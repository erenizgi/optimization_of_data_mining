{
    "author": "ArthurZucker",
    "message": "[`AutoDocstring`] Based on inspect parsing of the signature (#33771)\n\n* delete common docstring\n\n* nit\n\n* updates\n\n* push\n\n* fixup\n\n* move stuff around fixup\n\n* no need for dataclas\n\n* damn nice modular\n\n* add auto class docstring\n\n* style\n\n* modular update\n\n* import autodocstring\n\n* fixup\n\n* maybe add original doc!\n\n* more cleanup\n\n* remove class do cas well\n\n* update\n\n* nits\n\n* more celanup\n\n* fix\n\n* wups\n\n* small check\n\n* updatez\n\n* some fixes\n\n* fix doc\n\n* update\n\n* nits\n\n* try?\n\n* nit\n\n* some updates\n\n* a little bit better\n\n* where ever we did not have help we are not really adding it!\n\n* revert llama config\n\n* small fixes and small tests\n\n* test\n\n* fixup\n\n* more fix-copies\n\n* updates\n\n* updates\n\n* fix doc building\n\n* style\n\n* small fixes\n\n* nits\n\n* fix-copies\n\n* fix merge issues faster\n\n* fix merge conf\n\n* nits jamba\n\n* ?\n\n* working autodoc for model class and forward except returns and example\n\n* support return section and unpack kwargs description\n\n* nits and cleanup\n\n* fix-copies\n\n* fix-copies\n\n* nits\n\n* Add support for llava-like models\n\n* fixup\n\n* add class args subset support\n\n* add examples inferred from automodel/pipelines\n\n* update ruff\n\n* autodocstring for Aria, Albert + fixups\n\n* Fix empty return blocks\n\n* fix copies\n\n* fix copies\n\n* add autodoc for all fast image processors + align, altclip\n\n* fix copies\n\n* add auto_doc for audio_spectrogram, auto_former, bark, bamba\n\n* Drastically improve speed + add bart beit bert\n\n* add autodoc to all bert-like models\n\n* Fix broken doc\n\n* fix copies\n\n* fix auto_docstring after merge\n\n* add autodoc to models\n\n* add models\n\n* add models\n\n* add models and improve support for optional, and custom shape in args docstring\n\n* update fast image processors\n\n* refactor auto_method_docstring in args_doc\n\n* add models and fix docstring parsing\n\n* add models\n\n* add models\n\n* remove debugging\n\n* add models\n\n* add fix_auto_docstrings and improve args_docs\n\n* add support for additional_info in args docstring\n\n* refactor (almost) all models\n\n* fix check docstring\n\n* fix -copies\n\n* fill in all missing docstrings\n\n* fix copies\n\n* fix qwen3 moe docstring\n\n* add documentation\n\n* add back labels\n\n* update docs and fix can_return_tuple in modular files\n\n* fix LongformerForMaskedLM docstring\n\n* add auto_docstring to _toctree\n\n* remove auto_docstring tests temporarily\n\n* fix copyrights new files\n\n* fix can_return_tuple granite hybrid\n\n* fix fast beit\n\n* Fix empty config doc\n\n* add support for COMMON_CUSTOM_ARGS in check_docstrings and add missing models\n\n* fix code block not closed flava\n\n* fix can_return_tuple sam hq\n\n* Fix Flaubert dataclass\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "5f5ccfdc545d94202ded13217613032c81151aa5",
    "files": [
        {
            "sha": "0dc62cc6f957b40ba8ba90e707b882734f26544c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -21,6 +21,8 @@\n       title: Adding a new model to Transformers\n     - local: modular_transformers\n       title: Modular Transformers\n+    - local: auto_docstring\n+      title: Document your models\n     - local: task_summary\n       title: What ðŸ¤— Transformers can do\n     - local: tasks_explained"
        },
        {
            "sha": "19058c00eb24234eecf64f68df9a721c106d03a5",
            "filename": "docs/source/en/auto_docstring.md",
            "status": "added",
            "additions": 279,
            "deletions": 0,
            "changes": 279,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fauto_docstring.md?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -0,0 +1,279 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Utilizing the @auto_docstring Decorator\n+\n+The `@auto_docstring` decorator in the Hugging Face Transformers library helps generate docstrings for model classes and their methods, which will be used to build the documentation for the library. It aims to improve consistency and reduce boilerplate by automatically including standard argument descriptions and allowing for targeted overrides and additions.\n+\n+---\n+\n+## ðŸ“œ How it Works\n+\n+The `@auto_docstring` decorator constructs docstrings by:\n+\n+1.  **Signature Inspection:** It inspects the signature (arguments, types, defaults) of the decorated class's `__init__` method or the decorated function.\n+2.  **Centralized Docstring Fetching:** It retrieves predefined docstrings for common arguments (e.g., `input_ids`, `attention_mask`) from internal library sources (like `ModelArgs` or `ImageProcessorArgs` in `utils/args_doc.py`).\n+3.  **Overriding or Adding Arguments Descriptions:**\n+    * **Direct Docstring Block:** It incorporates custom docstring content from an `r\"\"\" \"\"\"` (or `\"\"\" \"\"\"`) block below the method signature or within the `__init__` docstring. This is for documenting new arguments or overriding standard descriptions.\n+    * **Decorator Arguments (`custom_args`):** A `custom_args` docstring block can be passed to the decorator to provide docstrings for specific arguments directly in the decorator call. This can be used to define the docstring block for new arguments once if they are repeated in multiple places in the modeling file.\n+4.  **Adding Classes and Functions Introduction:**\n+    * **`custom_intro` argument:** Allows prepending a custom introductory paragraph to a class or function docstring.\n+    * **Automatic Introduction Generation:** For model classes with standard naming patterns (like `ModelForCausalLM`) or belonging to a pipeline, the decorator automatically generates an appropriate introductory paragraph using `ClassDocstring` in `utils/args_doc.py` as the source.\n+5.  **Templating:** The decorator uses a templating system, allowing predefined docstrings to include dynamic information deduced from the `auto_modules` of the library, such as `{{processor_class}}` or `{{config_class}}`.\n+6.  **Deducing Relevant Examples:** The decorator attempts to find appropriate usage examples based on the model's task or pipeline compatibility. It extracts checkpoint information from the model's configuration class to provide concrete examples with real model identifiers.\n+7.  **Adding Return Value Documentation:** For methods like `forward`, the decorator can automatically generate the \"Returns\" section based on the method's return type annotation. For example, for a method returning a `ModelOutput` subclass, it will extracts field descriptions from that class's docstring to create a comprehensive return value description. A custom `Returns` section can also be manually specified in the function docstring block.\n+8.  **Unrolling Kwargs Typed With Unpack Operator:** For specific methods (defined in `UNROLL_KWARGS_METHODS`) or classes (defined in `UNROLL_KWARGS_CLASSES`), the decorator processes `**kwargs` parameters that are typed with `Unpack[KwargsTypedDict]`. It extracts the documentation from the TypedDict and adds each parameter to the function's docstring. Currently, this functionality is only supported for `FastImageProcessorKwargs`.\n+\n+\n+---\n+\n+## ðŸš€ How to Use @auto_docstring\n+\n+### 1. Importing the Decorator\n+Import the decorator into your modeling file:\n+\n+```python\n+from ...utils import auto_docstring\n+```\n+\n+### 2. Applying to Classes\n+Place `@auto_docstring` directly above the class definition. It uses the `__init__` method's signature and its docstring for parameter descriptions.\n+\n+```python\n+from transformers.modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring\n+\n+@auto_docstring\n+class MyAwesomeModel(PreTrainedModel):\n+    def __init__(self, config, custom_parameter: int = 10, another_custom_arg: str = \"default\"):\n+        r\"\"\"\n+        custom_parameter (`int`, *optional*, defaults to 10):\n+            Description of the custom_parameter for MyAwesomeModel.\n+        another_custom_arg (`str`, *optional*, defaults to \"default\"):\n+            Documentation for another unique argument.\n+        \"\"\"\n+        super().__init__(config)\n+        self.custom_parameter = custom_parameter\n+        self.another_custom_arg = another_custom_arg\n+        # ... rest of your init\n+\n+    # ... other methods\n+```\n+\n+#### Advanced Class Decoration:\n+\n+Arguments can be passed directly to `@auto_docstring` for more control:\n+\n+```python\n+@auto_docstring(\n+    custom_intro=\"\"\"This model performs specific synergistic operations.\n+    It builds upon the standard Transformer architecture with unique modifications.\"\"\",\n+    custom_args=\"\"\"\n+    custom_parameter (`type`, *optional*, defaults to `default_value`):\n+        A concise description for custom_parameter if not defined or overriding the description in `args_doc.py`.\n+    internal_helper_arg (`type`, *optional*, defaults to `default_value`):\n+        A concise description for internal_helper_arg if not defined or overriding the description in `args_doc.py`.\n+    \"\"\"\n+)\n+class MySpecialModel(PreTrainedModel):\n+    def __init__(self, config: ConfigType, custom_parameter: \"type\" = \"default_value\", internal_helper_arg=None):\n+        # ...\n+```\n+\n+Or:\n+\n+```python\n+@auto_docstring(\n+    custom_intro=\"\"\"This model performs specific synergistic operations.\n+    It builds upon the standard Transformer architecture with unique modifications.\"\"\",\n+)\n+class MySpecialModel(PreTrainedModel):\n+    def __init__(self, config: ConfigType, custom_parameter: \"type\" = \"default_value\", internal_helper_arg=None):\n+        r\"\"\"\n+        custom_parameter (`type`, *optional*, defaults to `default_value`):\n+            A concise description for custom_parameter if not defined or overriding the description in `args_doc.py`.\n+        internal_helper_arg (`type`, *optional*, defaults to `default_value`):\n+            A concise description for internal_helper_arg if not defined or overriding the description in `args_doc.py`.\n+        \"\"\"\n+        # ...\n+```\n+\n+### 3. Applying to Functions (e.g., `forward` method)\n+Apply the decorator above method definitions, such as the `forward` method.\n+\n+```python\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        new_custom_argument: Optional[torch.Tensor] = None,\n+        arg_documented_in_args_doc: Optional[torch.Tensor] = None,\n+        # ... other arguments\n+    ) -> Union[Tuple, ModelOutput]: # The description of the return value will automatically be generated from the ModelOutput class docstring.\n+        r\"\"\"\n+        new_custom_argument (`torch.Tensor`, *optional*):\n+            Description of this new custom argument and its expected shape or type.\n+        \"\"\"\n+        # ...\n+```\n+\n+#### Advanced Function Decoration:\n+\n+Arguments can be passed directly to `@auto_docstring` for more control. `Returns` and `Examples` sections can also be manually specified:\n+\n+```python\n+MODEL_COMMON_CUSTOM_ARGS = r\"\"\"\n+    common_arg_1 (`torch.Tensor`, *optional*, defaults to `default_value`):\n+        Description of common_arg_1\n+    common_arg_2 (`torch.Tensor`, *optional*, defaults to `default_value`):\n+        Description of common_arg_2\n+    ...\n+\"\"\"\n+\n+class MyModel(PreTrainedModel):\n+    # ...\n+    @auto_docstring(\n+        custom_intro=\"\"\"\n+        This is a custom introduction for the function.\n+        \"\"\"\n+        custom_args=MODEL_COMMON_CUSTOM_ARGS\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        common_arg_1: Optional[torch.Tensor] = None,\n+        common_arg_2: Optional[torch.Tensor] = None,\n+        #...\n+        function_specific_argument: Optional[torch.Tensor] = None,\n+        # ... other arguments\n+    ) -> torch.Tensor:\n+        r\"\"\"\n+        function_specific_argument (`torch.Tensor`, *optional*):\n+            Description of an argument specific to this function\n+\n+        Returns:\n+            `torch.Tensor`: For a function returning a generic type, a custom \"Returns\" section can be specified.\n+\n+        Example:\n+\n+        (To override the default example with a custom one or to add an example for a model class that does not have a pipeline)\n+\n+        ```python\n+        ...\n+        ```\n+        \"\"\"\n+        # ...\n+```\n+\n+---\n+\n+### âœï¸ Documenting Arguments: Approach & Priority\n+\n+1.  **Standard Arguments (e.g., `input_ids`, `attention_mask`, `pixel_values`, `encoder_hidden_states` etc.):**\n+    * `@auto_docstring` retrieves descriptions from a central source. Do not redefine these locally if their description and shape are the same as in `args_doc.py`.\n+\n+2.  **New or Custom Arguments:**\n+    * **Primary Method:** Document these within an `r\"\"\" \"\"\"` docstring block following the signature (for functions) or in the `__init__` method's docstring (for class parameters).\n+    * **Format:**\n+        ```\n+        argument_name (`type`, *optional*, defaults to `X`):\n+            Description of the argument.\n+            Explain its purpose, expected shape/type if complex, and default behavior.\n+            This can span multiple lines.\n+        ```\n+    * Include `type` in backticks.\n+    * Add \"*optional*\" if the argument is not required (has a default value).\n+    * Add \"defaults to `X`\" if it has a default value (no need to specify \"defaults to `None`\" if the default value is `None`).\n+\n+3.  **Overriding Standard Arguments:**\n+    * If a standard argument behaves differently (e.g., different expected shape, model-specific behavior), provide its complete description in the local `r\"\"\" \"\"\"` docstring. This local definition takes precedence.\n+    * The `labels` argument is often customized per model and typically requires a specific docstring.\n+\n+4.  **Using Decorator Arguments for Overrides or New Arguments (`custom_args`):**\n+    * New or custom arguments docstrings can also be passed to `@auto_docstring` as a `custom_args` argument. This can be used to define the docstring block for new arguments once if they are repeated in multiple places in the modeling file.\n+\n+---\n+\n+### Usage with [modular files](./modular_transformers)\n+\n+When working with modular files, follow these guidelines for applying the `@auto_docstring` decorator:\n+\n+- **For standalone models in modular files:**\n+  Apply the `@auto_docstring` decorator just as you would in regular modeling files.\n+\n+- **For models inheriting from other library models:**\n+  - When inheriting from a parent model, decorators (including `@auto_docstring`) are automatically carried over to the generated modeling file without needing to add them in your modular file.\n+  - If you need to modify the `@auto_docstring` behavior, apply the customized decorator in your modular file, making sure to *include all other decorators* that were present on the original function/class.\n+\n+  > **Warning**: When overriding any decorator in a modular file, you must include ALL decorators that were applied to that function/class in the parent model. If you only override some decorators, the others won't be included in the generated modeling file.\n+\n+\n+**Note**: The `check_auto_docstrings` tool doesn't check modular files directly, but it will check (and modify when using `--fix_and_overwrite`) the generated modeling files. If issues are found in the generated files, you'll need to update your modular files accordingly.\n+\n+---\n+\n+## âœ… Checking Your Docstrings with `check_auto_docstrings`\n+\n+The library includes a utility script to validate docstrings. This check is typically run during Continuous Integration (CI).\n+\n+#### What it Checks:\n+\n+* **Decorator Presence:** Ensures `@auto_docstring` is applied to relevant model classes and public methods. (TODO)\n+* **Argument Completeness & Consistency:**\n+    * Flags arguments in the signature that are not known standard arguments and lack a local description.\n+    * Ensures documented arguments exist in the signature. (TODO)\n+    * Verifies that types and default values in the docstring match the signature. (TODO)\n+* **Placeholder Detection:** Reminds you to complete placeholders like `<fill_type>` or `<fill_docstring>`.\n+* **Formatting:** Adherence to the expected docstring style.\n+\n+#### Running the Check Locally:\n+\n+Run this check locally before committing. The common command is:\n+\n+```bash\n+make fix-copies\n+```\n+\n+Alternatively, to only perform docstrings and auto-docstring checks, you can use:\n+\n+```bash\n+python utils/check_docstrings.py # to only check files included in the diff without fixing them\n+# Or: python utils/check_docstrings.py --fix_and_overwrite # to fix and overwrite the files in the diff\n+# Or: python utils/check_docstrings.py --fix_and_overwrite --check_all # to fix and overwrite all files\n+```\n+\n+#### Workflow with the Checker:\n+\n+1.  Add `@auto_docstring(...)` to the class or method.\n+2.  For new, custom, or overridden arguments, add descriptions in an `r\"\"\" \"\"\"` block.\n+3.  Run `make fix-copies` (or the `check_docstrings.py` utility).\n+    * For unrecognized arguments lacking documentation, the utility will create placeholder entries.\n+4.  Manually edit these placeholders with accurate types and descriptions.\n+5.  Re-run the check to ensure all issues are resolved.\n+\n+---\n+\n+## ðŸ”‘ Key Takeaways & Best Practices\n+\n+* Use `@auto_docstring` for new PyTorch model classes (`PreTrainedModel` subclasses) and their primary for methods (e.g., `forward`, `get_text_features` etc.).\n+* For classes, the `__init__` method's docstring is the main source for parameter descriptions when using `@auto_docstring` on the class.\n+* Rely on standard docstrings; do not redefine common arguments unless their behavior is different in your specific model.\n+* Document new or custom arguments clearly.\n+* Run `check_docstrings` locally and iteratively.\n+\n+By following these guidelines, you help maintain consistent and informative documentation for the Hugging Face Transformers library ðŸ¤—."
        },
        {
            "sha": "3122fcee2cbf2ab67cdf94cc6b1266f1a343f337",
            "filename": "src/transformers/commands/add_fast_image_processor.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -396,9 +396,7 @@ def add_fast_image_processor_file(\n \n     content_header = get_fast_image_processing_content_header(content_base_file)\n     content_base_file = (\n-        f\"@add_start_docstrings(\\n\"\n-        f'    \"Constructs a fast {fast_image_processor_name.replace(\"ImageProcessorFast\", \"\")} image processor.\",\\n'\n-        f\"    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\\n)\\n\"\n+        f\"@auto_docstring(\\n\"\n         f\"class {fast_image_processor_name}(BaseImageProcessorFast):\\n\"\n         \"    # This generated class can be used as a starting point for the fast image processor.\\n\"\n         \"    # if the image processor is only used for simple augmentations, such as resizing, center cropping, rescaling, or normalizing,\\n\"\n@@ -422,9 +420,7 @@ def add_fast_image_processor_file(\n         f'__all__ = [\"{fast_image_processor_name}\"]\\n'\n     )\n \n-    imports = (\n-        \"\\n\\nfrom ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\\n\"\n-    )\n+    imports = \"\\n\\nfrom ...image_processing_utils_fast import BaseImageProcessorFast\\n\"\n     image_utils_imports = []\n     if default_args_dict.get(\"resample\") is not None and \"PILImageResampling\" in default_args_dict.get(\"resample\"):\n         image_utils_imports.append(\"PILImageResampling\")\n@@ -442,7 +438,7 @@ def add_fast_image_processor_file(\n         image_utils_imports.sort()\n         imports += f\"from ...image_utils import {', '.join(image_utils_imports)}\\n\"\n \n-    imports += \"from ...utils import add_start_docstrings\\n\"\n+    imports += \"from ...utils import auto_docstring\\n\"\n \n     content = content_header + imports + \"\\n\\n\" + content_base_file\n "
        },
        {
            "sha": "064122bfa7e2daca1ec2aaace0deaa36f66785f7",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 103,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -46,7 +46,7 @@\n from .processing_utils import Unpack\n from .utils import (\n     TensorType,\n-    add_start_docstrings,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -190,107 +190,7 @@ class DefaultFastImageProcessorKwargs(TypedDict, total=False):\n     device: Optional[\"torch.device\"]\n \n \n-BASE_IMAGE_PROCESSOR_FAST_DOCSTRING = r\"\"\"\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n-            `do_resize` parameter in the `preprocess` method.\n-        size (`dict`, *optional*, defaults to `self.size`):\n-            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n-            method.\n-        default_to_square (`bool`, *optional*, defaults to `self.default_to_square`):\n-            Whether to default to a square image when resizing, if size is an int.\n-        resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n-            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-            overridden by the `resample` parameter in the `preprocess` method.\n-        do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n-            `preprocess` method.\n-        crop_size (`Dict[str, int]` *optional*, defaults to `self.crop_size`):\n-            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n-            method.\n-        do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n-            `do_rescale` parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `self.rescale_factor`):\n-            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n-            overridden by the `rescale_factor` parameter in the `preprocess` method.\n-        do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n-            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n-            overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-            Whether to convert the image to RGB.\n-        return_tensors (`str` or `TensorType`, *optional*, defaults to `self.return_tensors`):\n-            Returns stacked tensors if set to `pt, otherwise returns a list of tensors.\n-        data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.data_format`):\n-            Only `ChannelDimension.FIRST` is supported. Added for compatibility with slow processors.\n-        input_data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.input_data_format`):\n-            The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-            from the input image. Can be one of:\n-            - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-            - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-            - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        device (`torch.device`, *optional*, defaults to `self.device`):\n-            The device to process the images on. If unset, the device is inferred from the input images.\"\"\"\n-\n-BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS = r\"\"\"\n-    Preprocess an image or batch of images.\n-\n-    Args:\n-        images (`ImageInput`):\n-            Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n-            passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-        do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-            Whether to resize the image.\n-        size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n-            Describes the maximum input dimensions to the model.\n-        resample (`PILImageResampling` or `InterpolationMode`, *optional*, defaults to `self.resample`):\n-            Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-            has an effect if `do_resize` is set to `True`.\n-        do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-            Whether to center crop the image.\n-        crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-            Size of the output image after applying `center_crop`.\n-        do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-            Whether to rescale the image.\n-        rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-            Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-        do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-            Whether to normalize the image.\n-        image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-            Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-        image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-            Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-            `True`.\n-        do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-            Whether to convert the image to RGB.\n-        return_tensors (`str` or `TensorType`, *optional*, defaults to `self.return_tensors`):\n-            Returns stacked tensors if set to `pt, otherwise returns a list of tensors.\n-        data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.data_format`):\n-            Only `ChannelDimension.FIRST` is supported. Added for compatibility with slow processors.\n-        input_data_format (`ChannelDimension` or `str`, *optional*, defaults to `self.input_data_format`):\n-            The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-            from the input image. Can be one of:\n-            - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-            - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-            - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        device (`torch.device`, *optional*, defaults to `self.device`):\n-            The device to process the images on. If unset, the device is inferred from the input images.\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"Constructs a fast base image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n class BaseImageProcessorFast(BaseImageProcessor):\n     resample = None\n     image_mean = None\n@@ -666,7 +566,7 @@ def _validate_preprocess_kwargs(\n             data_format=data_format,\n         )\n \n-    @add_start_docstrings(BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS)\n+    @auto_docstring\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[DefaultFastImageProcessorKwargs]) -> BatchFeature:\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n         # Set default kwargs from self. This ensures that if a kwarg is not provided"
        },
        {
            "sha": "b896b9646b5b0b9c93c9c9c107b08e5567e0c2dc",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 48,
            "deletions": 173,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -41,22 +41,12 @@\n     is_torch_greater_or_equal_than_2_2,\n     prune_linear_layer,\n )\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_albert import AlbertConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"albert/albert-base-v2\"\n-_CONFIG_FOR_DOC = \"AlbertConfig\"\n-\n \n def load_tf_weights_in_albert(model, config, tf_checkpoint_path):\n     \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n@@ -553,12 +543,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class AlbertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = AlbertConfig\n     load_tf_weights = load_tf_weights_in_albert\n     base_model_prefix = \"albert\"\n@@ -617,81 +603,16 @@ class AlbertForPreTrainingOutput(ModelOutput):\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-ALBERT_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Args:\n-        config ([`AlbertConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-ALBERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare ALBERT Model transformer outputting raw hidden-states without any specific head on top.\",\n-    ALBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AlbertModel(AlbertPreTrainedModel):\n     config_class = AlbertConfig\n     base_model_prefix = \"albert\"\n \n     def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n \n         self.config = config\n@@ -733,12 +654,7 @@ def _prune_heads(self, heads_to_prune: Dict[int, List[int]]) -> None:\n             inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n             self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -826,12 +742,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Albert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a\n     `sentence order prediction (classification)` head.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n+    \"\"\"\n )\n class AlbertForPreTraining(AlbertPreTrainedModel):\n     _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n@@ -855,8 +770,7 @@ def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n     def get_input_embeddings(self) -> nn.Embedding:\n         return self.albert.embeddings.word_embeddings\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=AlbertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -881,8 +795,6 @@ def forward(\n             (see `input_ids` docstring) Indices should be in `[0, 1]`. `0` indicates original order (sequence A, then\n             sequence B), `1` indicates switched order (sequence B, then sequence A).\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -981,10 +893,7 @@ def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n         return logits\n \n \n-@add_start_docstrings(\n-    \"Albert Model with a `language modeling` head on top.\",\n-    ALBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AlbertForMaskedLM(AlbertPreTrainedModel):\n     _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n \n@@ -1007,8 +916,7 @@ def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n     def get_input_embeddings(self) -> nn.Embedding:\n         return self.albert.embeddings.word_embeddings\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1028,8 +936,6 @@ def forward(\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -1093,12 +999,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Albert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n     output) e.g. for GLUE tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n+    \"\"\"\n )\n class AlbertForSequenceClassification(AlbertPreTrainedModel):\n     def __init__(self, config: AlbertConfig):\n@@ -1113,14 +1018,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"textattack/albert-base-v2-imdb\",\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"'LABEL_1'\",\n-        expected_loss=0.12,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1194,13 +1092,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AlbertForTokenClassification(AlbertPreTrainedModel):\n     def __init__(self, config: AlbertConfig):\n         super().__init__(config)\n@@ -1218,12 +1110,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1277,13 +1164,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AlbertForQuestionAnswering(AlbertPreTrainedModel):\n     def __init__(self, config: AlbertConfig):\n         super().__init__(config)\n@@ -1295,16 +1176,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"twmkn9/albert-base-v2-squad2\",\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        qa_target_start_index=12,\n-        qa_target_end_index=13,\n-        expected_output=\"'a nice puppet'\",\n-        expected_loss=7.36,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1319,16 +1191,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.albert(\n@@ -1380,13 +1242,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Albert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    ALBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AlbertForMultipleChoice(AlbertPreTrainedModel):\n     def __init__(self, config: AlbertConfig):\n         super().__init__(config)\n@@ -1398,12 +1254,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ALBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1418,6 +1269,30 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[AlbertForPreTrainingOutput, Tuple]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n+            [`PreTrainedTokenizer.encode`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where *num_choices* is the size of the second dimension of the input tensors. (see"
        },
        {
            "sha": "bdebd31a266cd36a637a4cc698a6644e77fc21cb",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 22,
            "deletions": 168,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -31,154 +31,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_align import AlignConfig, AlignTextConfig, AlignVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"kakaobrain/align-base\"\n-_CONFIG_FOR_DOC = \"AlignConfig\"\n-\n-\n-ALIGN_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`AlignConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-ALIGN_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-ALIGN_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`EfficientNetImageProcessor.__call__`] for details.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-ALIGN_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-       input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`EfficientNetImageProcessor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n \n @dataclass\n class AlignVisionModelOutput(ModelOutput):\n@@ -1165,12 +1023,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+@auto_docstring\n class AlignPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = AlignConfig\n     base_model_prefix = \"align\"\n     supports_gradient_checkpointing = True\n@@ -1194,15 +1048,20 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-@add_start_docstrings(\n-    \"\"\"The text model from ALIGN without any head or projection on top.\"\"\",\n-    ALIGN_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The text model from ALIGN without any head or projection on top.\n+    \"\"\"\n )\n class AlignTextModel(AlignPreTrainedModel):\n     config_class = AlignTextConfig\n     _no_split_modules = [\"AlignTextEmbeddings\"]\n \n     def __init__(self, config: AlignTextConfig, add_pooling_layer: bool = True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -1220,8 +1079,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    @add_start_docstrings_to_model_forward(ALIGN_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPoolingAndCrossAttentions, config_class=AlignTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1235,8 +1093,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1321,9 +1177,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"The vision model from ALIGN without any head or projection on top.\"\"\",\n-    ALIGN_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The vision model from ALIGN without any head or projection on top.\n+    \"\"\"\n )\n class AlignVisionModel(AlignPreTrainedModel):\n     config_class = AlignVisionConfig\n@@ -1350,17 +1207,14 @@ def __init__(self, config: AlignVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.convolution\n \n-    @add_start_docstrings_to_model_forward(ALIGN_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPoolingAndNoAttention, config_class=AlignVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1410,7 +1264,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(ALIGN_START_DOCSTRING)\n+@auto_docstring\n class AlignModel(AlignPreTrainedModel):\n     config_class = AlignConfig\n \n@@ -1444,7 +1298,7 @@ def __init__(self, config: AlignConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ALIGN_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1497,7 +1351,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(ALIGN_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1542,8 +1396,7 @@ def get_image_features(\n \n         return image_features\n \n-    @add_start_docstrings_to_model_forward(ALIGN_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AlignOutput, config_class=AlignConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1559,7 +1412,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, AlignOutput]:\n         r\"\"\"\n-        Returns:\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "5637bc6deec35c7732da9b5555e2befb13776072",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 19,
            "deletions": 163,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -32,125 +32,12 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_altclip import AltCLIPConfig, AltCLIPTextConfig, AltCLIPVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"BAAI/AltCLIP\"\n-_CONFIG_FOR_DOC = \"AltCLIPConfig\"\n-\n-\n-ALTCLIP_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`CLIPConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-ALTCLIP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-ALTCLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-ALTCLIP_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n \n # contrastive loss function, adapted from\n # https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n@@ -1076,12 +963,8 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         return embeddings\n \n \n+@auto_docstring\n class AltCLIPPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = AltCLIPConfig\n     base_model_prefix = \"altclip\"\n     supports_gradient_checkpointing = True\n@@ -1144,8 +1027,7 @@ def __init__(self, config: AltCLIPVisionConfig):\n         self.encoder = AltCLIPEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-    @add_start_docstrings_to_model_forward(ALTCLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=AltCLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1154,10 +1036,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1205,8 +1083,7 @@ def __init__(self, config: AltCLIPVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @add_start_docstrings_to_model_forward(ALTCLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=AltCLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1216,8 +1093,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1248,9 +1123,8 @@ def forward(\n         )\n \n \n-class AltRobertaModel(AltCLIPPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n     cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n     all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n@@ -1261,13 +1135,17 @@ class AltRobertaModel(AltCLIPPreTrainedModel):\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n \n     .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n-\n     \"\"\"\n-\n+)\n+class AltRobertaModel(AltCLIPPreTrainedModel):\n     config_class = AltCLIPTextConfig\n \n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->AltRoberta\n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -1293,6 +1171,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @auto_docstring\n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n@@ -1310,26 +1189,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1444,8 +1303,7 @@ def set_input_embeddings(self, value: nn.Embedding) -> None:\n     def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> nn.Embedding:\n         return super().resize_token_embeddings(new_num_tokens)\n \n-    @add_start_docstrings_to_model_forward(ALTCLIP_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPoolingAndProjection, config_class=AltCLIPTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1461,8 +1319,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndProjection]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1551,7 +1407,7 @@ def __init__(self, config: AltCLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ALTCLIP_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1598,7 +1454,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(ALTCLIP_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1646,8 +1502,7 @@ def get_image_features(\n \n         return image_features\n \n-    @add_start_docstrings_to_model_forward(ALTCLIP_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AltCLIPOutput, config_class=AltCLIPConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1662,7 +1517,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, AltCLIPOutput]:\n         r\"\"\"\n-        Returns:\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "9a02e1c58d257f354671f3b38d04ef7f7f5fd85a",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 28,
            "deletions": 210,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -34,12 +34,10 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     LossKwargs,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n-    replace_return_docstrings,\n )\n from ...utils.import_utils import is_torch_available\n from ..auto import AutoModel\n@@ -60,9 +58,6 @@\n logger = logging.get_logger(__name__)\n \n \n-_CONFIG_FOR_DOC = \"AriaConfig\"\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class AriaTextRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -656,11 +651,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class AriaTextPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n-    \"\"\"\n-\n     config_class = AriaTextConfig\n     base_model_prefix = \"model\"\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n@@ -687,27 +679,7 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n \n \n-ARIA_TEXT_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`AriaTextConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Aria Model outputting raw hidden-states without any specific head on top.\",\n-    ARIA_TEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AriaPreTrainedModel(PreTrainedModel):\n     config_class = AriaConfig\n     base_model_prefix = \"\"\n@@ -773,88 +745,8 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-ARIA_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare AriaText Model outputting raw hidden-states without any specific head on top.\",\n-    ARIA_TEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class AriaTextModel(AriaTextPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`AriaTextDecoderLayer`]\n-\n-    Args:\n-        config: AriaTextConfig\n-    \"\"\"\n-\n     def __init__(self, config: AriaTextConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -878,7 +770,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1100,21 +992,8 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-_CONFIG_FOR_TEXT_DOC = \"AriaTextConfig\"\n-\n-\n+@auto_docstring\n class AriaTextForCausalLM(AriaTextPreTrainedModel, GenerationMixin):\n-    \"\"\"\n-    Aria model for causal language modeling tasks.\n-\n-    This class extends `LlamaForCausalLM` to incorporate the Mixture of Experts (MoE) approach,\n-    allowing for more efficient and scalable language modeling.\n-\n-    Args:\n-        config (`AriaTextConfig`):\n-            Configuration object for the model.\n-    \"\"\"\n-\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n@@ -1146,8 +1025,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_TEXT_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1164,19 +1042,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n \n@@ -1304,61 +1173,10 @@ class AriaModelOutputWithPast(BaseModelOutputWithPast):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-ARIA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor`, *optional*):\n-            Input token IDs.\n-        pixel_values (`torch.FloatTensor`, *optional*):\n-            Pixel values of the images.\n-        pixel_mask (`torch.LongTensor`, *optional*):\n-            Mask for the pixel values.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            Attention mask.\n-        position_ids (`torch.LongTensor`, *optional*):\n-            Position IDs.\n-        past_key_values (`List[torch.FloatTensor]`, *optional*):\n-            Past key values for efficient processing.\n-        inputs_embeds (`torch.FloatTensor`, *optional*):\n-            Input embeddings.\n-        labels (`torch.LongTensor`, *optional*):\n-            Labels for computing the language modeling loss.\n-        use_cache (`bool`, *optional*):\n-            Whether to use the model's cache mechanism.\n-        output_attentions (`bool`, *optional*):\n-            Whether to output attention weights.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether to output hidden states.\n-        return_dict (`bool`, *optional*):\n-            Whether to return a `ModelOutput` object.\n-        logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n-            If an `int`, calculate logits for the last `logits_to_keep` tokens, or all `input_ids` if `0`.\n-            Otherwise, slice according to the 1D tensor in the sequence length dimension\n-        cache_position (`torch.LongTensor`, *optional*):\n-            Cache positions.\n-        **loss_kwargs:\n-            Additional keyword arguments for loss calculation.\n-\"\"\"\n-\n-ARIA_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config (`AriaConfig`):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"The Aria model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n-    ARIA_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Aria model which consists of a vision backbone and a language model, without a language modeling head.\n+    \"\"\"\n )\n class AriaModel(AriaPreTrainedModel):\n     _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n@@ -1411,7 +1229,7 @@ def get_image_features(\n         return image_features\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1501,11 +1319,13 @@ def _create_patch_attention_mask(self, pixel_mask):\n         return (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n \n \n-@add_start_docstrings(\n-    \"\"\"Aria model for conditional generation tasks.\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Aria model for conditional generation tasks.\n+\n     This model combines a vision tower, a multi-modal projector, and a language model\n-    to perform tasks that involve both image and text inputs.\"\"\",\n-    ARIA_START_DOCSTRING,\n+    to perform tasks that involve both image and text inputs.\n+    \"\"\"\n )\n class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n@@ -1548,8 +1368,7 @@ def multi_modal_projector(self):\n         return self.model.multi_modal_projector\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1569,12 +1388,11 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`).\n-                Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n-                computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `AriaForConditionalGeneration`).\n+            Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n+            computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "c741a9b2c4e14bbea3961c5f9d17c92ad325bf23",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 16,
            "deletions": 102,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -33,22 +33,10 @@\n     validate_preprocess_arguments,\n )\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils import (\n-    PreTokenizedInput,\n-    TextInput,\n-)\n-from ...utils import (\n-    LossKwargs,\n-    TensorType,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...tokenization_utils import PreTokenizedInput, TextInput\n+from ...utils import LossKwargs, TensorType, auto_docstring, can_return_tuple, logging\n from ...utils.import_utils import is_torch_available\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoTokenizer\n from ..llama.configuration_llama import LlamaConfig\n@@ -76,11 +64,6 @@\n     from torch import nn\n \n \n-_CONFIG_FOR_DOC = \"AriaConfig\"\n-_CONFIG_FOR_TEXT_DOC = \"AriaTextConfig\"\n-ARIA_TEXT_INPUTS_DOCSTRING = None\n-\n-\n def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert):\n     \"\"\"\n     Compute the matrix multiplication (GEMM) for each expert sequentially. This approach is computationally inefficient, especially when dealing with a large number of experts.\n@@ -1229,11 +1212,8 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n         self.mlp = AriaTextMoELayer(config)\n \n \n+@auto_docstring\n class AriaTextPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n-    \"\"\"\n-\n     config_class = AriaTextConfig\n     base_model_prefix = \"model\"\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n@@ -1297,17 +1277,6 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n class AriaTextForCausalLM(AriaTextPreTrainedModel, LlamaForCausalLM):\n-    \"\"\"\n-    Aria model for causal language modeling tasks.\n-\n-    This class extends `LlamaForCausalLM` to incorporate the Mixture of Experts (MoE) approach,\n-    allowing for more efficient and scalable language modeling.\n-\n-    Args:\n-        config (`AriaTextConfig`):\n-            Configuration object for the model.\n-    \"\"\"\n-\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(self, config: AriaTextConfig):\n@@ -1319,8 +1288,7 @@ def __init__(self, config: AriaTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_TEXT_DOC)\n+    @auto_docstring\n     def forward(self, **super_kwargs):\n         super().forward(self, **super_kwargs)\n \n@@ -1333,58 +1301,6 @@ class AriaModelOutputWithPast(LlavaModelOutputWithPast):\n     pass\n \n \n-ARIA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor`, *optional*):\n-            Input token IDs.\n-        pixel_values (`torch.FloatTensor`, *optional*):\n-            Pixel values of the images.\n-        pixel_mask (`torch.LongTensor`, *optional*):\n-            Mask for the pixel values.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            Attention mask.\n-        position_ids (`torch.LongTensor`, *optional*):\n-            Position IDs.\n-        past_key_values (`List[torch.FloatTensor]`, *optional*):\n-            Past key values for efficient processing.\n-        inputs_embeds (`torch.FloatTensor`, *optional*):\n-            Input embeddings.\n-        labels (`torch.LongTensor`, *optional*):\n-            Labels for computing the language modeling loss.\n-        use_cache (`bool`, *optional*):\n-            Whether to use the model's cache mechanism.\n-        output_attentions (`bool`, *optional*):\n-            Whether to output attention weights.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether to output hidden states.\n-        return_dict (`bool`, *optional*):\n-            Whether to return a `ModelOutput` object.\n-        logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n-            If an `int`, calculate logits for the last `logits_to_keep` tokens, or all `input_ids` if `0`.\n-            Otherwise, slice according to the 1D tensor in the sequence length dimension\n-        cache_position (`torch.LongTensor`, *optional*):\n-            Cache positions.\n-        **loss_kwargs:\n-            Additional keyword arguments for loss calculation.\n-\"\"\"\n-\n-ARIA_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config (`AriaConfig`):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n class AriaModel(LlavaModel):\n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n@@ -1440,8 +1356,6 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n-    @can_return_tuple\n-    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1515,16 +1429,17 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"Aria model for conditional generation tasks.\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Aria model for conditional generation tasks.\n+\n     This model combines a vision tower, a multi-modal projector, and a language model\n-    to perform tasks that involve both image and text inputs.\"\"\",\n-    ARIA_START_DOCSTRING,\n+    to perform tasks that involve both image and text inputs.\n+    \"\"\"\n )\n class AriaForConditionalGeneration(LlavaForConditionalGeneration):\n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1544,12 +1459,11 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`).\n-                Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n-                computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `AriaForConditionalGeneration`).\n+            Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n+            computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "90fc3362159398c2104ec5501bbde6c73339946e",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 22,
            "deletions": 81,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -25,24 +25,12 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import auto_docstring, logging\n from .configuration_audio_spectrogram_transformer import ASTConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"ASTConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 1214, 768]\n-\n-# Audio classification docstring\n-_SEQ_CLASS_CHECKPOINT = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n-_SEQ_CLASS_EXPECTED_OUTPUT = \"'Speech'\"\n-_SEQ_CLASS_EXPECTED_LOSS = 0.17\n-\n \n class ASTEmbeddings(nn.Module):\n     \"\"\"\n@@ -388,12 +376,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class ASTPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ASTConfig\n     base_model_prefix = \"audio_spectrogram_transformer\"\n     main_input_name = \"input_values\"\n@@ -420,48 +404,7 @@ def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> No\n             module.distillation_token.data.zero_()\n \n \n-AUDIO_SPECTROGRAM_TRANSFORMER_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`ASTConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-AUDIO_SPECTROGRAM_TRANSFORMER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n-            Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n-            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n-            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n-            tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare AST Model transformer outputting raw hidden-states without any specific head on top.\",\n-    AUDIO_SPECTROGRAM_TRANSFORMER_START_DOCSTRING,\n-)\n+@auto_docstring\n class ASTModel(ASTPreTrainedModel):\n     def __init__(self, config: ASTConfig) -> None:\n         super().__init__(config)\n@@ -486,14 +429,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(AUDIO_SPECTROGRAM_TRANSFORMER_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor] = None,\n@@ -502,6 +438,14 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n+            Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n+            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n+            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n+            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n+            tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -555,12 +499,11 @@ def forward(self, hidden_state):\n         return hidden_state\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Audio Spectrogram Transformer model with an audio classification head on top (a linear layer on top of the pooled\n     output) e.g. for datasets like AudioSet, Speech Commands v2.\n-    \"\"\",\n-    AUDIO_SPECTROGRAM_TRANSFORMER_START_DOCSTRING,\n+    \"\"\"\n )\n class ASTForAudioClassification(ASTPreTrainedModel):\n     def __init__(self, config: ASTConfig) -> None:\n@@ -575,15 +518,7 @@ def __init__(self, config: ASTConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(AUDIO_SPECTROGRAM_TRANSFORMER_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_SEQ_CLASS_CHECKPOINT,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n-        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor] = None,\n@@ -594,6 +529,12 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n+        input_values (`torch.FloatTensor` of shape `(batch_size, max_length, num_mel_bins)`):\n+            Float values mel features extracted from the raw audio waveform. Raw audio waveform can be obtained by\n+            loading a `.flac` or `.wav` audio file into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via\n+            the soundfile library (`pip install soundfile`). To prepare the array into `input_features`, the\n+            [`AutoFeatureExtractor`] should be used for extracting the mel features, padding and conversion into a\n+            tensor of type `torch.FloatTensor`. See [`~ASTFeatureExtractor.__call__`]\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the audio classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "8cb92caa1a4beed6bf2ee68468412149521817e2",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 151,
            "deletions": 171,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,22 +27,15 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n-from ...modeling_outputs import (\n-    BaseModelOutput,\n-    ModelOutput,\n-    SampleTSPredictionOutput,\n-    Seq2SeqTSPredictionOutput,\n-)\n+from ...modeling_outputs import BaseModelOutput, ModelOutput, SampleTSPredictionOutput, Seq2SeqTSPredictionOutput\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import auto_docstring, logging\n from .configuration_autoformer import AutoformerConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"AutoformerConfig\"\n-\n \n @dataclass\n class AutoFormerDecoderOutput(ModelOutput):\n@@ -888,6 +881,7 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class AutoformerPreTrainedModel(PreTrainedModel):\n     config_class = AutoformerConfig\n     base_model_prefix = \"model\"\n@@ -908,154 +902,6 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-AUTOFORMER_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`AutoformerConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-AUTOFORMER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Past values of the time series, that serve as context in order to predict the future. These values may\n-            contain lags, i.e. additional values from the past which are added in order to serve as \"extra context\".\n-            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n-            `static_categorical_features`, `static_real_features`, `past_time_features`).\n-\n-            The sequence length here is equal to `context_length` + `max(config.lags_sequence)`.\n-\n-            Missing values need to be replaced with zeros.\n-\n-        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`, *optional*):\n-            Optional time features, which the model internally will add to `past_values`. These could be things like\n-            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n-            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n-            time-series is. Age features have small values for distant past time steps and increase monotonically the\n-            more we approach the current time step.\n-\n-            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n-            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n-            Transformer requires to provide additional time features.\n-\n-            The Autoformer only learns additional embeddings for `static_categorical_features`.\n-\n-        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n-            `[0, 1]`:\n-\n-            - 1 for values that are **observed**,\n-            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n-\n-        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n-            Optional static categorical features for which the model will learn an embedding, which it will add to the\n-            values of the time series.\n-\n-            Static categorical features are features which have the same value for all time steps (static over time).\n-\n-            A typical example of a static categorical feature is a time series ID.\n-\n-        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n-            Optional static real features which the model will add to the values of the time series.\n-\n-            Static real features are features which have the same value for all time steps (static over time).\n-\n-            A typical example of a static real feature is promotion information.\n-\n-        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)`):\n-            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n-            Transformer needs to learn to output, given the `past_values`.\n-\n-            See the demo notebook and code snippets for details.\n-\n-            Missing values need to be replaced with zeros.\n-\n-        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`, *optional*):\n-            Optional time features, which the model internally will add to `future_values`. These could be things like\n-            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n-            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n-            time-series is. Age features have small values for distant past time steps and increase monotonically the\n-            more we approach the current time step.\n-\n-            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n-            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n-            Transformer requires to provide additional features.\n-\n-            The Autoformer only learns additional embeddings for `static_categorical_features`.\n-\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on certain token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Mask to avoid performing attention on certain token indices. By default, a causal mask will be used, to\n-            make sure the model can only look at previous inputs in order to predict the future.\n-\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.TimeSeriesTransformerEncoder with TimeSeriesTransformer->Autoformer,TimeSeries->Autoformer\n class AutoformerEncoder(AutoformerPreTrainedModel):\n     \"\"\"\n@@ -1417,10 +1263,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare Autoformer Model outputting raw hidden-states without any specific head on top.\",\n-    AUTOFORMER_START_DOCSTRING,\n-)\n+@auto_docstring\n class AutoformerModel(AutoformerPreTrainedModel):\n     def __init__(self, config: AutoformerConfig):\n         super().__init__(config)\n@@ -1595,8 +1438,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n-    @add_start_docstrings_to_model_forward(AUTOFORMER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AutoformerModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         past_values: torch.Tensor,\n@@ -1618,7 +1460,74 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[AutoformerModelOutput, Tuple]:\n         r\"\"\"\n-        Returns:\n+        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Past values of the time series, that serve as context in order to predict the future. These values may\n+            contain lags, i.e. additional values from the past which are added in order to serve as \"extra context\".\n+            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n+            `static_categorical_features`, `static_real_features`, `past_time_features`).\n+\n+            The sequence length here is equal to `context_length` + `max(config.lags_sequence)`.\n+\n+            Missing values need to be replaced with zeros.\n+        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`, *optional*):\n+            Optional time features, which the model internally will add to `past_values`. These could be things like\n+            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n+            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n+            time-series is. Age features have small values for distant past time steps and increase monotonically the\n+            more we approach the current time step.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional time features.\n+\n+            The Autoformer only learns additional embeddings for `static_categorical_features`.\n+        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n+            `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n+            Optional static categorical features for which the model will learn an embedding, which it will add to the\n+            values of the time series.\n+\n+            Static categorical features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static categorical feature is a time series ID.\n+        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n+            Optional static real features which the model will add to the values of the time series.\n+\n+            Static real features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static real feature is promotion information.\n+        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)`):\n+            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n+            Transformer needs to learn to output, given the `past_values`.\n+\n+            See the demo notebook and code snippets for details.\n+\n+            Missing values need to be replaced with zeros.\n+        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`, *optional*):\n+            Optional time features, which the model internally will add to `future_values`. These could be things like\n+            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n+            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n+            time-series is. Age features have small values for distant past time steps and increase monotonically the\n+            more we approach the current time step.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional features.\n+\n+            The Autoformer only learns additional embeddings for `static_categorical_features`.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n \n         Examples:\n \n@@ -1753,10 +1662,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The Autoformer Model with a distribution head on top for time-series forecasting.\",\n-    AUTOFORMER_START_DOCSTRING,\n-)\n+@auto_docstring\n class AutoformerForPrediction(AutoformerPreTrainedModel):\n     def __init__(self, config: AutoformerConfig):\n         super().__init__(config)\n@@ -1797,8 +1703,7 @@ def output_distribution(self, params, loc=None, scale=None, trailing_n=None) ->\n             sliced_params = [p[:, -trailing_n:] for p in params]\n         return self.distribution_output.distribution(sliced_params, loc=loc, scale=scale)\n \n-    @add_start_docstrings_to_model_forward(AUTOFORMER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqTSPredictionOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         past_values: torch.Tensor,\n@@ -1821,7 +1726,82 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Seq2SeqTSPredictionOutput, Tuple]:\n         r\"\"\"\n-        Returns:\n+        past_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Past values of the time series, that serve as context in order to predict the future. These values may\n+            contain lags, i.e. additional values from the past which are added in order to serve as \"extra context\".\n+            The `past_values` is what the Transformer encoder gets as input (with optional additional features, such as\n+            `static_categorical_features`, `static_real_features`, `past_time_features`).\n+\n+            The sequence length here is equal to `context_length` + `max(config.lags_sequence)`.\n+\n+            Missing values need to be replaced with zeros.\n+        past_time_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_features)`, *optional*):\n+            Optional time features, which the model internally will add to `past_values`. These could be things like\n+            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n+            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n+            time-series is. Age features have small values for distant past time steps and increase monotonically the\n+            more we approach the current time step.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional time features.\n+\n+            The Autoformer only learns additional embeddings for `static_categorical_features`.\n+        past_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Boolean mask to indicate which `past_values` were observed and which were missing. Mask values selected in\n+            `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+        static_categorical_features (`torch.LongTensor` of shape `(batch_size, number of static categorical features)`, *optional*):\n+            Optional static categorical features for which the model will learn an embedding, which it will add to the\n+            values of the time series.\n+\n+            Static categorical features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static categorical feature is a time series ID.\n+        static_real_features (`torch.FloatTensor` of shape `(batch_size, number of static real features)`, *optional*):\n+            Optional static real features which the model will add to the values of the time series.\n+\n+            Static real features are features which have the same value for all time steps (static over time).\n+\n+            A typical example of a static real feature is promotion information.\n+        future_values (`torch.FloatTensor` of shape `(batch_size, prediction_length)`):\n+            Future values of the time series, that serve as labels for the model. The `future_values` is what the\n+            Transformer needs to learn to output, given the `past_values`.\n+\n+            See the demo notebook and code snippets for details.\n+\n+            Missing values need to be replaced with zeros.\n+        future_time_features (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_features)`, *optional*):\n+            Optional time features, which the model internally will add to `future_values`. These could be things like\n+            \"month of year\", \"day of the month\", etc. encoded as vectors (for instance as Fourier features). These\n+            could also be so-called \"age\" features, which basically help the model know \"at which point in life\" a\n+            time-series is. Age features have small values for distant past time steps and increase monotonically the\n+            more we approach the current time step.\n+\n+            These features serve as the \"positional encodings\" of the inputs. So contrary to a model like BERT, where\n+            the position encodings are learned from scratch internally as parameters of the model, the Time Series\n+            Transformer requires to provide additional features.\n+\n+            The Autoformer only learns additional embeddings for `static_categorical_features`.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n+            Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n+            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n+            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n+        future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n+            Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n+            in `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+\n+            This mask is used to filter out missing values for the final loss calculation.\n \n         Examples:\n "
        },
        {
            "sha": "0e00263584936dd61642c3af77b0ee936947c22a",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 16,
            "deletions": 132,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -31,21 +31,11 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torchdynamo_compiling,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n from ..auto import AutoModel\n from .configuration_aya_vision import AyaVisionConfig\n \n \n-_CONFIG_FOR_DOC = \"AyaVisionConfig\"\n-\n-\n class AyaVisionMultiModalProjector(nn.Module):\n     def __init__(self, config: AyaVisionConfig):\n         super().__init__()\n@@ -96,27 +86,7 @@ def pixel_shuffle(self, image_features):  # B, S, D\n         return image_features\n \n \n-AYA_VISION_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`AyaVisionConfig`] or [`AyaVisionVisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Aya Vision Model outputting raw hidden-states without any specific head on top.\",\n-    AYA_VISION_START_DOCSTRING,\n-)\n+@auto_docstring\n class AyaVisionPreTrainedModel(PreTrainedModel):\n     config_class = AyaVisionConfig\n     base_model_prefix = \"\"\n@@ -218,86 +188,10 @@ class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-AYA_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`AyaVisionProcessor`] uses\n-            [`CLIPImageProcessor`] for processing images).\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n-            The index of the layer to select the vision feature. If multiple indices are provided,\n-            the vision feature of the corresponding indices will be concatenated to form the\n-            vision features.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"The AyaVision model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n-    AYA_VISION_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The AyaVision model which consists of a vision backbone and a language model, without a language modeling head.\n+    \"\"\"\n )\n class AyaVisionModel(AyaVisionPreTrainedModel):\n     _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n@@ -363,7 +257,7 @@ def get_image_features(\n         return image_features\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(AYA_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -454,9 +348,10 @@ def forward(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-@add_start_docstrings(\n-    \"\"\"The AyaVision model which consists of a vision backbone and a language model.\"\"\",\n-    AYA_VISION_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The AYA_VISION model which consists of a vision backbone and a language model.\n+    \"\"\"\n )\n class AyaVisionForConditionalGeneration(AyaVisionPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n@@ -499,8 +394,7 @@ def multi_modal_projector(self):\n         return self.model.multi_modal_projector\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(AYA_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AyaVisionCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -522,20 +416,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "7dcd206bab8222454ca2be10f6681c085d2a117a",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 32,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -29,17 +29,12 @@\n \n from ...activations import ACT2FN\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    logging,\n-)\n+from ...utils import logging\n from .configuration_aya_vision import AyaVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"AyaVisionConfig\"\n-\n \n class AyaVisionMultiModalProjector(nn.Module):\n     def __init__(self, config: AyaVisionConfig):\n@@ -91,14 +86,6 @@ def pixel_shuffle(self, image_features):  # B, S, D\n         return image_features\n \n \n-AYA_VISION_START_DOCSTRING = None\n-AYA_VISION_INPUTS_DOCSTRING = None\n-\n-\n-@add_start_docstrings(\n-    \"The bare Aya Vision Model outputting raw hidden-states without any specific head on top.\",\n-    AYA_VISION_START_DOCSTRING,\n-)\n class AyaVisionPreTrainedModel(LlavaPreTrainedModel):\n     _supports_quantized_cache = False\n     _supports_static_cache = False\n@@ -127,10 +114,6 @@ class AyaVisionModel(LlavaModel):\n     pass\n \n \n-@add_start_docstrings(\n-    \"\"\"The AyaVision model which consists of a vision backbone and a language model.\"\"\",\n-    AYA_VISION_START_DOCSTRING,\n-)\n class AyaVisionForConditionalGeneration(LlavaForConditionalGeneration):\n     def forward(\n         self,\n@@ -153,20 +136,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "86316881493285a5ca9e2f3f068dd00141c45de6",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 10,
            "deletions": 130,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -41,13 +41,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, can_return_tuple, logging\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -66,8 +60,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"BambaConfig\"\n-\n \n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n@@ -1014,27 +1006,7 @@ def forward(\n         return outputs\n \n \n-BAMBA_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BambaConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BambaModel outputting raw hidden-states without any specific head on top.\",\n-    BAMBA_START_DOCSTRING,\n-)\n+@auto_docstring\n class BambaPreTrainedModel(PreTrainedModel):\n     config_class = BambaConfig\n     base_model_prefix = \"model\"\n@@ -1064,91 +1036,8 @@ def _init_weights(self, module):\n             module.D.data.fill_(1.0)\n \n \n-BAMBA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`HybridMambaAttentionDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            A HybridMambaAttentionDynamicCache object containing pre-computed hidden-states (keys and values in the\n-            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.\n-            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and\n-            `(batch_size, d_inner, d_state)` respectively.\n-            See the `HybridMambaAttentionDynamicCache` class for more details.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        output_router_logits (`bool`, *optional*):\n-            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-            should not be returned during inference.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Bamba Model outputting raw hidden-states without any specific head on top.\",\n-    BAMBA_START_DOCSTRING,\n-)\n-# Adapted from transformers.models.jamba.modeling_jamba.JambaModel\n+@auto_docstring\n class BambaModel(BambaPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`BambaDecoderLayer`]\n-\n-    Args:\n-        config: BambaConfig\n-    \"\"\"\n-\n     def __init__(self, config: BambaConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1175,7 +1064,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1414,6 +1303,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n         return mamba_mask\n \n \n+@auto_docstring\n class BambaForCausalLM(BambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1447,8 +1337,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1465,19 +1354,10 @@ def forward(\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "954dfd11ab68463785b0cc52c3b4d5c7a0e636de",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 10,
            "deletions": 139,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -43,23 +43,11 @@\n     segment_sum,\n )\n \n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-)\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    logging,\n-    replace_return_docstrings,\n-)\n-from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_flash_attn_2_available,\n-    is_mamba_2_ssm_available,\n-)\n+from ...utils import auto_docstring, can_return_tuple, logging\n+from ...utils.import_utils import is_causal_conv1d_available, is_flash_attn_2_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n \n@@ -82,8 +70,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"BambaConfig\"\n-\n \n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n@@ -782,27 +768,7 @@ def forward(\n         return outputs\n \n \n-BAMBA_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BambaConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BambaModel outputting raw hidden-states without any specific head on top.\",\n-    BAMBA_START_DOCSTRING,\n-)\n+@auto_docstring\n class BambaPreTrainedModel(PreTrainedModel):\n     config_class = BambaConfig\n     base_model_prefix = \"model\"\n@@ -832,91 +798,8 @@ def _init_weights(self, module):\n             module.D.data.fill_(1.0)\n \n \n-BAMBA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`HybridMambaAttentionDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            A HybridMambaAttentionDynamicCache object containing pre-computed hidden-states (keys and values in the\n-            self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-            Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.\n-            Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and\n-            `(batch_size, d_inner, d_state)` respectively.\n-            See the `HybridMambaAttentionDynamicCache` class for more details.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        output_router_logits (`bool`, *optional*):\n-            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-            should not be returned during inference.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Bamba Model outputting raw hidden-states without any specific head on top.\",\n-    BAMBA_START_DOCSTRING,\n-)\n-# Adapted from transformers.models.jamba.modeling_jamba.JambaModel\n+@auto_docstring\n class BambaModel(BambaPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`BambaDecoderLayer`]\n-\n-    Args:\n-        config: BambaConfig\n-    \"\"\"\n-\n     def __init__(self, config: BambaConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -943,7 +826,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1183,9 +1066,6 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n \n \n class BambaForCausalLM(LlamaForCausalLM):\n-    @can_return_tuple\n-    @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1202,19 +1082,10 @@ def forward(\n         **kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "7a564393f9e38261f0465e7ccabef0ebb2219b01",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 40,
            "deletions": 157,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -34,8 +34,7 @@\n from ...modeling_outputs import CausalLMOutputWithPast, MaskedLMOutput\n from ...modeling_utils import PreTrainedModel, get_parameter_device\n from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     is_accelerate_available,\n     is_torch_accelerator_available,\n     logging,\n@@ -62,10 +61,6 @@\n logger = logging.get_logger(__name__)\n \n \n-_CHECKPOINT_FOR_DOC = \"suno/bark-small\"\n-_CONFIG_FOR_DOC = \"BarkConfig\"\n-\n-\n class BarkSelfAttention(nn.Module):\n     # adapted from GPTNeoSelfAttention and Bark code\n     # BarkSelfAttention can have two attention type, i.e full attention or causal attention\n@@ -368,12 +363,8 @@ def forward(\n         return outputs  # hidden_states, ((present), attentions)\n \n \n+@auto_docstring\n class BarkPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BarkConfig\n     supports_gradient_checkpointing = False\n     _supports_flash_attn_2 = True\n@@ -418,134 +409,6 @@ def device(self) -> torch.device:\n         return get_parameter_device(self)\n \n \n-BARK_MODEL_START_DOCSTRING = \"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`{config}`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-BARK_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BarkConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-BARK_FINE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        codebook_idx (`int`):\n-            Index of the codebook that will be predicted.\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, number_of_codebooks)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it. Initially, indices of the first two codebooks are obtained from the `coarse` sub-model. The rest is\n-            predicted recursively by attending the previously predicted channels. The model predicts on windows of\n-            length 1024.\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*): NOT IMPLEMENTED YET.\n-        input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. If\n-            `past_key_values` is used, optionally only the last `input_embeds` have to be input (see\n-            `past_key_values`). This is useful if you want more control over how to convert `input_ids` indices into\n-            associated vectors than the model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-BARK_CAUSAL_MODEL_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it. Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `input_ids` of shape `(batch_size, sequence_length)`.\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-            Here, due to `Bark` particularities, if `past_key_values` is used, `input_embeds` will be ignored and you\n-            have to use `input_ids`. If `past_key_values` is not used and `use_cache` is set to `True`, `input_embeds`\n-            is used in priority instead of `input_ids`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # GPT2-like autoregressive model\n class BarkCausalModel(BarkPreTrainedModel, GenerationMixin):\n     config_class = BarkSubModelConfig\n@@ -639,7 +502,7 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, **kwarg\n             \"attention_mask\": attention_mask,\n         }\n \n-    @add_start_docstrings_to_model_forward(BARK_CAUSAL_MODEL_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -654,6 +517,13 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithPast]:\n+        r\"\"\"\n+        input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+            Here, due to `Bark` particularities, if `past_key_values` is used, `input_embeds` will be ignored and you\n+            have to use `input_ids`. If `past_key_values` is not used and `use_cache` is set to `True`, `input_embeds`\n+            is used in priority instead of `input_ids`.\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -803,10 +673,11 @@ def _reorder_cache(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"Bark semantic (or text) model. It shares the same architecture as the coarse model.\n-    It is a GPT-2 like autoregressive model with a language modeling head on top.\"\"\",\n-    BARK_MODEL_START_DOCSTRING.format(config=\"BarkSemanticConfig\"),\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Bark semantic (or text) model. It shares the same architecture as the coarse model.\n+    It is a GPT-2 like autoregressive model with a language modeling head on top.\n+    \"\"\"\n )\n class BarkSemanticModel(BarkCausalModel):\n     base_model_prefix = \"semantic\"\n@@ -912,11 +783,12 @@ def generate(\n         return semantic_output\n \n \n-@add_start_docstrings(\n-    \"\"\"Bark coarse acoustics model.\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Bark coarse acoustics model.\n     It shares the same architecture as the semantic (or text) model. It is a GPT-2 like autoregressive model with a\n-    language modeling head on top.\"\"\",\n-    BARK_MODEL_START_DOCSTRING.format(config=\"BarkCoarseConfig\"),\n+    language modeling head on top.\n+    \"\"\"\n )\n class BarkCoarseModel(BarkCausalModel):\n     base_model_prefix = \"coarse_acoustics\"\n@@ -1133,10 +1005,11 @@ def generate(\n         return coarse_output\n \n \n-@add_start_docstrings(\n-    \"\"\"Bark fine acoustics model. It is a non-causal GPT-like model with `config.n_codes_total` embedding layers and\n-    language modeling heads, one for each codebook.\"\"\",\n-    BARK_MODEL_START_DOCSTRING.format(config=\"BarkFineConfig\"),\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Bark fine acoustics model. It is a non-causal GPT-like model with `config.n_codes_total` embedding layers and\n+    language modeling heads, one for each codebook.\n+    \"\"\"\n )\n class BarkFineModel(BarkPreTrainedModel):\n     base_model_prefix = \"fine_acoustics\"\n@@ -1293,7 +1166,7 @@ def tie_weights(self):\n             if hasattr(module, \"_tie_weights\"):\n                 module._tie_weights()\n \n-    @add_start_docstrings_to_model_forward(BARK_FINE_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         codebook_idx: int,  # an additional idx corresponding to the id of the codebook that will be predicted\n@@ -1307,6 +1180,17 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n+        r\"\"\"\n+        codebook_idx (`int`):\n+            Index of the codebook that will be predicted.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            NOT IMPLEMENTED YET.\n+        input_embeds (`torch.FloatTensor` of shape `(batch_size, input_sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. If\n+            `past_key_values` is used, optionally only the last `input_embeds` have to be input (see\n+            `past_key_values`). This is useful if you want more control over how to convert `input_ids` indices into\n+            associated vectors than the model's internal embedding lookup matrix.\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1541,8 +1425,8 @@ def generate(\n         return fine_input\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The full Bark model, a text-to-speech model composed of 4 sub-models:\n     - [`BarkSemanticModel`] (also referred to as the 'text' model): a causal auto-regressive transformer model that\n       takes\n@@ -1557,8 +1441,7 @@ def generate(\n \n     It should be noted that each of the first three modules can support conditional speaker embeddings to condition the\n     output sound according to specific predefined voice.\n-    \"\"\",\n-    BARK_START_DOCSTRING,\n+    \"\"\"\n )\n class BarkModel(BarkPreTrainedModel):\n     config_class = BarkConfig"
        },
        {
            "sha": "a9f9c31ac0f45d900659d53aa6d0510656a3f3dd",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 177,
            "deletions": 303,
            "changes": 480,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -43,14 +43,7 @@\n     Seq2SeqSequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_end_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from .configuration_bart import BartConfig\n \n \n@@ -60,22 +53,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"facebook/bart-base\"\n-_CONFIG_FOR_DOC = \"BartConfig\"\n-\n-# Base model docstring\n-_EXPECTED_OUTPUT_SHAPE = [1, 8, 768]\n-\n-# SequenceClassification docstring\n-_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"valhalla/bart-large-sst2\"\n-_SEQ_CLASS_EXPECTED_LOSS = 0.0\n-_SEQ_CLASS_EXPECTED_OUTPUT = \"'POSITIVE'\"\n-\n-# QuestionAsnwering docstring\n-_CHECKPOINT_FOR_QA = \"valhalla/bart-large-finetuned-squadv1\"\n-_QA_EXPECTED_LOSS = 0.59\n-_QA_EXPECTED_OUTPUT = \"' nice puppet'\"\n-\n \n def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n     \"\"\"\n@@ -739,6 +716,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+@auto_docstring\n class BartPreTrainedModel(PreTrainedModel):\n     config_class = BartConfig\n     base_model_prefix = \"model\"\n@@ -787,163 +765,6 @@ def __init_subclass__(self):\n         )\n \n \n-BART_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BartConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BART_GENERATION_EXAMPLE = r\"\"\"\n-    Summarization example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n-\n-    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-\n-    >>> ARTICLE_TO_SUMMARIZE = (\n-    ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n-    ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n-    ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n-    ... )\n-    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n-\n-    >>> # Generate Summary\n-    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n-    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-    'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n-    ```\n-\n-    Mask filling example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n-\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n-    >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n-\n-    >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n-    >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n-    >>> logits = model(input_ids).logits\n-\n-    >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n-    >>> probs = logits[0, masked_index].softmax(dim=0)\n-    >>> values, predictions = probs.topk(5)\n-\n-    >>> tokenizer.decode(predictions).split()\n-    ['not', 'good', 'healthy', 'great', 'very']\n-    ```\n-\"\"\"\n-\n-BART_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n-            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n-\n-            For translation and summarization training, `decoder_input_ids` should be provided. If no\n-            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n-            for denoising pre-training following the paper.\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-            than the model's internal embedding lookup matrix.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n-            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n-            input (see `past_key_values`). This is useful if you want more control over how to convert\n-            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n-\n-            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n-            of `inputs_embeds`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class BartEncoder(BartPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -1419,10 +1240,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare BART Model outputting raw hidden-states without any specific head on top.\",\n-    BART_START_DOCSTRING,\n-)\n+@auto_docstring\n class BartModel(BartPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n \n@@ -1465,13 +1283,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n-    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Seq2SeqModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1490,6 +1302,35 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqModelOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n+            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        \"\"\"\n         # different to other models, Bart automatically creates decoder_input_ids from\n         # input_ids if no decoder_input_ids are provided\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n@@ -1560,8 +1401,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The BART Model with a language modeling head. Can be used for summarization.\", BART_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The BART Model with a language modeling head. Can be used for summarization.\n+    \"\"\"\n )\n class BartForConditionalGeneration(BartPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n@@ -1610,9 +1453,7 @@ def _tie_weights(self):\n             self.model._tie_weights()\n             self._tie_or_clone_weights(self.lm_head, self.model.shared)\n \n-    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n-    @add_end_docstrings(BART_GENERATION_EXAMPLE)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1633,12 +1474,78 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n+            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        Returns:\n+        Example summarization:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n+\n+        >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n+\n+        >>> ARTICLE_TO_SUMMARIZE = (\n+        ...     \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n+        ...     \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n+        ...     \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n+        ... )\n+        >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n+\n+        >>> # Generate Summary\n+        >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n+        >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        'PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions'\n+        ```\n+\n+        Mask filling example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, BartForConditionalGeneration\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n+        >>> model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n+\n+        >>> TXT = \"My friends are <mask> but they eat too many carbs.\"\n+        >>> input_ids = tokenizer([TXT], return_tensors=\"pt\")[\"input_ids\"]\n+        >>> logits = model(input_ids).logits\n+\n+        >>> masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n+        >>> probs = logits[0, masked_index].softmax(dim=0)\n+        >>> values, predictions = probs.topk(5)\n+\n+        >>> tokenizer.decode(predictions).split()\n+        ['not', 'good', 'healthy', 'great', 'very']\n+        ```\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1709,12 +1616,11 @@ def _reorder_cache(past_key_values, beam_idx):\n         return reordered_past\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Bart model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n     tasks.\n-    \"\"\",\n-    BART_START_DOCSTRING,\n+    \"\"\"\n )\n class BartForSequenceClassification(BartPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n@@ -1732,14 +1638,7 @@ def __init__(self, config: BartConfig, **kwargs):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n-        output_type=Seq2SeqSequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n-        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1759,6 +1658,33 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n+            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n@@ -1839,13 +1765,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BART Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    BART_START_DOCSTRING,\n-)\n+@auto_docstring\n class BartForQuestionAnswering(BartPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n \n@@ -1861,14 +1781,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BART_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_QA,\n-        output_type=Seq2SeqQuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_loss=_QA_EXPECTED_LOSS,\n-        expected_output=_QA_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1889,14 +1802,33 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n-            are not taken into account for computing the loss.\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n+            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n+\n+            For translation and summarization training, `decoder_input_ids` should be provided. If no\n+            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n+            for denoising pre-training following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if start_positions is not None and end_positions is not None:\n@@ -1978,11 +1910,10 @@ def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BART decoder with a language modeling head on top (linear layer with weights tied to the input embeddings).\n-    \"\"\",\n-    BART_START_DOCSTRING,\n+    \"\"\"\n )\n class BartForCausalLM(BartPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -2017,7 +1948,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.decoder\n \n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2035,72 +1966,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                if the model is configured as a decoder.\n-            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n-                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n-                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n-\n-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n-                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n-                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n \n-        Returns:\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "5f090b92760a5be3fbcfc958514073d1b947a4fc",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 16,
            "deletions": 23,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -21,7 +21,6 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n@@ -40,24 +39,21 @@\n     validate_kwargs,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, add_start_docstrings\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TensorType, auto_docstring\n \n \n class BeitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    do_reduce_labels: Optional[bool]\n-\n-\n-@add_start_docstrings(\n-    \"Constructs a fast Beit image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     \"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n         Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n         is used for background, and background itself is not included in all classes of a dataset (e.g.\n         ADE20k). The background label will be replaced by 255.\n-    \"\"\",\n-)\n+    \"\"\"\n+\n+    do_reduce_labels: Optional[bool]\n+\n+\n+@auto_docstring\n class BeitImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n     image_mean = IMAGENET_STANDARD_MEAN\n@@ -72,6 +68,9 @@ class BeitImageProcessorFast(BaseImageProcessorFast):\n     do_reduce_labels = False\n     valid_kwargs = BeitFastImageProcessorKwargs\n \n+    def __init__(self, **kwargs: Unpack[BeitFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n     @classmethod\n     def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n         \"\"\"\n@@ -170,23 +169,17 @@ def __call__(self, images, segmentation_maps=None, **kwargs):\n         # be passed in as positional arguments.\n         return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n \n-    @deprecate_kwarg(\"reduce_labels\", new_name=\"do_reduce_labels\", version=\"4.41.0\")\n-    @add_start_docstrings(\n-        \"Constructs a fast Beit image processor.\",\n-        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-        \"\"\"\n-        do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n-            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n-            is used for background, and background itself is not included in all classes of a dataset (e.g.\n-            ADE20k). The background label will be replaced by 255.\n-        \"\"\",\n-    )\n+    @auto_docstring\n     def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n+        **kwargs: Unpack[BeitFastImageProcessorKwargs],\n     ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n         # Set default kwargs from self. This ensures that if a kwarg is not provided\n         # by the user, it gets its default value from the instance, or is set to None."
        },
        {
            "sha": "24e16ee8bf1f7e00e482e0e8cde816c120261b44",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 25,
            "deletions": 102,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,25 +36,16 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import compile_compatible_method_lru_cache, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import auto_docstring, logging, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_beit import BeitConfig\n \n \n logger = logging.get_logger(__name__)\n \n # General docstring\n-_CONFIG_FOR_DOC = \"BeitConfig\"\n \n # Base docstring\n-_CHECKPOINT_FOR_DOC = \"microsoft/beit-base-patch16-224-pt22k\"\n _EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n \n # Image classification docstring\n@@ -741,12 +732,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class BeitPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BeitConfig\n     base_model_prefix = \"beit\"\n     main_input_name = \"pixel_values\"\n@@ -784,48 +771,13 @@ def _init_weights(self, module):\n                 module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n \n \n-BEIT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`BeitConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BEIT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n-            [`BeitImageProcessor.__call__`] for details.\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Beit Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BEIT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BeitModel(BeitPreTrainedModel):\n     def __init__(self, config: BeitConfig, add_pooling_layer: bool = True) -> None:\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -851,14 +803,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BeitModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n@@ -933,12 +878,13 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-@add_start_docstrings(\n-    \"\"\"Beit Model transformer with a 'language' modeling head on top. BEiT does masked image modeling by predicting\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Beit Model transformer with a 'language' modeling head on top. BEiT does masked image modeling by predicting\n     visual tokens of a Vector-Quantize Variational Autoencoder (VQ-VAE), whereas other vision models like ViT and DeiT\n     predict RGB pixel values. As a result, this class is incompatible with [`AutoModelForMaskedImageModeling`], so you\n-    will need to use [`BeitForMaskedImageModeling`] directly if you wish to do masked image modeling with BEiT.\"\"\",\n-    BEIT_START_DOCSTRING,\n+    will need to use [`BeitForMaskedImageModeling`] directly if you wish to do masked image modeling with BEiT.\n+    \"\"\"\n )\n class BeitForMaskedImageModeling(BeitPreTrainedModel):\n     def __init__(self, config: BeitConfig) -> None:\n@@ -954,8 +900,7 @@ def __init__(self, config: BeitConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -970,14 +915,11 @@ def forward(\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n-\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n \n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1035,12 +977,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Beit Model transformer with an image classification head on top (a linear layer on top of the average of the final\n     hidden states of the patch tokens) e.g. for ImageNet.\n-    \"\"\",\n-    BEIT_START_DOCSTRING,\n+    \"\"\"\n )\n class BeitForImageClassification(BeitPreTrainedModel):\n     def __init__(self, config: BeitConfig) -> None:\n@@ -1055,13 +996,7 @@ def __init__(self, config: BeitConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -1361,12 +1296,7 @@ def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor:\n         return output\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Beit Model transformer with a semantic segmentation head on top e.g. for ADE20k, CityScapes.\n-    \"\"\",\n-    BEIT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BeitForSemanticSegmentation(BeitPreTrainedModel):\n     def __init__(self, config: BeitConfig) -> None:\n         super().__init__(config)\n@@ -1419,8 +1349,7 @@ def compute_loss(self, logits, auxiliary_logits, labels):\n \n         return loss\n \n-    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=SemanticSegmenterOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -1436,8 +1365,6 @@ def forward(\n             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1`, a classification loss is computed (Cross-Entropy).\n \n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1514,11 +1441,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BEiT backbone, to be used with frameworks like DETR and MaskFormer.\n-    \"\"\",\n-    BEIT_START_DOCSTRING,\n+    \"\"\"\n )\n class BeitBackbone(BeitPreTrainedModel, BackboneMixin):\n     def __init__(self, config):\n@@ -1554,18 +1480,15 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    @add_start_docstrings_to_model_forward(BEIT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Tensor,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> BackboneOutput:\n-        \"\"\"\n-        Returns:\n-\n+        r\"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "dd738ea96ebd4dfbe93e178ebe7fdf82e1074d17",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 71,
            "deletions": 268,
            "changes": 339,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -29,10 +29,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -46,42 +43,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    get_torch_version,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring, get_torch_version, logging\n from .configuration_bert import BertConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google-bert/bert-base-uncased\"\n-_CONFIG_FOR_DOC = \"BertConfig\"\n-\n-# TokenClassification docstring\n-_CHECKPOINT_FOR_TOKEN_CLASSIFICATION = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n-_TOKEN_CLASS_EXPECTED_OUTPUT = (\n-    \"['O', 'I-ORG', 'I-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O', 'I-LOC', 'O', 'I-LOC', 'I-LOC'] \"\n-)\n-_TOKEN_CLASS_EXPECTED_LOSS = 0.01\n-\n-# QuestionAnswering docstring\n-_CHECKPOINT_FOR_QA = \"deepset/bert-base-cased-squad2\"\n-_QA_EXPECTED_OUTPUT = \"'a nice puppet'\"\n-_QA_EXPECTED_LOSS = 7.41\n-_QA_TARGET_START_INDEX = 14\n-_QA_TARGET_END_INDEX = 15\n-\n-# SequenceClassification docstring\n-_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION = \"textattack/bert-base-uncased-yelp-polarity\"\n-_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_1'\"\n-_SEQ_CLASS_EXPECTED_LOSS = 0.01\n-\n \n def load_tf_weights_in_bert(model, config, tf_checkpoint_path):\n     \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n@@ -821,12 +788,8 @@ def forward(self, sequence_output, pooled_output):\n         return prediction_scores, seq_relationship_score\n \n \n+@auto_docstring\n class BertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BertConfig\n     load_tf_weights = load_tf_weights_in_bert\n     base_model_prefix = \"bert\"\n@@ -886,79 +849,8 @@ class BertForPreTrainingOutput(ModelOutput):\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-BERT_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BertConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Bert Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BERT_START_DOCSTRING,\n-)\n-class BertModel(BertPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n     cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n     all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n@@ -968,10 +860,15 @@ class BertModel(BertPreTrainedModel):\n     to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n     \"\"\"\n-\n+)\n+class BertModel(BertPreTrainedModel):\n     _no_split_modules = [\"BertEmbeddings\", \"BertLayer\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -1000,12 +897,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1022,26 +914,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1169,12 +1041,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Bert Model with two heads on top as done during the pretraining: a `masked language modeling` head and a `next\n     sentence prediction (classification)` head.\n-    \"\"\",\n-    BERT_START_DOCSTRING,\n+    \"\"\"\n )\n class BertForPreTraining(BertPreTrainedModel):\n     _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n@@ -1195,8 +1066,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=BertForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1212,20 +1082,16 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BertForPreTrainingOutput]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n-                config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n-                the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-            next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n-                pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n-\n-                - 0 indicates sequence B is a continuation of sequence A,\n-                - 1 indicates sequence B is a random sequence.\n-            kwargs (`Dict[str, any]`, *optional*, defaults to `{}`):\n-                Used to hide legacy arguments that have been deprecated.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n+            the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n+            pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n \n-        Returns:\n+            - 0 indicates sequence B is a continuation of sequence A,\n+            - 1 indicates sequence B is a random sequence.\n \n         Example:\n \n@@ -1280,8 +1146,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"Bert Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BERT_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Bert Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n )\n class BertLMHeadModel(BertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n@@ -1305,12 +1173,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutputWithCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1330,28 +1193,10 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n@@ -1402,7 +1247,7 @@ def _reorder_cache(self, past_key_values, beam_idx):\n         return reordered_past\n \n \n-@add_start_docstrings(\"\"\"Bert Model with a `language modeling` head on top.\"\"\", BERT_START_DOCSTRING)\n+@auto_docstring\n class BertForMaskedLM(BertPreTrainedModel):\n     _tied_weights_keys = [\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n@@ -1428,14 +1273,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"'paris'\",\n-        expected_loss=0.88,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1518,9 +1356,10 @@ def can_generate(cls) -> bool:\n         return False\n \n \n-@add_start_docstrings(\n-    \"\"\"Bert Model with a `next sentence prediction (classification)` head on top.\"\"\",\n-    BERT_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Bert Model with a `next sentence prediction (classification)` head on top.\n+    \"\"\"\n )\n class BertForNextSentencePrediction(BertPreTrainedModel):\n     def __init__(self, config):\n@@ -1532,8 +1371,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=NextSentencePredictorOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1556,8 +1394,6 @@ def forward(\n             - 0 indicates sequence B is a continuation of sequence A,\n             - 1 indicates sequence B is a random sequence.\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -1620,12 +1456,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Bert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n     output) e.g. for GLUE tasks.\n-    \"\"\",\n-    BERT_START_DOCSTRING,\n+    \"\"\"\n )\n class BertForSequenceClassification(BertPreTrainedModel):\n     def __init__(self, config):\n@@ -1643,14 +1478,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_SEQUENCE_CLASSIFICATION,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_SEQ_CLASS_EXPECTED_OUTPUT,\n-        expected_loss=_SEQ_CLASS_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1723,13 +1551,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    BERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BertForMultipleChoice(BertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1744,12 +1566,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1764,6 +1581,30 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n@@ -1817,13 +1658,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Bert Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    BERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BertForTokenClassification(BertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1839,14 +1674,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_TOKEN_CLASSIFICATION,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_TOKEN_CLASS_EXPECTED_OUTPUT,\n-        expected_loss=_TOKEN_CLASS_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1900,13 +1728,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Bert Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    BERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BertForQuestionAnswering(BertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1918,16 +1740,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_QA,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        qa_target_start_index=_QA_TARGET_START_INDEX,\n-        qa_target_end_index=_QA_TARGET_END_INDEX,\n-        expected_output=_QA_EXPECTED_OUTPUT,\n-        expected_loss=_QA_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1942,16 +1755,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.bert("
        },
        {
            "sha": "6dee1db6fbdc7b93050ac88dd661029c8f7671af",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 12,
            "deletions": 122,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,20 +27,14 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     logging,\n-    replace_return_docstrings,\n )\n from .configuration_bert_generation import BertGenerationConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google/bert_for_seq_generation_L-24_bbc_encoder\"\n-_CONFIG_FOR_DOC = \"BertGenerationConfig\"\n-\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->BertGeneration\n class BertGenerationSelfOutput(nn.Module):\n@@ -583,12 +577,8 @@ def forward(self, input_ids=None, position_ids=None, inputs_embeds=None, past_ke\n         return embeddings\n \n \n+@auto_docstring\n class BertGenerationPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BertGenerationConfig\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n@@ -612,67 +602,10 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-BERT_GENERATION_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BertGenerationConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BERT_GENERATION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BertGeneration model transformer outputting raw hidden-states without any specific head on top.\",\n-    BERT_GENERATION_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare BertGeneration model transformer outputting raw hidden-states without any specific head on top.\n+    \"\"\"\n )\n class BertGenerationEncoder(BertGenerationPreTrainedModel):\n     \"\"\"\n@@ -715,12 +648,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(BERT_GENERATION_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPastAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -737,24 +665,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`: `1` for\n-            tokens that are NOT MASKED, `0` for MASKED tokens.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -860,9 +770,10 @@ def _tie_weights(self):\n             self.bias = self.decoder.bias\n \n \n-@add_start_docstrings(\n-    \"\"\"BertGeneration Model with a `language modeling` head on top for CLM fine-tuning.\"\"\",\n-    BERT_GENERATION_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    BertGeneration Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n )\n class BertGenerationDecoder(BertGenerationPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n@@ -886,8 +797,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n         self.lm_head.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BERT_GENERATION_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -906,30 +816,10 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\n-        Returns:\n \n         Example:\n "
        },
        {
            "sha": "5bae8894716ec8ae4e2fa910a7bcf0e71754fd8c",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 57,
            "deletions": 198,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -38,22 +38,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_big_bird import BigBirdConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google/bigbird-roberta-base\"\n-_CONFIG_FOR_DOC = \"BigBirdConfig\"\n-\n \n _TRIVIA_QA_MAPPING = {\n     \"big_bird_attention\": \"attention/self\",\n@@ -1743,12 +1733,8 @@ def forward(self, sequence_output, pooled_output):\n         return prediction_scores, seq_relationship_score\n \n \n+@auto_docstring\n class BigBirdPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BigBirdConfig\n     load_tf_weights = load_tf_weights_in_big_bird\n     base_model_prefix = \"bert\"\n@@ -1773,67 +1759,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-BIG_BIRD_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`BigBirdConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BIG_BIRD_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n @dataclass\n class BigBirdForPreTrainingOutput(ModelOutput):\n     \"\"\"\n@@ -1903,10 +1828,7 @@ class BigBirdForQuestionAnsweringModelOutput(ModelOutput):\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-@add_start_docstrings(\n-    \"The bare BigBird Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BIG_BIRD_START_DOCSTRING,\n-)\n+@auto_docstring\n class BigBirdModel(BigBirdPreTrainedModel):\n     \"\"\"\n \n@@ -1921,6 +1843,10 @@ class BigBirdModel(BigBirdPreTrainedModel):\n     \"\"\"\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.attention_type = self.config.attention_type\n         self.config = config\n@@ -1964,12 +1890,7 @@ def set_attention_type(self, value: str):\n         self.attention_type = value\n         self.encoder.set_attention_type(value)\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1987,25 +1908,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[BaseModelOutputWithPoolingAndCrossAttentions, Tuple[torch.FloatTensor]]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -2266,8 +2168,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=BigBirdForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2294,10 +2195,6 @@ def forward(\n \n             - 0 indicates sequence B is a continuation of sequence A,\n             - 1 indicates sequence B is a random sequence.\n-        kwargs (`Dict[str, any]`, *optional*, defaults to `{}`):\n-            Used to hide legacy arguments that have been deprecated.\n-\n-        Returns:\n \n         Example:\n \n@@ -2353,7 +2250,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\"\"\"BigBird Model with a `language modeling` head on top.\"\"\", BIG_BIRD_START_DOCSTRING)\n+@auto_docstring\n class BigBirdForMaskedLM(BigBirdPreTrainedModel):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n \n@@ -2379,8 +2276,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2402,8 +2298,6 @@ def forward(\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -2496,8 +2390,10 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n         return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n \n \n-@add_start_docstrings(\n-    \"\"\"BigBird Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BIG_BIRD_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    BigBird Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n )\n class BigBirdForCausalLM(BigBirdPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n@@ -2521,12 +2417,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutputWithCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2546,27 +2437,10 @@ def forward(\n         **kwargs,\n     ) -> Union[CausalLMOutputWithCrossAttentions, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -2646,12 +2520,11 @@ def forward(self, features, **kwargs):\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BigBird Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n-    \"\"\",\n-    BIG_BIRD_START_DOCSTRING,\n+    \"\"\"\n )\n class BigBirdForSequenceClassification(BigBirdPreTrainedModel):\n     def __init__(self, config):\n@@ -2664,8 +2537,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2685,8 +2557,6 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -2774,13 +2644,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BigBird Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    BIG_BIRD_START_DOCSTRING,\n-)\n+@auto_docstring\n class BigBirdForMultipleChoice(BigBirdPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -2792,14 +2656,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(\n-        BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\")\n-    )\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2814,6 +2671,30 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[MultipleChoiceModelOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n@@ -2867,13 +2748,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BigBird Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    BIG_BIRD_START_DOCSTRING,\n-)\n+@auto_docstring\n class BigBirdForTokenClassification(BigBirdPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -2889,12 +2764,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2966,15 +2836,13 @@ def forward(self, encoder_output):\n         return hidden_states\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BigBird Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    BIG_BIRD_START_DOCSTRING,\n-)\n+@auto_docstring\n class BigBirdForQuestionAnswering(BigBirdPreTrainedModel):\n     def __init__(self, config, add_pooling_layer=False):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n \n         config.num_labels = 2\n@@ -2987,13 +2855,12 @@ def __init__(self, config, add_pooling_layer=False):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIG_BIRD_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=BigBirdForQuestionAnsweringModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n-        question_lengths: Optional[torch.Tensor] = None,\n+        question_lengths: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n@@ -3005,16 +2872,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[BigBirdForQuestionAnsweringModelOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-\n-        Returns:\n+        question_lengths (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*):\n+            The lengths of the questions in the batch.\n \n         Example:\n "
        },
        {
            "sha": "efcf2dc44360d92f2dfa6f359cb9a766cd07e33b",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 110,
            "deletions": 271,
            "changes": 381,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,21 +36,12 @@\n     Seq2SeqSequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_end_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google/bigbird-pegasus-large-arxiv\"\n-_CONFIG_FOR_DOC = \"BigBirdPegasusConfig\"\n _EXPECTED_OUTPUT_SHAPE = [1, 7, 1024]\n \n \n@@ -1564,6 +1555,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+@auto_docstring\n class BigBirdPegasusPreTrainedModel(PreTrainedModel):\n     config_class = BigBirdPegasusConfig\n     base_model_prefix = \"model\"\n@@ -1594,149 +1586,6 @@ def dummy_inputs(self):\n         return dummy_inputs\n \n \n-BIGBIRD_PEGASUS_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BigBirdPegasusConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BIGBIRD_PEGASUS_GENERATION_EXAMPLE = r\"\"\"\n-    Summarization example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n-\n-    >>> model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n-    >>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n-\n-    >>> ARTICLE_TO_SUMMARIZE = (\n-    ...     \"The dominant sequence transduction models are based on complex recurrent or convolutional neural \"\n-    ...     \"networks in an encoder-decoder configuration. The best performing models also connect the encoder \"\n-    ...     \"and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, \"\n-    ...     \"based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \"\n-    ...     \"Experiments on two machine translation tasks show these models to be superior in quality \"\n-    ...     \"while being more parallelizable and requiring significantly less time to train.\"\n-    ... )\n-    >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=4096, return_tensors=\"pt\", truncation=True)\n-\n-    >>> # Generate Summary\n-    >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=15)\n-    >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-    'dominant sequence models are based on recurrent or convolutional neural networks .'\n-    ```\n-\"\"\"\n-\n-BIGBIRD_PEGASUS_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Provide for translation and summarization training. By default, the model will create this tensor by\n-            shifting the `input_ids` to the right, following the paper.\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            If you want to change padding behavior, you should read\n-            [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n-            [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n-\n-        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-            than the model's internal embedding lookup matrix.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n-            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n-            input (see `past_key_values`). This is useful if you want more control over how to convert\n-            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n-\n-            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n-            of `inputs_embeds`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-BIGBIRD_PEGASUS_STANDALONE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`ProphetNetTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class BigBirdPegasusEncoder(BigBirdPegasusPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -2293,10 +2142,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare BigBirdPegasus Model outputting raw hidden-states without any specific head on top.\",\n-    BIGBIRD_PEGASUS_START_DOCSTRING,\n-)\n+@auto_docstring\n class BigBirdPegasusModel(BigBirdPegasusPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n \n@@ -2334,14 +2180,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n-    @add_start_docstrings_to_model_forward(BIGBIRD_PEGASUS_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Seq2SeqModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n-    # Copied from transformers.models.bart.modeling_bart.BartModel.forward with Bart->BigBirdPegasus\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2360,6 +2199,23 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqModelOutput]:\n+        r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Provide for translation and summarization training. By default, the model will create this tensor by\n+            shifting the `input_ids` to the right, following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read\n+            [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n+            [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n+        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        \"\"\"\n         # different to other models, BigBirdPegasus automatically creates decoder_input_ids from\n         # input_ids if no decoder_input_ids are provided\n         if decoder_input_ids is None and decoder_inputs_embeds is None:\n@@ -2430,9 +2286,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The BigBirdPegasus Model with a language modeling head. Can be used for summarization.\",\n-    BIGBIRD_PEGASUS_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The BigBirdPegasus Model with a language modeling head. Can be used for summarization.\n+    \"\"\"\n )\n # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n class BigBirdPegasusForConditionalGeneration(BigBirdPegasusPreTrainedModel, GenerationMixin):\n@@ -2482,9 +2339,8 @@ def _tie_weights(self):\n             self.model._tie_weights()\n             self._tie_or_clone_weights(self.lm_head, self.model.shared)\n \n-    @add_start_docstrings_to_model_forward(BIGBIRD_PEGASUS_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n-    @add_end_docstrings(BIGBIRD_PEGASUS_GENERATION_EXAMPLE)\n+    @auto_docstring\n+    # Ignore copy\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2505,12 +2361,49 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Provide for translation and summarization training. By default, the model will create this tensor by\n+            shifting the `input_ids` to the right, following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read\n+            [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n+            [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n+        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        Returns:\n+        Example summarization:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, BigBirdPegasusForConditionalGeneration\n+\n+        >>> model = BigBirdPegasusForConditionalGeneration.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n+\n+        >>> ARTICLE_TO_SUMMARIZE = (\n+        ...     \"The dominant sequence transduction models are based on complex recurrent or convolutional neural \"\n+        ...     \"networks in an encoder-decoder configuration. The best performing models also connect the encoder \"\n+        ...     \"and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, \"\n+        ...     \"based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. \"\n+        ...     \"Experiments on two machine translation tasks show these models to be superior in quality \"\n+        ...     \"while being more parallelizable and requiring significantly less time to train.\"\n+        ... )\n+        >>> inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=4096, return_tensors=\"pt\", truncation=True)\n+\n+        >>> # Generate Summary\n+        >>> summary_ids = model.generate(inputs[\"input_ids\"], num_beams=4, max_length=15)\n+        >>> tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        'dominant sequence models are based on recurrent or convolutional neural networks .'\n+        ```\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -2581,12 +2474,11 @@ def _reorder_cache(past_key_values, beam_idx):\n         return reordered_past\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BigBirdPegasus model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g.\n     for GLUE tasks.\n-    \"\"\",\n-    BIGBIRD_PEGASUS_START_DOCSTRING,\n+    \"\"\"\n )\n class BigBirdPegasusForSequenceClassification(BigBirdPegasusPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n@@ -2604,13 +2496,7 @@ def __init__(self, config: BigBirdPegasusConfig, **kwargs):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIGBIRD_PEGASUS_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Seq2SeqSequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    # Copied from transformers.models.bart.modeling_bart.BartForSequenceClassification.forward\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2630,6 +2516,21 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqSequenceClassifierOutput]:\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Provide for translation and summarization training. By default, the model will create this tensor by\n+            shifting the `input_ids` to the right, following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read\n+            [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n+            [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n+        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n@@ -2710,13 +2611,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BigBirdPegasus Model with a span classification head on top for extractive question-answering tasks like SQuAD (a\n-    linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    BIGBIRD_PEGASUS_START_DOCSTRING,\n-)\n+@auto_docstring\n class BigBirdPegasusForQuestionAnswering(BigBirdPegasusPreTrainedModel):\n     _tied_weights_keys = [\"encoder.embed_tokens.weight\", \"decoder.embed_tokens.weight\"]\n \n@@ -2732,13 +2627,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIGBIRD_PEGASUS_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Seq2SeqQuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n-    # Copied from transformers.models.bart.modeling_bart.BartForQuestionAnswering.forward\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -2759,14 +2648,21 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Seq2SeqQuestionAnsweringModelOutput]:\n         r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (*sequence_length*). Position outside of the sequence\n-            are not taken into account for computing the loss.\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Provide for translation and summarization training. By default, the model will create this tensor by\n+            shifting the `input_ids` to the right, following the paper.\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            If you want to change padding behavior, you should read\n+            [`modeling_bigbird_pegasus._prepare_decoder_attention_mask`] and modify to your needs. See diagram 1 in\n+            [the paper](https://arxiv.org/abs/1910.13461) for more information on the default strategy.\n+        decoder_head_mask (`torch.Tensor` of shape `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if start_positions is not None and end_positions is not None:\n@@ -2882,7 +2778,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.decoder\n \n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2900,72 +2796,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n \n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                if the model is configured as a decoder.\n-            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n-                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n-                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n-\n-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n-                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n-                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\n-        Returns:\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "7fdb5a561899b67183ed0da19a9922740c542d22",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 15,
            "deletions": 118,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -33,19 +33,14 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     logging,\n )\n from .configuration_biogpt import BioGptConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"microsoft/biogpt\"\n-_CONFIG_FOR_DOC = \"BioGptConfig\"\n-\n \n # copied from transformers.models.opt.modeling_opt.OPTLearnedPositionalEmbedding with OPT->BioGpt\n # TODO @ArthurZucker bring copied from back\n@@ -444,12 +439,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class BioGptPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BioGptConfig\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n@@ -472,76 +463,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-BIOGPT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`~BioGptConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BIOGPT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-            than the model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BioGPT Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BIOGPT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BioGptModel(BioGptPreTrainedModel):\n     def __init__(self, config: BioGptConfig):\n         super().__init__(config)\n@@ -571,12 +493,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    @add_start_docstrings_to_model_forward(BIOGPT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPastAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -719,8 +636,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"BioGPT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", BIOGPT_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    BioGPT Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n )\n class BioGptForCausalLM(BioGptPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"output_projection.weight\"]\n@@ -740,12 +659,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.output_projection = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(BIOGPT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutputWithCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -815,13 +729,7 @@ def _reorder_cache(past_key_values, beam_idx):\n         return reordered_past\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BioGPT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    BIOGPT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BioGptForTokenClassification(BioGptPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -837,12 +745,7 @@ def __init__(self, config):\n \n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIOGPT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -907,8 +810,8 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The BioGpt Model transformer with a sequence classification head on top (linear layer).\n \n     [`BioGptForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n@@ -919,8 +822,7 @@ def forward(\n     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n     each row of the batch).\n-    \"\"\",\n-    BIOGPT_START_DOCSTRING,\n+    \"\"\"\n )\n class BioGptForSequenceClassification(BioGptPreTrainedModel):\n     def __init__(self, config: BioGptConfig):\n@@ -932,12 +834,7 @@ def __init__(self, config: BioGptConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIOGPT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "46c0e94f3d190b0b5157aa3e2a3ba7c38f3f41cc",
            "filename": "src/transformers/models/bit/image_processing_bit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -14,18 +14,12 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for BiT.\"\"\"\n \n-from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    BaseImageProcessorFast,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...utils import add_start_docstrings\n+from ...utils import auto_docstring\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast Bit image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n class BitImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "f1a6ee34e72c516666cf3c5e06f6926ba8fa4244",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 13,
            "deletions": 80,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -32,30 +32,13 @@\n     ImageClassifierOutputWithNoAttention,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_bit import BitConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"BitConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"google/bit-50\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 2048, 7, 7]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"google/bit-50\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tiger cat\"\n-\n \n def get_padding_value(padding=None, kernel_size=7, stride=1, dilation=1) -> Tuple[Tuple, bool]:\n     r\"\"\"\n@@ -646,12 +629,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class BitPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BitConfig\n     base_model_prefix = \"bit\"\n     main_input_name = \"pixel_values\"\n@@ -672,35 +651,7 @@ def _init_weights(self, module):\n             nn.init.constant_(module.bias, 0)\n \n \n-BIT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`BitConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BIT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`BitImageProcessor.__call__`]\n-            for details.\n-\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BiT model outputting raw features without any specific head on top.\",\n-    BIT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BitModel(BitPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -719,14 +670,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n     ) -> BaseModelOutputWithPoolingAndNoAttention:\n@@ -757,12 +701,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BiT Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\n     ImageNet.\n-    \"\"\",\n-    BIT_START_DOCSTRING,\n+    \"\"\"\n )\n class BitForImageClassification(BitPreTrainedModel):\n     def __init__(self, config):\n@@ -777,13 +720,7 @@ def __init__(self, config):\n         # initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutputWithNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -834,11 +771,10 @@ def forward(\n         return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BiT backbone, to be used with frameworks like DETR and MaskFormer.\n-    \"\"\",\n-    BIT_START_DOCSTRING,\n+    \"\"\"\n )\n class BitBackbone(BitPreTrainedModel, BackboneMixin):\n     def __init__(self, config):\n@@ -851,14 +787,11 @@ def __init__(self, config):\n         # initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BIT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n     ) -> BackboneOutput:\n-        \"\"\"\n-        Returns:\n-\n+        r\"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "48076bfb784e0be02a0381dcac6120b2ec7c2af2",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 10,
            "deletions": 122,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -34,15 +34,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_bitnet import BitNetConfig\n \n \n@@ -53,7 +45,6 @@\n \n \n logger = logging.get_logger(__name__)\n-_CONFIG_FOR_DOC = \"BitNetConfig\"\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n@@ -332,27 +323,7 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-BITNET_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BitNetConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BitNet Model outputting raw hidden-states without any specific head on top.\",\n-    BITNET_START_DOCSTRING,\n-)\n+@auto_docstring\n class BitNetPreTrainedModel(PreTrainedModel):\n     config_class = BitNetConfig\n     base_model_prefix = \"model\"\n@@ -381,88 +352,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-BITNET_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare BitNet Model outputting raw hidden-states without any specific head on top.\",\n-    BITNET_START_DOCSTRING,\n-)\n+@auto_docstring\n class BitNetModel(BitNetPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`BitNetDecoderLayer`]\n-\n-    Args:\n-        config: BitNetConfig\n-    \"\"\"\n-\n     def __init__(self, config: BitNetConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -486,7 +377,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(BITNET_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -708,6 +599,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+@auto_docstring\n class BitNetForCausalLM(BitNetPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = None\n@@ -741,8 +633,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(BITNET_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -759,13 +650,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        Args:\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, transformers.,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, transformers., config.vocab_size]`.\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, transformers.,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, transformers., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "0c0d133cb5d1676201628be90a72f2dfaded709d",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -22,9 +22,7 @@\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import (\n-    logging,\n-)\n+from ...utils import logging\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -40,8 +38,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"microsoft/bitnet-b1.58-2B-4T\"\n-\n \n class BitNetRMSNorm(LlamaRMSNorm):\n     pass\n@@ -132,13 +128,10 @@ def forward(\n         **super_kwargs,\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        Args:\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, transformers.,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, transformers., config.vocab_size]`.\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, transformers.,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, transformers., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "1eef9276428174340d39f9ba4fd63af43f79af54",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 89,
            "deletions": 230,
            "changes": 319,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,22 +36,13 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_end_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from ..blenderbot_small import BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel\n from .configuration_blenderbot import BlenderbotConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"BlenderbotConfig\"\n-_CHECKPOINT_FOR_DOC = \"facebook/blenderbot-400M-distill\"\n-\n \n # Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n@@ -453,6 +444,7 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class BlenderbotPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotConfig\n     base_model_prefix = \"model\"\n@@ -481,147 +473,6 @@ def dummy_inputs(self):\n         return dummy_inputs\n \n \n-BLENDERBOT_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BlenderbotConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BLENDERBOT_GENERATION_EXAMPLE = r\"\"\"\n-    Conversation example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n-\n-    >>> mname = \"facebook/blenderbot-400M-distill\"\n-    >>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n-    >>> tokenizer = AutoTokenizer.from_pretrained(mname)\n-    >>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n-    >>> print(\"Human: \", UTTERANCE)\n-    Human:  My friends are cool but they eat too many carbs.\n-\n-    >>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n-    >>> reply_ids = model.generate(**inputs)\n-    >>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n-    Bot: That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?\n-\n-    >>> REPLY = \"I'm not sure\"\n-    >>> print(\"Human: \", REPLY)\n-    Human: I'm not sure\n-\n-    >>> NEXT_UTTERANCE = (\n-    ...     \"My friends are cool but they eat too many carbs.</s> <s>That's unfortunate. \"\n-    ...     \"Are they trying to lose weight or are they just trying to be healthier?</s> \"\n-    ...     \"<s> I'm not sure.\"\n-    ... )\n-    >>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n-    >>> next_reply_ids = model.generate(**inputs)\n-    >>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n-    Bot:   I see. Well, it's good that they're trying to change their eating habits.\n-    ```\n-\"\"\"\n-\n-BLENDERBOT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            Blenderbot uses the `bos_token_id` as the starting token for `decoder_input_ids` generation. If\n-            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-            than the model's internal embedding lookup matrix.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n-            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n-            input (see `past_key_values`). This is useful if you want more control over how to convert\n-            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n-\n-            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n-            of `inputs_embeds`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class BlenderbotEncoder(BlenderbotPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -1051,10 +902,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare Blenderbot Model outputting raw hidden-states without any specific head on top.\",\n-    BLENDERBOT_START_DOCSTRING,\n-)\n+@auto_docstring\n class BlenderbotModel(BlenderbotPreTrainedModel):\n     _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n \n@@ -1097,8 +945,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n-    @add_start_docstrings_to_model_forward(BLENDERBOT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1118,7 +965,26 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            Blenderbot uses the `bos_token_id` as the starting token for `decoder_input_ids` generation. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n \n         Example:\n \n@@ -1192,8 +1058,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The Blenderbot Model with a language modeling head. Can be used for summarization.\", BLENDERBOT_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Blenderbot Model with a language modeling head. Can be used for summarization.\n+    \"\"\"\n )\n class BlenderbotForConditionalGeneration(BlenderbotPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n@@ -1252,9 +1120,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(BLENDERBOT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n-    @add_end_docstrings(BLENDERBOT_GENERATION_EXAMPLE)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1275,12 +1141,62 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            Blenderbot uses the `bos_token_id` as the starting token for `decoder_input_ids` generation. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        Returns:\n+        Example conversation:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, BlenderbotForConditionalGeneration\n+\n+        >>> mname = \"facebook/blenderbot-400M-distill\"\n+        >>> model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n+        >>> tokenizer = AutoTokenizer.from_pretrained(mname)\n+        >>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n+        >>> print(\"Human: \", UTTERANCE)\n+        Human:  My friends are cool but they eat too many carbs.\n+\n+        >>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n+        >>> reply_ids = model.generate(**inputs)\n+        >>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n+        Bot: That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?\n+\n+        >>> REPLY = \"I'm not sure\"\n+        >>> print(\"Human: \", REPLY)\n+        Human: I'm not sure\n+\n+        >>> NEXT_UTTERANCE = (\n+        ...     \"My friends are cool but they eat too many carbs.</s> <s>That's unfortunate. \"\n+        ...     \"Are they trying to lose weight or are they just trying to be healthier?</s> \"\n+        ...     \"<s> I'm not sure.\"\n+        ... )\n+        >>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n+        >>> next_reply_ids = model.generate(**inputs)\n+        >>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n+        Bot:   I see. Well, it's good that they're trying to change their eating habits.\n+        ```\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1394,7 +1310,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.decoder\n \n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1412,72 +1328,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                if the model is configured as a decoder.\n-            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n-                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n-                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n-\n-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n-                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n-                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n \n-        Returns:\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "eefb467e6059620f60626f22286dd56d3d5f169b",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 89,
            "deletions": 230,
            "changes": 319,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -34,20 +34,12 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_end_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from .configuration_blenderbot_small import BlenderbotSmallConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"BlenderbotSmallConfig\"\n-\n \n # Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n@@ -441,6 +433,7 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotSmallConfig\n     base_model_prefix = \"model\"\n@@ -469,147 +462,6 @@ def dummy_inputs(self):\n         return dummy_inputs\n \n \n-BLENDERBOT_SMALL_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BlenderbotSmallConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BLENDERBOT_SMALL_GENERATION_EXAMPLE = r\"\"\"\n-    Conversation example:\n-\n-    ```python\n-    >>> from transformers import AutoTokenizer, BlenderbotSmallForConditionalGeneration\n-\n-    >>> mname = \"facebook/blenderbot_small-90M\"\n-    >>> model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n-    >>> tokenizer = AutoTokenizer.from_pretrained(mname)\n-    >>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n-    >>> print(\"Human: \", UTTERANCE)\n-    Human:  My friends are cool but they eat too many carbs.\n-\n-    >>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n-    >>> reply_ids = model.generate(**inputs)\n-    >>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n-    Bot:  what kind of carbs do they eat? i don't know much about carbs.\n-\n-    >>> REPLY = \"I'm not sure\"\n-    >>> print(\"Human: \", REPLY)\n-    Human: I'm not sure\n-\n-    >>> NEXT_UTTERANCE = (\n-    ...     \"My friends are cool but they eat too many carbs.__end__ __start__what kind of carbs do they eat? \"\n-    ...     \"i don't know much about carbs__end__ \"\n-    ...     \"__start__ I'm not sure.\"\n-    ... )\n-    >>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n-    >>> next_reply_ids = model.generate(**inputs)\n-    >>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n-    Bot:  they eat a lot of carbs. carbs are high in fat, protein, and fats.\n-    ```\n-\"\"\"\n-\n-BLENDERBOT_SMALL_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            BlenderbotSmall uses the `bos_token_id` as the starting token for `decoder_input_ids` generation. If\n-            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-        head_mask (`torch.Tensor` of shape `(encoder_layers, encoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the encoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        decoder_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the attention modules in the decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-            This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-            than the model's internal embedding lookup matrix.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n-            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n-            input (see `past_key_values`). This is useful if you want more control over how to convert\n-            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n-\n-            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n-            of `inputs_embeds`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class BlenderbotSmallEncoder(BlenderbotSmallPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -1032,10 +884,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare BlenderbotSmall Model outputting raw hidden-states without any specific head on top.\",\n-    BLENDERBOT_SMALL_START_DOCSTRING,\n-)\n+@auto_docstring\n class BlenderbotSmallModel(BlenderbotSmallPreTrainedModel):\n     _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"encoder.embed_tokens.weight\"]\n \n@@ -1065,8 +914,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.decoder\n \n-    @add_start_docstrings_to_model_forward(BLENDERBOT_SMALL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1086,7 +934,26 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            BlenderbotSmall uses the `bos_token_id` as the starting token for `decoder_input_ids` generation. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n \n         Example:\n \n@@ -1160,9 +1027,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\",\n-    BLENDERBOT_SMALL_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\n+    \"\"\"\n )\n class BlenderbotSmallForConditionalGeneration(BlenderbotSmallPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n@@ -1206,9 +1074,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(BLENDERBOT_SMALL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n-    @add_end_docstrings(BLENDERBOT_SMALL_GENERATION_EXAMPLE)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1229,12 +1095,62 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            BlenderbotSmall uses the `bos_token_id` as the starting token for `decoder_input_ids` generation. If\n+            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules in the decoder. Mask values selected in `[0,\n+            1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        Returns:\n+        Example Conversation:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, BlenderbotSmallForConditionalGeneration\n+\n+        >>> mname = \"facebook/blenderbot_small-90M\"\n+        >>> model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n+        >>> tokenizer = AutoTokenizer.from_pretrained(mname)\n+        >>> UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n+        >>> print(\"Human: \", UTTERANCE)\n+        Human:  My friends are cool but they eat too many carbs.\n+\n+        >>> inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n+        >>> reply_ids = model.generate(**inputs)\n+        >>> print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n+        Bot:  what kind of carbs do they eat? i don't know much about carbs.\n+\n+        >>> REPLY = \"I'm not sure\"\n+        >>> print(\"Human: \", REPLY)\n+        Human: I'm not sure\n+\n+        >>> NEXT_UTTERANCE = (\n+        ...     \"My friends are cool but they eat too many carbs.__end__ __start__what kind of carbs do they eat? \"\n+        ...     \"i don't know much about carbs__end__ \"\n+        ...     \"__start__ I'm not sure.\"\n+        ... )\n+        >>> inputs = tokenizer([NEXT_UTTERANCE], return_tensors=\"pt\")\n+        >>> next_reply_ids = model.generate(**inputs)\n+        >>> print(\"Bot: \", tokenizer.batch_decode(next_reply_ids, skip_special_tokens=True)[0])\n+        Bot:  they eat a lot of carbs. carbs are high in fat, protein, and fats.\n+        ```\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1348,7 +1264,7 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model.decoder\n \n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1366,72 +1282,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                if the model is configured as a decoder.\n-            encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used\n-                in the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n-                Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n-\n-                - 1 indicates the head is **not masked**,\n-                - 0 indicates the head is **masked**.\n-\n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n-                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n-                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n-\n-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n-                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n-                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n+            Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n \n-        Returns:\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "2b1d14f139049162590bcaabf923a08b194aaae5",
            "filename": "src/transformers/models/blip/image_processing_blip_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -14,15 +14,12 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for BLIP.\"\"\"\n \n-from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...utils import add_start_docstrings\n+from ...utils import auto_docstring\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast BLIP image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n class BlipImageProcessorFast(BaseImageProcessorFast):\n     # To be checked against the slow image processor\n     # None values left after checking can be removed"
        },
        {
            "sha": "548a362ebfdd872b155a2f2aa58204c6c8d5c3f8",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 26,
            "deletions": 156,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,22 +27,13 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_blip import BlipConfig, BlipTextConfig, BlipVisionConfig\n from .modeling_blip_text import BlipTextLMHeadModel, BlipTextModel\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"Salesforce/blip-vqa-base\"\n-\n \n # Copied from transformers.models.clip.modeling_clip.contrastive_loss\n def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n@@ -462,12 +453,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class BlipPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BlipConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n@@ -504,108 +491,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-BLIP_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BlipConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BLIP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoProcessor`]. See [`BlipProcessor.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-BLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`BlipImageProcessor`]. See [`BlipImageProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-BLIP_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoProcessor`]. See [`BlipProcessor.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`BlipImageProcessor`]. See [`BlipImageProcessor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-\n class BlipEncoder(nn.Module):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -707,8 +592,7 @@ def __init__(self, config: BlipVisionConfig):\n \n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=BlipVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -717,10 +601,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -759,11 +639,10 @@ def get_input_embeddings(self):\n         return self.embeddings\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     This model is going to be deprecated in future versions. Please use `BlipForConditionalGeneration`, `BlipForQuestionAnswering` or `BlipForImageTextRetrieval` depending on your usecase.\n-    \"\"\",\n-    BLIP_START_DOCSTRING,\n+    \"\"\"\n )\n class BlipModel(BlipPreTrainedModel):\n     config_class = BlipConfig\n@@ -810,7 +689,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n-    @add_start_docstrings_to_model_forward(BLIP_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -848,7 +727,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -890,7 +769,7 @@ def get_image_features(\n \n         return image_features\n \n-    @add_start_docstrings_to_model_forward(BLIP_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_multimodal_features(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -945,8 +824,7 @@ def get_multimodal_features(\n \n         return multimodal_features\n \n-    @add_start_docstrings_to_model_forward(BLIP_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BlipOutput, config_class=BlipConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -960,7 +838,8 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, BlipOutput]:\n         r\"\"\"\n-        Returns:\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n \n@@ -1042,14 +921,13 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP Model for image captioning. The model consists of a vision encoder and a text decoder. One can optionally pass\n     `input_ids` to the model, which serve as a text prompt, to make the text decoder continue the prompt. Otherwise,\n     the decoder starts generating text from the [BOS] (beginning-of-sequence) token. will start generating the caption\n     from the text input. If no text input is provided, the decoder will start with the [BOS] token only.\n-    \"\"\",\n-    BLIP_START_DOCSTRING,\n+    \"\"\"\n )\n class BlipForConditionalGeneration(BlipPreTrainedModel, GenerationMixin):\n     config_class = BlipConfig\n@@ -1075,8 +953,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_decoder.set_input_embeddings(value)\n \n-    @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BlipForConditionalGenerationModelOutput, config_class=BlipVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1089,8 +966,6 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, BlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1225,13 +1100,12 @@ def generate(\n         return outputs\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP Model for visual question answering. The model consists of a vision encoder, a text encoder as well as a text\n     decoder. The vision encoder will encode the input image, the text encoder will encode the input question together\n     with the encoding of the image, and the text decoder will output the answer to the question.\n-    \"\"\",\n-    BLIP_START_DOCSTRING,\n+    \"\"\"\n )\n class BlipForQuestionAnswering(BlipPreTrainedModel, GenerationMixin):\n     config_class = BlipConfig\n@@ -1259,8 +1133,7 @@ def get_input_embeddings(self):\n         # This will return shared embeddings if they are shared else specific to encoder.\n         return self.text_encoder.get_input_embeddings()\n \n-    @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BlipTextVisionModelOutput, config_class=BlipVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1275,8 +1148,6 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, BlipTextVisionModelOutput]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1459,13 +1330,12 @@ def generate(\n         return outputs\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP Model with a vision and text projector, and a classification head on top. The model is used in the context of\n     image-text retrieval. Given an image and a text, the model returns the probability of the text being relevant to\n     the image.\n-    \"\"\",\n-    BLIP_START_DOCSTRING,\n+    \"\"\"\n )\n class BlipForImageTextRetrieval(BlipPreTrainedModel):\n     config_class = BlipConfig\n@@ -1506,8 +1376,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_encoder.set_input_embeddings(value)\n \n-    @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BlipTextVisionModelOutput, config_class=BlipVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1520,7 +1389,8 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, BlipTextVisionModelOutput]:\n         r\"\"\"\n-        Returns:\n+        use_itm_head (`bool`, *optional*, defaults to `True`):\n+            Whether or not to use the image-text matching head.\n \n         Examples:\n "
        },
        {
            "sha": "ff5ebb952affd285e623e4251ce2ae0361d2849b",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 83,
            "deletions": 324,
            "changes": 407,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -35,23 +35,13 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    LossKwargs,\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import LossKwargs, ModelOutput, auto_docstring, logging, torch_int\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_blip_2 import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"Salesforce/blip2-opt-2.7b\"\n-\n \n @dataclass\n class Blip2ForConditionalGenerationModelOutput(ModelOutput):\n@@ -431,12 +421,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class Blip2PreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = Blip2Config\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n@@ -483,255 +469,6 @@ def _init_weights(self, module):\n             module.query_tokens.data.zero_()\n \n \n-BLIP_2_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`Blip2Config`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BLIP_2_QFORMER_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`Blip2QFormerConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BLIP_2_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for\n-            details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-BLIP_2_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it. Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-            [What are attention masks?](../glossary#attention-mask)\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n-            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n-\n-            To know more on how to prepare `decoder_input_ids` for pretraining take a look at [T5\n-            Training](./t5#training).\n-        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-BLIP_2_TEXT_WITH_PROJECTION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it. Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-BLIP_2_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for\n-            details.\n-\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n-            encoder-decoder language model (like T5) is used.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            Only relevant in case an encoder-decoder language model (like T5) is used.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\"\"\"\n-\n-BLIP2_IMAGE_TEXT_RETRIEVAL_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for\n-            details.\n-\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        use_image_text_matching_head (`bool`, *optional*):\n-            Whether to return the Image-Text Matching or Contrastive scores.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-BLIP2_QFORMER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n-            Hidden states to be used in the attention computation. If cross-attention,\n-            will be used for the query (i.e., key and value will use the encoder_hidden_states).\n-\n-        query_length (`int`, *optional*):\n-            Length of the query, usually based on the number of query tokens.\n-            If no value is provided, query_length will be inferred by the query_embeds.\n-\n-        attention_mask (`torch.FloatTensor`, *optional*):\n-            Attention mask of size `(batch, sequence_length)` where padding elements\n-            are indicated by 0.\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, `optional`):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, `optional`):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n-            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n-            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n-            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n-            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n-            `(batch_size, sequence_length)`.\n-\n-        use_cache (`bool`, `optional`):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Copied from transformers.models.blip.modeling_blip.BlipEncoder with Blip->Blip2\n class Blip2Encoder(nn.Module):\n     \"\"\"\n@@ -819,6 +556,7 @@ def forward(\n         )\n \n \n+@auto_docstring\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->Blip2, BLIP->BLIP_2\n class Blip2VisionModel(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n@@ -835,8 +573,7 @@ def __init__(self, config: Blip2VisionConfig):\n \n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Blip2VisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -845,10 +582,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1359,11 +1092,10 @@ def forward(\n         return embeddings\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP-2 Querying Transformer (Q-Former).\n-    \"\"\",\n-    BLIP_2_QFORMER_START_DOCSTRING,\n+    \"\"\"\n )\n class Blip2QFormerModel(Blip2PreTrainedModel):\n     _supports_attention_backend = False  # adds position on attn weights before last matmul\n@@ -1441,10 +1173,7 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n-    @add_start_docstrings_to_model_forward(BLIP2_QFORMER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions, config_class=Blip2QFormerConfig\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         query_embeds: torch.FloatTensor,\n@@ -1460,7 +1189,12 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n-        Returns:\n+        query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n+            Hidden states to be used in the attention computation. If cross-attention,\n+            will be used for the query (i.e., key and value will use the encoder_hidden_states).\n+        query_length (`int`, *optional*):\n+            Length of the query, usually based on the number of query tokens.\n+            If no value is provided, query_length will be inferred by the query_embeds.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1553,12 +1287,11 @@ def forward(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP-2 Model for generating text and image features. The model consists of a vision encoder, Querying Transformer\n     (Q-Former) and a language model.\n-    \"\"\",\n-    BLIP_2_START_DOCSTRING,\n+    \"\"\"\n )\n class Blip2Model(Blip2PreTrainedModel):\n     config_class = Blip2Config\n@@ -1611,7 +1344,7 @@ def _tie_weights(self):\n             self.language_model.encoder.embed_tokens = self.language_model.shared\n             self.language_model.decoder.embed_tokens = self.language_model.shared\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1625,6 +1358,23 @@ def get_text_features(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ):\n         r\"\"\"\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+            T5 uses the `pad_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n+            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n+\n+            To know more on how to prepare `decoder_input_ids` for pretraining take a look at [T5\n+            Training](./t5#training).\n+        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n         Returns:\n             text_outputs (`CausalLMOutputWithPast`, or `tuple(torch.FloatTensor)` if `return_dict=False`):\n                 The language model outputs. If `return_dict=True`, the output is a [`CausalLMOutputWithPast`] that\n@@ -1673,7 +1423,7 @@ def get_text_features(\n \n         return text_outputs\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1719,7 +1469,7 @@ def get_image_features(\n \n         return vision_outputs\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_qformer_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1780,8 +1530,7 @@ def get_qformer_features(\n \n         return query_outputs\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Blip2ForConditionalGenerationModelOutput, config_class=Blip2VisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1797,7 +1546,18 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n-        Returns:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            Only relevant in case an encoder-decoder language model (like T5) is used.\n \n         Examples:\n \n@@ -1917,12 +1677,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BLIP-2 Text Model with a projection layer on top (a linear layer on top of the pooled output).\n-    \"\"\",\n-    BLIP_2_START_DOCSTRING,\n-)\n+@auto_docstring\n class Blip2TextModelWithProjection(Blip2PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n@@ -1946,8 +1701,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_TEXT_WITH_PROJECTION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Blip2TextModelOutput, config_class=Blip2Config)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1958,8 +1712,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Blip2TextModelOutput]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -2016,12 +1768,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    BLIP-2 Vision Model with a projection layer on top (a linear layer on top of the pooled output).\n-    \"\"\",\n-    BLIP_2_START_DOCSTRING,\n-)\n+@auto_docstring\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n@@ -2043,8 +1790,7 @@ def __init__(self, config: Blip2Config):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Blip2VisionModelOutput, config_class=Blip2Config)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -2053,8 +1799,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Blip2VisionModelOutput]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -2123,8 +1867,8 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP-2 Model for generating text given an image and an optional text prompt. The model consists of a vision\n     encoder, Querying Transformer (Q-Former) and a language model.\n \n@@ -2136,8 +1880,7 @@ def forward(\n     Note that Flan-T5 checkpoints cannot be cast to float16. They are pre-trained using bfloat16.\n \n     </Tip>\n-    \"\"\",\n-    BLIP_2_START_DOCSTRING,\n+    \"\"\"\n )\n class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     config_class = Blip2Config\n@@ -2213,8 +1956,7 @@ def _preprocess_accelerate(self):\n         if hasattr(self.language_model, \"_hf_hook\"):\n             self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n \n-    @add_start_docstrings_to_model_forward(BLIP_2_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Blip2ForConditionalGenerationModelOutput, config_class=Blip2VisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -2231,7 +1973,18 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n-        Returns:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            Only relevant in case an encoder-decoder language model (like T5) is used.\n \n         Examples:\n \n@@ -2499,13 +2252,12 @@ def generate(\n         return outputs\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BLIP-2 Model with a vision and text projector, and a classification head on top. The model is used in the context\n     of image-text retrieval. Given an image and a text, the model returns the probability of the text being relevant to\n     the image.\n-    \"\"\",\n-    BLIP_2_START_DOCSTRING,\n+    \"\"\"\n )\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n@@ -2539,8 +2291,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    @add_start_docstrings_to_model_forward(BLIP2_IMAGE_TEXT_RETRIEVAL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Blip2ImageTextMatchingModelOutput, config_class=Blip2Config)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -2552,7 +2303,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Blip2ImageTextMatchingModelOutput]:\n         r\"\"\"\n-        Returns:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        use_image_text_matching_head (`bool`, *optional*):\n+            Whether to return the Image-Text Matching or Contrastive scores.\n \n         Examples:\n "
        },
        {
            "sha": "43f9ee6c20d357d7acfd3f00c06b9dbb5012b3db",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 73,
            "deletions": 147,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -25,7 +25,6 @@\n from torch.nn import functional as F\n \n from ...cache_utils import Cache, DynamicCache, StaticCache\n-from ...file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -37,6 +36,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n+    auto_docstring,\n     is_torch_flex_attn_available,\n     logging,\n )\n@@ -51,9 +51,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"bigscience/bloom-560m\"\n-_CONFIG_FOR_DOC = \"BloomConfig\"\n-\n \n def build_alibi_tensor(attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype) -> torch.Tensor:\n     \"\"\"\n@@ -442,6 +439,7 @@ def forward(\n         return outputs  # hidden_states, past_kv, attentions\n \n \n+@auto_docstring\n class BloomPreTrainedModel(PreTrainedModel):\n     config_class = BloomConfig\n     base_model_prefix = \"transformer\"\n@@ -472,94 +470,7 @@ def _init_weights(self, module: nn.Module):\n             module.weight.data.fill_(1.0)\n \n \n-BLOOM_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BloomConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BLOOM_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-\n-            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n-            `past_key_values`).\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Bloom Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BLOOM_START_DOCSTRING,\n-)\n+@auto_docstring\n class BloomModel(BloomPreTrainedModel):\n     def __init__(self, config: BloomConfig):\n         super().__init__(config)\n@@ -591,12 +502,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings: torch.Tensor):\n         self.word_embeddings = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(BLOOM_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPastAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -611,6 +517,19 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **deprecated_arguments,\n     ) -> Union[Tuple[torch.Tensor, ...], BaseModelOutputWithPastAndCrossAttentions]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        \"\"\"\n         if deprecated_arguments.pop(\"position_ids\", False) is not False:\n             # `position_ids` could have been `torch.Tensor` or `None` so defaulting pop to `False` allows to detect if users were passing explicitly `None`\n             warnings.warn(\n@@ -865,12 +784,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input\n     embeddings).\n-    \"\"\",\n-    BLOOM_START_DOCSTRING,\n+    \"\"\"\n )\n class BloomForCausalLM(BloomPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -951,12 +869,7 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n-    @add_start_docstrings_to_model_forward(BLOOM_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutputWithCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -973,6 +886,17 @@ def forward(\n         **deprecated_arguments,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n@@ -1056,8 +980,8 @@ def _reorder_cache(\n         return reordered_past\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The Bloom Model transformer with a sequence classification head on top (linear layer).\n \n     [`BloomForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n@@ -1068,8 +992,7 @@ def _reorder_cache(\n     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n     each row of the batch).\n-    \"\"\",\n-    BLOOM_START_DOCSTRING,\n+    \"\"\"\n )\n class BloomForSequenceClassification(BloomPreTrainedModel):\n     def __init__(self, config: BloomConfig):\n@@ -1081,12 +1004,7 @@ def __init__(self, config: BloomConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BLOOM_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1102,6 +1020,17 @@ def forward(\n         **deprecated_arguments,\n     ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutputWithPast]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1192,13 +1121,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Bloom Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    BLOOM_START_DOCSTRING,\n-)\n+@auto_docstring\n class BloomForTokenClassification(BloomPreTrainedModel):\n     def __init__(self, config: BloomConfig):\n         super().__init__(config)\n@@ -1217,12 +1140,7 @@ def __init__(self, config: BloomConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BLOOM_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1238,6 +1156,17 @@ def forward(\n         **deprecated_arguments,\n     ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1293,13 +1222,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    The BLOOM Model transformer with a span classification head on top for extractive question-answering tasks like\n-    SQuAD (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    BLOOM_START_DOCSTRING,\n-)\n+@auto_docstring\n class BloomForQuestionAnswering(BloomPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1309,7 +1232,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BLOOM_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1324,14 +1247,17 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n         r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0][0].shape[2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only `input_ids` that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n "
        },
        {
            "sha": "68d5b2c0f899a32186d74990372b268a7bdd836c",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 23,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -17,8 +17,6 @@\n from typing import Dict, Iterable, Optional, Tuple, Union\n \n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n     BatchFeature,\n     DefaultFastImageProcessorKwargs,\n@@ -31,7 +29,7 @@\n     reorder_images,\n )\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...utils import add_start_docstrings, is_torch_available, is_torchvision_available, is_torchvision_v2_available\n+from ...utils import auto_docstring, is_torch_available, is_torchvision_available, is_torchvision_v2_available\n \n \n if is_torch_available():\n@@ -95,22 +93,21 @@ def get_resize_output_image_size(\n \n \n class BridgeTowerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    size_divisor: Optional[int]\n-    do_pad: Optional[bool]\n-\n-\n-@add_start_docstrings(\n-    \"Constructs a fast BridgeTower image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     \"\"\"\n+    Args:\n         size_divisor (`int`, *optional*, defaults to 32):\n             The size by which to make sure both the height and width can be divided. Only has an effect if `do_resize`\n             is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess` method.\n         do_pad (`bool`, *optional*, defaults to `True`):\n             Whether to pad the image to the `(max_height, max_width)` of the images in the batch. Can be overridden by\n             the `do_pad` parameter in the `preprocess` method.\n-    \"\"\",\n-)\n+    \"\"\"\n+\n+    size_divisor: Optional[int]\n+    do_pad: Optional[bool]\n+\n+\n+@auto_docstring\n class BridgeTowerImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -129,17 +126,7 @@ class BridgeTowerImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    @add_start_docstrings(\n-        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n-        \"\"\"\n-            size_divisor (`int`, *optional*, defaults to 32):\n-                The size by which to make sure both the height and width can be divided. Only has an effect if `do_resize`\n-                is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess` method.\n-            do_pad (`bool`, *optional*, defaults to `True`):\n-                Whether to pad the image to the `(max_height, max_width)` of the images in the batch. Can be overridden by\n-                the `do_pad` parameter in the `preprocess` method.\n-        \"\"\",\n-    )\n+    @auto_docstring\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n "
        },
        {
            "sha": "e9ba3f272ce3d757c73e2761b2e13ec79998d240",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 42,
            "deletions": 140,
            "changes": 182,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -34,96 +34,15 @@\n )\n from ...modeling_utils import PreTrainedModel, apply_chunking_to_forward\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import auto_docstring, logging, torch_int\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"BridgeTowerConfig\"\n-_CHECKPOINT_FOR_DOC = \"BridgeTower/bridgetower-base\"\n _TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n \n \n-BRIDGETOWER_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`_ subclass. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`BridgeTowerConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BRIDGETOWER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See\n-            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details. [What are input\n-            IDs?](../glossary#input-ids)\n-\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-            [What are token type IDs?](../glossary#token-type-ids)\n-\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`BridgeTowerImageProcessor`]. See\n-            [`BridgeTowerImageProcessor.__call__`] for details.\n-\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-            `What are attention masks? <../glossary.html#attention-mask>`__\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n-            Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n-            This is useful if you want more control over how to convert `pixel_values` into patch embeddings.\n-\n-        image_token_type_idx (`int`, *optional*):\n-            - The token type ids for images.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n @dataclass\n class BridgeTowerModelOutput(ModelOutput):\n     \"\"\"\n@@ -1030,12 +949,8 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n     return incremental_indices.long() + padding_idx\n \n \n+@auto_docstring\n class BridgeTowerPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BridgeTowerConfig\n     base_model_prefix = \"bridgetower\"\n     supports_gradient_checkpointing = False\n@@ -1084,9 +999,8 @@ def forward(self, image, image_mask=None, interpolate_pos_encoding=False):\n         return self.visual(image.type(self.dtype), image_mask, interpolate_pos_encoding)\n \n \n-class BridgeTowerTextModel(BridgeTowerPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n     cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n     all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n@@ -1097,12 +1011,16 @@ class BridgeTowerTextModel(BridgeTowerPreTrainedModel):\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n \n     .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n-\n     \"\"\"\n-\n+)\n+class BridgeTowerTextModel(BridgeTowerPreTrainedModel):\n     config_class = BridgeTowerTextConfig\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -1128,6 +1046,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @auto_docstring\n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n@@ -1145,26 +1064,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1260,10 +1159,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare BridgeTower Model transformer outputting BridgeTowerModelOutput object without any specific head on\"\n-    \" top.\",\n-    BRIDGETOWER_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare BridgeTower Model transformer outputting BridgeTowerModelOutput object without any specific head on\n+    \"\"\"\n )\n class BridgeTowerModel(BridgeTowerPreTrainedModel):\n     def __init__(self, config):\n@@ -1328,8 +1227,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_model.set_input_embeddings(value)\n \n-    @add_start_docstrings_to_model_forward(BRIDGETOWER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BridgeTowerModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1348,6 +1246,11 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> Union[Tuple[torch.Tensor], BridgeTowerModelOutput]:\n         r\"\"\"\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n+            Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n+            This is useful if you want more control over how to convert `pixel_values` into patch embeddings.\n+        image_token_type_idx (`int`, *optional*):\n+            - The token type ids for images.\n         output_hidden_states (`bool`, *optional*):\n             If set to `True`, hidden states are returned as a list containing the hidden states of text, image, and\n             cross-modal components respectively. i.e. `(hidden_states_text, hidden_states_image,\n@@ -1357,7 +1260,6 @@ def forward(\n             `cross_modal_image_hidden_states` of each brdige layer.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels are currently not supported.\n-        Returns:\n \n         Examples:\n \n@@ -1612,11 +1514,10 @@ def forward(self, x):\n         return itm_score\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BridgeTower Model with a language modeling head on top as done during pretraining.\n-    \"\"\",\n-    BRIDGETOWER_START_DOCSTRING,\n+    \"\"\"\n )\n class BridgeTowerForMaskedLM(BridgeTowerPreTrainedModel):\n     _tied_weights_keys = [\"mlm_score.decoder.weight\"]\n@@ -1636,8 +1537,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.mlm_score.decoder = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(BRIDGETOWER_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1654,11 +1554,13 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n     ) -> Union[MaskedLMOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n+            Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n+            This is useful if you want more control over how to convert `pixel_values` into patch embeddings.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        Returns:\n \n         Examples:\n \n@@ -1720,12 +1622,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BridgeTower Model transformer with a classifier head on top (a linear layer on top of the final hidden state of the\n     [CLS] token) for image-to-text matching.\n-    \"\"\",\n-    BRIDGETOWER_START_DOCSTRING,\n+    \"\"\"\n )\n class BridgeTowerForImageAndTextRetrieval(BridgeTowerPreTrainedModel):\n     def __init__(self, config):\n@@ -1738,8 +1639,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BRIDGETOWER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1756,10 +1656,12 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n     ) -> Union[SequenceClassifierOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n+            Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n+            This is useful if you want more control over how to convert `pixel_values` into patch embeddings.\n         labels (`torch.LongTensor` of shape `(batch_size, 1)`, *optional*):\n             Labels for computing the image-text matching loss. 0 means the pairs don't match and 1 means they match.\n             The pairs with 0 will be skipped for calculation.\n-        Returns:\n \n         Examples:\n \n@@ -1832,11 +1734,10 @@ def forward(self, x):\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     BridgeTower Model with a image-text contrastive head on top computing image-text contrastive loss.\n-    \"\"\",\n-    BRIDGETOWER_START_DOCSTRING,\n+    \"\"\"\n )\n class BridgeTowerForContrastiveLearning(BridgeTowerPreTrainedModel):\n     def __init__(self, config):\n@@ -1852,8 +1753,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(BRIDGETOWER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BridgeTowerContrastiveOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1870,9 +1770,11 @@ def forward(\n         return_loss: Optional[bool] = None,\n     ) -> Union[BridgeTowerContrastiveOutput, Tuple[torch.FloatTensor]]:\n         r\"\"\"\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`, *optional*):\n+            Optionally, instead of passing `pixel_values`, you can choose to directly pass an embedded representation.\n+            This is useful if you want more control over how to convert `pixel_values` into patch embeddings.\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss.\n-        Returns:\n \n         Examples:\n "
        },
        {
            "sha": "100337d96b754680255dca2e94948d298f66cb57",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 52,
            "deletions": 125,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -31,100 +31,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_bros import BrosConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"jinho8345/bros-base-uncased\"\n-_CONFIG_FOR_DOC = \"BrosConfig\"\n-\n-\n-BROS_START_DOCSTRING = r\"\"\"\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`BrosConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-BROS_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`BrosProcessor`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n-            Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n-            (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner of the\n-            bounding box.\n-\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        bbox_first_token_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to indicate the first token of each bounding box. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n \n @dataclass\n class BrosSpadeOutput(ModelOutput):\n@@ -749,12 +661,8 @@ def forward(self, query_layer: torch.Tensor, key_layer: torch.Tensor):\n         return relation_score\n \n \n+@auto_docstring\n class BrosPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = BrosConfig\n     base_model_prefix = \"bros\"\n \n@@ -775,12 +683,13 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-@add_start_docstrings(\n-    \"The bare Bros Model transformer outputting raw hidden-states without any specific head on top.\",\n-    BROS_START_DOCSTRING,\n-)\n+@auto_docstring\n class BrosModel(BrosPreTrainedModel):\n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -806,8 +715,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(BROS_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPoolingAndCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -826,7 +734,10 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n-        Returns:\n+        bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n+            Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n+            (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner of the\n+            bounding box.\n \n         Examples:\n \n@@ -950,13 +861,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Bros Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    BROS_START_DOCSTRING,\n-)\n+@auto_docstring\n class BrosForTokenClassification(BrosPreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n \n@@ -973,8 +878,7 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    @add_start_docstrings_to_model_forward(BROS_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=TokenClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -991,8 +895,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n+        bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n+            Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n+            (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner of the\n+            bounding box.\n+        bbox_first_token_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to indicate the first token of each bounding box. Mask values selected in `[0, 1]`:\n \n-        Returns:\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n \n         Examples:\n \n@@ -1054,15 +965,14 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Bros Model with a token classification head on top (initial_token_layers and subsequent_token_layer on top of the\n     hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks. The initial_token_classifier is used to\n     predict the first token of each entity, and the subsequent_token_classifier is used to predict the subsequent\n     tokens within an entity. Compared to BrosForTokenClassification, this model is more robust to serialization errors\n     since it predicts next token from one token.\n-    \"\"\",\n-    BROS_START_DOCSTRING,\n+    \"\"\"\n )\n class BrosSpadeEEForTokenClassification(BrosPreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n@@ -1092,8 +1002,7 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    @add_start_docstrings_to_model_forward(BROS_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=BrosSpadeOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1111,7 +1020,19 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BrosSpadeOutput]:\n         r\"\"\"\n-        Returns:\n+        bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n+            Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n+            (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner of the\n+            bounding box.\n+        bbox_first_token_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to indicate the first token of each bounding box. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        initial_token_labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for the initial token classification.\n+        subsequent_token_labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for the subsequent token classification.\n \n         Examples:\n \n@@ -1200,12 +1121,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Bros Model with a token classification head on top (a entity_linker layer on top of the hidden-states output) e.g.\n     for Entity-Linking. The entity_linker is used to predict intra-entity links (one entity to another entity).\n-    \"\"\",\n-    BROS_START_DOCSTRING,\n+    \"\"\"\n )\n class BrosSpadeELForTokenClassification(BrosPreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n@@ -1224,8 +1144,7 @@ def __init__(self, config):\n \n         self.init_weights()\n \n-    @add_start_docstrings_to_model_forward(BROS_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=TokenClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1242,7 +1161,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n-        Returns:\n+        bbox ('torch.FloatTensor' of shape '(batch_size, num_boxes, 4)'):\n+            Bounding box coordinates for each token in the input sequence. Each bounding box is a list of four values\n+            (x1, y1, x2, y2), where (x1, y1) is the top left corner, and (x2, y2) is the bottom right corner of the\n+            bounding box.\n+        bbox_first_token_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to indicate the first token of each bounding box. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n \n         Examples:\n "
        },
        {
            "sha": "733f31307892bb4a02c74949f07f0430061033ee",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 94,
            "deletions": 220,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -26,10 +26,7 @@\n \n from ...activations import ACT2FN, gelu\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -42,39 +39,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    get_torch_version,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, get_torch_version, logging\n from .configuration_camembert import CamembertConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"almanach/camembert-base\"\n-_CONFIG_FOR_DOC = \"CamembertConfig\"\n-\n-\n-CAMEMBERT_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`CamembertConfig`]): Model configuration class with all the parameters of the\n-            model. Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->Camembert\n class CamembertEmbeddings(nn.Module):\n@@ -704,12 +674,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+@auto_docstring\n class CamembertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CamembertConfig\n     base_model_prefix = \"roberta\"\n     supports_gradient_checkpointing = True\n@@ -735,56 +701,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-CAMEMBERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead with Roberta->Camembert\n class CamembertClassificationHead(nn.Module):\n     \"\"\"Head for sentence-level classification tasks.\"\"\"\n@@ -840,10 +756,7 @@ def _tie_weights(self):\n             self.bias = self.decoder.bias\n \n \n-@add_start_docstrings(\n-    \"The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\",\n-    CAMEMBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class CamembertModel(CamembertPreTrainedModel):\n     \"\"\"\n \n@@ -864,6 +777,10 @@ class CamembertModel(CamembertPreTrainedModel):\n \n     # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.__init__ with Roberta->Camembert\n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -892,12 +809,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n     def forward(\n         self,\n@@ -915,26 +827,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1062,10 +954,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"CamemBERT Model with a `language modeling` head on top.\"\"\",\n-    CAMEMBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForMaskedLM(CamembertPreTrainedModel):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n@@ -1091,15 +980,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        mask=\"<mask>\",\n-        expected_output=\"' Paris'\",\n-        expected_loss=0.1,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1116,12 +997,19 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        kwargs (`Dict[str, any]`, *optional*, defaults to `{}`):\n-            Used to hide legacy arguments that have been deprecated.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1160,12 +1048,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n-    \"\"\",\n-    CAMEMBERT_START_DOCSTRING,\n+    \"\"\"\n )\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForSequenceClassification(CamembertPreTrainedModel):\n@@ -1180,14 +1067,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"cardiffnlp/twitter-roberta-base-emotion\",\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"'optimism'\",\n-        expected_loss=0.08,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1202,6 +1082,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1260,13 +1149,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CamemBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    CAMEMBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForMultipleChoice(CamembertPreTrainedModel):\n     def __init__(self, config):\n@@ -1279,14 +1162,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(\n-        CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\")\n-    )\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1301,10 +1177,35 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n             `input_ids` above)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n@@ -1355,13 +1256,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.\n-    for Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    CAMEMBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForTokenClassification(CamembertPreTrainedModel):\n     def __init__(self, config):\n@@ -1378,14 +1273,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"Jean-Baptiste/roberta-large-ner-english\",\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"['O', 'ORG', 'ORG', 'O', 'O', 'O', 'O', 'O', 'LOC', 'O', 'LOC', 'LOC']\",\n-        expected_loss=0.01,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1400,6 +1288,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n@@ -1441,13 +1338,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CamemBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`\n-    \"\"\",\n-    CAMEMBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForQuestionAnswering(CamembertPreTrainedModel):\n     def __init__(self, config):\n@@ -1460,14 +1351,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"deepset/roberta-base-squad2\",\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"' puppet'\",\n-        expected_loss=0.86,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1483,14 +1367,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1543,8 +1428,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"CamemBERT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", CAMEMBERT_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    CamemBERT Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n )\n # Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM with Roberta->Camembert, ROBERTA->CAMEMBERT, FacebookAI/roberta-base->almanach/camembert-base\n class CamembertForCausalLM(CamembertPreTrainedModel, GenerationMixin):\n@@ -1568,8 +1455,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(CAMEMBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1589,31 +1475,19 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n \n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n \n+            [What are token type IDs?](../glossary#token-type-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\n-        Returns:\n \n         Example:\n "
        },
        {
            "sha": "ae0bed5bd316d5b43abb669ac59753a703d4c1d6",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 42,
            "deletions": 145,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,21 +36,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from .configuration_canine import CanineConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google/canine-s\"\n-_CONFIG_FOR_DOC = \"CanineConfig\"\n-\n \n # Support up to 16 hash functions.\n _PRIMES = [31, 43, 59, 61, 73, 97, 103, 113, 137, 149, 157, 173, 181, 193, 211, 223]\n@@ -880,12 +871,8 @@ def forward(\n         return prediction_scores\n \n \n+@auto_docstring\n class CaninePreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CanineConfig\n     load_tf_weights = load_tf_weights_in_canine\n     base_model_prefix = \"canine\"\n@@ -908,73 +895,13 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-CANINE_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`CanineConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CANINE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare CANINE Model transformer outputting raw hidden-states without any specific head on top.\",\n-    CANINE_START_DOCSTRING,\n-)\n+@auto_docstring\n class CanineModel(CaninePreTrainedModel):\n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n         shallow_config = copy.deepcopy(config)\n@@ -1081,12 +1008,7 @@ def _repeat_molecules(self, molecules: torch.Tensor, char_seq_length: int) -> to\n         # `repeated`: [batch_size, char_seq_len, molecule_hidden_size]\n         return torch.cat([repeated, remainder_repeated], dim=-2)\n \n-    @add_start_docstrings_to_model_forward(CANINE_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CanineModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1246,12 +1168,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     CANINE Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled\n     output) e.g. for GLUE tasks.\n-    \"\"\",\n-    CANINE_START_DOCSTRING,\n+    \"\"\"\n )\n class CanineForSequenceClassification(CaninePreTrainedModel):\n     def __init__(self, config):\n@@ -1265,12 +1186,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CANINE_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1343,13 +1259,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CANINE Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    CANINE_START_DOCSTRING,\n-)\n+@auto_docstring\n class CanineForMultipleChoice(CaninePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1361,12 +1271,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CANINE_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1381,6 +1286,30 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n@@ -1434,13 +1363,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CANINE Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    CANINE_START_DOCSTRING,\n-)\n+@auto_docstring\n class CanineForTokenClassification(CaninePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1453,8 +1376,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CANINE_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=TokenClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1472,8 +1394,6 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -1540,13 +1460,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CANINE Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    CANINE_START_DOCSTRING,\n-)\n+@auto_docstring\n class CanineForQuestionAnswering(CaninePreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1558,14 +1472,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CANINE_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=\"Splend1dchan/canine-c-squad\",\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=\"'nice puppet'\",\n-        expected_loss=8.81,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1580,16 +1487,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.canine("
        },
        {
            "sha": "624450371bd5febc377f6a5506283bedd7ebea90",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 18,
            "deletions": 154,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,23 +27,17 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     LossKwargs,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     can_return_tuple,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n-    replace_return_docstrings,\n )\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n \n@@ -56,12 +50,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"ChameleonConfig\"\n-_CHECKPOINT_FOR_DOC = \"meta/chameleon-7b\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 7, 4096]\n-_SEQ_CLASS_EXPECTED_LOSS = 1.03\n-_SEQ_CLASS_EXPECTED_OUTPUT = \"'LABEL_0'\"\n-\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Chameleon\n class ChameleonRMSNorm(nn.Module):\n@@ -829,27 +817,7 @@ def convert_img2bpe(self, img_batch: torch.Tensor) -> torch.Tensor:\n         return img_tokens.to(device)\n \n \n-CHAMELEON_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ChameleonConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare chameleon Model outputting raw hidden-states without any specific head on top.\",\n-    CHAMELEON_START_DOCSTRING,\n-)\n+@auto_docstring\n class ChameleonPreTrainedModel(PreTrainedModel):\n     config_class = ChameleonConfig\n     base_model_prefix = \"model\"\n@@ -882,29 +850,12 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-CHAMELEON_VQ_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ChameleonVQVAEConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"The VQ-VAE model used in Chameleon for encoding/decoding images into discrete tokens.\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The VQ-VAE model used in Chameleon for encoding/decoding images into discrete tokens.\n     This model follows the \"Make-a-scene: Scene-based text-to-image generation with human priors\" paper from\n     [ Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman](https://arxiv.org/abs/2203.13131).\n-    \"\"\",\n-    CHAMELEON_VQ_START_DOCSTRING,\n+    \"\"\"\n )\n class ChameleonVQVAE(ChameleonPreTrainedModel):\n     config_class = ChameleonVQVAEConfig\n@@ -926,87 +877,8 @@ def encode(self, pixel_values: torch.LongTensor):\n         return quant, emb_loss, indices\n \n \n-CHAMELEON_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`ChameleonImageProcessor.__call__`] for details.\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Should always be a [`~cache_utils.Cache`] instance and the model will output the same cache instance.\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare chameleon Model outputting raw hidden-states without any specific head on top.\",\n-    CHAMELEON_START_DOCSTRING,\n-)\n+@auto_docstring\n class ChameleonModel(ChameleonPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`ChameleonDecoderLayer`]\n-\n-    Args:\n-        config: ChameleonConfig\n-    \"\"\"\n-\n     def __init__(self, config: ChameleonConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1047,13 +919,7 @@ def get_image_tokens(self, pixel_values: torch.FloatTensor):\n         bpe_toks = bpe_toks.view(batch_size, -1)\n         return bpe_toks\n \n-    @add_start_docstrings_to_model_forward(CHAMELEON_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1314,9 +1180,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-@add_start_docstrings(\n-    \"Chameleon Model with a head on top used for outputting logits for next token prediction.\",\n-    CHAMELEON_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Chameleon Model with a head on top used for outputting logits for next token prediction.\n+    \"\"\"\n )\n class ChameleonForConditionalGeneration(ChameleonPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -1349,8 +1216,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CHAMELEON_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1368,12 +1234,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "6ced07e0c29e523ce3be8775d16a8e1858887604",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -14,15 +14,12 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for Chinese-CLIP.\"\"\"\n \n-from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...utils import add_start_docstrings\n+from ...utils import auto_docstring\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast ChineseCLIP image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n class ChineseCLIPImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN"
        },
        {
            "sha": "bc1421e7157a39e6b8ff2dd469b7fbdde290a1e6",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 23,
            "deletions": 189,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -31,23 +31,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_chinese_clip import ChineseCLIPConfig, ChineseCLIPTextConfig, ChineseCLIPVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"OFA-Sys/chinese-clip-vit-base-patch16\"\n-_CONFIG_FOR_DOC = \"ChineseCLIPConfig\"\n-\n \n # https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n # Copied from transformers.models.clip.modeling_clip.contrastive_loss\n@@ -733,12 +722,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+@auto_docstring\n class ChineseCLIPPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ChineseCLIPConfig\n     base_model_prefix = \"chinese_clip\"\n     supports_gradient_checkpointing = True\n@@ -791,131 +776,6 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n \n-CHINESE_CLIP_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`ChineseCLIPConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CHINESE_CLIP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CHINESE_CLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`ChineseCLIPImageProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CHINESE_CLIP_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`ChineseCLIPImageProcessor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->ChineseCLIPText\n class ChineseCLIPTextEncoder(nn.Module):\n     def __init__(self, config):\n@@ -1098,8 +958,7 @@ def __init__(self, config: ChineseCLIPVisionConfig):\n         self.encoder = ChineseCLIPVisionEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-    @add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1108,9 +967,6 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1145,9 +1001,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The text model from CHINESE_CLIP without any head or projection on top.\",\n-    CHINESE_CLIP_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The text model from CHINESE_CLIP without any head or projection on top.\n+    \"\"\"\n )\n class ChineseCLIPTextModel(ChineseCLIPPreTrainedModel):\n     \"\"\"\n@@ -1166,6 +1023,10 @@ class ChineseCLIPTextModel(ChineseCLIPPreTrainedModel):\n     _no_split_modules = [\"ChineseCLIPTextEmbeddings\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -1191,12 +1052,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(CHINESE_CLIP_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1213,26 +1069,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1328,9 +1164,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"The vision model from CHINESE_CLIP without any head or projection on top.\"\"\",\n-    CHINESE_CLIP_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The vision model from CHINESE_CLIP without any head or projection on top.\n+    \"\"\"\n )\n class ChineseCLIPVisionModel(ChineseCLIPPreTrainedModel):\n     config_class = ChineseCLIPVisionConfig\n@@ -1346,8 +1183,7 @@ def __init__(self, config: ChineseCLIPVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ChineseCLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1357,8 +1193,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1389,7 +1223,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(CHINESE_CLIP_START_DOCSTRING)\n+@auto_docstring\n class ChineseCLIPModel(ChineseCLIPPreTrainedModel):\n     config_class = ChineseCLIPConfig\n \n@@ -1425,7 +1259,7 @@ def __init__(self, config: ChineseCLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CHINESE_CLIP_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1475,7 +1309,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(CHINESE_CLIP_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1527,8 +1361,7 @@ def get_image_features(\n \n         return image_features\n \n-    @add_start_docstrings_to_model_forward(CHINESE_CLIP_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ChineseCLIPOutput, config_class=ChineseCLIPConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1543,7 +1376,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, ChineseCLIPOutput]:\n         r\"\"\"\n-        Returns:\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "a08849071701c32886d82ab6bc29138310db416b",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 47,
            "deletions": 171,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -31,21 +31,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_clap import ClapAudioConfig, ClapConfig, ClapTextConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"laion/clap-htsat-fused\"\n-\n \n # Adapted from: https://github.com/LAION-AI/CLAP/blob/6ad05a971ba0622f6acee8c41993e0d02bbed639/src/open_clip/utils.py#L191\n def interpolate(hidden_states, ratio):\n@@ -1017,109 +1008,6 @@ def forward(\n         )\n \n \n-CLAP_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ClapConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CLAP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CLAP_AUDIO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n-        is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n-            Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n-            the features.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CLAP_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class ClapProjectionLayer(nn.Module):\n     def __init__(self, config: Union[ClapAudioConfig, ClapTextConfig]):\n         super().__init__()\n@@ -1663,12 +1551,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+@auto_docstring\n class ClapPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ClapConfig\n     base_model_prefix = \"clap\"\n     supports_gradient_checkpointing = False\n@@ -1709,8 +1593,7 @@ def __init__(self, config: ClapAudioConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.audio_encoder.patch_embed.proj\n \n-    @add_start_docstrings_to_model_forward(CLAP_AUDIO_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=ClapAudioConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_features: Optional[torch.FloatTensor] = None,\n@@ -1720,7 +1603,12 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        Returns:\n+        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n+            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n+        is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n+            Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n+            the features.\n \n         Examples:\n \n@@ -1754,9 +1642,8 @@ def forward(\n         )\n \n \n-class ClapTextModel(ClapPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n     cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n     all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n@@ -1767,12 +1654,16 @@ class ClapTextModel(ClapPreTrainedModel):\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n \n     .. _*Attention is all you need*: https://arxiv.org/abs/1706.03762\n-\n     \"\"\"\n-\n+)\n+class ClapTextModel(ClapPreTrainedModel):\n     config_class = ClapTextConfig\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -1790,6 +1681,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1806,26 +1698,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1921,7 +1793,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(CLAP_START_DOCSTRING)\n+@auto_docstring\n class ClapModel(ClapPreTrainedModel):\n     config_class = ClapConfig\n \n@@ -1957,7 +1829,7 @@ def __init__(self, config: ClapConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CLAP_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -2005,7 +1877,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(CLAP_AUDIO_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_audio_features(\n         self,\n         input_features: Optional[torch.Tensor] = None,\n@@ -2016,6 +1888,13 @@ def get_audio_features(\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n+        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n+            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n+        is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n+            Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n+            the features.\n+\n         Returns:\n             audio_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The audio embeddings obtained by\n             applying the projection layer to the pooled output of [`ClapAudioModel`].\n@@ -2051,8 +1930,7 @@ def get_audio_features(\n \n         return audio_features\n \n-    @add_start_docstrings_to_model_forward(CLAP_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ClapOutput, config_class=ClapConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2066,7 +1944,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, ClapOutput]:\n         r\"\"\"\n-        Returns:\n+        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n+            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n+        is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n+            Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n+            the features.\n \n         Examples:\n \n@@ -2149,12 +2034,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CLAP Text Model with a projection layer on top (a linear layer on top of the pooled output).\n-    \"\"\",\n-    CLAP_START_DOCSTRING,\n-)\n+@auto_docstring\n class ClapTextModelWithProjection(ClapPreTrainedModel):\n     config_class = ClapTextConfig\n \n@@ -2171,8 +2051,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.word_embeddings = value\n \n-    @add_start_docstrings_to_model_forward(CLAP_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ClapTextModelOutput, config_class=ClapTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -2183,8 +2062,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, ClapTextModelOutput]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -2225,12 +2102,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CLAP Audio Model with a projection layer on top (a linear layer on top of the pooled output).\n-    \"\"\",\n-    CLAP_START_DOCSTRING,\n-)\n+@auto_docstring\n class ClapAudioModelWithProjection(ClapPreTrainedModel):\n     config_class = ClapAudioConfig\n     main_input_name = \"input_features\"\n@@ -2245,8 +2117,7 @@ def __init__(self, config: ClapAudioConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.audio_model.audio_encoder.patch_embed.proj\n \n-    @add_start_docstrings_to_model_forward(CLAP_AUDIO_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ClapAudioModelOutput, config_class=ClapAudioConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_features: Optional[torch.FloatTensor] = None,\n@@ -2256,7 +2127,12 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, ClapAudioModelOutput]:\n         r\"\"\"\n-        Returns:\n+        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n+            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n+        is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n+            Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n+            the features.\n \n         Examples:\n "
        },
        {
            "sha": "accd41475dd76344ec25e251270361cbd8a63d14",
            "filename": "src/transformers/models/clip/image_processing_clip_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -14,15 +14,12 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for CLIP.\"\"\"\n \n-from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...utils import add_start_docstrings\n+from ...utils import auto_docstring\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast CLIP image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n class CLIPImageProcessorFast(BaseImageProcessorFast):\n     # To be checked against the slow image processor\n     # None values left after checking can be removed"
        },
        {
            "sha": "1ddf6bf8f79fc6bd445ae4330a0dea810392dfec",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 29,
            "deletions": 191,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -25,29 +25,12 @@\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging, torch_int\n from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"CLIPConfig\"\n-_CHECKPOINT_FOR_DOC = \"openai/clip-vit-base-patch32\"\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"openai/clip-vit-base-patch32\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"LABEL_0\"\n-\n \n # contrastive loss function, adapted from\n # https://sachinruk.github.io/blog/2021-03-07-clip.html\n@@ -460,12 +443,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class CLIPPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CLIPConfig\n     base_model_prefix = \"clip\"\n     supports_gradient_checkpointing = True\n@@ -529,110 +508,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-CLIP_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`CLIPConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CLIP_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CLIP_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CLIP_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class CLIPEncoder(nn.Module):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -745,8 +620,7 @@ def __init__(self, config: CLIPTextConfig):\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -755,10 +629,6 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n     ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -824,9 +694,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"The text model from CLIP without any head or projection on top.\"\"\",\n-    CLIP_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The text model from CLIP without any head or projection on top.\n+    \"\"\"\n )\n class CLIPTextModel(CLIPPreTrainedModel):\n     config_class = CLIPTextConfig\n@@ -846,8 +717,7 @@ def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -857,8 +727,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -895,19 +763,14 @@ def __init__(self, config: CLIPVisionConfig):\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n     ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -937,9 +800,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"The vision model from CLIP without any head or projection on top.\"\"\",\n-    CLIP_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The vision model from CLIP without any head or projection on top.\n+    \"\"\"\n )\n class CLIPVisionModel(CLIPPreTrainedModel):\n     config_class = CLIPVisionConfig\n@@ -956,8 +820,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -966,9 +829,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n-        Returns:\n-\n-        Examples:\n+        Example:\n \n         ```python\n         >>> from PIL import Image\n@@ -996,7 +857,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(CLIP_START_DOCSTRING)\n+@auto_docstring\n class CLIPModel(CLIPPreTrainedModel):\n     config_class = CLIPConfig\n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\", \"CLIPVisionEmbeddings\"]\n@@ -1036,7 +897,7 @@ def __init__(self, config: CLIPConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1080,7 +941,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1129,8 +990,7 @@ def get_image_features(\n         return image_features\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CLIPOutput, config_class=CLIPConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1143,7 +1003,8 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> CLIPOutput:\n         r\"\"\"\n-        Returns:\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n \n@@ -1218,12 +1079,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output).\n-    \"\"\",\n-    CLIP_START_DOCSTRING,\n-)\n+@auto_docstring\n class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config_class = CLIPTextConfig\n \n@@ -1247,8 +1103,7 @@ def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CLIPTextModelOutput, config_class=CLIPTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1258,8 +1113,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n     ) -> CLIPTextModelOutput:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1292,12 +1145,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output).\n-    \"\"\",\n-    CLIP_START_DOCSTRING,\n-)\n+@auto_docstring\n class CLIPVisionModelWithProjection(CLIPPreTrainedModel):\n     config_class = CLIPVisionConfig\n     main_input_name = \"pixel_values\"\n@@ -1317,8 +1165,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CLIPVisionModelOutput, config_class=CLIPVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1327,8 +1174,6 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> CLIPVisionModelOutput:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1365,12 +1210,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     CLIP vision encoder with an image classification head on top (a linear layer on top of the pooled final hidden states of\n     the patch tokens) e.g. for ImageNet.\n-    \"\"\",\n-    CLIP_START_DOCSTRING,\n+    \"\"\"\n )\n class CLIPForImageClassification(CLIPPreTrainedModel):\n     main_input_name = \"pixel_values\"\n@@ -1391,13 +1235,7 @@ def __init__(self, config: CLIPConfig) -> None:\n         self.post_init()\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CLIP_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "2321932812b3bbe7dc2ea83aadf0fdb199564eac",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 21,
            "deletions": 151,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,23 +27,13 @@\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_clipseg import CLIPSegConfig, CLIPSegTextConfig, CLIPSegVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n-_CHECKPOINT_FOR_DOC = \"CIDAS/clipseg-rd64-refined\"\n-\n-\n # contrastive loss function, adapted from\n # https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html\n def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n@@ -434,12 +424,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class CLIPSegPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CLIPSegConfig\n     base_model_prefix = \"clip\"\n     supports_gradient_checkpointing = True\n@@ -486,106 +472,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-CLIPSEG_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`CLIPSegConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CLIPSEG_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CLIPSEG_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `True`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-CLIPSEG_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `True`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Copied from transformers.models.altclip.modeling_altclip.AltCLIPEncoder with AltCLIP->CLIPSeg\n class CLIPSegEncoder(nn.Module):\n     \"\"\"\n@@ -696,9 +582,7 @@ def __init__(self, config: CLIPSegTextConfig):\n         # For `pooled_output` computation\n         self.eos_token_id = config.eos_token_id\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPSegTextConfig)\n-    # Adapted from transformers.models.clip.modeling_clip.CLIPTextTransformer.forward with clip->clipseg, CLIP->CLIPSeg\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -708,10 +592,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -798,8 +678,7 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPSegTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -810,8 +689,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -848,8 +725,7 @@ def __init__(self, config: CLIPSegVisionConfig):\n         self.encoder = CLIPSegEncoder(config)\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPSegVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor],\n@@ -858,10 +734,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = True,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -906,8 +778,7 @@ def __init__(self, config: CLIPSegVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=CLIPSegVisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -917,8 +788,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -947,7 +816,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(CLIPSEG_START_DOCSTRING)\n+@auto_docstring\n class CLIPSegModel(CLIPSegPreTrainedModel):\n     config_class = CLIPSegConfig\n \n@@ -983,7 +852,7 @@ def __init__(self, config: CLIPSegConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_text_features(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1030,7 +899,7 @@ def get_text_features(\n \n         return text_features\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_VISION_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -1081,8 +950,7 @@ def get_image_features(\n \n         return image_features\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CLIPSegOutput, config_class=CLIPSegConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1096,7 +964,8 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, CLIPSegOutput]:\n         r\"\"\"\n-        Returns:\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n \n@@ -1334,11 +1203,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     CLIPSeg model with a Transformer-based decoder on top for zero-shot and one-shot image segmentation.\n-    \"\"\",\n-    CLIPSEG_START_DOCSTRING,\n+    \"\"\"\n )\n class CLIPSegForImageSegmentation(CLIPSegPreTrainedModel):\n     config_class = CLIPSegConfig\n@@ -1385,8 +1253,7 @@ def get_conditional_embeddings(\n \n         return conditional_embeddings\n \n-    @add_start_docstrings_to_model_forward(CLIPSEG_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CLIPSegImageSegmentationOutput, config_class=CLIPSegTextConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.FloatTensor] = None,\n@@ -1406,8 +1273,11 @@ def forward(\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-\n-        Returns:\n+        conditional_pixel_values (`torch.FloatTensor`, *optional*):\n+            The pixel values of the conditional images.\n+        conditional_embeddings (`torch.FloatTensor` of shape `(batch_size, config.projection_dim)`, *optional*):\n+            The conditional embeddings for the query images. If provided, the model will use this instead of computing\n+            the embeddings from the conditional_pixel_values.\n \n         Examples:\n "
        },
        {
            "sha": "677858fe804ea5ccf4c39ab7cfccb1135db30570",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 23,
            "deletions": 144,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -38,10 +38,8 @@\n from ...pytorch_utils import Conv1D, isin_mps_friendly\n from ...utils import (\n     ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     logging,\n-    replace_return_docstrings,\n )\n from .configuration_clvp import (\n     ClvpConfig,\n@@ -52,8 +50,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"susnato/clvp_dev\"\n-\n \n # Copied from transformers.models.clip.modeling_clip.contrastive_loss\n def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n@@ -807,12 +803,8 @@ def forward(\n         return torch.concat([mel_spec, text_embeds], dim=1)\n \n \n+@auto_docstring\n class ClvpPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ClvpConfig\n     base_model_prefix = \"clvp\"\n     supports_gradient_checkpointing = True\n@@ -851,122 +843,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-CLVP_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ClvpConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-CLVP_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`):\n-            Indicates log mel-spectrogram representations for audio returned by [`ClvpFeatureExtractor`].\n-        conditioning_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n-            inputs_embeds for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.\n-        text_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n-            inputs_embeds for the text encoder model passed in place of `input_ids`.\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding text token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-CLVP_DECODER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        past_key_values (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`):\n-            Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n-            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n-            their past given to this model should not be passed as `input_ids` as they have already been computed.\n-        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If `past_key_values` is used, `attention_mask` needs to contain the masking strategy that was used for\n-            `past_key_values`. In other words, the `attention_mask` always has to have the length:\n-            `len(past_key_values) + len(input_ids)`\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-\n-            If `past_key_values` is used, optionally only the last `inputs_embeds` have to be input (see\n-            `past_key_values`).\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class ClvpEncoder(ClvpPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -1158,7 +1034,7 @@ def _prune_heads(self, heads_to_prune):\n         for layer, heads in heads_to_prune.items():\n             self.layers[layer].attn.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(CLVP_DECODER_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1302,10 +1178,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The bare Clvp decoder model outputting raw hidden-states without any specific head on top.\",\n-    CLVP_START_DOCSTRING,\n-)\n+@auto_docstring\n class ClvpModel(ClvpPreTrainedModel):\n     def __init__(self, config: ClvpDecoderConfig):\n         super().__init__(config)\n@@ -1324,7 +1197,7 @@ def set_input_embeddings(self, value):\n     def get_decoder(self):\n         return self.decoder\n \n-    @add_start_docstrings_to_model_forward(CLVP_DECODER_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1373,9 +1246,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"The CLVP decoder model with a language modelling head on top.\",\n-    CLVP_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The CLVP decoder model with a language modelling head on top.\n+    \"\"\"\n )\n class ClvpForCausalLM(ClvpPreTrainedModel, GenerationMixin):\n     def __init__(self, config):\n@@ -1516,7 +1390,7 @@ def prepare_inputs_for_generation(\n         )\n         return model_inputs\n \n-    @add_start_docstrings_to_model_forward(CLVP_DECODER_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1603,11 +1477,10 @@ def _reorder_cache(\n         )\n \n \n-@add_start_docstrings(\n-    \"The composite CLVP model with a text encoder, speech encoder and speech decoder model.\"\n-    \"The speech decoder model generates the speech_ids from the text and the text encoder and speech encoder works\"\n-    \"together to filter out the best speech_ids.\",\n-    CLVP_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The composite CLVP model with a text encoder, speech encoder and speech decoder model.\n+    \"\"\"\n )\n class ClvpModelForConditionalGeneration(ClvpPreTrainedModel, GenerationMixin):\n     config_class = ClvpConfig\n@@ -1831,8 +1704,7 @@ def get_speech_features(\n \n         return outputs[0]\n \n-    @add_start_docstrings_to_model_forward(CLVP_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ClvpOutput, config_class=ClvpConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1846,7 +1718,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, ClvpOutput]:\n         r\"\"\"\n-        Returns:\n+        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`):\n+            Indicates log mel-spectrogram representations for audio returned by [`ClvpFeatureExtractor`].\n+        conditioning_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n+            inputs_embeds for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.\n+        text_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n+            inputs_embeds for the text encoder model passed in place of `input_ids`.\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "d421432d77ffa22d687312bfc82db9d59e4ee649",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 18,
            "deletions": 114,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,9 +27,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     is_torch_flex_attn_available,\n     logging,\n )\n@@ -44,9 +42,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"Salesforce/codegen-2B-mono\"\n-_CONFIG_FOR_DOC = \"CodeGenConfig\"\n-\n \n # Copied from transformers.models.gptj.modeling_gptj.create_sinusoidal_positions\n def create_sinusoidal_positions(num_pos: int, dim: int) -> torch.Tensor:\n@@ -296,12 +291,8 @@ def forward(\n         return outputs  # hidden_states, present, (attentions)\n \n \n+@auto_docstring\n class CodeGenPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CodeGenConfig\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n@@ -331,93 +322,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-CODEGEN_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`CodeGenConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CODEGEN_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoProcenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_attention_heads,)` or `(n_layer, num_attention_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_dim)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare CodeGen Model transformer outputting raw hidden-states without any specific head on top.\",\n-    CODEGEN_START_DOCSTRING,\n-)\n+@auto_docstring\n class CodeGenModel(CodeGenPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -441,12 +346,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.wte = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(CODEGEN_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -463,6 +363,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n+        r\"\"\"\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -711,11 +617,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The CodeGen Model transformer with a language modeling head on top.\n-    \"\"\",\n-    CODEGEN_START_DOCSTRING,\n+    \"\"\"\n )\n class CodeGenForCausalLM(CodeGenPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -734,12 +639,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(CODEGEN_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -758,6 +658,10 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_dim)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`"
        },
        {
            "sha": "9611d92ca6d796165431181e6fd38684982f5134",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 10,
            "deletions": 129,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -42,15 +42,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_cohere import CohereConfig\n \n \n@@ -62,8 +54,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"CohereConfig\"\n-\n \n class CohereLayerNorm(nn.Module):\n     def __init__(self, hidden_size=None, eps=1e-5, bias=False):\n@@ -370,27 +360,7 @@ def forward(\n         return outputs\n \n \n-COHERE_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`CohereConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Cohere Model outputting raw hidden-states without any specific head on top.\",\n-    COHERE_START_DOCSTRING,\n-)\n+@auto_docstring\n class CoherePreTrainedModel(PreTrainedModel):\n     config_class = CohereConfig\n     base_model_prefix = \"model\"\n@@ -419,88 +389,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-COHERE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Cohere Model outputting raw hidden-states without any specific head on top.\",\n-    COHERE_START_DOCSTRING,\n-)\n+@auto_docstring\n class CohereModel(CoherePreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CohereDecoderLayer`]\n-\n-    Args:\n-        config: CohereConfig\n-    \"\"\"\n-\n     def __init__(self, config: CohereConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -524,7 +414,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -746,6 +636,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+@auto_docstring\n class CohereForCausalLM(CoherePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -781,8 +672,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -799,19 +689,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "a44aebcead74cbfb41e4f9ea9c714dc8be00bb37",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -51,8 +51,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"CohereConfig\"\n-\n \n class CohereLayerNorm(nn.Module):\n     def __init__(self, hidden_size=None, eps=1e-5, bias=False):\n@@ -330,19 +328,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "689b535f579cf3fbeb54f8712ac1c559cfa41043",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 129,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -33,15 +33,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere2 import Cohere2Config\n \n@@ -54,8 +46,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"Cohere2Config\"\n-\n \n class Cohere2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Cohere2Config, device=None):\n@@ -390,27 +380,7 @@ def forward(\n         return outputs\n \n \n-COHERE2_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`Cohere2Config`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Cohere2 Model outputting raw hidden-states without any specific head on top.\",\n-    COHERE2_START_DOCSTRING,\n-)\n+@auto_docstring\n class Cohere2PreTrainedModel(PreTrainedModel):\n     config_class = Cohere2Config\n     base_model_prefix = \"model\"\n@@ -439,87 +409,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-COHERE2_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Cohere2 Model outputting raw hidden-states without any specific head on top.\",\n-    COHERE2_START_DOCSTRING,\n-)\n+@auto_docstring\n class Cohere2Model(Cohere2PreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Cohere2DecoderLayer`]\n-    Args:\n-        config: Cohere2Config\n-    \"\"\"\n-\n     def __init__(self, config: Cohere2Config):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -543,8 +434,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -739,6 +629,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+@auto_docstring\n class Cohere2ForCausalLM(Cohere2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -774,8 +665,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -792,19 +682,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "256a597a415baec53faab317ecb3528692d165d1",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -26,7 +26,7 @@\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import add_start_docstrings_to_model_forward, can_return_tuple, logging\n+from ...utils import logging\n from ...utils.deprecation import deprecate_kwarg\n from ..cohere.modeling_cohere import (\n     CohereAttention,\n@@ -41,9 +41,6 @@\n from ..gemma2.modeling_gemma2 import Gemma2Model\n \n \n-COHERE2_INPUTS_DOCSTRING = None  # Will be picked up by modular\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -444,20 +441,11 @@ class Cohere2PreTrainedModel(CoherePreTrainedModel):\n \n \n class Cohere2Model(Gemma2Model):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Cohere2DecoderLayer`]\n-    Args:\n-        config: Cohere2Config\n-    \"\"\"\n-\n     def __init__(self, config: Cohere2Config):\n         super().__init__(config)\n         self.norm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n         self.rotary_emb = Cohere2RotaryEmbedding(config=config)\n \n-    @can_return_tuple\n-    @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n-    @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "3714a9a6f22464bfad2fab9ecf9af14af0d4e6ba",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 9,
            "deletions": 74,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -24,38 +24,16 @@\n \n from ...cache_utils import Cache\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring\n from .configuration_colpali import ColPaliConfig\n \n \n-_CONFIG_FOR_DOC = \"ColPaliConfig\"\n-\n-COLPALI_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ColPaliConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare ColPali model outputting raw hidden-states without any specific head on top.\",\n-    COLPALI_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare ColPali model outputting raw hidden-states without any specific head on top.\n+    \"\"\"\n )\n+@auto_docstring\n class ColPaliPreTrainedModel(PreTrainedModel):\n     config_class = ColPaliConfig\n     base_model_prefix = \"model\"\n@@ -118,47 +96,8 @@ class ColPaliForRetrievalOutput(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-COLPALI_FOR_RETRIEVAL_INPUT_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-            [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`PaliGemmaProcessor`] uses\n-            [`SiglipImageProcessor`] for processing images). If none, ColPali will only process text (query embeddings).\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-            [What are attention masks?](../glossary#attention-mask)\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        kwargs (`Dict[str, Any]`, *optional*):\n-            Additional key word arguments passed along to the vlm backbone model.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     In our proposed ColPali approach, we leverage VLMs to construct efficient multi-vector embeddings directly\n     from document images (â€œscreenshotsâ€) for document retrieval. We train the model to maximize the similarity\n     between these document embeddings and the corresponding query embeddings, using the late interaction method\n@@ -187,8 +126,7 @@ def __init__(self, config: ColPaliConfig):\n \n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(COLPALI_FOR_RETRIEVAL_INPUT_DOCSTRING)\n-    @replace_return_docstrings(output_type=ColPaliForRetrievalOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -199,9 +137,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n         **kwargs,\n     ) -> Union[Tuple, ColPaliForRetrievalOutput]:\n-        r\"\"\"\n-        Returns:\n-        \"\"\"\n         if \"pixel_values\" in kwargs:\n             kwargs[\"pixel_values\"] = kwargs[\"pixel_values\"].to(dtype=self.dtype)\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions"
        },
        {
            "sha": "c1448fe61284adccac5f7d0af2cee71cacad09e9",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 32,
            "deletions": 53,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -9,8 +9,6 @@\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n     SizeDict,\n@@ -33,7 +31,7 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n-    add_start_docstrings,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -68,6 +66,26 @@\n \n \n class ConditionalDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+        If `pad_size` is provided, the image will be padded to the specified dimensions.\n+        Otherwise, the image will be padded to the maximum height and width of the batch.\n+    pad_size (`Dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+        height and width in the batch.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    \"\"\"\n+\n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n     do_pad: Optional[bool]\n@@ -275,29 +293,7 @@ def prepare_coco_panoptic_annotation(\n     return new_target\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast ConditionalDetr image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    \"\"\"\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-            Whether to return segmentation masks.\n-    \"\"\",\n-)\n+@auto_docstring\n @requires(backends=(\"torchvision\", \"torch\"))\n class ConditionalDetrImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -571,9 +567,15 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @add_start_docstrings(\n-        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n-        \"\"\"\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n         annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n             List of annotations associated with the image or batch of images. If annotation is for object\n             detection, the annotations should be a dictionary with the following keys:\n@@ -587,32 +589,9 @@ def pad(\n             - \"file_name\" (`str`): The file name of the image.\n         format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n             Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-            Whether to return segmentation masks.\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n-        \"\"\",\n-    )\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n+        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once("
        },
        {
            "sha": "0b63dd333073900f562d8f2e15d6d785f21739c9",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 41,
            "deletions": 93,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -25,15 +25,7 @@\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithCrossAttentions, Seq2SeqModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_timm_available,\n-    logging,\n-    replace_return_docstrings,\n-    requires_backends,\n-)\n+from ...utils import ModelOutput, auto_docstring, is_timm_available, logging, requires_backends\n from ...utils.backbone_utils import load_backbone\n from .configuration_conditional_detr import ConditionalDetrConfig\n \n@@ -44,9 +36,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"ConditionalDetrConfig\"\n-_CHECKPOINT_FOR_DOC = \"microsoft/conditional-detr-resnet-50\"\n-\n \n @dataclass\n class ConditionalDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n@@ -1031,6 +1020,8 @@ def forward(self, x):\n         return x\n \n \n+@auto_docstring\n+\n # Copied from transformers.models.detr.modeling_detr.DetrPreTrainedModel with Detr->ConditionalDetr\n class ConditionalDetrPreTrainedModel(PreTrainedModel):\n     config_class = ConditionalDetrConfig\n@@ -1062,61 +1053,6 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-CONDITIONAL_DETR_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`ConditionalDetrConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CONDITIONAL_DETR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it.\n-\n-            Pixel values can be obtained using [`AutoImageProcessor`]. See [`ConditionalDetrImageProcessor.__call__`]\n-            for details.\n-\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n-            Not used by default. Can be used to mask object queries.\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Copied from transformers.models.detr.modeling_detr.DetrEncoder with Detr->ConditionalDetr,DETR->ConditionalDETR\n class ConditionalDetrEncoder(ConditionalDetrPreTrainedModel):\n     \"\"\"\n@@ -1433,12 +1369,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The bare Conditional DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw\n     hidden-states without any specific head on top.\n-    \"\"\",\n-    CONDITIONAL_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class ConditionalDetrModel(ConditionalDetrPreTrainedModel):\n     def __init__(self, config: ConditionalDetrConfig):\n@@ -1474,8 +1409,7 @@ def unfreeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(True)\n \n-    @add_start_docstrings_to_model_forward(CONDITIONAL_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ConditionalDetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1489,7 +1423,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], ConditionalDetrModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n \n         Examples:\n \n@@ -1624,12 +1565,11 @@ def forward(self, x):\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CONDITIONAL_DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Conditional DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n     top, for tasks such as COCO detection.\n-    \"\"\",\n-    CONDITIONAL_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class ConditionalDetrForObjectDetection(ConditionalDetrPreTrainedModel):\n     def __init__(self, config: ConditionalDetrConfig):\n@@ -1657,8 +1597,7 @@ def _set_aux_loss(self, outputs_class, outputs_coord):\n         # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n \n-    @add_start_docstrings_to_model_forward(CONDITIONAL_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ConditionalDetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1673,14 +1612,20 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], ConditionalDetrObjectDetectionOutput]:\n         r\"\"\"\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n             following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n             respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n             in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1788,13 +1733,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    CONDITIONAL_DETR Model (consisting of a backbone and encoder-decoder Transformer) with a segmentation head on top,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Conditional DETR Model (consisting of a backbone and encoder-decoder Transformer) with a segmentation head on top,\n     for tasks such as COCO panoptic.\n-\n-    \"\"\",\n-    CONDITIONAL_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class ConditionalDetrForSegmentation(ConditionalDetrPreTrainedModel):\n     def __init__(self, config: ConditionalDetrConfig):\n@@ -1818,8 +1761,7 @@ def __init__(self, config: ConditionalDetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONDITIONAL_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ConditionalDetrSegmentationOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1834,6 +1776,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], ConditionalDetrSegmentationOutput]:\n         r\"\"\"\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss, DICE/F-1 loss and Focal loss. List of dicts, each\n             dictionary containing at least the following 3 keys: 'class_labels', 'boxes' and 'masks' (the class labels,\n@@ -1842,8 +1792,6 @@ def forward(\n             `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)` and the masks a\n             `torch.FloatTensor` of shape `(number of bounding boxes in the image, height, width)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "0a8f142d1d3e176fd184d946c4808573ba066045",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 44,
            "deletions": 151,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -35,15 +35,15 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    auto_docstring,\n+    logging,\n+)\n from .configuration_convbert import ConvBertConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"YituTech/conv-bert-base\"\n-_CONFIG_FOR_DOC = \"ConvBertConfig\"\n-\n \n def load_tf_weights_in_convbert(model, config, tf_checkpoint_path):\n     \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n@@ -229,12 +229,8 @@ def forward(\n         return embeddings\n \n \n+@auto_docstring\n class ConvBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ConvBertConfig\n     load_tf_weights = load_tf_weights_in_convbert\n     base_model_prefix = \"convbert\"\n@@ -783,74 +779,7 @@ def forward(\n         return output\n \n \n-CONVBERT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`ConvBertConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CONVBERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare ConvBERT Model transformer outputting raw hidden-states without any specific head on top.\",\n-    CONVBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class ConvBertModel(ConvBertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -878,12 +807,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(CONVBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -965,7 +889,7 @@ def forward(self, generator_hidden_states: torch.FloatTensor) -> torch.FloatTens\n         return hidden_states\n \n \n-@add_start_docstrings(\"\"\"ConvBERT Model with a `language modeling` head on top.\"\"\", CONVBERT_START_DOCSTRING)\n+@auto_docstring\n class ConvBertForMaskedLM(ConvBertPreTrainedModel):\n     _tied_weights_keys = [\"generator.lm_head.weight\"]\n \n@@ -985,12 +909,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, word_embeddings):\n         self.generator_lm_head = word_embeddings\n \n-    @add_start_docstrings_to_model_forward(CONVBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1070,12 +989,11 @@ def forward(self, hidden_states: torch.Tensor, **kwargs) -> torch.Tensor:\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     ConvBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n-    \"\"\",\n-    CONVBERT_START_DOCSTRING,\n+    \"\"\"\n )\n class ConvBertForSequenceClassification(ConvBertPreTrainedModel):\n     def __init__(self, config):\n@@ -1088,12 +1006,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1165,13 +1078,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    ConvBERT Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    CONVBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class ConvBertForMultipleChoice(ConvBertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1183,14 +1090,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(\n-        CONVBERT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\")\n-    )\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1205,6 +1105,31 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n@@ -1258,13 +1183,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    ConvBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    CONVBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class ConvBertForTokenClassification(ConvBertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1280,12 +1199,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1339,13 +1253,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    ConvBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    CONVBERT_START_DOCSTRING,\n-)\n+@auto_docstring\n class ConvBertForQuestionAnswering(ConvBertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1357,12 +1265,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVBERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1377,16 +1280,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.convbert("
        },
        {
            "sha": "0d38e7f42046517e7b4dc2b76c0366e21ce7579c",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 20,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -18,8 +18,6 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n     group_images_by_shape,\n@@ -36,7 +34,7 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n-    add_start_docstrings,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -54,18 +52,16 @@\n \n \n class ConvNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    crop_pct (`float`, *optional*):\n+        Percentage of the image to crop. Only has an effect if size < 384. Can be\n+        overridden by `crop_pct` in the`preprocess` method.\n+    \"\"\"\n+\n     crop_pct: Optional[float]\n \n \n-@add_start_docstrings(\n-    r\"Constructs a fast ConvNeXT image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    \"\"\"\n-        crop_pct (`float`, *optional*):\n-            Percentage of the image to crop. Only has an effect if size < 384. Can be\n-            overridden by `crop_pct` in the`preprocess` method.\n-    \"\"\",\n-)\n+@auto_docstring\n class ConvNextImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_STANDARD_MEAN\n@@ -81,14 +77,7 @@ class ConvNextImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[ConvNextFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    @add_start_docstrings(\n-        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n-        \"\"\"\n-        crop_pct (`float`, *optional*):\n-            Percentage of the image to crop. Only has an effect if size < 384. Can be\n-            overridden by `crop_pct` in the`preprocess` method.\n-        \"\"\",\n-    )\n+    @auto_docstring\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[ConvNextFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n "
        },
        {
            "sha": "0f36abfb8fc91a6a6f978249fde1eda3563c16a2",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 13,
            "deletions": 80,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -29,30 +29,13 @@\n     ImageClassifierOutputWithNoAttention,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_convnext import ConvNextConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"ConvNextConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/convnext-tiny-224\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 768, 7, 7]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"facebook/convnext-tiny-224\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n-\n \n # Copied from transformers.models.beit.modeling_beit.drop_path\n def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n@@ -268,12 +251,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class ConvNextPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ConvNextConfig\n     base_model_prefix = \"convnext\"\n     main_input_name = \"pixel_values\"\n@@ -295,35 +274,7 @@ def _init_weights(self, module):\n                 module.layer_scale_parameter.data.fill_(self.config.layer_scale_init_value)\n \n \n-CONVNEXT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`ConvNextConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CONVNEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n-            [`ConvNextImageProcessor.__call__`] for details.\n-\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare ConvNext model outputting raw features without any specific head on top.\",\n-    CONVNEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class ConvNextModel(ConvNextPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -338,14 +289,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVNEXT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -383,12 +327,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     ConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\n     ImageNet.\n-    \"\"\",\n-    CONVNEXT_START_DOCSTRING,\n+    \"\"\"\n )\n class ConvNextForImageClassification(ConvNextPreTrainedModel):\n     def __init__(self, config):\n@@ -405,13 +348,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVNEXT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutputWithNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -466,11 +403,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     ConvNeXt backbone, to be used with frameworks like DETR and MaskFormer.\n-    \"\"\",\n-    CONVNEXT_START_DOCSTRING,\n+    \"\"\"\n )\n class ConvNextBackbone(ConvNextPreTrainedModel, BackboneMixin):\n     def __init__(self, config):\n@@ -490,17 +426,14 @@ def __init__(self, config):\n         # initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVNEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> BackboneOutput:\n-        \"\"\"\n-        Returns:\n-\n+        r\"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "9fd15612fc8ebefbc36afa9840a39264face28d8",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 79,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -29,30 +29,13 @@\n     ImageClassifierOutputWithNoAttention,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_convnextv2 import ConvNextV2Config\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"ConvNextV2Config\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/convnextv2-tiny-1k-224\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 768, 7, 7]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"facebook/convnextv2-tiny-1k-224\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n-\n \n # Copied from transformers.models.beit.modeling_beit.drop_path\n def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n@@ -288,12 +271,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class ConvNextV2PreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = ConvNextV2Config\n     base_model_prefix = \"convnextv2\"\n     main_input_name = \"pixel_values\"\n@@ -315,34 +294,7 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-CONVNEXTV2_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`ConvNextV2Config`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CONVNEXTV2_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`ConvNextImageProcessor`]. See\n-            [`ConvNextImageProcessor.__call__`] for details.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare ConvNextV2 model outputting raw features without any specific head on top.\",\n-    CONVNEXTV2_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.convnext.modeling_convnext.ConvNextModel with CONVNEXT->CONVNEXTV2, ConvNext->ConvNextV2\n class ConvNextV2Model(ConvNextV2PreTrainedModel):\n     def __init__(self, config):\n@@ -358,14 +310,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVNEXTV2_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -403,12 +348,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\n     ImageNet.\n-    \"\"\",\n-    CONVNEXTV2_START_DOCSTRING,\n+    \"\"\"\n )\n # Copied from transformers.models.convnext.modeling_convnext.ConvNextForImageClassification with CONVNEXT->CONVNEXTV2,ConvNext->ConvNextV2,convnext->convnextv2\n class ConvNextV2ForImageClassification(ConvNextV2PreTrainedModel):\n@@ -426,13 +370,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVNEXTV2_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutputWithNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -487,11 +425,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     ConvNeXT V2 backbone, to be used with frameworks like DETR and MaskFormer.\n-    \"\"\",\n-    CONVNEXTV2_START_DOCSTRING,\n+    \"\"\"\n )\n # Copied from transformers.models.convnext.modeling_convnext.ConvNextBackbone with CONVNEXT->CONVNEXTV2,ConvNext->ConvNextV2,facebook/convnext-tiny-224->facebook/convnextv2-tiny-1k-224\n class ConvNextV2Backbone(ConvNextV2PreTrainedModel, BackboneMixin):\n@@ -512,17 +449,14 @@ def __init__(self, config):\n         # initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CONVNEXTV2_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> BackboneOutput:\n-        \"\"\"\n-        Returns:\n-\n+        r\"\"\"\n         Examples:\n \n         ```python"
        },
        {
            "sha": "e437672aa58e0bd4006399d22f802028e8b74b63",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 24,
            "deletions": 87,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,15 +27,12 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import auto_docstring, logging\n from .configuration_cpmant import CpmAntConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"openbmb/cpm-ant-10b\"\n-_CONFIG_FOR_DOC = \"CpmAntConfig\"\n-\n \n class CpmAntLayerNorm(nn.Module):\n     \"\"\"\n@@ -523,12 +520,8 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n+@auto_docstring\n class CpmAntPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CpmAntConfig\n     base_model_prefix = \"cpmant\"\n \n@@ -551,45 +544,7 @@ def _init_weights(self, module):\n             module.relative_attention_bias.data.normal_(mean=0.0, std=self.config.init_std)\n \n \n-CPMANT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters\n-        config ([`~CpmAntConfig`]): Model configuration class with all the parameters of the\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CPMANT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.Tensor` of shape `(batch_size, seq_len)`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`CPMAntTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare CPMAnt Model outputting raw hidden-states without any specific head on top.\",\n-    CPMANT_START_DOCSTRING,\n-)\n+@auto_docstring\n class CpmAntModel(CpmAntPreTrainedModel):\n     def __init__(self, config: CpmAntConfig):\n         super().__init__(config)\n@@ -628,12 +583,7 @@ def _prepare_attention_mask(self, input_ids, span, context, length):\n         attention_mask = mask_1d.view(batch, seqlen, 1) & mask_1d.view(batch, 1, seqlen) & attention_mask\n         return attention_mask\n \n-    @add_start_docstrings_to_model_forward(CPMANT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -644,6 +594,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n         **kwargs,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPast]:\n+        r\"\"\"\n+        input_ids (`torch.Tensor` of shape `(batch_size, seq_len)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`CPMAntTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -731,11 +690,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The CPMAnt Model with a language modeling head on top (linear layer with weights tied to the input embeddings).\n-    \"\"\",\n-    CPMANT_START_DOCSTRING,\n+    \"\"\"\n )\n class CpmAntForCausalLM(CpmAntPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -750,12 +708,7 @@ def __init__(self, config: CpmAntConfig):\n         )\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CPMANT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutputWithPast,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -769,31 +722,15 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-        Args:\n-            input_ids (`torch.Tensor` of shape `(batch_size, seq_len)`):\n-                Indices of input sequence tokens in the vocabulary.\n+        input_ids (`torch.Tensor` of shape `(batch_size, seq_len)`):\n+            Indices of input sequence tokens in the vocabulary.\n \n-                Indices can be obtained using [`CPMAntTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n+            Indices can be obtained using [`CPMAntTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n \n-                [What are input IDs?](../glossary#input-ids)\n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers.\n-            labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                CPMAnt will process attention mask automatically, this parameter is a dummy parameter for\n-                text-generation pipeline.\n+            [What are input IDs?](../glossary#input-ids)\n+        labels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss.\n \n         Example:\n "
        },
        {
            "sha": "58042c64abbe6612f5f5ed0d1776fb45850eaf19",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 64,
            "deletions": 232,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,16 +36,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, ModelOutput, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from ..auto import AutoModel\n from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n from .generation_csm import CsmGenerationMixin\n@@ -58,7 +49,6 @@\n \n \n logger = logging.get_logger(__name__)\n-_CONFIG_FOR_DOC = \"CsmConfig\"\n \n \n @dataclass\n@@ -120,30 +110,12 @@ class CsmOutputWithPast(ModelOutput):\n     backbone_loss: Optional[torch.FloatTensor] = None\n \n \n-START_DOCSTRING_BASE = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`{config_class}`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-CSM_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmConfig\"))\n-\n-\n-@add_start_docstrings(\n-    \"The bare Csm Model outputting raw hidden-states without any specific head on top.\",\n-    CSM_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare Csm Model outputting raw hidden-states without any specific head on top.\n+    \"\"\"\n )\n+@auto_docstring\n class CsmPreTrainedModel(PreTrainedModel):\n     config_class = CsmConfig\n     base_model_prefix = \"model\"\n@@ -450,104 +422,8 @@ def forward(\n         return outputs\n \n \n-CSM_DEPTH_DECODER_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmDepthDecoderConfig\"))\n-\n-\n-INPUTS_DOCSTRING_BASE = r\"\"\"\n-    Args:\n-        {input_ids_docstring}\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-DEPTH_DECODER_INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\"\"\"\n-\n-\n-CSM_DEPTH_DECODER_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(\n-    INPUTS_DOCSTRING_BASE.format(input_ids_docstring=DEPTH_DECODER_INPUT_IDS_DOCSTRING)\n-)\n-\n-\n-@add_start_docstrings(\n-    \"The bare CsmDepthDecoderModel outputting raw hidden-states without any specific head on top.\",\n-    CSM_DEPTH_DECODER_START_DOCSTRING,\n-)\n+@auto_docstring\n class CsmDepthDecoderModel(CsmPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n-\n-    Args:\n-        config: CsmDepthDecoderConfig\n-    \"\"\"\n-\n     config_class = CsmDepthDecoderConfig\n \n     def __init__(self, config):\n@@ -573,7 +449,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -838,13 +714,12 @@ def forward(self, hidden_states, cache_position=None):\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The CsmDepthDecoder Model transformer, with a [`CsmCodebooksHead`] on top,\n     which can be seen a position-specific language modeling head, allowing to use a different linear layer for each codebook\n     (e.g. position 0 is the first codebook and uses the first codebook head, etc.)\n-    \"\"\",\n-    CSM_DEPTH_DECODER_START_DOCSTRING,\n+    \"\"\"\n )\n class CsmDepthDecoderForCausalLM(CsmPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = None\n@@ -873,8 +748,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -892,43 +766,13 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, CsmDepthDecoderForCausalLM\n-\n-        >>> model = CsmDepthDecoderForCausalLM.from_pretrained(\"meta-csm_depth_decoder/CsmDepthDecoder-2-7b-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-csm_depth_decoder/CsmDepthDecoder-2-7b-hf\")\n-\n-        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n-        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n-        ```\n-            backbone_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, backbone_hidden_size)`, *optional*):\n-                The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n-                is provided in the `input_ids` argument.\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        backbone_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, backbone_hidden_size)`, *optional*):\n+            The last hidden state of the backbone model. Such input is required when the first codebook token (the one generated by the backbone model)\n+            is provided in the `input_ids` argument.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1018,33 +862,8 @@ def forward(self, input_ids):\n         return input_embeds\n \n \n-INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n-            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n-            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n-\n-            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\"\"\"\n-\n-\n-CSM_BACKBONE_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n-\n-\n-@add_start_docstrings(\n-    \"The bare CsmBackboneModel Model outputting raw hidden-states without any specific head on top.\",\n-    CSM_START_DOCSTRING,\n-)\n+@auto_docstring\n class CsmBackboneModel(CsmPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n-\n-    Args:\n-        config: CsmBackboneModelConfig\n-    \"\"\"\n-\n     def __init__(self, config):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1067,7 +886,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_BACKBONE_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1081,6 +900,18 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n+            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n+            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n+\n+            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1286,14 +1117,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-CSM_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The Csm model consists of two llama-like auto-regressive transformer models: a backbone model that predicts the first codebook token and a depth decoder that predicts the other codebook tokens.\n-    \"\"\",\n-    CSM_START_DOCSTRING,\n+    \"\"\"\n )\n class CsmForConditionalGeneration(CsmPreTrainedModel, CsmGenerationMixin):\n     _tied_weights_keys = [\n@@ -1481,8 +1308,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CsmOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1501,27 +1327,33 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CsmOutputWithPast]:\n         r\"\"\"\n-            input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`, *optional*):\n-                Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n-                If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n-                where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n-                the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n-\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n-                Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n-                - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n-                - `-100` will be ignored in the loss computation\n-                - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n-\n-                Such labels can be prepared using `output_labels=True` when calling [`CsmProcessor`].\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                Kept for compatibility. Does not support another value than:\n-                1. `0`, which is equivalent to keeping all logits, used in the training regime\n-                2. `1`, which is equivalent to keeping only the last logit, used in the generation regime\n-\n-        Returns:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n+            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n+            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n+\n+            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`, *optional*):\n+            Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n+            If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n+            where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n+            the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n+            Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n+            - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n+            - `-100` will be ignored in the loss computation\n+            - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n+\n+            Such labels can be prepared using `output_labels=True` when calling [`CsmProcessor`].\n+        logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+            Kept for compatibility. Does not support another value than:\n+            1. `0`, which is equivalent to keeping all logits, used in the training regime\n+            2. `1`, which is equivalent to keeping only the last logit, used in the generation regime\n \n         Example:\n "
        },
        {
            "sha": "8b7a54d2765960b9a052c181de5ffcad71d159bf",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 58,
            "deletions": 200,
            "changes": 258,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -22,20 +22,10 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from ..llama.modeling_llama import (\n     KwargsForCausalLM,\n@@ -47,15 +37,11 @@\n     LlamaRMSNorm,\n     LlamaRotaryEmbedding,\n )\n-from .configuration_csm import (\n-    CsmConfig,\n-    CsmDepthDecoderConfig,\n-)\n+from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n from .generation_csm import CsmGenerationMixin\n \n \n logger = logging.get_logger(__name__)\n-_CONFIG_FOR_DOC = \"CsmConfig\"\n \n \n @dataclass\n@@ -117,33 +103,12 @@ class CsmOutputWithPast(ModelOutput):\n     backbone_loss: Optional[torch.FloatTensor] = None\n \n \n-START_DOCSTRING_BASE = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`{config_class}`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-CSM_DEPTH_DECODER_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmDepthDecoderConfig\"))\n-\n-\n-CSM_START_DOCSTRING = r\"\"\"{}\"\"\".format(START_DOCSTRING_BASE.format(config_class=\"CsmConfig\"))\n-\n-\n-@add_start_docstrings(\n-    \"The bare Csm Model outputting raw hidden-states without any specific head on top.\",\n-    CSM_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The bare Csm Model outputting raw hidden-states without any specific head on top.\n+    \"\"\"\n )\n+@auto_docstring\n class CsmPreTrainedModel(PreTrainedModel):\n     config_class = CsmConfig\n     base_model_prefix = \"model\"\n@@ -177,104 +142,6 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-INPUTS_DOCSTRING_BASE = r\"\"\"\n-    Args:\n-        {input_ids_docstring}\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-DEPTH_DECODER_INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\"\"\"\n-\n-\n-INPUT_IDS_DOCSTRING = r\"\"\"input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n-            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n-            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n-\n-            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\"\"\"\n-\n-\n-CSM_DEPTH_DECODER_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(\n-    INPUTS_DOCSTRING_BASE.format(input_ids_docstring=DEPTH_DECODER_INPUT_IDS_DOCSTRING)\n-)\n-\n-\n-CSM_BACKBONE_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n-\n-\n # manually specify names for correct naming when converting from modualr\n class CsmRMSNorm(LlamaRMSNorm):\n     pass\n@@ -296,18 +163,8 @@ class CsmDecoderLayer(LlamaDecoderLayer):\n     pass\n \n \n-@add_start_docstrings(\n-    \"The bare CsmDepthDecoderModel outputting raw hidden-states without any specific head on top.\",\n-    CSM_DEPTH_DECODER_START_DOCSTRING,\n-)\n+@auto_docstring\n class CsmDepthDecoderModel(LlamaModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n-\n-    Args:\n-        config: CsmDepthDecoderConfig\n-    \"\"\"\n-\n     config_class = CsmDepthDecoderConfig\n \n     def __init__(self, config):\n@@ -316,7 +173,7 @@ def __init__(self, config):\n         self.inputs_embeds_projector = nn.Linear(config.backbone_hidden_size, config.hidden_size, bias=False)\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -455,13 +312,12 @@ def forward(self, hidden_states, cache_position=None):\n         return hidden_states\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The CsmDepthDecoder Model transformer, with a [`CsmCodebooksHead`] on top,\n     which can be seen a position-specific language modeling head, allowing to use a different linear layer for each codebook\n     (e.g. position 0 is the first codebook and uses the first codebook head, etc.)\n-    \"\"\",\n-    CSM_DEPTH_DECODER_START_DOCSTRING,\n+    \"\"\"\n )\n class CsmDepthDecoderForCausalLM(LlamaForCausalLM, GenerationMixin):\n     _tied_weights_keys = None\n@@ -503,8 +359,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_DEPTH_DECODER_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -596,36 +451,34 @@ def forward(self, input_ids):\n         return input_embeds\n \n \n-@add_start_docstrings(\n-    \"The bare CsmBackboneModel Model outputting raw hidden-states without any specific head on top.\",\n-    CSM_START_DOCSTRING,\n-)\n+@auto_docstring\n class CsmBackboneModel(LlamaModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CsmDecoderLayer`]\n-\n-    Args:\n-        config: CsmBackboneModelConfig\n-    \"\"\"\n-\n     def __init__(self, config):\n         super().__init__(config)\n         self.embed_tokens = CsmBackboneModelEmbeddings(config)\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_BACKBONE_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(self, **super_kwargs):\n-        return super().forward(**super_kwargs)\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n+            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n+            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n \n+            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n \n-CSM_INPUTS_DOCSTRING = r\"\"\"{}\"\"\".format(INPUTS_DOCSTRING_BASE.format(input_ids_docstring=INPUT_IDS_DOCSTRING))\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n \n+            [What are input IDs?](../glossary#input-ids)\n+        \"\"\"\n+        return super().forward(**super_kwargs)\n \n-@add_start_docstrings(\n-    \"\"\"\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The Csm model consists of two llama-like auto-regressive transformer models: a backbone model that predicts the first codebook token and a depth decoder that predicts the other codebook tokens.\n-    \"\"\",\n-    CSM_START_DOCSTRING,\n+    \"\"\"\n )\n class CsmForConditionalGeneration(CsmPreTrainedModel, CsmGenerationMixin):\n     _tied_weights_keys = [\n@@ -813,8 +666,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(CSM_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CsmOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -833,27 +685,33 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CsmOutputWithPast]:\n         r\"\"\"\n-            input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`, *optional*):\n-                Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n-                If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n-                where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n-                the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n-\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n-                Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n-                - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n-                - `-100` will be ignored in the loss computation\n-                - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n-\n-                Such labels can be prepared using `output_labels=True` when calling [`CsmProcessor`].\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                Kept for compatibility. Does not support another value than:\n-                1. `0`, which is equivalent to keeping all logits, used in the training regime\n-                2. `1`, which is equivalent to keeping only the last logit, used in the generation regime\n-\n-        Returns:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length, num_codebooks) or (batch_size, sequence_length)`):\n+            1. (batch_size, sequence_length): corresponds to the input sequence prepared with the processor from the text prompt. Such input\n+            requires `input_values` to be provided so that audio can be encoded in codebook tokens and then merged with the text tokens.\n+\n+            2. (batch_size, sequence_length, num_codebooks): codebook tokens generated during the autoregressive decoding. Such input is not meant to be used by end users.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        input_values_cutoffs (`torch.Tensor` of shape `(batch_size, max_num_audio)`, *optional*):\n+            Specify the end positions of audio segments within each batch entry, relative to the concatenated audio input.\n+            If a batch entry has fewer segments than the maximum, it is padded with -1. For example, in a batch of 2 sequences\n+            where the first contains 2 audio segments of length l1, and the second contains 1 audio segment of length l2,\n+            the input_values_cutoffs would be: [[l1, 2 * l1], [l2, -1]].\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[config.audio_token_id, -100, -101]`.\n+            Requires targeted `input_values` to be provided as audio tokens will be infered from it using the `codec_model`.\n+            - `config.audio_token_id` indicates an audio frames (considering sequence length elements as frames)\n+            - `-100` will be ignored in the loss computation\n+            - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n+\n+            Such labels can be prepared using `output_labels=True` when calling [`CsmProcessor`].\n+        logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+            Kept for compatibility. Does not support another value than:\n+            1. `0`, which is equivalent to keeping all logits, used in the training regime\n+            2. `1`, which is equivalent to keeping only the last logit, used in the generation regime\n \n         Example:\n "
        },
        {
            "sha": "1896d6ea4130412225ccb65f22cd58e1e2abe568",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 48,
            "deletions": 108,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -26,14 +26,15 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    auto_docstring,\n+    logging,\n+)\n from .configuration_ctrl import CTRLConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"CTRLConfig\"\n-\n \n def angle_defn(pos, i, d_model_size):\n     angle_rates = 1 / torch.pow(10000, (2 * (i // 2)) / d_model_size)\n@@ -205,12 +206,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class CTRLPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CTRLConfig\n     base_model_prefix = \"transformer\"\n \n@@ -231,87 +228,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-CTRL_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`CTRLConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CTRL_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n-            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n-            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n-\n-            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n-            `input_ids`.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n-            [`PreTrainedTokenizer.encode`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        past_key_values (`Tuple[Tuple[torch.FloatTensor]]` of length `config.n_layers`):\n-            Contains pre-computed hidden-states (key and values in the attention blocks) as computed by the model (see\n-            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n-            their past given to this model should not be passed as input ids as they have already been computed.\n-        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, input_ids_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare CTRL Model transformer outputting raw hidden-states without any specific head on top.\",\n-    CTRL_START_DOCSTRING,\n-)\n+@auto_docstring\n class CTRLModel(CTRLPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -345,8 +262,7 @@ def _prune_heads(self, heads_to_prune):\n         for layer, heads in heads_to_prune.items():\n             self.h[layer].multi_head_attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(CTRL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -363,7 +279,17 @@ def forward(\n         **kwargs,  # NOOP kwargs, for now\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPast]:\n         r\"\"\"\n-        Returns:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n+            [`PreTrainedTokenizer.encode`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n \n         Example:\n \n@@ -498,12 +424,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The CTRL Model transformer with a language modeling head on top (linear layer with weights tied to the input\n     embeddings).\n-    \"\"\",\n-    CTRL_START_DOCSTRING,\n+    \"\"\"\n )\n class CTRLLMHeadModel(CTRLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -522,8 +447,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(CTRL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -541,13 +465,22 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n+            [`PreTrainedTokenizer.encode`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n \n-        Returns:\n-\n         Example:\n \n         ```python\n@@ -647,17 +580,16 @@ def _reorder_cache(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The CTRL Model transformer with a sequence classification head on top (linear layer).\n     [`CTRLForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n     (e.g. GPT-2) do. Since it does classification on the last token, it requires to know the position of the last\n     token. If a `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in\n     each row. If no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot\n     guess the padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last\n     value in each row of the batch).\n-    \"\"\",\n-    CTRL_START_DOCSTRING,\n+    \"\"\"\n )\n class CTRLForSequenceClassification(CTRLPreTrainedModel):\n     def __init__(self, config):\n@@ -669,8 +601,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CTRL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -687,13 +618,22 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, input_ids_length)`):\n+            `input_ids_length` = `sequence_length` if `past_key_values` is `None` else `past_key_values[0].shape[-2]`\n+            (`sequence_length` of input past key value states). Indices of input sequence tokens in the vocabulary.\n+\n+            If `past_key_values` is used, only input IDs that do not have their past calculated should be passed as\n+            `input_ids`.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.__call__`] and\n+            [`PreTrainedTokenizer.encode`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n \n-        Returns:\n-\n         Example of single-label classification:\n \n         ```python"
        },
        {
            "sha": "eb9294dd432973ef3d6afb0d8a0bc557c04c2131",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 65,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -23,26 +23,14 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n from ...modeling_outputs import ImageClassifierOutputWithNoAttention, ModelOutput\n from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from .configuration_cvt import CvtConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"CvtConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"microsoft/cvt-13\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 384, 14, 14]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"microsoft/cvt-13\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n-\n \n @dataclass\n class BaseModelOutputWithCLSToken(ModelOutput):\n@@ -523,12 +511,8 @@ def forward(self, pixel_values, output_hidden_states=False, return_dict=True):\n         )\n \n \n+@auto_docstring\n class CvtPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = CvtConfig\n     base_model_prefix = \"cvt\"\n     main_input_name = \"pixel_values\"\n@@ -550,36 +534,13 @@ def _init_weights(self, module):\n                 )\n \n \n-CVT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`CvtConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-CVT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`CvtImageProcessor.__call__`]\n-            for details.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Cvt Model transformer outputting raw hidden-states without any specific head on top.\",\n-    CVT_START_DOCSTRING,\n-)\n+@auto_docstring\n class CvtModel(CvtPreTrainedModel):\n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n         self.encoder = CvtEncoder(config)\n@@ -593,14 +554,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(CVT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithCLSToken,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -632,12 +586,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Cvt Model transformer with an image classification head on top (a linear layer on top of the final hidden state of\n     the [CLS] token) e.g. for ImageNet.\n-    \"\"\",\n-    CVT_START_DOCSTRING,\n+    \"\"\"\n )\n class CvtForImageClassification(CvtPreTrainedModel):\n     def __init__(self, config):\n@@ -654,13 +607,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(CVT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutputWithNoAttention,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "1db385b52d8ad3343b4239fec1f36d5808f4e8dd",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 27,
            "deletions": 80,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -32,21 +32,11 @@\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import compile_compatible_method_lru_cache\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_torchdynamo_compiling,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, is_torchdynamo_compiling, torch_int\n from ...utils.backbone_utils import load_backbone\n from .configuration_d_fine import DFineConfig\n \n \n-_CONFIG_FOR_DOC = \"DFineConfig\"\n-\n-\n def multi_scale_deformable_attention_v2(\n     value: Tensor,\n     value_spatial_shapes: Tensor,\n@@ -922,64 +912,11 @@ def get_contrastive_denoising_training_group(\n     return input_query_class, input_query_bbox, attn_mask, denoising_meta_values\n \n \n-DFine_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DFineConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DFine_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`DFineImageProcessor.__call__`] for details.\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n-            Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n-            following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n-            respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n-            in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n def _get_clones(partial_module, N):\n     return nn.ModuleList([partial_module() for i in range(N)])\n \n \n+@auto_docstring\n class DFinePreTrainedModel(PreTrainedModel):\n     config_class = DFineConfig\n     base_model_prefix = \"d_fine\"\n@@ -1366,11 +1303,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting raw hidden states without any head on top.\n-    \"\"\",\n-    DFine_START_DOCSTRING,\n+    \"\"\"\n )\n class DFineModel(DFinePreTrainedModel):\n     def __init__(self, config: DFineConfig):\n@@ -1495,8 +1431,7 @@ def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dt\n \n         return anchors, valid_mask\n \n-    @add_start_docstrings_to_model_forward(DFine_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DFineModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1510,7 +1445,17 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DFineModelOutput]:\n         r\"\"\"\n-        Returns:\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n+        labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n+            Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n+            following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n+            respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n+            in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n \n         Examples:\n \n@@ -1709,12 +1654,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting bounding boxes and logits to be further\n     decoded into scores and classes.\n-    \"\"\",\n-    DFine_START_DOCSTRING,\n+    \"\"\"\n )\n class DFineForObjectDetection(DFinePreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n@@ -1756,8 +1700,7 @@ def _set_aux_loss(self, outputs_class, outputs_coord):\n         # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n \n-    @add_start_docstrings_to_model_forward(DFine_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DFineObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1772,14 +1715,18 @@ def forward(\n         **loss_kwargs,\n     ) -> Union[Tuple[torch.FloatTensor], DFineObjectDetectionOutput]:\n         r\"\"\"\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n             following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n             respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n             in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "03c513f0d5864e84c385b645d455f8ec08ff0b7c",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 26,
            "deletions": 76,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,20 +27,15 @@\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     logging,\n-    replace_return_docstrings,\n )\n from ...utils.backbone_utils import load_backbone\n from .configuration_dab_detr import DabDetrConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"DabDetrConfig\"\n-_CHECKPOINT_FOR_DOC = \"IDEA-Research/dab_detr-base\"\n-\n \n @dataclass\n # Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n@@ -859,6 +854,7 @@ def forward(self, input_tensor):\n \n \n # Modified from transformers.models.detr.modeling_detr.DetrPreTrainedModel with Detr->DabDetr\n+@auto_docstring\n class DabDetrPreTrainedModel(PreTrainedModel):\n     config_class = DabDetrConfig\n     base_model_prefix = \"model\"\n@@ -894,61 +890,6 @@ def _init_weights(self, module):\n             module.class_embed.bias.data.fill_(bias_value)\n \n \n-DAB_DETR_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DabDetrConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DAB_DETR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it.\n-\n-            Pixel values can be obtained using [`AutoImageProcessor`]. See [`DetrImageProcessor.__call__`]\n-            for details.\n-\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n-            Not used by default. Can be used to mask object queries.\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n # Modified from transformers.models.detr.modeling_detr.DetrEncoder with Detr->DabDetr,DETR->ConditionalDETR\n class DabDetrEncoder(DabDetrPreTrainedModel):\n     \"\"\"\n@@ -1273,12 +1214,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The bare DAB-DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw\n     hidden-states, intermediate hidden states, reference points, output coordinates without any specific head on top.\n-    \"\"\",\n-    DAB_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DabDetrModel(DabDetrPreTrainedModel):\n     def __init__(self, config: DabDetrConfig):\n@@ -1338,8 +1278,7 @@ def unfreeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(True)\n \n-    @add_start_docstrings_to_model_forward(DAB_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DabDetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1353,7 +1292,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DabDetrModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n \n         Examples:\n \n@@ -1543,12 +1489,11 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         return weights\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DAB_DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n     top, for tasks such as COCO detection.\n-    \"\"\",\n-    DAB_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DabDetrForObjectDetection(DabDetrPreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n@@ -1587,8 +1532,7 @@ def _set_aux_loss(self, outputs_class, outputs_coord):\n         # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n \n-    @add_start_docstrings_to_model_forward(DAB_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DabDetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1603,14 +1547,20 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DabDetrObjectDetectionOutput]:\n         r\"\"\"\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n             following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n             respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n             in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "519a4bcd44fb2d28d03e6714d4ae3db8bff356a6",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 27,
            "deletions": 76,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -24,19 +24,10 @@\n import torch.nn.functional as F\n \n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    replace_return_docstrings,\n-)\n+from ...utils import ModelOutput, auto_docstring\n from .configuration_dac import DacConfig\n \n \n-# General docstring\n-_CONFIG_FOR_DOC = \"DacConfig\"\n-\n-\n @dataclass\n class DacOutput(ModelOutput):\n     \"\"\"\n@@ -479,11 +470,8 @@ def forward(self, hidden_state):\n         return hidden_state\n \n \n+@auto_docstring\n class DacPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n-    \"\"\"\n-\n     config_class = DacConfig\n     base_model_prefix = \"dac\"\n     main_input_name = \"input_values\"\n@@ -556,36 +544,10 @@ def remove_weight_norm(self):\n             nn.utils.remove_weight_norm(layer.res_unit3.conv2)\n \n \n-DAC_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DacConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DAC_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_values (`torch.Tensor` of shape `(batch_size, 1, time_steps)`).\n-            Audio data to encode,\n-        n_quantizers (`int`, *optional*):\n-            Number of quantizers to use. If `None`, all quantizers are used. Default is `None`.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The DAC (Descript Audio Codec) model.\",\n-    DAC_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The DAC (Descript Audio Codec) model.\n+    \"\"\"\n )\n class DacModel(DacPreTrainedModel):\n     def __init__(self, config: DacConfig):\n@@ -604,25 +566,18 @@ def __init__(self, config: DacConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @replace_return_docstrings(output_type=DacEncoderOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def encode(\n         self,\n         input_values: torch.Tensor,\n         n_quantizers: Optional[int] = None,\n         return_dict: Optional[bool] = None,\n     ):\n-        \"\"\"\n-        Encode given audio data and return quantized latent codes\n-\n-        Args:\n-            input_values (`torch.Tensor of shape `(batch_size, 1, time_steps)`):\n-                Input audio data to encode,\n-            n_quantizers (int, *optional*):\n-                Number of quantizers to use. If None, all quantizers are used. Default is None.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        Returns:\n-\n+        r\"\"\"\n+        input_values (`torch.Tensor of shape `(batch_size, 1, time_steps)`):\n+            Input audio data to encode,\n+        n_quantizers (int, *optional*):\n+            Number of quantizers to use. If None, all quantizers are used. Default is None.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.return_dict\n \n@@ -638,27 +593,20 @@ def encode(\n \n         return DacEncoderOutput(loss, quantized_representation, audio_codes, projected_latents)\n \n-    @replace_return_docstrings(output_type=DacDecoderOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def decode(\n         self,\n         quantized_representation: Optional[torch.Tensor] = None,\n         audio_codes: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n     ):\n-        \"\"\"Decode given latent codes and return audio data\n-\n-        Args:\n-            quantized_representation (torch.Tensor of shape `(batch_size, dimension, time_steps)`, *optional*):\n-                Quantized continuous representation of input.\n-            audio_codes (`torch.Tensor` of shape `(batch_size, num_codebooks, time_steps)`, *optional*):\n-                The codebook indices for each codebook, representing the quantized discrete\n-                representation of the input. This parameter should be provided if you want\n-                to decode directly from the audio codes (it will overwrite quantized_representation).\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\n-        Returns:\n-\n+        r\"\"\"\n+        quantized_representation (torch.Tensor of shape `(batch_size, dimension, time_steps)`, *optional*):\n+            Quantized continuous representation of input.\n+        audio_codes (`torch.Tensor` of shape `(batch_size, num_codebooks, time_steps)`, *optional*):\n+            The codebook indices for each codebook, representing the quantized discrete\n+            representation of the input. This parameter should be provided if you want\n+            to decode directly from the audio codes (it will overwrite quantized_representation).\n         \"\"\"\n \n         if quantized_representation is None and audio_codes is None:\n@@ -676,16 +624,19 @@ def decode(\n \n         return DacDecoderOutput(audio_values)\n \n-    @add_start_docstrings_to_model_forward(DAC_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DacOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: torch.Tensor,\n         n_quantizers: Optional[int] = None,\n         return_dict: Optional[bool] = None,\n     ):\n-        \"\"\"\n-        Returns:\n+        r\"\"\"\n+        input_values (`torch.Tensor` of shape `(batch_size, 1, time_steps)`):\n+            Audio data to encode.\n+        n_quantizers (`int`, *optional*):\n+            Number of quantizers to use. If `None`, all quantizers are used. Default is `None`.\n+\n         Examples:\n \n         ```python"
        },
        {
            "sha": "3db2fc0c685f59a8b2004cfba3bec3e652658199",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 46,
            "deletions": 138,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -26,13 +26,7 @@\n     XVectorOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_peft_available,\n-    logging,\n-)\n+from ...utils import auto_docstring, is_peft_available, logging\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n@@ -42,12 +36,6 @@\n \n logger = logging.get_logger(__name__)\n \n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/data2vec-audio-base-960h\"\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"Data2VecAudioConfig\"\n-\n \n class Data2VecAudioConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n@@ -769,12 +757,8 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n+@auto_docstring\n class Data2VecAudioPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = Data2VecAudioConfig\n     base_model_prefix = \"data2vec_audio\"\n     main_input_name = \"input_values\"\n@@ -970,70 +954,10 @@ def compute_num_masked_span(input_length):\n     return spec_aug_mask\n \n \n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n-\n-\n-DATA2VEC_AUDIO_START_DOCSTRING = r\"\"\"\n-    Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and\n-    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and\n-    Michael Auli.\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving etc.).\n-\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`Data2VecAudioConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DATA2VEC_AUDIO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file\n-            into an array of type *List[float]* or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install\n-            soundfile*). To prepare the array into *input_values*, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.\n-        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            <Tip warning={true}>\n-\n-            `attention_mask` should be passed if the corresponding processor has `config.return_attention_mask ==\n-            True`, which is the case for all pre-trained Data2Vec Audio models. Be aware that that even with\n-            `attention_mask`, zero-padded inputs will have slightly different outputs compared to non-padded inputs\n-            because there are more than one convolutional layer in the positional encodings. For a more detailed\n-            explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).\n-\n-            </Tip>\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput\n \n \n-@add_start_docstrings(\n-    \"The bare Data2VecAudio Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n+@auto_docstring\n class Data2VecAudioModel(Data2VecAudioPreTrainedModel):\n     def __init__(self, config: Data2VecAudioConfig):\n         super().__init__(config)\n@@ -1105,14 +1029,7 @@ def _mask_hidden_states(\n \n         return hidden_states\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Data2VecAudioBaseModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1122,6 +1039,11 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Data2VecAudioBaseModelOutput]:\n+        r\"\"\"\n+        mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n+            masked extracted features in *config.proj_codevector_dim* space.\n+        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1168,17 +1090,20 @@ def forward(\n \n _HIDDEN_STATES_START_POSITION = 2\n \n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n-_CTC_EXPECTED_LOSS = 66.95\n-\n \n-@add_start_docstrings(\n-    \"\"\"Data2VecAudio Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Data2VecAudio Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n+    \"\"\"\n )\n class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel):\n     def __init__(self, config):\n+        r\"\"\"\n+        target_lang (`str`, *optional*):\n+            Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or\n+            adapter.<lang>.bin. Only relevant when using an instance of [`Data2VecAudioForCTC`] with adapters. Uses 'eng' by\n+            default.\n+        \"\"\"\n         super().__init__(config)\n \n         self.data2vec_audio = Data2VecAudioModel(config)\n@@ -1218,14 +1143,7 @@ def freeze_feature_encoder(self):\n         \"\"\"\n         self.data2vec_audio.feature_extractor._freeze_parameters()\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_CTC_EXPECTED_OUTPUT,\n-        expected_loss=_CTC_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1297,12 +1215,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Data2VecAudio Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like\n+    SUPERB Keyword Spotting.\n     \"\"\"\n-    Data2VecAudio Model with a sequence classification head on top (a linear layer over the pooled output) for tasks\n-    like SUPERB Keyword Spotting.\n-    \"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n )\n class Data2VecAudioForSequenceClassification(Data2VecAudioPreTrainedModel):\n     def __init__(self, config):\n@@ -1349,13 +1266,7 @@ def freeze_base_model(self):\n         for param in self.data2vec_audio.parameters():\n             param.requires_grad = False\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1366,6 +1277,11 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, SequenceClassifierOutput]:\n         r\"\"\"\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1419,12 +1335,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecAudio Model with a frame classification head on top for tasks like Speaker Diarization.\n-    \"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n+@auto_docstring\n class Data2VecAudioForAudioFrameClassification(Data2VecAudioPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1469,13 +1380,7 @@ def freeze_base_model(self):\n         for param in self.data2vec_audio.parameters():\n             param.requires_grad = False\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1486,6 +1391,11 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, TokenClassifierOutput]:\n         r\"\"\"\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1585,11 +1495,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Data2VecAudio Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n-    \"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n+    \"\"\"\n )\n class Data2VecAudioForXVector(Data2VecAudioPreTrainedModel):\n     def __init__(self, config):\n@@ -1653,13 +1562,7 @@ def _conv_out_length(input_length, kernel_size, stride):\n \n         return input_lengths\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=XVectorOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_values: Optional[torch.Tensor],\n@@ -1670,6 +1573,11 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[Tuple, XVectorOutput]:\n         r\"\"\"\n+        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n+            into an array of type `List[float]` or a `numpy.ndarray`, *e.g.* via the soundfile library (`pip install\n+            soundfile`). To prepare the array into `input_values`, the [`AutoProcessor`] should be used for padding and\n+            conversion into a tensor of type `torch.FloatTensor`. See [`Data2VecAudioProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "a1d747476df80b9cf579f59d4a411caf84f26341",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 49,
            "deletions": 211,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,13 +36,7 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, logging\n from .configuration_data2vec_text import Data2VecTextConfig\n \n \n@@ -51,10 +45,6 @@\n \n _HIDDEN_STATES_START_POSITION = 2\n \n-# General docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/data2vec-text-base\"\n-_CONFIG_FOR_DOC = \"Data2VecTextConfig\"\n-\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->Data2VecText\n class Data2VecTextForTextEmbeddings(nn.Module):\n@@ -581,12 +571,8 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+@auto_docstring\n class Data2VecTextPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = Data2VecTextConfig\n     base_model_prefix = \"data2vec_text\"\n     supports_gradient_checkpointing = True\n@@ -611,79 +597,7 @@ def _init_weights(self, module):\n                 module.weight.data.fill_(1.0)\n \n \n-DATA2VECTEXT_START_DOCSTRING = r\"\"\"\n-    Data2VecText was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and\n-    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and\n-    Michael Auli.\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`Data2VecTextConfig`]): Model configuration class with all the parameters of the\n-            model. Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DATA2VECTEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Data2VecText Model for text transformer outputting raw hidden-states without any specific head on top.\",\n-    DATA2VECTEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class Data2VecTextModel(Data2VecTextPreTrainedModel):\n     \"\"\"\n \n@@ -701,6 +615,10 @@ class Data2VecTextModel(Data2VecTextPreTrainedModel):\n     \"\"\"\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -726,12 +644,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward(\n         self,\n@@ -749,26 +662,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -864,8 +757,10 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"Data2VecText Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", DATA2VECTEXT_START_DOCSTRING\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Data2VecText Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n )\n class Data2VecTextForCausalLM(Data2VecTextPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n@@ -888,8 +783,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -909,31 +803,10 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-\n-        Returns:\n \n         Example:\n \n@@ -1005,7 +878,7 @@ def _reorder_cache(self, past_key_values, beam_idx):\n         return reordered_past\n \n \n-@add_start_docstrings(\"\"\"data2vec Model with a `language modeling` head on top.\"\"\", DATA2VECTEXT_START_DOCSTRING)\n+@auto_docstring\n class Data2VecTextForMaskedLM(Data2VecTextPreTrainedModel):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n@@ -1030,13 +903,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        mask=\"<mask>\",\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1057,8 +924,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        kwargs (`Dict[str, any]`, *optional*, defaults to *{}*):\n-            Used to hide legacy arguments that have been deprecated.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1129,12 +994,11 @@ def _tie_weights(self):\n             self.bias = self.decoder.bias\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Data2VecText Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n-    \"\"\",\n-    DATA2VECTEXT_START_DOCSTRING,\n+    \"\"\"\n )\n class Data2VecTextForSequenceClassification(Data2VecTextPreTrainedModel):\n     def __init__(self, config):\n@@ -1148,12 +1012,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1226,13 +1085,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecText Model with a multiple choice classification head on top (a linear layer on top of the pooled output\n-    and a softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    DATA2VECTEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class Data2VecTextForMultipleChoice(Data2VecTextPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1244,14 +1097,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(\n-        DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, num_choices, sequence_length\")\n-    )\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1266,10 +1112,34 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n             `input_ids` above)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n@@ -1320,13 +1190,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecText Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g.\n-    for Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    DATA2VECTEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class Data2VecTextForTokenClassification(Data2VecTextPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1342,12 +1206,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1426,13 +1285,7 @@ def forward(self, features, **kwargs):\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecText Model with a span classification head on top for extractive question-answering tasks like SQuAD (a\n-    linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    DATA2VECTEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class Data2VecTextForQuestionAnswering(Data2VecTextPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1444,12 +1297,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DATA2VECTEXT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1464,16 +1312,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.data2vec_text("
        },
        {
            "sha": "33dae045d48e421134d486474a9db2e9b0a9aac9",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 14,
            "deletions": 93,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -34,30 +34,12 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import compile_compatible_method_lru_cache, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import auto_docstring, logging, torch_int\n from .configuration_data2vec_vision import Data2VecVisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"Data2VecVisionConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/data2vec-vision-base\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"facebook/data2vec-vision-base-ft1k\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"remote control, remote\"\n-\n \n @dataclass\n # Copied from transformers.models.beit.modeling_beit.BeitModelOutputWithPooling with Beit->Data2VecVision\n@@ -754,13 +736,9 @@ def forward(\n         )\n \n \n+@auto_docstring\n # Copied from transformers.models.beit.modeling_beit.BeitPreTrainedModel with Beit->Data2VecVision,beit->data2vec_vision\n class Data2VecVisionPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = Data2VecVisionConfig\n     base_model_prefix = \"data2vec_vision\"\n     main_input_name = \"pixel_values\"\n@@ -798,49 +776,14 @@ def _init_weights(self, module):\n                 module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n \n \n-DATA2VEC_VISION_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`Data2VecVisionConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DATA2VEC_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n-            [`BeitImageProcessor.__call__`] for details.\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Data2VecVision Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DATA2VEC_VISION_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.beit.modeling_beit.BeitModel with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,True->False\n class Data2VecVisionModel(Data2VecVisionPreTrainedModel):\n     def __init__(self, config: Data2VecVisionConfig, add_pooling_layer: bool = False) -> None:\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `False`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -866,14 +809,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_VISION_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Data2VecVisionModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n@@ -949,12 +885,11 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Data2VecVision Model transformer with an image classification head on top (a linear layer on top of the average of\n     the final hidden states of the patch tokens) e.g. for ImageNet.\n-    \"\"\",\n-    DATA2VEC_VISION_START_DOCSTRING,\n+    \"\"\"\n )\n # Copied from transformers.models.beit.modeling_beit.BeitForImageClassification with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,beit->data2vec_vision\n class Data2VecVisionForImageClassification(Data2VecVisionPreTrainedModel):\n@@ -970,13 +905,7 @@ def __init__(self, config: Data2VecVisionConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_VISION_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=ImageClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -1287,12 +1216,7 @@ def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor:\n         return output\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecVision Model transformer with a semantic segmentation head on top e.g. for ADE20k, CityScapes.\n-    \"\"\",\n-    DATA2VEC_VISION_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.beit.modeling_beit.BeitForSemanticSegmentation with BEIT->DATA2VEC_VISION,Beit->Data2VecVision,microsoft/beit-base-finetuned-ade-640-640->facebook/data2vec-vision-base,beit->data2vec_vision\n class Data2VecVisionForSemanticSegmentation(Data2VecVisionPreTrainedModel):\n     def __init__(self, config: Data2VecVisionConfig) -> None:\n@@ -1346,8 +1270,7 @@ def compute_loss(self, logits, auxiliary_logits, labels):\n \n         return loss\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=SemanticSegmenterOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -1363,8 +1286,6 @@ def forward(\n             Ground truth semantic segmentation maps for computing the loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels > 1`, a classification loss is computed (Cross-Entropy).\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "58934d2e86a7182bbdd8996078c29fe412ff4c49",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 155,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -4,19 +4,8 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_outputs import (\n-    CausalLMOutput,\n-    SequenceClassifierOutput,\n-    TokenClassifierOutput,\n-    Wav2Vec2BaseModelOutput,\n-    XVectorOutput,\n-)\n+from ...modeling_outputs import Wav2Vec2BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-)\n from ..wav2vec2.modeling_wav2vec2 import (\n     Wav2Vec2Adapter,\n     Wav2Vec2Encoder,\n@@ -33,20 +22,6 @@\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n-_HIDDEN_STATES_START_POSITION = 2\n-\n-# General docstring\n-_CONFIG_FOR_DOC = \"Data2VecAudioConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/data2vec-audio-base-960h\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 292, 768]\n-\n-# CTC docstring\n-_CTC_EXPECTED_OUTPUT = \"'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL'\"\n-_CTC_EXPECTED_LOSS = 66.95\n-\n-\n class Data2VecAudioConvLayer(nn.Module):\n     def __init__(self, config, layer_id=0):\n         super().__init__()\n@@ -143,11 +118,6 @@ class Data2VecAudioAdapter(Wav2Vec2Adapter):\n \n \n class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = Data2VecAudioConfig\n     base_model_prefix = \"data2vec_audio\"\n     main_input_name = \"input_values\"\n@@ -190,67 +160,9 @@ def load_adapter(self):\n         raise AttributeError(\"Not needed for Data2VecAudio\")\n \n \n-DATA2VEC_AUDIO_START_DOCSTRING = r\"\"\"\n-    Data2VecAudio was proposed in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and\n-    Language](https://arxiv.org/pdf/2202.03555) by Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu and\n-    Michael Auli.\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving etc.).\n-\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`Data2VecAudioConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DATA2VEC_AUDIO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a *.flac* or *.wav* audio file\n-            into an array of type *List[float]* or a *numpy.ndarray*, *e.g.* via the soundfile library (*pip install\n-            soundfile*). To prepare the array into *input_values*, the [`AutoProcessor`] should be used for padding and\n-            conversion into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.\n-        attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing convolution and attention on padding token indices. Mask values selected in `[0,\n-            1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            <Tip warning={true}>\n-\n-            `attention_mask` should be passed if the corresponding processor has `config.return_attention_mask ==\n-            True`, which is the case for all pre-trained Data2Vec Audio models. Be aware that that even with\n-            `attention_mask`, zero-padded inputs will have slightly different outputs compared to non-padded inputs\n-            because there are more than one convolutional layer in the positional encodings. For a more detailed\n-            explanation, see [here](https://github.com/huggingface/transformers/issues/25621#issuecomment-1713759349).\n-\n-            </Tip>\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n Data2VecAudioBaseModelOutput = Wav2Vec2BaseModelOutput\n \n \n-@add_start_docstrings(\n-    \"The bare Data2VecAudio Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n class Data2VecAudioModel(Data2VecAudioPreTrainedModel, Wav2Vec2Model):\n     def __init__(self, config: Data2VecAudioConfig):\n         Data2VecAudioPreTrainedModel.__init__(config)\n@@ -279,22 +191,10 @@ def freeze_feature_encoder(self):\n         \"\"\"\n         self.feature_extractor._freeze_parameters()\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=Data2VecAudioBaseModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n     def forward(self, **super_kwargs):\n         return super().forward(**super_kwargs)\n \n \n-@add_start_docstrings(\n-    \"\"\"Data2VecAudio Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel, Wav2Vec2ForCTC):\n     def __init__(self, config):\n         Data2VecAudioPreTrainedModel.__init__(config)\n@@ -323,71 +223,20 @@ def freeze_base_model(self):\n     def tie_weights(self):\n         raise AttributeError(\"Not needed for Data2VecAudio\")\n \n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=CausalLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_CTC_EXPECTED_OUTPUT,\n-        expected_loss=_CTC_EXPECTED_LOSS,\n-    )\n     def forward(self, **super_kwargs):\n         return super().forward(**super_kwargs)\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecAudio Model with a sequence classification head on top (a linear layer over the pooled output) for tasks\n-    like SUPERB Keyword Spotting.\n-    \"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n class Data2VecAudioForSequenceClassification(Wav2Vec2ForSequenceClassification):\n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-    )\n-    def forward(self, **super_kwargs):\n-        return super().forward(**super_kwargs)\n+    pass\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecAudio Model with a frame classification head on top for tasks like Speaker Diarization.\n-    \"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n class Data2VecAudioForAudioFrameClassification(Wav2Vec2ForAudioFrameClassification):\n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-    )\n-    def forward(self, **super_kwargs):\n-        return super().forward(**super_kwargs)\n+    pass\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    Data2VecAudio Model with an XVector feature extraction head on top for tasks like Speaker Verification.\n-    \"\"\",\n-    DATA2VEC_AUDIO_START_DOCSTRING,\n-)\n class Data2VecAudioForXVector(Wav2Vec2ForXVector):\n-    @add_start_docstrings_to_model_forward(DATA2VEC_AUDIO_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=XVectorOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"audio\",\n-    )\n-    def forward(self, **super_kwargs):\n-        return super().forward(**super_kwargs)\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "050476355c93f561eb6172d430a3402372cb95cf",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 14,
            "deletions": 129,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -28,13 +28,7 @@\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_dbrx import DbrxConfig\n \n \n@@ -49,8 +43,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"DbrxConfig\"\n-\n \n class DbrxRotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n@@ -807,27 +799,7 @@ def forward(\n         return outputs\n \n \n-DBRX_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DbrxConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DBRX Model outputting raw hidden-states without any specific head on top.\",\n-    DBRX_START_DOCSTRING,\n-)\n+@auto_docstring\n class DbrxPreTrainedModel(PreTrainedModel):\n     config_class = DbrxConfig\n     base_model_prefix = \"transformer\"\n@@ -860,88 +832,7 @@ def _init_weights(self, module: nn.Module):\n             module.w2.data.normal_(mean=0.0, std=std)\n \n \n-DBRX_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        output_router_logits (`bool`, *optional*):\n-            Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-            should not be returned during inference.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DBRX Model outputting raw hidden-states without any specific head on top.\",\n-    DBRX_START_DOCSTRING,\n-)\n+@auto_docstring\n class DbrxModel(DbrxPreTrainedModel):\n     \"\"\"Transformer decoder consisting of *config.num_hidden_layers*. Each layer is a [`DbrxBlock`] layer.\n \n@@ -971,7 +862,7 @@ def get_input_embeddings(self) -> nn.Embedding:\n     def set_input_embeddings(self, value: nn.Embedding):\n         self.wte = value\n \n-    @add_start_docstrings_to_model_forward(DBRX_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1236,7 +1127,11 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-@add_start_docstrings(\"The DBRX Model transformer for causal language modeling.\", DBRX_START_DOCSTRING)\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The DBRX Model transformer for causal language modeling.\n+    \"\"\"\n+)\n class DbrxForCausalLM(DbrxPreTrainedModel, GenerationMixin):\n     def __init__(self, config: DbrxConfig):\n         super().__init__(config)\n@@ -1268,8 +1163,7 @@ def set_decoder(self, decoder: DbrxModel):\n     def get_decoder(self) -> DbrxModel:\n         return self.transformer\n \n-    @add_start_docstrings_to_model_forward(DBRX_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1288,19 +1182,10 @@ def forward(\n         **kwargs,\n     ) -> Union[Tuple, MoeCausalLMOutputWithPast]:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "b23c189befd710809aaf701276966e0511474517",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 14,
            "deletions": 151,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -30,25 +30,11 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import auto_docstring, logging\n from .configuration_deberta import DebertaConfig\n \n \n logger = logging.get_logger(__name__)\n-_CONFIG_FOR_DOC = \"DebertaConfig\"\n-_CHECKPOINT_FOR_DOC = \"microsoft/deberta-base\"\n-\n-# Masked LM docstring\n-_CHECKPOINT_FOR_MASKED_LM = \"lsanochkin/deberta-large-feedback\"\n-_MASKED_LM_EXPECTED_OUTPUT = \"' Paris'\"\n-_MASKED_LM_EXPECTED_LOSS = \"0.54\"\n-\n-# QuestionAnswering docstring\n-_CHECKPOINT_FOR_QA = \"Palak/microsoft_deberta-large_squad\"\n-_QA_EXPECTED_OUTPUT = \"' a nice puppet'\"\n-_QA_EXPECTED_LOSS = 0.14\n-_QA_TARGET_START_INDEX = 12\n-_QA_TARGET_END_INDEX = 14\n \n \n class DebertaLayerNorm(nn.Module):\n@@ -632,12 +618,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class DebertaPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DebertaConfig\n     base_model_prefix = \"deberta\"\n     _keys_to_ignore_on_load_unexpected = [\"position_embeddings\"]\n@@ -665,71 +647,7 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-DEBERTA_START_DOCSTRING = r\"\"\"\n-    The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled\n-    Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It's build\n-    on top of BERT/RoBERTa with two improvements, i.e. disentangled attention and enhanced mask decoder. With those two\n-    improvements, it out perform BERT/RoBERTa on a majority of tasks with 80GB pretraining data.\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-\n-    Parameters:\n-        config ([`DebertaConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DEBERTA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DebertaModel(DebertaPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -754,12 +672,7 @@ class PreTrainedModel\n         \"\"\"\n         raise NotImplementedError(\"The prune function is not implemented in DeBERTa model.\")\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -933,7 +846,7 @@ def forward(self, sequence_output, word_embeddings):\n         return prediction_scores\n \n \n-@add_start_docstrings(\"\"\"DeBERTa Model with a `language modeling` head on top.\"\"\", DEBERTA_START_DOCSTRING)\n+@auto_docstring\n class DebertaForMaskedLM(DebertaPreTrainedModel):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n \n@@ -964,15 +877,7 @@ def set_output_embeddings(self, new_embeddings):\n             self.lm_predictions.lm_head.dense = new_embeddings\n             self.lm_predictions.lm_head.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_MASKED_LM,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        mask=\"[MASK]\",\n-        expected_output=_MASKED_LM_EXPECTED_OUTPUT,\n-        expected_loss=_MASKED_LM_EXPECTED_LOSS,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1050,12 +955,11 @@ def output_dim(self):\n         return self.config.hidden_size\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n+    \"\"\"\n )\n class DebertaForSequenceClassification(DebertaPreTrainedModel):\n     def __init__(self, config):\n@@ -1082,12 +986,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.deberta.set_input_embeddings(new_embeddings)\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1168,13 +1067,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    DeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DebertaForTokenClassification(DebertaPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1187,12 +1080,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1241,13 +1129,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    DeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DebertaForQuestionAnswering(DebertaPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1259,16 +1141,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_QA,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_QA_EXPECTED_OUTPUT,\n-        expected_loss=_QA_EXPECTED_LOSS,\n-        qa_target_start_index=_QA_TARGET_START_INDEX,\n-        qa_target_end_index=_QA_TARGET_END_INDEX,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1282,16 +1155,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.deberta("
        },
        {
            "sha": "06462ed477575d369178ca5a650125d72586f11b",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 151,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -32,17 +32,12 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import auto_docstring, logging\n from .configuration_deberta_v2 import DebertaV2Config\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"DebertaV2Config\"\n-_CHECKPOINT_FOR_DOC = \"microsoft/deberta-v2-xlarge\"\n-_QA_TARGET_START_INDEX = 2\n-_QA_TARGET_END_INDEX = 9\n-\n \n # Copied from transformers.models.deberta.modeling_deberta.DebertaSelfOutput with DebertaLayerNorm->LayerNorm\n class DebertaV2SelfOutput(nn.Module):\n@@ -703,12 +698,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class DebertaV2PreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DebertaV2Config\n     base_model_prefix = \"deberta\"\n     _keys_to_ignore_on_load_unexpected = [\"position_embeddings\"]\n@@ -733,71 +724,7 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-DEBERTA_START_DOCSTRING = r\"\"\"\n-    The DeBERTa model was proposed in [DeBERTa: Decoding-enhanced BERT with Disentangled\n-    Attention](https://arxiv.org/abs/2006.03654) by Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen. It's build\n-    on top of BERT/RoBERTa with two improvements, i.e. disentangled attention and enhanced mask decoder. With those two\n-    improvements, it out perform BERT/RoBERTa on a majority of tasks with 80GB pretraining data.\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-\n-    Parameters:\n-        config ([`DebertaV2Config`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DEBERTA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert *input_ids* indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.deberta.modeling_deberta.DebertaModel with Deberta->DebertaV2\n class DebertaV2Model(DebertaV2PreTrainedModel):\n     def __init__(self, config):\n@@ -823,12 +750,7 @@ class PreTrainedModel\n         \"\"\"\n         raise NotImplementedError(\"The prune function is not implemented in DeBERTa model.\")\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1000,7 +922,7 @@ def forward(self, sequence_output, word_embeddings):\n         return prediction_scores\n \n \n-@add_start_docstrings(\"\"\"DeBERTa Model with a `language modeling` head on top.\"\"\", DEBERTA_START_DOCSTRING)\n+@auto_docstring\n class DebertaV2ForMaskedLM(DebertaV2PreTrainedModel):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n     _keys_to_ignore_on_load_unexpected = r\"mask_predictions.*\"\n@@ -1031,13 +953,7 @@ def set_output_embeddings(self, new_embeddings):\n             self.lm_predictions.lm_head.dense = new_embeddings\n             self.lm_predictions.lm_head.bias = new_embeddings.bias\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MaskedLMOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        mask=\"[MASK]\",\n-    )\n+    @auto_docstring\n     # Copied from transformers.models.deberta.modeling_deberta.DebertaForMaskedLM.forward with Deberta->DebertaV2\n     def forward(\n         self,\n@@ -1117,12 +1033,11 @@ def output_dim(self):\n         return self.config.hidden_size\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n+    \"\"\"\n )\n class DebertaV2ForSequenceClassification(DebertaV2PreTrainedModel):\n     def __init__(self, config):\n@@ -1149,12 +1064,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.deberta.set_input_embeddings(new_embeddings)\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=SequenceClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     # Copied from transformers.models.deberta.modeling_deberta.DebertaForSequenceClassification.forward with Deberta->DebertaV2\n     def forward(\n         self,\n@@ -1236,13 +1146,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    DeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n # Copied from transformers.models.deberta.modeling_deberta.DebertaForTokenClassification with Deberta->DebertaV2\n class DebertaV2ForTokenClassification(DebertaV2PreTrainedModel):\n     def __init__(self, config):\n@@ -1256,12 +1160,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1310,13 +1209,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    DeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n-    layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DebertaV2ForQuestionAnswering(DebertaV2PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1328,14 +1221,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=QuestionAnsweringModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        qa_target_start_index=_QA_TARGET_START_INDEX,\n-        qa_target_end_index=_QA_TARGET_END_INDEX,\n-    )\n+    @auto_docstring\n     # Copied from transformers.models.deberta.modeling_deberta.DebertaForQuestionAnswering.forward with Deberta->DebertaV2\n     def forward(\n         self,\n@@ -1350,16 +1236,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, QuestionAnsweringModelOutput]:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         outputs = self.deberta(\n@@ -1410,13 +1286,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    DeBERTa Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n-    softmax) e.g. for RocStories/SWAG tasks.\n-    \"\"\",\n-    DEBERTA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DebertaV2ForMultipleChoice(DebertaV2PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1441,12 +1311,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.deberta.set_input_embeddings(new_embeddings)\n \n-    @add_start_docstrings_to_model_forward(DEBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=MultipleChoiceModelOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "c577d6f17c659bc7bab1396e2f767c1c5ae849db",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 44,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -30,20 +30,15 @@\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n from ...utils import (\n     ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     logging,\n-    replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_decision_transformer import DecisionTransformerConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"edbeeching/decision-transformer-gym-hopper-medium\"\n-_CONFIG_FOR_DOC = \"DecisionTransformerConfig\"\n-\n \n # Copied from transformers.models.gpt2.modeling_gpt2.load_tf_weights_in_gpt2\n def load_tf_weights_in_gpt2(model, config, gpt2_checkpoint_path):\n@@ -449,12 +444,8 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DecisionTransformerConfig\n     load_tf_weights = load_tf_weights_in_gpt2\n     base_model_prefix = \"transformer\"\n@@ -795,36 +786,11 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-DECISION_TRANSFORMER_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n-    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`~DecisionTransformerConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DECISION_TRANSFORMER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        states (`torch.FloatTensor` of shape `(batch_size, episode_length, state_dim)`):\n-            The states for each step in the trajectory\n-        actions (`torch.FloatTensor` of shape `(batch_size, episode_length, act_dim)`):\n-            The actions taken by the \"expert\" policy for the current state, these are masked for auto regressive\n-            prediction\n-        rewards (`torch.FloatTensor` of shape `(batch_size, episode_length, 1)`):\n-            The rewards for each state, action\n-        returns_to_go (`torch.FloatTensor` of shape `(batch_size, episode_length, 1)`):\n-            The returns for each state in the trajectory\n-        timesteps (`torch.LongTensor` of shape `(batch_size, episode_length)`):\n-            The timestep for each step in the trajectory\n-        attention_mask (`torch.FloatTensor` of shape `(batch_size, episode_length)`):\n-            Masking, used to mask the actions when performing autoregressive prediction\n-\"\"\"\n-\n-\n-@add_start_docstrings(\"The Decision Transformer Model\", DECISION_TRANSFORMER_START_DOCSTRING)\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Decision Transformer Model\n+    \"\"\"\n+)\n class DecisionTransformerModel(DecisionTransformerPreTrainedModel):\n     \"\"\"\n \n@@ -858,8 +824,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DECISION_TRANSFORMER_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @replace_return_docstrings(output_type=DecisionTransformerOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         states: Optional[torch.FloatTensor] = None,\n@@ -873,7 +838,17 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DecisionTransformerOutput]:\n         r\"\"\"\n-        Returns:\n+        states (`torch.FloatTensor` of shape `(batch_size, episode_length, state_dim)`):\n+            The states for each step in the trajectory\n+        actions (`torch.FloatTensor` of shape `(batch_size, episode_length, act_dim)`):\n+            The actions taken by the \"expert\" policy for the current state, these are masked for auto regressive\n+            prediction\n+        rewards (`torch.FloatTensor` of shape `(batch_size, episode_length, 1)`):\n+            The rewards for each state, action\n+        returns_to_go (`torch.FloatTensor` of shape `(batch_size, episode_length, 1)`):\n+            The returns for each state in the trajectory\n+        timesteps (`torch.LongTensor` of shape `(batch_size, episode_length)`):\n+            The timestep for each step in the trajectory\n \n         Examples:\n "
        },
        {
            "sha": "8388f743f2af9550887421e24436d276bcb88271",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 128,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -22,15 +22,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n \n@@ -41,7 +33,6 @@\n \n \n logger = logging.get_logger(__name__)\n-_CONFIG_FOR_DOC = \"DeepseekV3Config\"\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n@@ -511,27 +502,7 @@ def forward(\n         return outputs\n \n \n-DEEPSEEK_V3_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DeepseekV3Config`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DeepseekV3 Model outputting raw hidden-states without any specific head on top.\",\n-    DEEPSEEK_V3_START_DOCSTRING,\n-)\n+@auto_docstring\n class DeepseekV3PreTrainedModel(PreTrainedModel):\n     config_class = DeepseekV3Config\n     base_model_prefix = \"model\"\n@@ -562,88 +533,8 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n \n \n-DEEPSEEK_V3_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DeepseekV3 Model outputting raw hidden-states without any specific head on top.\",\n-    DEEPSEEK_V3_START_DOCSTRING,\n-)\n+@auto_docstring\n class DeepseekV3Model(DeepseekV3PreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DeepseekV3DecoderLayer`]\n-\n-    Args:\n-        config: DeepseekV3Config\n-    \"\"\"\n-\n     _keys_to_ignore_on_load_unexpected = [r\"model\\.layers\\.61.*\"]\n \n     def __init__(self, config: DeepseekV3Config):\n@@ -669,7 +560,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -891,6 +782,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+@auto_docstring\n class DeepseekV3ForCausalLM(DeepseekV3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -924,8 +816,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -942,19 +833,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "cd25fb7a9d75f64a01371f46e7ce45c30ec1a263",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 32,
            "deletions": 53,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -9,8 +9,6 @@\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n     SizeDict,\n@@ -33,7 +31,7 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n-    add_start_docstrings,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -59,6 +57,26 @@\n \n \n class DeformableDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+        If `pad_size` is provided, the image will be padded to the specified dimensions.\n+        Otherwise, the image will be padded to the maximum height and width of the batch.\n+    pad_size (`Dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+        height and width in the batch.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    \"\"\"\n+\n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n     do_pad: Optional[bool]\n@@ -266,29 +284,7 @@ def prepare_coco_panoptic_annotation(\n     return new_target\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast DeformableDetr image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    \"\"\"\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-            Whether to return segmentation masks.\n-    \"\"\",\n-)\n+@auto_docstring\n @requires(backends=(\"torchvision\", \"torch\"))\n class DeformableDetrImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -562,9 +558,15 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @add_start_docstrings(\n-        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n-        \"\"\"\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n         annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n             List of annotations associated with the image or batch of images. If annotation is for object\n             detection, the annotations should be a dictionary with the following keys:\n@@ -578,32 +580,9 @@ def pad(\n             - \"file_name\" (`str`): The file name of the image.\n         format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n             Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-            Whether to return segmentation masks.\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n-        \"\"\",\n-    )\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n+        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once("
        },
        {
            "sha": "1dbcb2ffb3f1c8714d416b303d978d9feac00f80",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 26,
            "deletions": 76,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -32,11 +32,9 @@\n from ...pytorch_utils import meshgrid\n from ...utils import (\n     ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     is_timm_available,\n     logging,\n-    replace_return_docstrings,\n     requires_backends,\n )\n from ...utils.backbone_utils import load_backbone\n@@ -52,9 +50,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"DeformableDetrConfig\"\n-_CHECKPOINT_FOR_DOC = \"sensetime/deformable-detr\"\n-\n \n @use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n class MultiScaleDeformableAttention(nn.Module):\n@@ -966,6 +961,7 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class DeformableDetrPreTrainedModel(PreTrainedModel):\n     config_class = DeformableDetrConfig\n     base_model_prefix = \"model\"\n@@ -1022,61 +1018,6 @@ def _init_weights(self, module):\n             nn.init.normal_(module.level_embed)\n \n \n-DEFORMABLE_DETR_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DeformableDetrConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DEFORMABLE_DETR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it.\n-\n-            Pixel values can be obtained using [`AutoImageProcessor`]. See [`DeformableDetrImageProcessor.__call__`]\n-            for details.\n-\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n-            Not used by default. Can be used to mask object queries.\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class DeformableDetrEncoder(DeformableDetrPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* deformable attention layers. Each layer is a\n@@ -1417,12 +1358,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The bare Deformable DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw\n     hidden-states without any specific head on top.\n-    \"\"\",\n-    DEFORMABLE_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DeformableDetrModel(DeformableDetrPreTrainedModel):\n     def __init__(self, config: DeformableDetrConfig):\n@@ -1595,8 +1535,7 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n         object_query = self.enc_output_norm(self.enc_output(object_query))\n         return object_query, output_proposals\n \n-    @add_start_docstrings_to_model_forward(DEFORMABLE_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DeformableDetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1610,7 +1549,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DeformableDetrModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n \n         Examples:\n \n@@ -1823,12 +1769,11 @@ def forward(self, x):\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Deformable DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on\n     top, for tasks such as COCO detection.\n-    \"\"\",\n-    DEFORMABLE_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DeformableDetrForObjectDetection(DeformableDetrPreTrainedModel):\n     # When using clones, all layers > 0 will be clones, but layer 0 *is* required\n@@ -1868,8 +1813,7 @@ def __init__(self, config: DeformableDetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEFORMABLE_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DeformableDetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1884,14 +1828,20 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DeformableDetrObjectDetectionOutput]:\n         r\"\"\"\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n             following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n             respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n             in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "3aafeaf50c09455cffeecb3776eb3598c8ceccf2",
            "filename": "src/transformers/models/deit/image_processing_deit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -14,19 +14,16 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for DeiT.\"\"\"\n \n-from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n     IMAGENET_STANDARD_STD,\n     PILImageResampling,\n )\n-from ...utils import add_start_docstrings\n+from ...utils import auto_docstring\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast DeiT image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n class DeiTImageProcessorFast(BaseImageProcessorFast):\n     # To be checked against the slow image processor\n     # None values left after checking can be removed"
        },
        {
            "sha": "1b2ba1036570dcadea0adb0b56d53a86dfb84582",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 23,
            "deletions": 100,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -32,31 +32,12 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    ModelOutput,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_deit import DeiTConfig\n \n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-_CONFIG_FOR_DOC = \"DeiTConfig\"\n-\n-# Base docstring\n-_CHECKPOINT_FOR_DOC = \"facebook/deit-base-distilled-patch16-224\"\n-_EXPECTED_OUTPUT_SHAPE = [1, 198, 768]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"facebook/deit-base-distilled-patch16-224\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n-\n \n class DeiTEmbeddings(nn.Module):\n     \"\"\"\n@@ -460,12 +441,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class DeiTPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DeiTConfig\n     base_model_prefix = \"deit\"\n     main_input_name = \"pixel_values\"\n@@ -495,48 +472,15 @@ def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> No\n                 module.mask_token.data.zero_()\n \n \n-DEIT_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`DeiTConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DEIT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n-            [`DeiTImageProcessor.__call__`] for details.\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DeiT Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DEIT_START_DOCSTRING,\n-)\n+@auto_docstring\n class DeiTModel(DeiTPreTrainedModel):\n     def __init__(self, config: DeiTConfig, add_pooling_layer: bool = True, use_mask_token: bool = False) -> None:\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        use_mask_token (`bool`, *optional*, defaults to `False`):\n+            Whether to use a mask token for masked image modeling.\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -560,14 +504,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(DEIT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPooling,\n-        config_class=_CONFIG_FOR_DOC,\n-        modality=\"vision\",\n-        expected_output=_EXPECTED_OUTPUT_SHAPE,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -646,17 +583,17 @@ def forward(self, hidden_states):\n         return pooled_output\n \n \n-@add_start_docstrings(\n-    \"\"\"DeiT Model with a decoder on top for masked image modeling, as proposed in [SimMIM](https://arxiv.org/abs/2111.09886).\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    DeiT Model with a decoder on top for masked image modeling, as proposed in [SimMIM](https://arxiv.org/abs/2111.09886).\n \n     <Tip>\n \n     Note that we provide a script to pre-train this model on custom data in our [examples\n     directory](https://github.com/huggingface/transformers/tree/main/examples/pytorch/image-pretraining).\n \n     </Tip>\n-    \"\"\",\n-    DEIT_START_DOCSTRING,\n+    \"\"\"\n )\n class DeiTForMaskedImageModeling(DeiTPreTrainedModel):\n     def __init__(self, config: DeiTConfig) -> None:\n@@ -676,8 +613,7 @@ def __init__(self, config: DeiTConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEIT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=MaskedImageModelingOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -692,8 +628,6 @@ def forward(\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n \n-        Returns:\n-\n         Examples:\n         ```python\n         >>> from transformers import AutoImageProcessor, DeiTForMaskedImageModeling\n@@ -765,12 +699,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DeiT Model transformer with an image classification head on top (a linear layer on top of the final hidden state of\n     the [CLS] token) e.g. for ImageNet.\n-    \"\"\",\n-    DEIT_START_DOCSTRING,\n+    \"\"\"\n )\n class DeiTForImageClassification(DeiTPreTrainedModel):\n     def __init__(self, config: DeiTConfig) -> None:\n@@ -785,8 +718,7 @@ def __init__(self, config: DeiTConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEIT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n@@ -803,8 +735,6 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n \n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -912,17 +842,16 @@ class token).\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DeiT Model transformer with image classification heads on top (a linear layer on top of the final hidden state of\n     the [CLS] token and a linear layer on top of the final hidden state of the distillation token) e.g. for ImageNet.\n \n     .. warning::\n \n            This model supports inference-only. Fine-tuning with distillation (i.e. with a teacher) is not yet\n            supported.\n-    \"\"\",\n-    DEIT_START_DOCSTRING,\n+    \"\"\"\n )\n class DeiTForImageClassificationWithTeacher(DeiTPreTrainedModel):\n     def __init__(self, config: DeiTConfig) -> None:\n@@ -942,13 +871,7 @@ def __init__(self, config: DeiTConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEIT_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n-        output_type=DeiTForImageClassificationWithTeacherOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "71ff5df6ba8f40c295631591d444b704fcddb8d6",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 6,
            "deletions": 47,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -20,49 +20,16 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...file_utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    replace_return_docstrings,\n-)\n from ...modeling_outputs import DepthEstimatorOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import load_backbone\n from .configuration_depth_anything import DepthAnythingConfig\n \n \n logger = logging.get_logger(__name__)\n \n # General docstring\n-_CONFIG_FOR_DOC = \"DepthAnythingConfig\"\n-\n-\n-DEPTH_ANYTHING_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`DepthAnythingConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DEPTH_ANYTHING_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`DPTImageProcessor.__call__`]\n-            for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n \n \n class DepthAnythingReassembleLayer(nn.Module):\n@@ -242,12 +209,8 @@ def forward(self, hidden_states, size=None):\n \n # Modified from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->DepthAnything,dpt->depth_anything\n # avoiding sdpa and flash_attn_2 support, it's done in the backend\n+@auto_docstring\n class DepthAnythingPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DepthAnythingConfig\n     base_model_prefix = \"depth_anything\"\n     main_input_name = \"pixel_values\"\n@@ -360,11 +323,10 @@ def forward(self, hidden_states: List[torch.Tensor], patch_height, patch_width)\n         return predicted_depth\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Depth Anything Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n-    \"\"\",\n-    DEPTH_ANYTHING_START_DOCSTRING,\n+    \"\"\"\n )\n class DepthAnythingForDepthEstimation(DepthAnythingPreTrainedModel):\n     _no_split_modules = [\"DPTViTEmbeddings\"]\n@@ -379,8 +341,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEPTH_ANYTHING_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DepthEstimatorOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -393,8 +354,6 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n             Ground truth depth estimation maps for computing the loss.\n \n-        Returns:\n-\n         Examples:\n         ```python\n         >>> from transformers import AutoImageProcessor, AutoModelForDepthEstimation"
        },
        {
            "sha": "7d3440bde3382ade809b0899d4f6e88ba0e0f610",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -36,21 +36,18 @@\n     is_scaled_image,\n     is_torch_available,\n     make_list_of_images,\n-    pil_torch_interpolation_mapping,\n     to_numpy_array,\n     valid_images,\n )\n-from ...utils import (\n-    TensorType,\n-    filter_out_non_signature_kwargs,\n-    logging,\n-    requires_backends,\n-)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_torchvision_available, logging, requires_backends\n \n \n if is_torch_available():\n     import torch\n \n+if is_torchvision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n \n logger = logging.get_logger(__name__)\n "
        },
        {
            "sha": "669fcc00eb8df2610844fd0408bd207513ca4f64",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -18,7 +18,6 @@\n \n from ...image_processing_base import BatchFeature\n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n     BaseImageProcessorFast,\n     group_images_by_shape,\n     reorder_images,\n@@ -31,7 +30,7 @@\n )\n from ...utils import (\n     TensorType,\n-    add_start_docstrings,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -60,10 +59,7 @@\n         from torchvision.transforms import functional as F\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast DepthPro image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-)\n+@auto_docstring\n @requires(backends=(\"torchvision\", \"torch\"))\n class DepthProImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR"
        },
        {
            "sha": "9eef406a669cb448b0ab8bc45327d193491bd771",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 12,
            "deletions": 78,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -22,16 +22,8 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from ..auto import AutoModel\n from .configuration_depth_pro import DepthProConfig\n \n@@ -631,62 +623,10 @@ def forward(self, features: List[torch.Tensor]) -> List[torch.Tensor]:\n \n \n # General docstring\n-_CONFIG_FOR_DOC = \"DepthProConfig\"\n-\n-\n-DEPTH_PRO_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`DepthProConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DEPTH_PRO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`DPTImageProcessor.__call__`]\n-            for details.\n-\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-DEPTH_PRO_FOR_DEPTH_ESTIMATION_START_DOCSTRING = r\"\"\"\n-    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n-    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n-    behavior.\n-\n-    Parameters:\n-        config ([`DepthProConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-        use_fov_model (`bool`, *optional*, defaults to `True`):\n-            Whether to use `DepthProFovModel` to generate the field of view.\n-\"\"\"\n \n \n+@auto_docstring\n class DepthProPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DepthProConfig\n     base_model_prefix = \"depth_pro\"\n     main_input_name = \"pixel_values\"\n@@ -712,10 +652,7 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n \n-@add_start_docstrings(\n-    \"The bare DepthPro Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DEPTH_PRO_START_DOCSTRING,\n-)\n+@auto_docstring\n class DepthProModel(DepthProPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -728,8 +665,7 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.encoder.image_encoder.model.get_input_embeddings()\n \n-    @add_start_docstrings_to_model_forward(DEPTH_PRO_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -739,8 +675,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, DepthProOutput]:\n         r\"\"\"\n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1087,14 +1021,17 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return predicted_depth\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DepthPro Model with a depth estimation head on top (consisting of 3 convolutional layers).\n-    \"\"\",\n-    DEPTH_PRO_FOR_DEPTH_ESTIMATION_START_DOCSTRING,\n+    \"\"\"\n )\n class DepthProForDepthEstimation(DepthProPreTrainedModel):\n     def __init__(self, config, use_fov_model=None):\n+        r\"\"\"\n+        use_fov_model (bool, *optional*):\n+            Whether to use the field of view model.\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n         self.use_fov_model = use_fov_model if use_fov_model is not None else self.config.use_fov_model\n@@ -1114,8 +1051,7 @@ def __init__(self, config, use_fov_model=None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DEPTH_PRO_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DepthProDepthEstimatorOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1129,8 +1065,6 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n             Ground truth depth estimation maps for computing the loss.\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "cf9d4e52d8cdd797958ec4e00c1c83f432f5d172",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 32,
            "deletions": 53,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -21,8 +21,6 @@\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n     SizeDict,\n@@ -49,7 +47,7 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n-    add_start_docstrings,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n@@ -284,36 +282,34 @@ def prepare_coco_panoptic_annotation(\n \n \n class DetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+        Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+    do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+        Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n+        bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+        Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+        If `pad_size` is provided, the image will be padded to the specified dimensions.\n+        Otherwise, the image will be padded to the maximum height and width of the batch.\n+    pad_size (`Dict[str, int]`, *optional*):\n+        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+        height and width in the batch.\n+    return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+        Whether to return segmentation masks.\n+    \"\"\"\n+\n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n     do_pad: Optional[bool]\n     pad_size: Optional[Dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n-@add_start_docstrings(\n-    \"Constructs a fast Detr image processor.\",\n-    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n-    \"\"\"\n-        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n-            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-            Whether to return segmentation masks.\n-    \"\"\",\n-)\n+@auto_docstring\n @requires(backends=(\"torchvision\", \"torch\"))\n class DetrImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n@@ -587,9 +583,15 @@ def pad(\n \n         return image, pixel_mask, annotation\n \n-    @add_start_docstrings(\n-        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n-        \"\"\"\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[DetrFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n         annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n             List of annotations associated with the image or batch of images. If annotation is for object\n             detection, the annotations should be a dictionary with the following keys:\n@@ -603,32 +605,9 @@ def pad(\n             - \"file_name\" (`str`): The file name of the image.\n         format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n             Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n-        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n-            Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n-            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n-            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-            If `pad_size` is provided, the image will be padded to the specified dimensions.\n-            Otherwise, the image will be padded to the maximum height and width of the batch.\n-        pad_size (`Dict[str, int]`, *optional*):\n-            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-            height and width in the batch.\n-        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n-            Whether to return segmentation masks.\n         masks_path (`str` or `pathlib.Path`, *optional*):\n             Path to the directory containing the segmentation masks.\n-        \"\"\",\n-    )\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        masks_path: Optional[Union[str, pathlib.Path]] = None,\n-        **kwargs: Unpack[DetrFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n+        \"\"\"\n         if \"pad_and_return_pixel_mask\" in kwargs:\n             kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n             logger.warning_once("
        },
        {
            "sha": "c23258fad1789bee877e9e4a459d7fcdf7f0b1a7",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 38,
            "deletions": 84,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -27,11 +27,9 @@\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n+    auto_docstring,\n     is_timm_available,\n     logging,\n-    replace_return_docstrings,\n     requires_backends,\n )\n from ...utils.backbone_utils import load_backbone\n@@ -44,9 +42,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"DetrConfig\"\n-_CHECKPOINT_FOR_DOC = \"facebook/detr-resnet-50\"\n-\n \n @dataclass\n class DetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n@@ -787,6 +782,7 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class DetrPreTrainedModel(PreTrainedModel):\n     config_class = DetrConfig\n     base_model_prefix = \"model\"\n@@ -817,60 +813,6 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-DETR_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DetrConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DETR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it.\n-\n-            Pixel values can be obtained using [`AutoImageProcessor`]. See [`DetrImageProcessor.__call__`] for details.\n-\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n-            Not used by default. Can be used to mask object queries.\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class DetrEncoder(DetrPreTrainedModel):\n     \"\"\"\n     Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n@@ -1161,12 +1103,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The bare DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw hidden-states without\n     any specific head on top.\n-    \"\"\",\n-    DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DetrModel(DetrPreTrainedModel):\n     def __init__(self, config: DetrConfig):\n@@ -1202,8 +1143,7 @@ def unfreeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(True)\n \n-    @add_start_docstrings_to_model_forward(DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1217,7 +1157,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DetrModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n \n         Examples:\n \n@@ -1351,12 +1298,11 @@ def forward(self, x):\n         return x\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DETR Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on top, for tasks\n     such as COCO detection.\n-    \"\"\",\n-    DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DetrForObjectDetection(DetrPreTrainedModel):\n     def __init__(self, config: DetrConfig):\n@@ -1376,8 +1322,7 @@ def __init__(self, config: DetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DetrObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1392,14 +1337,20 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DetrObjectDetectionOutput]:\n         r\"\"\"\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss. List of dicts, each dictionary containing at least the\n             following 2 keys: 'class_labels' and 'boxes' (the class labels and bounding boxes of an image in the batch\n             respectively). The class labels themselves should be a `torch.LongTensor` of len `(number of bounding boxes\n             in the image,)` and the boxes a `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python\n@@ -1490,13 +1441,11 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     DETR Model (consisting of a backbone and encoder-decoder Transformer) with a segmentation head on top, for tasks\n     such as COCO panoptic.\n-\n-    \"\"\",\n-    DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class DetrForSegmentation(DetrPreTrainedModel):\n     def __init__(self, config: DetrConfig):\n@@ -1519,8 +1468,7 @@ def __init__(self, config: DetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @add_start_docstrings_to_model_forward(DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=DetrSegmentationOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1535,6 +1483,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], DetrSegmentationOutput]:\n         r\"\"\"\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n         labels (`List[Dict]` of len `(batch_size,)`, *optional*):\n             Labels for computing the bipartite matching loss, DICE/F-1 loss and Focal loss. List of dicts, each\n             dictionary containing at least the following 3 keys: 'class_labels', 'boxes' and 'masks' (the class labels,\n@@ -1543,8 +1499,6 @@ def forward(\n             `torch.FloatTensor` of shape `(number of bounding boxes in the image, 4)` and the masks a\n             `torch.FloatTensor` of shape `(number of bounding boxes in the image, height, width)`.\n \n-        Returns:\n-\n         Examples:\n \n         ```python"
        },
        {
            "sha": "bc896482d3aea1f82ea5b47c9f18241d6fd31a3a",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 168,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5",
            "patch": "@@ -48,16 +48,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_diffllama import DiffLlamaConfig\n \n \n@@ -69,9 +60,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"kajuma/DiffLlama-0.3B-handcut\"\n-_CONFIG_FOR_DOC = \"DiffLlamaConfig\"\n-\n \n class DiffLlamaMLP(nn.Module):\n     def __init__(self, config):\n@@ -579,27 +567,7 @@ def forward(\n         return outputs\n \n \n-DIFFLLAMA_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DiffLlamaConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DiffLlama Model outputting raw hidden-states without any specific head on top.\",\n-    DIFFLLAMA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DiffLlamaPreTrainedModel(PreTrainedModel):\n     config_class = DiffLlamaConfig\n     base_model_prefix = \"model\"\n@@ -667,88 +635,8 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-DIFFLLAMA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DiffLlama Model outputting raw hidden-states without any specific head on top.\",\n-    DIFFLLAMA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DiffLlamaModel(DiffLlamaPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DiffLlamaDecoderLayer`]\n-\n-    Args:\n-        config: DiffLlamaConfig\n-    \"\"\"\n-\n     def __init__(self, config: DiffLlamaConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -772,7 +660,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -994,6 +882,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+@auto_docstring\n class DiffLlamaForCausalLM(DiffLlamaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1027,8 +916,7 @@ def get_decoder(self):\n         return self.model\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1045,19 +933,10 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n \n@@ -1112,8 +991,8 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The DiffLlama Model transformer with a sequence classification head on top (linear layer).\n \n     [`DiffLlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n@@ -1124,8 +1003,7 @@ def forward(\n     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n     each row of the batch).\n-    \"\"\",\n-    DIFFLLAMA_START_DOCSTRING,\n+    \"\"\"\n )\n class DiffLlamaForSequenceClassification(DiffLlamaPreTrainedModel):\n     def __init__(self, config):\n@@ -1144,7 +1022,7 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1213,13 +1091,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-The DiffLlama Model transformer with a span classification head on top for extractive question-answering tasks like\n-SQuAD (a linear layer on top of the hidden-states output to compute `span start logits` and `span end logits`).\n-    \"\"\",\n-    DIFFLLAMA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DiffLlamaForQuestionAnswering(DiffLlamaPreTrainedModel):\n     base_model_prefix = \"transformer\"\n \n@@ -1238,7 +1110,7 @@ def set_input_embeddings(self, value):\n         self.transformer.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1252,17 +1124,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-\n         outputs: BaseModelOutputWithPast = self.transformer(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1293,13 +1154,7 @@ def forward(\n         )\n \n \n-@add_start_docstrings(\n-    \"\"\"\n-    The DiffLlama Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n-    output) e.g. for Named-Entity-Recognition (NER) tasks.\n-    \"\"\",\n-    DIFFLLAMA_START_DOCSTRING,\n-)\n+@auto_docstring\n class DiffLlamaForTokenClassification(DiffLlamaPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1324,12 +1179,7 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=TokenClassifierOutput,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "79e46bd112abd26420b956376762ed734b492da4",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "2fb1be118cbf0aefc6dd647572d07cbba3f02c72",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 122,
            "changes": 141,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "35e5e02e86f6c64c5cd1b1ef9a72c0c47c57c1d9",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 18,
            "deletions": 123,
            "changes": 141,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "226653cd4097de0141e99707d6e9eff099ebc870",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "1a241884145df28afbac43239763508242e87aee",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 65,
            "deletions": 140,
            "changes": 205,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "56bb290cccca2ea07ac8c22d7d4efb1c10bd2ddf",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 26,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c1fd629466d1dd44d293b7666201b993e23a5685",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 14,
            "deletions": 82,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c707d4fcfe70df932807f833998ed509da0d3d81",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 78,
            "deletions": 127,
            "changes": 205,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d772ddb85b3f9db14cfa2c01f68769c588d6a4ac",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 14,
            "deletions": 83,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ef956e5000ca30a69d6d7a5bc03dde09355ac99b",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 16,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d629cf622c452a85235b290dcade85ca31a71fdf",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 70,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0ebfa3d479ca7535e72cab20f85752c7c60252d3",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 53,
            "deletions": 212,
            "changes": 265,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "83136bfba2b92f70c69003f793ab2797e6e88fbc",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 30,
            "deletions": 247,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b1c27f961bca50afd2ebf8c10f8860b1a3b332ac",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 24,
            "deletions": 232,
            "changes": 256,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "cc7b3bc72f1926fb1f8215a04cfcf29954c7ee03",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 32,
            "deletions": 65,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8aaced73980ff195d614ba7d4d2f1760c4117fd2",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 33,
            "deletions": 107,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c07627fa792fa3de9cd67356a523e2fc5c6a8414",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 115,
            "deletions": 223,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "28c6d249c723dd8e418eca5794f7b586308aef5a",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 32,
            "deletions": 129,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c47f87b408a74a98a105cf688fffe697c7d830de",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 15,
            "deletions": 47,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d2d80af9fe9d6fc32f4932ad6a45cba7e2d56511",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 74,
            "deletions": 166,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7b588d8fa44e761a3e2672da5831e8ae5b03832e",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 20,
            "deletions": 80,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ed78a8a7e5b938184dcd9d38254f79e2cf4de108",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 59,
            "deletions": 140,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8eb2fc4b1c2a254a8c53dfac5f09c68e0338b895",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 196,
            "deletions": 207,
            "changes": 403,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "09e0606f78551eebb6059a8d72b26d769e721ff4",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 31,
            "deletions": 90,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f116d339084d3b5bb453788842616648fa55abd6",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 138,
            "deletions": 229,
            "changes": 367,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0a88a7b9fd84b024ca907b25b9fceeee6639603f",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 54,
            "deletions": 155,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "37292f06876897a10201abd442ba6a136f0d0dbf",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 24,
            "deletions": 89,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "39f65179b82bdf2c9b62654f23a17020c19b318f",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 63,
            "deletions": 144,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b77e802a2029d6875f16efda87021f5cb602865d",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 24,
            "deletions": 171,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ef3cd7309fd89d528e0471b770df028193d1efa9",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 28,
            "deletions": 116,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7344bd0da0ed6f9724f27f4c922930bb1b0ede72",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 16,
            "deletions": 149,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "fd7bae385549d38b185d757039d6749c7b27d6ae",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "04f3edbf6eccabb614154636fccc90499969bd3c",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 150,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9a448370dd1cfa0232aa0db9ae904f58b6075c94",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d7a5e87e01851043bf71ad9b0f92cefce46623bc",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 14,
            "deletions": 30,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d9fbe9a41a122b74aa12c330830f64a58e0f90a5",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 51,
            "deletions": 145,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ab1db5eb74e7760f91e285cc44ae1d24645d62ac",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 36,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c63b00fe6b3a61efcf6e0106e6c22a588795e216",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 18,
            "deletions": 147,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "607a3b7f693b03051dc0f41f48b2889a00fb35a7",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 149,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "98ee428e2c420d2c8df842510739fadabeaf4231",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 16,
            "deletions": 149,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "4664efcad2e390f7692eea920db2bce62dbc5c24",
            "filename": "src/transformers/models/glm4/modular_glm4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 19,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b9bb0dba2a63a4d71c2483b5a5da5da0d762427f",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 9,
            "deletions": 68,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3b8b4c2f56077be5b23a2e107af2169c9083e6f5",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 17,
            "deletions": 34,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f99598c4dcc278e94d729ed9ca3434316ee4ae80",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 124,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "80a2a23c2f195ac4f5bac4a49ddff614dcedf6d0",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 97,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "4b3853b43691cd4dd7ad754eae4761b0a4eab986",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 97,
            "deletions": 205,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c2c21bb1c3a4da9964432ab05f67c7b4818c5975",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 64,
            "deletions": 129,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b67bd37c6b2949a90e4856f1d04a61b97505de51",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 78,
            "deletions": 178,
            "changes": 256,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "00eaa2b325fd26ba08602f4231feb1f499e0d754",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 16,
            "deletions": 177,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8e01a82b03f01a8f8ef904b1840f1fa4905b08a4",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 14,
            "deletions": 79,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b5c11163a61e58d2b2d00377743d02a0244d2f91",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 9,
            "deletions": 108,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9e6210604e199b731e1c2687ab28f0f83b69dbe5",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 32,
            "deletions": 148,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "52eea02179996b18333f91dae27bc01db63be5f4",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 10,
            "deletions": 128,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "bfce41e6fac0696dd65f62c6bfd1e2a8dbb4d948",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 18,
            "deletions": 125,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "df6fd0f9aae6645307b1b36ab830b5ca6a2805b8",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 136,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "603f8414219208dde23891124e343173dff05c52",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 9,
            "deletions": 134,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f9dc0ec3ea6a6f38a879ff13abd1e24cb0bca4df",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 119,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0845ba7b69634b29d7701887c8caab03b1503221",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 9,
            "deletions": 132,
            "changes": 141,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b89775955b0e1ecfa09c9395f47c5906d69e3ffc",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 110,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "69166a160705db20d5a198e3caab6f3e67115478",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 32,
            "deletions": 53,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b75a5612c9af0d96b22554395cd455f3d495efc6",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 30,
            "deletions": 91,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "69628249b3b81e8d5af6b088eeb170ded2d6c648",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 137,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "5c83db0ab7254bd185afc7051476c10cc85a385a",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 16,
            "deletions": 149,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ed26ff852ff3d9a50a24569892dcc822536bb892",
            "filename": "src/transformers/models/helium/modular_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 15,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "4411a5048812b96e94e72a6ad27ab2acd9442dba",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 46,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "2b4c73a1a5a864173ca115e83affbbb6b0250398",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 45,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "613d8796c869417940ca362c1784945087b49ca0",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 26,
            "deletions": 107,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "575239e4d82ed54da9ed504177bdd42283d9e6f2",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 28,
            "deletions": 117,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b3e3d24cc0e87d804f6597a2fa694628424e84f3",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 129,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9d322e5ecab15c179503ff700adfcc8a68b9e6a2",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 44,
            "deletions": 154,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "80c9af0f91e402fb51e31be2e611a3b6b50c119e",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 23,
            "deletions": 111,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a07e480d5c3a4b6a4052f79b71f4a9ee147f8e7e",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 47,
            "deletions": 172,
            "changes": 219,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "90e15821a3f4021a1cb95ae04184b6a50df61949",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 33,
            "deletions": 150,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "956987290aff2f62619105205c6d7a1db4cc44b0",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 14,
            "deletions": 84,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "cf31e6ec8596b9f7c95fd0b8045a8e3cb06d2c51",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 12,
            "deletions": 43,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "68f6a04c5a2a27a84edff1f0cae49d04ff3137c9",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 38,
            "deletions": 117,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "98acd1bd13bee9006f70922db743762b08acf9b1",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 201,
            "deletions": 188,
            "changes": 389,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d5aa8efc46ba14950fefed627228c81c78b7d3e4",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 69,
            "deletions": 139,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "48eaa4fa85d6c2bbb6bf177df83e0505a9ecde03",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 69,
            "deletions": 138,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "44a0022ee4f724394f8816db7a4e5e550c84e2ad",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a6005133fca872460e4cf3f963ed4fd5b674c247",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 20,
            "deletions": 181,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "71fdcdf9d4f6353f413b603001d77bd7d3a1d732",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 85,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "90127ba70f966d730625e96217a446f6fda8f2e9",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 16,
            "deletions": 138,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a5ba0337bb39a855c3b02450813b928bbebbe6fe",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 19,
            "deletions": 173,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "6ff52d755868ba26601879e957002ddf6cf1a69d",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 16,
            "deletions": 147,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "e241898bd33efb728783045d07f1db162db88f1c",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 100,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "980996b2820daaa1776248cb70c0c5a3a40c5186",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 57,
            "deletions": 223,
            "changes": 280,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "68d05e509f97a90bb664f4ac3ebfb8d8bc130abe",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 45,
            "deletions": 118,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "418685892f9acb8597c722e01392df344fb0ed25",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 12,
            "deletions": 33,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "4333e2c4c87253abf4dd02a6d64450e6d456fbcd",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 106,
            "deletions": 130,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "35ce04b183c63b71887c49fd100f4ee4cac8ff52",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 28,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3605001d8a48ed2663c82c8b021dfc9f9f739e30",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 71,
            "deletions": 210,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "e6e21ce897da7366aa8931a77c4788bcbbfe1490",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 188,
            "deletions": 223,
            "changes": 411,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9d8774ade8f3c22cf40960bdf2116d2e45cfccf7",
            "filename": "src/transformers/models/levit/image_processing_levit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "837d59979ac142c2216968ff80c623883b73ea66",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 12,
            "deletions": 76,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9839e13f3666d29bce2e3db5a339604b20b988d2",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 36,
            "deletions": 126,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "1ea1f124279592dbb8f2db59ce2fbebfb670cb62",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a25f05ea7c365653a6d798473913b589e77a52a6",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 18,
            "deletions": 168,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "63aa53bc60d7f3046f1508eaa19ac62af3b2d877",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 18,
            "deletions": 34,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "50a6dc1fc811b42713d9fe04c6118836611d3907",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 15,
            "deletions": 166,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "e149bf1c625c307ef3e597b95270d30fc10abbbf",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ebe0e6c599099661a6012c4b82a32f61381a3e54",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 135,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8d4b3c48ba9e6abc9824b1b5d1baeff60903605d",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 27,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "2113224a9d0e5bd153212f2d7f8112d47a61a177",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 26,
            "deletions": 134,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "484030b3e38a2af0b02cc50a020ff167fe961138",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 26,
            "deletions": 138,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b00fc91813cebfd739ef1e8d188572dbb524227f",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 15,
            "deletions": 109,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "dc7a324c441fa17244bc6b0165648b2f50e589bc",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 27,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a4271297d030135bd7f0cf8260b72e5fcb05728f",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 43,
            "deletions": 125,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "34e13b504623ea723b1cffdf103e1d388ec07fac",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 35,
            "deletions": 41,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ea178f35940c9e885a567181ad3ef2bd51f21f4b",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 107,
            "deletions": 179,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7cb9aa8e9481c912c62f9daa40e601ab9c129f6f",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 101,
            "deletions": 187,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b921dbda67ec9314addb7fa574c51511580ffcf9",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 233,
            "deletions": 203,
            "changes": 436,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8e73378df56eb56a5f6cdf8718e770dc41ca0aee",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 67,
            "deletions": 123,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3e01afd4329e1420a17f58ec6d08a3c2fb950459",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 66,
            "deletions": 151,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7f2497849f15d800b14db93f90182aaaaca6699a",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 20,
            "deletions": 80,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f097aed9867321b84e76da51838ed411b2438a3e",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 86,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f544641e0d6504e68fc938633e13c7f368280489",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 76,
            "deletions": 220,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f2ebe388de3e69292080641170c5d19e72029040",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 36,
            "deletions": 113,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7650cfcd0fdd883f6c7f0024560236b676b148f1",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 12,
            "deletions": 90,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "4745e153c20df40514d0f250ae98f2074fed8f5d",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 52,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "560b8eca9bd2d36bff4b924b9e3932a55ee1d1aa",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 167,
            "deletions": 282,
            "changes": 449,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "557cdb1fee752d389154d2af81ff40f308789ade",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 62,
            "deletions": 222,
            "changes": 284,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "5d19456c3c81d8b789cd9c51dda430a21d9494ff",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 8,
            "deletions": 59,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "96f879d31ff95e2d0f02b7df74c943442f3dd336",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 32,
            "deletions": 72,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ac7ed954b9625cd1ea7bc1a81023e5cd0b9d39e1",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 157,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3c57c816fab19659802d18998936e06a1fe30ad3",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 126,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7494afdddc86144758bda0e77cc84d6b9752089c",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 28,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "23a7c663ed2ae4eee85943fc25166bb1a34fd711",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 158,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "726d2e99588d08e1f0dfb463329d341a2e0353a4",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d4f14416424b9a302358960bbd995fffa9540409",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 9,
            "deletions": 59,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7e55288b41f2124ed1f7ad6ff8f10a9db6d66d8d",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 9,
            "deletions": 63,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b9d266ae47f9d5341048c475452641adf4e19467",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 141,
            "deletions": 301,
            "changes": 442,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7c0c18a8df20cf9489a842f6c92a15593bf68f92",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 53,
            "deletions": 202,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "e716553a6d102fcd5d32af50f9e28c5397812287",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b77dd75ebf90b8b05f4bee383022272c84780588",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 12,
            "deletions": 65,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3edcd2d7b155d9b8eef1941ff0b8f001db2e677b",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f3cfc9098dc9fdc5bbea71d7694fafd3127ac34c",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 79,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b642f5c12e281ffa946cf288438cd54c637daca9",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 17,
            "deletions": 80,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7f800bfd34c74a877ae54c42ecad2adb8d797d52",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 79,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c0e990971527e9189209355445abbbcf070b730d",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 98,
            "deletions": 142,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "137673cfa590c4fb1ff0091c67c831d41270d176",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 97,
            "deletions": 141,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d849d1945ba0e9d56465696830db89c54c09b946",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 108,
            "deletions": 227,
            "changes": 335,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "edb1a70279ea63085971a3272a3f1cea5d742bbe",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 84,
            "deletions": 124,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ef705af4169b0e4f319d7d8d5bdd72c177aed63c",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 44,
            "deletions": 220,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "1e1a18c5596f65e948e81f6d702bc4f1a20b878f",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 35,
            "deletions": 138,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "10e91c988a74cec0ddf17194492bc57b2b672c32",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 73,
            "deletions": 129,
            "changes": 202,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "615df3d7d0b98e0e4f243a6b9eea5316b155d21e",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 42,
            "deletions": 146,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "500b2ca4312dd94f50d8a6f21ae37576580b30a7",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 192,
            "deletions": 217,
            "changes": 409,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f906c1a93c23a315072c99c8b539b94e13a0b6cc",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 164,
            "deletions": 233,
            "changes": 397,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "2b6930ff3a296d6fcdc47cce9ae853395363b701",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 144,
            "deletions": 215,
            "changes": 359,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "1c386234968147d70f1d37934980405a4ac07e2c",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 221,
            "deletions": 328,
            "changes": 549,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3e1ce01041b541c194929aae59f19d6289bd835b",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 17,
            "deletions": 166,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "79002aee3178b945807a72cded19e1786a9a4e88",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 64,
            "deletions": 152,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "1f40b40c5d4157ba8bd3938fded082fb67596620",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 43,
            "deletions": 148,
            "changes": 191,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "410dd25a966fa408ce808ed085f678fceab7d6b0",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 10,
            "deletions": 128,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f87befb4594df469f85d17eae4c71f1ad6b9695f",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 128,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "88f884dc2e43b84b90fd4571cc24594441744bfe",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 139,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a5ae345e5d9c3071417cd96c503d100536efaf9f",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 30,
            "deletions": 77,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b45e4219e8e4e8941a0d8015fbb3adb4dbb4266a",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 20,
            "deletions": 57,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "55aa53c40ffd95fd53ba5e8cfd824d3931c7fd5f",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 19,
            "deletions": 119,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "1f115cb8a7b66239feb0e9eb730788d1e5b59047",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 16,
            "deletions": 224,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "5affe9ca8dff1cbde6f9245742f291d51903c272",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 38,
            "deletions": 165,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8e8114451fbdfb9e971cd8b4de8d3fcdc44d61f3",
            "filename": "src/transformers/models/owlvit/image_processing_owlvit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fimage_processing_owlvit_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "eee133346289748c1c7af52d8e562c4bef4deb1f",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 38,
            "deletions": 163,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b584a3b1318fc597d3c6d9a825d246986a529b6b",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 44,
            "deletions": 125,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "983961f3b4e210c76a08030f4aad32f6ef1fb6fc",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 70,
            "deletions": 102,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "e5785974be5b4baafbc6dd42244cbf251b47ad67",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 52,
            "deletions": 81,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9d0eafa9c94dac911814fb18305bb80142745785",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 77,
            "deletions": 219,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9656b3afe77f022bff7b307419cce34173450771",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 38,
            "deletions": 138,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3dcdd1ffcb7312b31261c262a514b9af7b10dd08",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "14c969a4123a1cc2fe368786ca1114df85a83e19",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 123,
            "deletions": 199,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "2e53ce9bb4c965084c3b5541a772827b07283085",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 15,
            "deletions": 146,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "35df0f78cf2baf2bbfe256396ff0c5bf8bf048ce",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 16,
            "deletions": 149,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "d0728df4638318a669d286539d487c0a6d965453",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 149,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a373e723676436b505bbe377470e295f9f5b4ca0",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 42,
            "deletions": 125,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "5b3a585bc1dfd37fd128f133101f6b864f457d03",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 36,
            "deletions": 94,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "35c24718fb04c76bf203ba1d0328a5908d174b61",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 15,
            "deletions": 131,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "243c81a0c8586a15d92d7f72d822205d78d70841",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 71,
            "deletions": 275,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0b5bd193c08ed4af7c80cdc3bd63cea4d7a15ce0",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 18,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "61d5ada7122bdd5ca753311a44495a8edf8185ab",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 51,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "22157b29c77554fc8ff55b94069d06bdddc990ea",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 125,
            "deletions": 243,
            "changes": 368,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0dcbfd15c55f17b5ae95186dfa75abc7acc09337",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 18,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "9fa2ff37431ae203db12789e9c68b2ba0f8a9aee",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 59,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "5191a6a49012753f8f26dcd1559e9c6f4bb2d7dc",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 37,
            "deletions": 120,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f4336cc1e305744cad7001df04eb22c34fdad9fb",
            "filename": "src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 15,
            "deletions": 54,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ae7275a4695fe11b9dd63fae6b38d2a13d24556f",
            "filename": "src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py",
            "status": "modified",
            "additions": 16,
            "deletions": 53,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0f7cfcd224dca00b7d83f9db4542c4c6308a3c2a",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 72,
            "deletions": 223,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "6ad7a155d4ab74b477826db19b75bb32a734fc77",
            "filename": "src/transformers/models/pvt/image_processing_pvt_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fimage_processing_pvt_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c4b63ee74380cf59c42139b39944cc192ee4bd13",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 69,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7c0fe075b69de1f43a6800c390da53d18598bde2",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 79,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "2ddff4d203c2eb6bc7e6af19a30dcc7db1793d35",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 168,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "4849fe4e2094c77553dc6de9b67aa2c8a497f45b",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 90,
            "deletions": 197,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b1a90d9969fe5eadf30a373a2cfc81fa6ca7cb01",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 85,
            "deletions": 176,
            "changes": 261,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "fa2aa76159684003b1e45009196e2f882738c1d9",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 38,
            "deletions": 120,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "6a8e3080a6d73a56918d9fae294c861457084598",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 20,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b71fe618136581a7f8c05281a8357b3de06e2766",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 25,
            "deletions": 131,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "7bc3606c5b26924f12f516e6f9c23056ba0b8969",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 17,
            "deletions": 164,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ef7fd4f280414e2963fa26111c69a83b25fc0e5c",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 27,
            "deletions": 76,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "ca9783cc4ca14bbc6b1b92bc50728f198179c1cb",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 40,
            "deletions": 121,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "bc5a770a8c36346da6d3ac6e8955f8bb5cb85902",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 18,
            "deletions": 168,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "f914faa57fe4c9c5708fde3aed02b3887716fca8",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 17,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "8b3219c9c39bf6e41f68f8de8cd3efbc1f866b96",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "6c2ac9e46111a9eb68d79bbc82bf753041aca071",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 18,
            "deletions": 169,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3c673b6079b31de7e1d9876befe7f6a5603bead3",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "e2456afafa9d32fa90a81fe3be29427c7316c5f9",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 150,
            "deletions": 131,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "315edd29864aeec3457a1b189fc65ab7f020da14",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 93,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "3422fbad2b25fb1d3e2a6eea656ceca82e6d1805",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 114,
            "deletions": 150,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "b510e3565a0fe24ca399e91495b2c57e96f5af9e",
            "filename": "src/transformers/models/regnet/modeling_regnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 66,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "6a8ba4bf112599d468e672c3aa0e741ccd703297",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 52,
            "deletions": 198,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "c913bb1bdd993e0465092af88a797fce22345a87",
            "filename": "src/transformers/models/resnet/modeling_resnet.py",
            "status": "modified",
            "additions": 13,
            "deletions": 80,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "a88cf0c9c63bd074ecfe51a66d54bdbe111586e2",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 98,
            "deletions": 220,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "0f3aeaf4d04a376c2d766fed104daf7e497be5ef",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 108,
            "deletions": 210,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        },
        {
            "sha": "42e9fb94e4e51a5c89b05f3b8734393b868700bc",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 207,
            "deletions": 272,
            "changes": 479,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f5ccfdc545d94202ded13217613032c81151aa5/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=5f5ccfdc545d94202ded13217613032c81151aa5"
        }
    ],
    "stats": {
        "total": 64958,
        "additions": 18216,
        "deletions": 46742
    }
}