{
    "author": "faaany",
    "message": "add xpu memory check  (#37969)\n\nadd xpu check",
    "sha": "ff5ef95db7f38270eda4b893fb285385b246799a",
    "files": [
        {
            "sha": "f104d7c45f863cb0af2ed65ec3cafd1b98e03a4d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff5ef95db7f38270eda4b893fb285385b246799a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff5ef95db7f38270eda4b893fb285385b246799a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ff5ef95db7f38270eda4b893fb285385b246799a",
            "patch": "@@ -114,6 +114,7 @@\n     is_torch_npu_available,\n     is_torch_sdpa_available,\n     is_torch_xla_available,\n+    is_torch_xpu_available,\n     logging,\n     replace_return_docstrings,\n     strtobool,\n@@ -1286,11 +1287,14 @@ def _get_device_map(\n         if hf_quantizer is not None:\n             max_memory = hf_quantizer.adjust_max_memory(max_memory)\n \n-        # CUDA: `max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU, which we\n+        # `max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU, which we\n         # can use to allocate parameters.\n         for device_name in max_memory.keys():\n             if isinstance(device_name, int):  # it's a GPU device\n-                unused_memory = torch.cuda.memory_reserved(device_name) - torch.cuda.memory_allocated(device_name)\n+                if is_torch_xpu_available():\n+                    unused_memory = torch.xpu.memory_reserved(device_name) - torch.xpu.memory_allocated(device_name)\n+                else:\n+                    unused_memory = torch.cuda.memory_reserved(device_name) - torch.cuda.memory_allocated(device_name)\n                 max_memory[device_name] += unused_memory\n         device_map_kwargs[\"max_memory\"] = max_memory\n "
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}