{
    "author": "gante",
    "message": "[deprecations] Remove generate-related deprecations up to v4.56 (#40729)\n\nremove generate-related deprecations up to v4.56",
    "sha": "de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
    "files": [
        {
            "sha": "05caed152c6e0715ed4b656c006cc5f035a79d9b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -564,12 +564,6 @@ def validate(self, strict=False):\n             )\n         # 1.4. Watermarking attributes\n         if self.watermarking_config is not None:\n-            if not (isinstance(self.watermarking_config, (WatermarkingConfig, SynthIDTextWatermarkingConfig))):\n-                minor_issues[\"watermarking_config\"] = (\n-                    \"`watermarking_config` as a dict is deprecated and will be removed in v4.54.0. Please construct \"\n-                    \"`watermarking_config` object with `WatermarkingConfig` or `SynthIDTextWatermarkingConfig` class.\"\n-                )\n-                self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n             self.watermarking_config.validate()\n \n         # 2. Validation of attribute combinations"
        },
        {
            "sha": "699a177fc6c127f6a6ad731dd7ccd8cd2222d97e",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -545,10 +545,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "6ccb502766cb180c079a2a4305901e69bbbf164e",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -376,10 +376,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "49ddd30ce75553391d44c2c66b8f88b4e30015c4",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -19,8 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n-\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n@@ -230,17 +228,5 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types)\n \n-    @property\n-    def sliding_window_pattern(self):\n-        warnings.warn(\n-            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n-            FutureWarning,\n-        )\n-        return self._sliding_window_pattern\n-\n-    @sliding_window_pattern.setter\n-    def sliding_window_pattern(self, value):\n-        self._sliding_window_pattern = value\n-\n \n __all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "85d18429f9ce54938100c60c6f64406e11877b70",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n from typing import Callable, Optional\n \n import torch\n@@ -250,18 +249,6 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types)\n \n-    @property\n-    def sliding_window_pattern(self):\n-        warnings.warn(\n-            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n-            FutureWarning,\n-        )\n-        return self._sliding_window_pattern\n-\n-    @sliding_window_pattern.setter\n-    def sliding_window_pattern(self, value):\n-        self._sliding_window_pattern = value\n-\n \n class Cohere2RotaryEmbedding(CohereRotaryEmbedding):\n     pass"
        },
        {
            "sha": "1f80a48455fe9f1e15e228fbb30182a43662fcb6",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -910,10 +910,6 @@ def forward(\n \n         inputs_embeds = nn.functional.dropout(inputs_embeds, p=self.emb_pdrop, training=self.training)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "78db13c42d68d30840d5f170b54a7797501bb591",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -570,15 +570,6 @@ def forward(\n         if use_cache:\n             if past_key_values is None:\n                 past_key_values = DynamicCache(config=self.config)\n-            elif isinstance(past_key_values, tuple):\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n-                    \"You should pass an instance of `Cache` instead, e.g. \"\n-                    \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = DynamicCache(config=self.config)\n \n             if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n                 past_key_values = EncoderDecoderCache(past_key_values, DynamicCache(config=self.config))"
        },
        {
            "sha": "c6e248a30fcf2e0badb838bcb07ec4e1ba7ac72b",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -760,10 +760,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "c32c15b65ecd51c5870fe117c4dfc49f662c472c",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n from typing import Any, Optional, Union\n \n from ...configuration_utils import PretrainedConfig, layer_type_validation\n@@ -243,18 +242,6 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types)\n \n-    @property\n-    def sliding_window_pattern(self):\n-        warnings.warn(\n-            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n-            FutureWarning,\n-        )\n-        return self._sliding_window_pattern\n-\n-    @sliding_window_pattern.setter\n-    def sliding_window_pattern(self, value):\n-        self._sliding_window_pattern = value\n-\n \n class Gemma3Config(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "4d2ca5423b2b274256ae292d53964a76d324c7e0",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -14,7 +14,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import copy\n-import warnings\n from collections.abc import Callable\n from typing import Any, Optional, Union\n \n@@ -254,18 +253,6 @@ def __init__(\n             ]\n         layer_type_validation(self.layer_types)\n \n-    @property\n-    def sliding_window_pattern(self):\n-        warnings.warn(\n-            \"The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\",\n-            FutureWarning,\n-        )\n-        return self._sliding_window_pattern\n-\n-    @sliding_window_pattern.setter\n-    def sliding_window_pattern(self, value):\n-        self._sliding_window_pattern = value\n-\n \n class Gemma3Config(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "cdd0f622bd862fe5a3cdc444f1e317631fd66a61",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -405,10 +405,6 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "b6e7751414f7500aa3db4c223dcfc91da627f465",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -38,14 +38,11 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n from ...utils.generic import check_model_inputs\n from .configuration_glm4v import Glm4vConfig, Glm4vTextConfig, Glm4vVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class Glm4vRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -304,18 +301,7 @@ def forward(\n         query_states, key_states, value_states = (\n             self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n         )\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n         query_states = query_states.transpose(0, 1).unsqueeze(0)"
        },
        {
            "sha": "770d611707bbee70660f5d4a5adab3cc803c3ded",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -38,15 +38,12 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm4v_moe import Glm4vMoeConfig, Glm4vMoeTextConfig, Glm4vMoeVisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class Glm4vMoeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -648,18 +645,7 @@ def forward(\n         query_states, key_states, value_states = (\n             self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n         )\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n         query_states = query_states.transpose(0, 1).unsqueeze(0)"
        },
        {
            "sha": "f7cf160cbb218db07268c890087928d75137e545",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -571,10 +571,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "4a8dd649c99a9d00cedcdcd5fe033e265bcff7b2",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -428,10 +428,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "1d808304c3062d0db669046731c484214eef8f58",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -306,10 +306,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "e25548d90f0ca35e668acaffe6e5505bca2a067b",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -450,10 +450,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "5681398972fc7ac796faaa70d611f0d83499af9c",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -654,10 +654,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "cb4258c1a1ac79716ca8195663234837ffc8ff95",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -673,10 +673,6 @@ def forward(\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "1ef28d710e2d88cd5f3d0ed464f154f7dc7e4fb3",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -649,10 +649,6 @@ def forward(\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "e0dfaeaf700fcee95886f815a5aeef558944ff02",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -921,10 +921,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "10dc2b629fbf5d3fb025f572afd127828f3c6e6d",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -868,10 +868,6 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "18bfacb755921ffe922fa14c9f385686b2f286ba",
            "filename": "src/transformers/models/layoutlm/configuration_layoutlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"LayoutLM model configuration\"\"\"\n \n-import warnings\n from collections import OrderedDict\n from collections.abc import Mapping\n from typing import Any, Optional\n@@ -68,12 +67,6 @@ class LayoutLMConfig(PretrainedConfig):\n             The epsilon used by the layer normalization layers.\n         pad_token_id (`int`, *optional*, defaults to 0):\n             The value used to pad input_ids.\n-        position_embedding_type (`str`, *optional*, defaults to `\"absolute\"`):\n-            Type of position embedding. Choose one of `\"absolute\"`, `\"relative_key\"`, `\"relative_key_query\"`. For\n-            positional embeddings use `\"absolute\"`. For more information on `\"relative_key\"`, please refer to\n-            [Self-Attention with Relative Position Representations (Shaw et al.)](https://huggingface.co/papers/1803.02155).\n-            For more information on `\"relative_key_query\"`, please refer to *Method 4* in [Improve Transformer Models\n-            with Better Relative Position Embeddings (Huang et al.)](https://huggingface.co/papers/2009.13658).\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n@@ -113,7 +106,6 @@ def __init__(\n         initializer_range=0.02,\n         layer_norm_eps=1e-12,\n         pad_token_id=0,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         max_2d_position_embeddings=1024,\n         **kwargs,\n@@ -131,22 +123,9 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self._position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.max_2d_position_embeddings = max_2d_position_embeddings\n \n-    @property\n-    def position_embedding_type(self):\n-        warnings.warn(\n-            \"The `position_embedding_type` attribute is deprecated and will be removed in v4.55.\",\n-            FutureWarning,\n-        )\n-        return self._position_embedding_type\n-\n-    @position_embedding_type.setter\n-    def position_embedding_type(self, value):\n-        self._position_embedding_type = value\n-\n \n class LayoutLMOnnxConfig(OnnxConfig):\n     def __init__("
        },
        {
            "sha": "34c2083df85a9ba0a1472ae76785cc3313ed0809",
            "filename": "src/transformers/models/markuplm/configuration_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fconfiguration_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fconfiguration_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fconfiguration_markuplm.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -14,8 +14,6 @@\n # limitations under the License.\n \"\"\"MarkupLM model configuration\"\"\"\n \n-import warnings\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n \n@@ -120,7 +118,6 @@ def __init__(\n         subs_pad_id=1001,\n         xpath_unit_hidden_size=32,\n         max_depth=50,\n-        position_embedding_type=\"absolute\",\n         use_cache=True,\n         classifier_dropout=None,\n         **kwargs,\n@@ -143,7 +140,6 @@ def __init__(\n         self.type_vocab_size = type_vocab_size\n         self.initializer_range = initializer_range\n         self.layer_norm_eps = layer_norm_eps\n-        self._position_embedding_type = position_embedding_type\n         self.use_cache = use_cache\n         self.classifier_dropout = classifier_dropout\n         # additional properties\n@@ -154,17 +150,5 @@ def __init__(\n         self.subs_pad_id = subs_pad_id\n         self.xpath_unit_hidden_size = xpath_unit_hidden_size\n \n-    @property\n-    def position_embedding_type(self):\n-        warnings.warn(\n-            \"The `position_embedding_type` attribute is deprecated and will be removed in v4.55.\",\n-            FutureWarning,\n-        )\n-        return self._position_embedding_type\n-\n-    @position_embedding_type.setter\n-    def position_embedding_type(self, value):\n-        self._position_embedding_type = value\n-\n \n __all__ = [\"MarkupLMConfig\"]"
        },
        {
            "sha": "119f4a4d1afbc7972f56292ee0b4c534d3f4d228",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -1098,10 +1098,6 @@ def forward(\n             )\n             use_cache = False\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "27c08626115da1c27489ddb59f2b19c1999fc774",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -1275,10 +1275,6 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "277d601df04b1aabb63a35ecfbe810ef669045d7",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -788,10 +788,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "4c7cfd236ac5402a07487afd18ca999d978368bb",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -472,10 +472,6 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "295af2a6c736c0b38ce90d9fbf958aca8545feb9",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -974,10 +974,6 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "b3303b363daed5f8570c58902aefe123b45bb73c",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -205,18 +205,7 @@ def forward(\n         query_states, key_states, value_states = (\n             self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n         )\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n         query_states = query_states.transpose(0, 1).unsqueeze(0)"
        },
        {
            "sha": "f9540485e65606b5521e3889258ef57360e28c0a",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -823,10 +823,6 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "bb9fda02df18792d3cbb52e8b0353c3edad45e16",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -347,18 +347,7 @@ def forward(\n         query_states, key_states, value_states = (\n             self.qkv(hidden_states).reshape(seq_length, 3, self.num_heads, -1).permute(1, 0, 2, 3).unbind(0)\n         )\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `rotary_pos_emb` (2D tensor of RoPE theta values), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.54 `rotary_pos_emb` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n \n         query_states = query_states.transpose(0, 1).unsqueeze(0)"
        },
        {
            "sha": "5413f54ee584644d5fa927b154196d5eb6d9518d",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -700,10 +700,6 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n "
        },
        {
            "sha": "38d092e9aa66fa5fa2cf1995793d6540bc0e2cc0",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -141,9 +141,6 @@\n     is_spqr_available,\n     is_sudachi_available,\n     is_sudachi_projection_available,\n-    is_tensorflow_probability_available,\n-    is_tensorflow_text_available,\n-    is_tf2onnx_available,\n     is_tf_available,\n     is_tiktoken_available,\n     is_timm_available,\n@@ -514,14 +511,6 @@ def require_jinja(test_case):\n     return unittest.skipUnless(is_jinja_available(), \"test requires jinja\")(test_case)\n \n \n-def require_tf2onnx(test_case):\n-    logger.warning_once(\n-        \"TensorFlow test-related code, including `require_tf2onnx`, is deprecated and will be removed in \"\n-        \"Transformers v4.55\"\n-    )\n-    return unittest.skipUnless(is_tf2onnx_available(), \"test requires tf2onnx\")(test_case)\n-\n-\n def require_onnx(test_case):\n     return unittest.skipUnless(is_onnx_available(), \"test requires ONNX\")(test_case)\n \n@@ -716,49 +705,13 @@ def require_intel_extension_for_pytorch(test_case):\n     )(test_case)\n \n \n-def require_tensorflow_probability(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires TensorFlow probability.\n-\n-    These tests are skipped when TensorFlow probability isn't installed.\n-\n-    \"\"\"\n-    logger.warning_once(\n-        \"TensorFlow test-related code, including `require_tensorflow_probability`, is deprecated and will be \"\n-        \"removed in Transformers v4.55\"\n-    )\n-    return unittest.skipUnless(is_tensorflow_probability_available(), \"test requires TensorFlow probability\")(\n-        test_case\n-    )\n-\n-\n def require_torchaudio(test_case):\n     \"\"\"\n     Decorator marking a test that requires torchaudio. These tests are skipped when torchaudio isn't installed.\n     \"\"\"\n     return unittest.skipUnless(is_torchaudio_available(), \"test requires torchaudio\")(test_case)\n \n \n-def require_tf(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires TensorFlow. These tests are skipped when TensorFlow isn't installed.\n-    \"\"\"\n-    logger.warning_once(\n-        \"TensorFlow test-related code, including `require_tf`, is deprecated and will be removed in Transformers v4.55\"\n-    )\n-    return unittest.skipUnless(is_tf_available(), \"test requires TensorFlow\")(test_case)\n-\n-\n-def require_flax(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires JAX & Flax. These tests are skipped when one / both are not installed\n-    \"\"\"\n-    logger.warning_once(\n-        \"JAX test-related code, including `require_flax`, is deprecated and will be removed in Transformers v4.55\"\n-    )\n-    return unittest.skipUnless(is_flax_available(), \"test requires JAX & Flax\")(test_case)\n-\n-\n def require_sentencepiece(test_case):\n     \"\"\"\n     Decorator marking a test that requires SentencePiece. These tests are skipped when SentencePiece isn't installed.\n@@ -794,18 +747,6 @@ def require_tokenizers(test_case):\n     return unittest.skipUnless(is_tokenizers_available(), \"test requires tokenizers\")(test_case)\n \n \n-def require_tensorflow_text(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires tensorflow_text. These tests are skipped when tensroflow_text isn't\n-    installed.\n-    \"\"\"\n-    logger.warning_once(\n-        \"TensorFlow test-related code, including `require_tensorflow_text`, is deprecated and will be \"\n-        \"removed in Transformers v4.55\"\n-    )\n-    return unittest.skipUnless(is_tensorflow_text_available(), \"test requires tensorflow_text\")(test_case)\n-\n-\n def require_keras_nlp(test_case):\n     \"\"\"\n     Decorator marking a test that requires keras_nlp. These tests are skipped when keras_nlp isn't installed."
        },
        {
            "sha": "22c7f67972ee61404e2fdd1a62762433d62843e8",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de5cbe8b793368b79d392bb51decc5fc36e9ca6a/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=de5cbe8b793368b79d392bb51decc5fc36e9ca6a",
            "patch": "@@ -303,9 +303,6 @@\n         \"local_attention\",\n         \"local_rope_theta\",\n     ],\n-    # position_embedding_type not used and deprecated. Should be deleted in v4.55\n-    \"LayoutLMConfig\": [\"position_embedding_type\"],\n-    \"MarkupLMConfig\": [\"position_embedding_type\"],\n     \"SmolLM3Config\": [\"no_rope_layer_interval\"],\n     \"Gemma3nVisionConfig\": [\"architecture\", \"do_pooling\", \"model_args\"],  # this is for use in `timm`\n }"
        }
    ],
    "stats": {
        "total": 313,
        "additions": 6,
        "deletions": 307
    }
}