{
    "author": "ArthurZucker",
    "message": "Update ux cb (#39845)\n\n* clenaup\n\n* nits\n\n* updates\n\n* fix logging\n\n* push updates?\n\n* just passexception\n\n* update\n\n* nits\n\n* fix\n\n* add tokencount\n\n* style",
    "sha": "6ea646a03ab6f7ac330218815dfe0941a799c343",
    "files": [
        {
            "sha": "75c133190733a10a52506381a959aa5a77917761",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ea646a03ab6f7ac330218815dfe0941a799c343/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ea646a03ab6f7ac330218815dfe0941a799c343/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=6ea646a03ab6f7ac330218815dfe0941a799c343",
            "patch": "@@ -10,26 +10,27 @@\n torch.set_float32_matmul_precision(\"high\")\n \n model_id = \"meta-llama/Llama-3.2-3b-Instruct\"\n-model = AutoModelForCausalLM.from_pretrained(\n-    model_id, attn_implementation=\"sdpa_paged\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n-).eval()\n+model = (\n+    AutoModelForCausalLM.from_pretrained(\n+        model_id,\n+        attn_implementation=\"paged_attention|kernels-community/flash-attn\",\n+        torch_dtype=torch.bfloat16,\n+    )\n+    .eval()\n+    .cuda()\n+)\n tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n \n generation_config = GenerationConfig(\n     max_new_tokens=512,\n+    # use_cuda_graph=False,\n     eos_token_id=tokenizer.eos_token_id,\n     pad_token_id=tokenizer.pad_token_id,\n-    use_cache=False,\n-    num_blocks=2048,\n-    block_size=128,\n-    do_sample=True,\n-    max_batch_tokens=1024,  # Maximum number of tokens to process in a single batch\n-    scheduler=\"prefill_first\",\n+    do_sample=False,\n )\n \n train_dataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\n-\n-# --- Example 1: Simple Version using generate_batch ---\n+train_dataset = train_dataset.select(range(500))  # Use only 5 examples for the simple version\n print(\"--- Running CB Generation Example ---\")\n \n \n@@ -41,19 +42,21 @@ def tokenize_function(examples):\n simple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\n \n start_time_simple = time.time()\n-# model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\", fullgraph=True)\n+model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n batch_outputs = model.generate_batch(\n     inputs=simple_batch_inputs,\n     generation_config=generation_config,\n )\n end_time_simple = time.time()\n-\n+token_count = 0\n for request in batch_outputs:\n     input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=False)\n     try:\n         output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=False)\n+        token_count += len(batch_outputs[request].generated_tokens[1:])\n     except Exception as e:\n         print(f\"Decoding failed for request {request}: {e}\")\n+        token_count += len(batch_outputs[request].generated_tokens[1:])\n         output_text = tokenizer.decode(batch_outputs[request].generated_tokens[1:], skip_special_tokens=False)\n     if len(output_text) > 0:\n         print(\"-\" * 20)\n@@ -65,7 +68,9 @@ def tokenize_function(examples):\n print(\"--- Finished CB Generation Example ---\\n\\n\")\n \n \n-print(f\"CB generation took: {end_time_simple - start_time_simple:.2f} seconds\")\n+print(\n+    f\"CB generation took: {end_time_simple - start_time_simple:.2f} seconds for {token_count} tokens. {token_count / (end_time_simple - start_time_simple)}tok/s\"\n+)\n \n \n # train_dataset = train_dataset.select(range(5))  # Use only 5 examples for the simple version"
        },
        {
            "sha": "a43b11fb40d3df8d19e6e29d40b05b558719769b",
            "filename": "src/transformers/generation/continuous_batching.py",
            "status": "modified",
            "additions": 90,
            "deletions": 156,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ea646a03ab6f7ac330218815dfe0941a799c343/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ea646a03ab6f7ac330218815dfe0941a799c343/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching.py?ref=6ea646a03ab6f7ac330218815dfe0941a799c343",
            "patch": "@@ -13,9 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import logging\n import queue\n-import statistics\n import threading\n import time\n from abc import ABC, abstractmethod\n@@ -29,11 +27,11 @@\n import torch.nn as nn\n from tokenizers import Tokenizer\n from tokenizers.decoders import DecodeStream\n-from torch.profiler import profile, schedule, tensorboard_trace_handler\n from tqdm import tqdm\n \n from ..configuration_utils import PretrainedConfig\n from ..generation.configuration_utils import GenerationConfig\n+from ..utils.logging import logging\n from ..utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n \n \n@@ -49,9 +47,7 @@ class RequestStatus(Enum):\n     FAILED = \"failed\"\n \n \n-# Setup your logger\n logger = logging.getLogger(__name__)\n-logger.setLevel(logging.WARNING)\n \n \n @dataclass\n@@ -159,8 +155,8 @@ def __init__(\n         generation_config: GenerationConfig,\n         device: torch.device,\n         dtype: torch.dtype = torch.float16,\n+        num_requests: int = 100,\n         layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n-        initial_prompt_shapes: Optional[list[list[int]]] = None,\n         tp_size: Optional[int] = None,\n     ) -> None:\n         \"\"\"Initialize a paged attention cache for efficient memory usage.\n@@ -179,32 +175,40 @@ def __init__(\n             if getattr(config, \"num_key_value_heads\", None) is None\n             else config.num_key_value_heads\n         )\n+        num_key_value_heads = self.num_key_value_heads\n+        if tp_size is not None and tp_size > 1:\n+            if num_key_value_heads % tp_size != 0:\n+                raise ValueError(\n+                    f\"Number of key value heads {num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n+                )\n+            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n+            self.num_key_value_heads //= tp_size\n+\n         self.head_dim = (\n             config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n         )\n         self.num_hidden_layers = config.num_hidden_layers\n \n         # Calculate optimal block size and number if not provided\n         num_blocks = getattr(generation_config, \"num_blocks\", None)\n-        block_size = getattr(generation_config, \"block_size\", None)\n-        if num_blocks is None or block_size is None:\n-            logger.info(\"Calculating optimal block size and number...\")\n-            num_blocks, block_size = compute_optimal_blocks(\n-                device, config, generation_config, initial_prompt_shapes or [], dtype, median_prefill_length=200\n-            )\n-            logger.info(f\"Using calculated num_blocks={num_blocks}, block_size={block_size}\")\n-\n+        block_size = getattr(generation_config, \"block_size\", 32)\n+        max_memory_percent = getattr(generation_config, \"max_memory\", 0.9)\n+        num_blocks, max_batch_tokens = compute_optimal_blocks(\n+            generation_config.max_new_tokens,\n+            block_size=block_size,\n+            head_dim=self.head_dim,\n+            num_layers=self.num_hidden_layers,\n+            num_heads=self.num_key_value_heads,\n+            max_memory_percent=max_memory_percent,\n+            dtype=dtype,\n+            num_blocks=num_blocks,\n+        )\n+        logger.warning(\n+            f\"Using calculated num_blocks={num_blocks}, block_size={block_size}, max concurrent requests {max_batch_tokens}\"\n+        )\n+        self.max_batch_tokens = max_batch_tokens\n         self.block_size = block_size\n         self.num_blocks = num_blocks\n-        num_key_value_heads = self.num_key_value_heads\n-        if tp_size is not None and tp_size > 1:\n-            if num_key_value_heads % tp_size != 0:\n-                raise ValueError(\n-                    f\"Number of key value heads {num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n-                )\n-            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n-            num_key_value_heads //= tp_size\n-\n         self.cache_shape = (num_key_value_heads, num_blocks, self.block_size, self.head_dim)\n \n         self.dtype = dtype\n@@ -249,7 +253,7 @@ def free_blocks(self, request_id: str) -> None:\n             blocks_to_free = self._block_tables.pop(request_id)\n             self._free_blocks.extend(blocks_to_free)\n         else:\n-            logger.warning(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n+            logger.info(f\"Attempted to free blocks for non-existent request_id: {request_id}\")\n \n     def get_num_free_blocks(self) -> int:\n         \"\"\"Returns the number of free blocks available.\"\"\"\n@@ -343,7 +347,7 @@ def schedule_batch(self, token_budget: int) -> list[RequestState]:\n     @traced\n     def has_pending_requests(self) -> bool:\n         \"\"\"Check if there are requests ready to be processed.\"\"\"\n-        return self.active_requests or self.waiting_requests\n+        return len(self.active_requests) or len(self.waiting_requests)\n \n     @abstractmethod\n     def finish_request(self, request_id: str, evict_from_cache: bool = True):\n@@ -595,94 +599,60 @@ def finish_request(self, request_id: str, evict_from_cache: bool = True):\n                 del self.active_requests[request_id]\n \n \n-@traced(standalone=True)\n-def compute_optimal_blocks(\n-    device: torch.device,\n-    config: PretrainedConfig,\n-    generation_config: GenerationConfig,\n-    inputs: list[list[int]],\n-    dtype: torch.dtype = torch.bfloat16,\n-    safety_margin: float = 0.9,\n-    median_prefill_length: Optional[int] = None,\n-):\n-    \"\"\"Calculate optimal number and size of blocks for the KV cache.\n-\n-    Args:\n-        device: The device where the model runs\n-        config: The model configuration\n-        generation_config: The generation configuration\n-        inputs: Sample input sequences to estimate memory requirements\n-        dtype: Data type for cache tensors\n-        safety_margin: Fraction of available memory to use\n-        median_prefill_length: Override for median prefill length calculation\n-\n-    Returns:\n-        Tuple of (num_blocks, block_size)\n-    \"\"\"\n-    # Extract model dimensions\n-    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n-    num_kv_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n-    num_hidden_layers = getattr(config, \"num_hidden_layers\", 40)\n-\n-    # Get available device memory\n-    if device.type == \"cuda\":\n-        device_properties = torch.cuda.get_device_properties(device)\n-        total_memory = device_properties.total_memory\n-        allocated_memory = torch.cuda.memory_allocated(device)\n+def get_device_and_memory():\n+    # Select best available device\n+    if torch.cuda.is_available():\n+        device = torch.device(\"cuda\")\n+        total_memory = torch.cuda.get_device_properties(device).total_memory\n         reserved_memory = torch.cuda.memory_reserved(device)\n-        available_memory = total_memory - max(allocated_memory, reserved_memory)\n-    elif device.type == \"mps\":\n-        logger.warning(\"MPS memory estimation is approximate. Using conservative defaults.\")\n-        return 2048, 256\n-    else:\n-        logger.warning(f\"Unsupported device type {device.type} for optimal block calculation. Using defaults.\")\n-        return 32, 128\n-\n-    # Apply safety margin\n-    available_memory = int(available_memory * safety_margin)\n-    if available_memory <= 0:\n-        logger.warning(\"Not enough available memory. Using minimum configuration.\")\n-        return 8, 128  # Minimum viable configuration\n-\n-    # Calculate memory per token\n-    dtype_size = torch.tensor([], dtype=dtype).element_size()\n-    memory_per_token = 2 * num_kv_heads * head_dim * dtype_size * num_hidden_layers  # For K and V caches\n-\n-    # Estimate sequence length requirements\n-    tokens_to_generate = getattr(generation_config, \"max_new_tokens\") or 20\n-\n-    if median_prefill_length is None and inputs:\n-        non_empty_inputs = [len(seq) for seq in inputs if seq]\n-        median_prefill_length = int(statistics.median(non_empty_inputs)) if non_empty_inputs else 64\n-    elif median_prefill_length is None:\n-        median_prefill_length = 64  # Reasonable default if no inputs provided\n-\n-    # Total sequence length including generated tokens\n-    seq_length = median_prefill_length + tokens_to_generate\n+        allocated_memory = torch.cuda.memory_allocated(device)\n \n-    # Calculate block parameters\n-    MIN_BLOCK_SIZE = 16\n+    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n+        device = torch.device(\"mps\")\n+        # MPS memory reporting (PyTorch 2.0+)\n+        total_memory = torch.mps.driver_allocated_memory()\n+        allocated_memory = total_memory - torch.mps.recommended_max_memory()\n+        reserved_memory = 0  # MPS does not track reserved separately\n \n-    # Estimate number of concurrent sequences\n-    per_sequence_memory = seq_length * memory_per_token\n-    max_concurrent_sequences = max(1, int(available_memory // per_sequence_memory))\n+    else:\n+        device = torch.device(\"cpu\")\n+        total_memory = None\n+        reserved_memory = 0\n+        allocated_memory = 0\n \n-    # Total tokens that can fit in memory\n-    total_tokens = available_memory // memory_per_token\n+    return device, total_memory, reserved_memory, allocated_memory\n \n-    # Calculate block size (rounded to power of 2)\n-    initial_block_size = max(MIN_BLOCK_SIZE, total_tokens // (max_concurrent_sequences * 2))\n-    block_size = 1 << (initial_block_size - 1).bit_length()  # Round to power of 2\n \n-    # Calculate number of blocks\n-    num_blocks = max(1, total_tokens // block_size)\n+@traced(standalone=True)\n+def compute_optimal_blocks(\n+    max_num_tokens,\n+    block_size,\n+    head_dim,\n+    num_heads,\n+    num_layers,\n+    max_memory_percent=0.9,\n+    num_blocks=None,\n+    dtype=torch.float16,\n+):\n+    device, total, reserved, allocated = get_device_and_memory()\n+    available_memory = int((total - max(allocated, reserved)) * max_memory_percent)\n \n-    logger.info(\n-        f\"Optimal cache: {num_blocks} blocks of size {block_size} \"\n-        f\"(can handle ~{num_blocks * block_size // seq_length} sequences of length {seq_length})\"\n+    dtype_size = torch.tensor([], dtype=dtype).element_size()\n+    bytes_per_token = 2 * num_heads * head_dim * dtype_size * num_layers\n+    if num_blocks is not None:\n+        # TODO\n+        max_possible_concurrent_requests = num_blocks * bytes_per_token\n+    # FIXME: forgot to add the inintial prompt length in the mix....\n+    max_possible_concurrent_requests = int(\n+        available_memory // (bytes_per_token * max_num_tokens * max_num_tokens // 4)\n     )\n-\n-    return int(num_blocks), int(block_size)\n+    if max_possible_concurrent_requests <= 0:\n+        logger.warning(\"you are trying to generate a bit too many tokens\")\n+        max_possible_concurrent_requests = 32\n+    max_concurrent_tokens = min(64, max_possible_concurrent_requests)\n+    # FIXME: Optimal means uses all memory\n+    optimal_num_blocks = max(((max_concurrent_tokens * max_num_tokens) // block_size) + 1, 64)\n+    return optimal_num_blocks, max_concurrent_tokens\n \n \n @dataclass\n@@ -775,11 +745,9 @@ def __init__(\n \n         self.requests_in_batch: list[RequestState] = []\n \n-        # Get batch size parameters from generation config\n-        self._configure_batch_parameters()\n-\n         # Set up metrics collector\n-        self.metrics = ContinuousBatchProcessorMetrics(self.max_batch_tokens)\n+        self.max_batch_tokens = cache.max_batch_tokens\n+        self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n \n         self.setup_static_tensors()\n \n@@ -847,25 +815,6 @@ def __repr__(self):\n             + self.get_model_kwargs().__repr__()\n         )\n \n-    @traced(standalone=True)\n-    def _configure_batch_parameters(self):\n-        \"\"\"Set up batch processing parameters based on generation config.\"\"\"\n-        # Calculate total cache capacity\n-        total_cache_tokens = self.cache.num_blocks * self.cache.block_size\n-\n-        # Get or calculate max tokens per batch\n-        user_batch_tokens = getattr(self.generation_config, \"max_batch_tokens\", None)\n-        if user_batch_tokens is not None:\n-            self.max_batch_tokens = user_batch_tokens\n-        else:\n-            # Default to 1/8 of total cache capacity, adjusted for context\n-            self.max_context_len = getattr(self.generation_config, \"max_position_embeddings\", 2048)\n-            recommended_batch_size = min(total_cache_tokens // 8, self.max_context_len)\n-            self.max_batch_tokens = max(64, recommended_batch_size)\n-\n-        # Context length and EOS token\n-        self.max_context_len = getattr(self.generation_config, \"max_position_embeddings\", 2048)\n-\n     @traced\n     def _get_new_requests(self):\n         \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n@@ -1041,6 +990,8 @@ def update_batch(self):\n                 self._maybe_send_output(state, token)\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n+        if self.cache.get_num_free_blocks() == 0:\n+            raise ValueError(\"No more free blocks\")\n \n     @traced\n     def has_pending_requests(self) -> bool:\n@@ -1062,7 +1013,9 @@ def fail_all_requests(self, error):\n         Args:\n             error: The error to report in the failure message\n         \"\"\"\n-        for state in self.scheduler.active_requests.values():\n+\n+        requests = list(self.scheduler.active_requests.values())\n+        for state in requests:\n             self._handle_request_error(error, state)\n             self.scheduler.finish_request(state.request_id)\n \n@@ -1296,6 +1249,7 @@ def _run_generation_loop(self):\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n+                num_requests=len(self.input_queue.queue),\n                 tp_size=getattr(self.model, \"tp_size\"),\n             )\n \n@@ -1324,33 +1278,10 @@ def _run_generation_loop(self):\n             )\n             self.batch_processor = batch_processor\n             is_first = True\n-\n-            if self.profile:\n-                tracing_schedule = schedule(skip_first=2, warmup=1, active=1, repeat=3, wait=1)\n-                trace_handler = tensorboard_trace_handler(\n-                    dir_name=\"/fsx/arthur/transformers\", use_gzip=True, worker_name=\"paged_compile\"\n-                )\n-                activities = [\n-                    torch.profiler.ProfilerActivity.CPU,\n-                    torch.profiler.ProfilerActivity.CUDA,\n-                ]\n-                with profile(\n-                    activities=activities,\n-                    schedule=tracing_schedule,\n-                    on_trace_ready=trace_handler,\n-                    record_shapes=False,\n-                    with_stack=True,\n-                ) as prof:\n-                    while not self.stop_event.is_set() or batch_processor.has_pending_requests():\n-                        self._inner_generation_loop(batch_processor, is_first)\n-                        if is_first:\n-                            is_first = False\n-                        prof.step()\n-            else:\n-                while not self.stop_event.is_set() or batch_processor.has_pending_requests():\n-                    self._inner_generation_loop(batch_processor, is_first)\n-                    if is_first:\n-                        is_first = False\n+            while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n+                self._inner_generation_loop(batch_processor, is_first)\n+                if is_first:\n+                    is_first = False\n \n         except Exception as e:\n             logger.error(f\"Error in generation loop: {e}\", exc_info=True)\n@@ -1363,6 +1294,8 @@ def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor, is_f\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n         batch_processor.prepare_next_batch()\n+        device, total, reserved, allocated = get_device_and_memory()\n+        logger.info(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n         if torch.cuda.is_available() and self.use_cuda_graph:\n             if is_first:\n                 self.warmup(batch_processor)\n@@ -1502,6 +1435,7 @@ def generate_batch(\n                                 results[req_id] = result\n                                 finished_count += 1\n                                 pbar.update(1)\n+                            logger.debug(manager.batch_processor.tokenizer.decode(result.generated_tokens))\n                         else:\n                             if not manager.is_running():\n                                 logger.error(\"Generation thread terminated unexpectedly.\")"
        },
        {
            "sha": "a7bf5ae57717706598a999d995a25e800cb93cda",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ea646a03ab6f7ac330218815dfe0941a799c343/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ea646a03ab6f7ac330218815dfe0941a799c343/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=6ea646a03ab6f7ac330218815dfe0941a799c343",
            "patch": "@@ -4,8 +4,11 @@\n from ..utils import is_flash_attn_2_available\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn import flash_attn_varlen_func  # noqa: F401\n+try:\n+    if is_flash_attn_2_available():\n+        from flash_attn import flash_attn_varlen_func  # noqa: F401\n+except Exception:\n+    pass\n \n \n def paged_attention_forward("
        },
        {
            "sha": "03e9cf531470fc6128fe5b3421c3baffeb7c056f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ea646a03ab6f7ac330218815dfe0941a799c343/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ea646a03ab6f7ac330218815dfe0941a799c343/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6ea646a03ab6f7ac330218815dfe0941a799c343",
            "patch": "@@ -2705,18 +2705,18 @@ def _check_and_adjust_attn_implementation(\n                     kernel_function = partial(attention_wrapper, implementation=kernel)\n                 elif kernel_name is not None:\n                     kernel_function = getattr(kernel, kernel_name)\n-                ALL_ATTENTION_FUNCTIONS.register(applicable_attn_implementation, kernel_function)\n+                ALL_ATTENTION_FUNCTIONS.register(attn_implementation, kernel_function)\n                 ALL_MASK_ATTENTION_FUNCTIONS.register(\n-                    applicable_attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"]\n+                    attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"]\n                 )\n             except Exception as e:\n                 logger.warning_once(\n                     f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n                     \"default attention implementation instead (sdpa if available, eager otherwise).\"\n                 )\n \n-                applicable_attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n-            return applicable_attn_implementation\n+                attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n+            return attn_implementation\n         else:\n             return self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)\n "
        }
    ],
    "stats": {
        "total": 294,
        "additions": 118,
        "deletions": 176
    }
}