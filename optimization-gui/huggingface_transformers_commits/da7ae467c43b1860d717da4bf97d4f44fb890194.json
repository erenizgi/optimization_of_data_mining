{
    "author": "ChengLyu",
    "message": "Fix cache get item return type hints (#37847)\n\nF: Fix cache return hints\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "da7ae467c43b1860d717da4bf97d4f44fb890194",
    "files": [
        {
            "sha": "9ebcd49882c4e1807c73e620cfe57c15fb4fc359",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/da7ae467c43b1860d717da4bf97d4f44fb890194/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da7ae467c43b1860d717da4bf97d4f44fb890194/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=da7ae467c43b1860d717da4bf97d4f44fb890194",
            "patch": "@@ -376,7 +376,7 @@ def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n                 self.key_cache.append(key_states)\n                 self.value_cache.append(value_states)\n \n-    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n+    def __getitem__(self, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n         sequence length.\n@@ -649,7 +649,7 @@ def evict_previous_layer(self, layer_idx: int):\n             self.key_cache[prev_layer_idx] = self.key_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n             self.value_cache[prev_layer_idx] = self.value_cache[prev_layer_idx].to(\"cpu\", non_blocking=True)\n \n-    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n+    def __getitem__(self, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n         \"Gets the cache for this layer to the device. Prefetches the next and evicts the previous layer.\"\n         if layer_idx < len(self):\n             # Evict the previous layer if necessary\n@@ -1473,7 +1473,7 @@ def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n         for layer_idx in range(len(cross_attention_cache.key_cache)):\n             self.is_updated[layer_idx] = bool(cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n-    def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n+    def __getitem__(self, layer_idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n         sequence length."
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}