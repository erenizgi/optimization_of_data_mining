{
    "author": "zhanluxianshen",
    "message": "Fix some code annotation typos. (#37102)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>",
    "sha": "c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf",
    "files": [
        {
            "sha": "10fd9c4bdca19bcda6402be1263300376a61c899",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf",
            "patch": "@@ -1184,7 +1184,7 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     Helper function to recursively take the diff between two nested dictionaries. The resulting diff only contains the\n     values from `dict_a` that are different from values in `dict_b`.\n \n-    dict_b : the default config dictionnary. We want to remove values that are in this one\n+    dict_b : the default config dictionary. We want to remove values that are in this one\n     \"\"\"\n     diff = {}\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}"
        },
        {
            "sha": "b8e6de57f5c958c978d0b4a919dd9702342fc409",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf",
            "patch": "@@ -290,7 +290,7 @@ def get_gguf_hf_weights_map(\n     # hack: ggufs have a different name for cohere\n     if model_type == \"cohere\":\n         model_type = \"command-r\"\n-    if model_type == \"qwen2_moe\":\n+    elif model_type == \"qwen2_moe\":\n         model_type = \"qwen2moe\"\n     arch = None\n     for key, value in MODEL_ARCH_NAMES.items():\n@@ -346,7 +346,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     Args:\n         gguf_checkpoint_path (`str`):\n             The path the to GGUF file to load\n-        return_tensors (`bool`, defaults to `True`):\n+        return_tensors (`bool`, defaults to `False`):\n             Whether to read the tensors from the file and return them. Not doing so is faster\n             and only loads the metadata in memory.\n     \"\"\""
        },
        {
            "sha": "d2ec0629e42146f0ce743c3129c32b58a3fa4f47",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf",
            "patch": "@@ -78,7 +78,7 @@ def from_pretrained(\n                 - a path to a *directory* containing a processor saved using the [`~BarkProcessor.save_pretrained`]\n                   method, e.g., `./my_model_directory/`.\n             speaker_embeddings_dict_path (`str`, *optional*, defaults to `\"speaker_embeddings_path.json\"`):\n-                The name of the `.json` file containing the speaker_embeddings dictionnary located in\n+                The name of the `.json` file containing the speaker_embeddings dictionary located in\n                 `pretrained_model_name_or_path`. If `None`, no speaker_embeddings is loaded.\n             **kwargs\n                 Additional keyword arguments passed along to both\n@@ -105,7 +105,7 @@ def from_pretrained(\n                 logger.warning(\n                     f\"\"\"`{os.path.join(pretrained_processor_name_or_path, speaker_embeddings_dict_path)}` does not exists\n                     , no preloaded speaker embeddings will be used - Make sure to provide a correct path to the json\n-                    dictionnary if wanted, otherwise set `speaker_embeddings_dict_path=None`.\"\"\"\n+                    dictionary if wanted, otherwise set `speaker_embeddings_dict_path=None`.\"\"\"\n                 )\n                 speaker_embeddings = None\n             else:\n@@ -135,7 +135,7 @@ def save_pretrained(\n                 Directory where the tokenizer files and the speaker embeddings will be saved (directory will be created\n                 if it does not exist).\n             speaker_embeddings_dict_path (`str`, *optional*, defaults to `\"speaker_embeddings_path.json\"`):\n-                The name of the `.json` file that will contains the speaker_embeddings nested path dictionnary, if it\n+                The name of the `.json` file that will contains the speaker_embeddings nested path dictionary, if it\n                 exists, and that will be located in `pretrained_model_name_or_path/speaker_embeddings_directory`.\n             speaker_embeddings_directory (`str`, *optional*, defaults to `\"speaker_embeddings/\"`):\n                 The name of the folder in which the speaker_embeddings arrays will be saved.\n@@ -246,7 +246,7 @@ def __call__(\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             voice_preset (`str`, `Dict[np.ndarray]`):\n                 The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g\n-                `\"en_speaker_1\"`, or directly a dictionnary of `np.ndarray` embeddings for each submodel of `Bark`. Or\n+                `\"en_speaker_1\"`, or directly a dictionary of `np.ndarray` embeddings for each submodel of `Bark`. Or\n                 it can be a valid file name of a local `.npz` single voice preset.\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:"
        },
        {
            "sha": "75bd0d75eeeb167823d38ec679dc72ad427c70fa",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=c94c6ed3972d3c9ed62c77a7964ae89f5df85ccf",
            "patch": "@@ -1150,7 +1150,7 @@ def create_and_tag_model_card(\n             The list of tags to add in the model card\n         token (`str`, *optional*):\n             Authentication token, obtained with `huggingface_hub.HfApi.login` method. Will default to the stored token.\n-        ignore_metadata_errors (`str`):\n+        ignore_metadata_errors (`bool`, *optional*, defaults to `False`):\n             If True, errors while parsing the metadata section will be ignored. Some information might be lost during\n             the process. Use it at your own risk.\n     \"\"\""
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}