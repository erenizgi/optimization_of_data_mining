{
    "author": "gante",
    "message": "[cleanup] remove old scripts in `/scripts` ðŸ§¹ ðŸ§¹  (#37676)\n\n* rm old files\n\n* not this one",
    "sha": "0f8c34b0a0c69bda865089198c08d2a1be97d12a",
    "files": [
        {
            "sha": "b24beedcd4fa15bc2536e1dc1be409d7a4d5a355",
            "filename": "scripts/benchmark/trainer-benchmark.py",
            "status": "removed",
            "additions": 0,
            "deletions": 448,
            "changes": 448,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Fbenchmark%2Ftrainer-benchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Fbenchmark%2Ftrainer-benchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fbenchmark%2Ftrainer-benchmark.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,448 +0,0 @@\n-#!/usr/bin/env python\n-\n-# HF Trainer benchmarking tool\n-#\n-# This tool can be used to run and compare multiple dimensions of the HF Trainers args.\n-#\n-# It then prints a report once in github format with all the information that needs to be shared\n-# with others and second time in a console-friendly format, so it's easier to use for tuning things up.\n-#\n-# The main idea is:\n-#\n-#     ./trainer-benchmark.py --base-cmd '<cmd args that don't change>' \\\n-#     --variations '--tf32 0|--tf32 1' '--fp16 0|--fp16 1|--bf16 1' \\\n-#     --target-metric-key train_samples_per_second\n-#\n-# The variations can be any command line argument that you want to compare and not just dtype as in\n-# the example.\n-#\n-# --variations allows you to compare variations in multiple dimensions.\n-#\n-# as the first dimension has 2 options and the second 3 in our example, this will run the trainer 6\n-# times adding one of:\n-#\n-#    1. --tf32 0 --fp16 0\n-#    2. --tf32 0 --fp16 1\n-#    3. --tf32 0 --bf16 1\n-#    4. --tf32 1 --fp16 0\n-#    5. --tf32 1 --fp16 1\n-#    6. --tf32 1 --bf16 1\n-#\n-# and print the results. This is just a cartesian product - and more than 2 dimensions can be used.\n-#\n-# If you want to rely on defaults, this:\n-#    --variations '--tf32 0|--tf32 1' '--fp16 0|--fp16 1|--bf16 1'\n-# is identical to this:\n-#    --variations '--tf32 0|--tf32 1' '|--fp16|--bf16'\n-#\n-# the leading empty variation in the 2nd dimension is a valid variation.\n-#\n-# So here we get the following 6 variations:\n-#\n-#    1. --tf32 0\n-#    2. --tf32 0 --fp16\n-#    3. --tf32 0 --bf16\n-#    4. --tf32 1\n-#    5. --tf32 1 --fp16\n-#    6. --tf32 1 --bf16\n-#\n-# In this particular case we don't know what the default tf32 setting is as it's normally\n-# pytorch-version dependent). That's why it's best to do an explicit setting of each variation:\n-#    `--tf32 0|--tf32 1`\n-#\n-# Here is a full example of a train:\n-#\n-# CUDA_VISIBLE_DEVICES=0 python ./scripts/benchmark/trainer-benchmark.py \\\n-# --base-cmd \\\n-# ' examples/pytorch/translation/run_translation.py --model_name_or_path google-t5/t5-small \\\n-# --output_dir output_dir --do_train --label_smoothing 0.1 --logging_strategy no \\\n-# --save_strategy no --per_device_train_batch_size 32 --max_source_length 512 \\\n-# --max_target_length 512 --num_train_epochs 1 --overwrite_output_dir \\\n-# --source_lang en --target_lang ro --dataset_name wmt16 --dataset_config \"ro-en\" \\\n-# --source_prefix \"translate English to Romanian: \" --warmup_steps 50 \\\n-# --max_train_samples 20000 --dataloader_num_workers 2 ' \\\n-# --target-metric-key train_samples_per_second --repeat-times 1 --variations \\\n-# '|--fp16|--bf16' '--tf32 0|--tf32 1' --report-metric-keys train_loss \\\n-# --repeat-times 1 --base-variation '--tf32 0'\n-#\n-# and here is a possible output:\n-#\n-#\n-# | Variation       |     Train |   Diff |   Train |\n-# |                 |   samples |      % |    loss |\n-# |                 |       per |        |         |\n-# |                 |    second |        |         |\n-# |:----------------|----------:|-------:|--------:|\n-# | --tf32 0        |    285.11 |      0 |    2.51 |\n-# | --tf32 1        |    342.09 |     20 |    2.51 |\n-# | --fp16 --tf32 0 |    423.49 |     49 |    2.51 |\n-# | --fp16 --tf32 1 |    423.13 |     48 |    2.51 |\n-# | --bf16 --tf32 0 |    416.80 |     46 |    2.52 |\n-# | --bf16 --tf32 1 |    415.87 |     46 |    2.52 |\n-#\n-#\n-# So you can quickly compare the different outcomes.\n-#\n-# Typically running each experiment once is enough, but if the environment is unstable you can\n-# re-run each multiple times, e.g., 3 using --repeat-times 3 and it will report the averaged results.\n-#\n-# By default it'll use the lowest result as the base line to use as 100% and then compare the rest to\n-# it as can be seen from the table above, but you can also specify which combination is the one to use as\n-# the baseline, e.g., to change to another entry use: --base-variation '--tf32 1 --fp16 0'\n-#\n-# --target-metric-key is there to tell the program which metrics to compare - the different metric keys are\n-# inside output_dir/all_results.json. e.g., to measure eval performance instead of train use:\n-#    --target-metric-key eval_samples_per_second\n-# but of course you will need to adjust the --base-cmd value in the example to perform evaluation as\n-# well (as currently it doesn't)\n-#\n-\n-import argparse\n-import datetime\n-import io\n-import itertools\n-import json\n-import math\n-import os\n-import platform\n-import re\n-import shlex\n-import subprocess\n-import sys\n-from pathlib import Path\n-from statistics import fmean\n-\n-import pandas as pd\n-import torch\n-from tqdm import tqdm\n-\n-import transformers\n-\n-\n-nan = float(\"nan\")\n-\n-\n-class Tee:\n-    \"\"\"\n-    A helper class to tee print's output into a file.\n-    Usage:\n-    sys.stdout = Tee(filename)\n-    \"\"\"\n-\n-    def __init__(self, filename):\n-        self.stdout = sys.stdout\n-        self.file = open(filename, \"a\")\n-\n-    def __getattr__(self, attr):\n-        return getattr(self.stdout, attr)\n-\n-    def write(self, msg):\n-        self.stdout.write(msg)\n-        # strip tqdm codes\n-        self.file.write(re.sub(r\"^.*\\r\", \"\", msg, 0, re.M))\n-\n-\n-def get_original_command(max_width=80, full_python_path=False):\n-    \"\"\"\n-    Return the original command line string that can be replayed nicely and wrapped for 80 char width.\n-\n-    Args:\n-        max_width (`int`, *optional*, defaults to 80):\n-            The width to wrap for.\n-        full_python_path (`bool`, `optional`, defaults to `False`):\n-             Whether to replicate the full path or just the last segment (i.e. `python`).\n-    \"\"\"\n-\n-    cmd = []\n-\n-    # deal with critical env vars\n-    env_keys = [\"CUDA_VISIBLE_DEVICES\"]\n-    for key in env_keys:\n-        val = os.environ.get(key, None)\n-        if val is not None:\n-            cmd.append(f\"{key}={val}\")\n-\n-    # python executable (not always needed if the script is executable)\n-    python = sys.executable if full_python_path else sys.executable.split(\"/\")[-1]\n-    cmd.append(python)\n-\n-    # now the normal args\n-    cmd += list(map(shlex.quote, sys.argv))\n-\n-    # split up into up to MAX_WIDTH lines with shell multi-line escapes\n-    lines = []\n-    current_line = \"\"\n-    while len(cmd) > 0:\n-        current_line += f\"{cmd.pop(0)} \"\n-        if len(cmd) == 0 or len(current_line) + len(cmd[0]) + 1 > max_width - 1:\n-            lines.append(current_line)\n-            current_line = \"\"\n-    return \"\\\\\\n\".join(lines)\n-\n-\n-def get_base_command(args, output_dir):\n-\n-    # unwrap multi-line input\n-    args.base_cmd = re.sub(r\"[\\\\\\n]+\", \" \", args.base_cmd)\n-\n-    # remove --output_dir if any and set our own\n-    args.base_cmd = re.sub(\"--output_dir\\s+[^\\s]+\", \"\", args.base_cmd)\n-    args.base_cmd += f\" --output_dir {output_dir}\"\n-\n-    # ensure we have --overwrite_output_dir\n-    args.base_cmd = re.sub(\"--overwrite_output_dir\\s+\", \"\", args.base_cmd)\n-    args.base_cmd += \" --overwrite_output_dir\"\n-\n-    return [sys.executable] + shlex.split(args.base_cmd)\n-\n-\n-def process_run_single(id, cmd, variation, output_dir, target_metric_key, metric_keys, verbose):\n-\n-    # Enable to debug everything but the run itself, to do it fast and see the progress.\n-    # This is useful for debugging the output formatting quickly - we can remove it later once\n-    # everybody is happy with the output\n-    if 0:\n-        import random\n-        from time import sleep\n-\n-        sleep(0)\n-        return dict(\n-            {k: random.uniform(0, 100) for k in metric_keys},\n-            **{target_metric_key: random.choice([nan, 10.31, 100.2, 55.6666, 222.22222222])},\n-        )\n-\n-    result = subprocess.run(cmd, capture_output=True, text=True)\n-\n-    if verbose:\n-        print(\"STDOUT\", result.stdout)\n-        print(\"STDERR\", result.stderr)\n-\n-    # save the streams\n-    prefix = variation.replace(\" \", \"-\")\n-    with open(Path(output_dir) / f\"log.{prefix}.stdout.txt\", \"w\") as f:\n-        f.write(result.stdout)\n-    with open(Path(output_dir) / f\"log.{prefix}.stderr.txt\", \"w\") as f:\n-        f.write(result.stderr)\n-\n-    if result.returncode != 0:\n-        if verbose:\n-            print(\"failed\")\n-        return {target_metric_key: nan}\n-\n-    with io.open(f\"{output_dir}/all_results.json\", \"r\", encoding=\"utf-8\") as f:\n-        metrics = json.load(f)\n-\n-    # filter out just the keys we want\n-    return {k: v for k, v in metrics.items() if k in metric_keys}\n-\n-\n-def process_run(\n-    id,\n-    cmd,\n-    variation_key,\n-    variation,\n-    longest_variation_len,\n-    target_metric_key,\n-    report_metric_keys,\n-    repeat_times,\n-    output_dir,\n-    verbose,\n-):\n-    results = []\n-    metrics = []\n-    preamble = f\"{id}: {variation:<{longest_variation_len}}\"\n-    outcome = f\"{preamble}: \"\n-    metric_keys = set(report_metric_keys + [target_metric_key])\n-    for i in tqdm(range(repeat_times), desc=preamble, leave=False):\n-        single_run_metrics = process_run_single(\n-            id, cmd, variation, output_dir, target_metric_key, metric_keys, verbose\n-        )\n-        result = single_run_metrics[target_metric_key]\n-        if not math.isnan(result):\n-            metrics.append(single_run_metrics)\n-            results.append(result)\n-            outcome += \"âœ“\"\n-        else:\n-            outcome += \"âœ˜\"\n-    outcome = f\"\\33[2K\\r{outcome}\"\n-    if len(metrics) > 0:\n-        mean_metrics = {k: fmean([x[k] for x in metrics]) for k in metrics[0].keys()}\n-        mean_target = round(mean_metrics[target_metric_key], 2)\n-        results_str = f\"{outcome} {mean_target}\"\n-        if len(metrics) > 1:\n-            results_str += f\" {tuple(round(x, 2) for x in results)}\"\n-        print(results_str)\n-        mean_metrics[variation_key] = variation\n-        return mean_metrics\n-    else:\n-        print(outcome)\n-        return {variation_key: variation, target_metric_key: nan}\n-\n-\n-def get_versions():\n-    properties = torch.cuda.get_device_properties(torch.device(\"cuda\"))\n-    return f\"\"\"\n-Datetime    : {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n-\n-Software:\n-transformers: {transformers.__version__}\n-torch       : {torch.__version__}\n-cuda        : {torch.version.cuda}\n-python      : {platform.python_version()}\n-\n-Hardware:\n-{torch.cuda.device_count()} GPUs      : {properties.name}, {properties.total_memory/2**30:0.2f}GB\n-\"\"\"\n-\n-\n-def process_results(results, target_metric_key, report_metric_keys, base_variation, output_dir):\n-\n-    df = pd.DataFrame(results)\n-    variation_key = \"variation\"\n-    diff_key = \"diff_%\"\n-\n-    sentinel_value = nan\n-    if base_variation is not None and len(df[df[variation_key] == base_variation]):\n-        # this may still return nan\n-        sentinel_value = df.loc[df[variation_key] == base_variation][target_metric_key].item()\n-    if math.isnan(sentinel_value):\n-        # as a fallback, use the minimal value as the sentinel\n-        sentinel_value = df.loc[df[target_metric_key] != nan][target_metric_key].min()\n-\n-    # create diff column if possible\n-    if not math.isnan(sentinel_value):\n-        df[diff_key] = df.apply(\n-            lambda r: round(100 * (r[target_metric_key] - sentinel_value) / sentinel_value)\n-            if not math.isnan(r[target_metric_key])\n-            else 0,\n-            axis=\"columns\",\n-        )\n-\n-    # re-order columns\n-    cols = [variation_key, target_metric_key, diff_key, *report_metric_keys]\n-    df = df.reindex(cols, axis=\"columns\")  # reorder cols\n-\n-    # capitalize\n-    df = df.rename(str.capitalize, axis=\"columns\")\n-\n-    # make the cols as narrow as possible\n-    df_github = df.rename(lambda c: c.replace(\"_\", \"<br>\"), axis=\"columns\")\n-    df_console = df.rename(lambda c: c.replace(\"_\", \"\\n\"), axis=\"columns\")\n-\n-    report = [\"\", \"Copy between the cut-here-lines and paste as is to github or a forum\"]\n-    report += [\"----------8<-----------------8<--------\"]\n-    report += [\"*** Results:\", df_github.to_markdown(index=False, floatfmt=\".2f\")]\n-    report += [\"```\"]\n-    report += [\"*** Setup:\", get_versions()]\n-    report += [\"*** The benchmark command line was:\", get_original_command()]\n-    report += [\"```\"]\n-    report += [\"----------8<-----------------8<--------\"]\n-    report += [\"*** Results (console):\", df_console.to_markdown(index=False, floatfmt=\".2f\")]\n-\n-    print(\"\\n\\n\".join(report))\n-\n-\n-def main():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\n-        \"--base-cmd\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Base cmd\",\n-    )\n-    parser.add_argument(\n-        \"--variations\",\n-        default=None,\n-        type=str,\n-        nargs=\"+\",\n-        required=True,\n-        help=\"Multi-dimensional variations, example: '|--fp16|--bf16' '|--tf32'\",\n-    )\n-    parser.add_argument(\n-        \"--base-variation\",\n-        default=None,\n-        type=str,\n-        help=\"Baseline variation to compare to. if None the minimal target value will be used to compare against\",\n-    )\n-    parser.add_argument(\n-        \"--target-metric-key\",\n-        default=None,\n-        type=str,\n-        required=True,\n-        help=\"Target metric key in output_dir/all_results.json, e.g., train_samples_per_second\",\n-    )\n-    parser.add_argument(\n-        \"--report-metric-keys\",\n-        default=\"\",\n-        type=str,\n-        help=\"Report metric keys - other metric keys from output_dir/all_results.json to report, e.g., train_loss. Use a single argument e.g., 'train_loss train_samples\",\n-    )\n-    parser.add_argument(\n-        \"--repeat-times\",\n-        default=1,\n-        type=int,\n-        help=\"How many times to re-run each variation - an average will be reported\",\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=\"output_benchmark\",\n-        type=str,\n-        help=\"The output directory where all the benchmark reports will go to and additionally this directory will be used to override --output_dir in the script that is being benchmarked\",\n-    )\n-    parser.add_argument(\n-        \"--verbose\",\n-        default=False,\n-        action=\"store_true\",\n-        help=\"Whether to show the outputs of each run or just the benchmark progress\",\n-    )\n-    args = parser.parse_args()\n-\n-    output_dir = args.output_dir\n-    Path(output_dir).mkdir(exist_ok=True)\n-    base_cmd = get_base_command(args, output_dir)\n-\n-    # split each dimension into its --foo variations\n-    dims = [list(map(str.strip, re.split(r\"\\|\", x))) for x in args.variations]\n-    # build a cartesian product of dimensions and convert those back into cmd-line arg strings,\n-    # while stripping white space for inputs that were empty\n-    variations = list(map(str.strip, map(\" \".join, itertools.product(*dims))))\n-    longest_variation_len = max(len(x) for x in variations)\n-\n-    # split wanted keys\n-    report_metric_keys = args.report_metric_keys.split()\n-\n-    # capture prints into a log file for convenience\n-    report_fn = f\"benchmark-report-{datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}.txt\"\n-    print(f\"\\nNote: each run's output is also logged under {output_dir}/log.*.std*.txt\")\n-    print(f\"and this script's output is also piped into {report_fn}\")\n-\n-    sys.stdout = Tee(report_fn)\n-\n-    print(f\"\\n*** Running {len(variations)} benchmarks:\")\n-    print(f\"Base command: {' '.join(base_cmd)}\")\n-\n-    variation_key = \"variation\"\n-    results = []\n-    for id, variation in enumerate(tqdm(variations, desc=\"Total completion: \", leave=False)):\n-        cmd = base_cmd + variation.split()\n-        results.append(\n-            process_run(\n-                id + 1,\n-                cmd,\n-                variation_key,\n-                variation,\n-                longest_variation_len,\n-                args.target_metric_key,\n-                report_metric_keys,\n-                args.repeat_times,\n-                output_dir,\n-                args.verbose,\n-            )\n-        )\n-\n-    process_results(results, args.target_metric_key, report_metric_keys, args.base_variation, output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "0c9c6025def71458e3944d0f2e083f707819cc09",
            "filename": "scripts/deberta_scrtipt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 85,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Fdeberta_scrtipt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Fdeberta_scrtipt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fdeberta_scrtipt.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,85 +0,0 @@\n-import time\n-\n-import torch\n-\n-from transformers import AutoModel, AutoTokenizer, pipeline\n-\n-\n-test_sentence = 'Do you [MASK] the muffin man?'\n-\n-# for comparison\n-bert = pipeline('fill-mask', model = 'bert-base-uncased')\n-print('\\n'.join([d['sequence'] for d in bert(test_sentence)]))\n-\n-\n-deberta = pipeline('fill-mask', model = 'microsoft/deberta-v3-base', model_kwargs={\"legacy\": False})\n-print('\\n'.join([d['sequence'] for d in deberta(test_sentence)]))\n-\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n-\n-tokenized_dict = tokenizer(\n-    [\"Is this working\",], [\"Not yet\",],\n-    return_tensors=\"pt\"\n-)\n-\n-deberta.model.forward = torch.compile(deberta.model.forward)\n-start=time.time()\n-deberta.model(**tokenized_dict)\n-end=time.time()\n-print(end-start)\n-\n-\n-start=time.time()\n-deberta.model(**tokenized_dict)\n-end=time.time()\n-print(end-start)\n-\n-\n-start=time.time()\n-deberta.model(**tokenized_dict)\n-end=time.time()\n-print(end-start)\n-\n-\n-model = AutoModel.from_pretrained('microsoft/deberta-base')\n-model.config.return_dict = False\n-model.config.output_hidden_states=False\n-input_tuple = (tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n-\n-\n-start=time.time()\n-traced_model = torch.jit.trace(model, input_tuple)\n-end=time.time()\n-print(end-start)\n-\n-\n-start=time.time()\n-traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n-end=time.time()\n-print(end-start)\n-\n-\n-start=time.time()\n-traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n-end=time.time()\n-print(end-start)\n-\n-\n-start=time.time()\n-traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n-end=time.time()\n-print(end-start)\n-\n-\n-start=time.time()\n-traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n-end=time.time()\n-print(end-start)\n-\n-\n-torch.jit.save(traced_model, \"compiled_deberta.pt\")\n-\n-\n-\n-# my_script_module = torch.jit.script(model)"
        },
        {
            "sha": "30983c410164f3b6c96b9a1f69d631515614d724",
            "filename": "scripts/fsmt/convert-allenai-wmt16.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fconvert-allenai-wmt16.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fconvert-allenai-wmt16.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fconvert-allenai-wmt16.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,71 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script acquires data and converts it to fsmt model\n-# it covers:\n-# - allenai/wmt16-en-de-dist-12-1\n-# - allenai/wmt16-en-de-dist-6-1\n-# - allenai/wmt16-en-de-12-1\n-\n-# this script needs to be run from the top level of the transformers repo\n-if [ ! -d \"src/transformers\" ]; then\n-    echo \"Error: This script needs to be run from the top of the transformers repo\"\n-    exit 1\n-fi\n-\n-mkdir data\n-\n-# get data (run once)\n-\n-cd data\n-gdown 'https://drive.google.com/uc?id=1x_G2cjvM1nW5hjAB8-vWxRqtQTlmIaQU'\n-gdown 'https://drive.google.com/uc?id=1oA2aqZlVNj5FarxBlNXEHpBS4lRetTzU'\n-gdown 'https://drive.google.com/uc?id=1Wup2D318QYBFPW_NKI1mfP_hXOfmUI9r'\n-tar -xvzf trans_ende_12-1_0.2.tar.gz\n-tar -xvzf trans_ende-dist_12-1_0.2.tar.gz\n-tar -xvzf trans_ende-dist_6-1_0.2.tar.gz\n-gdown 'https://drive.google.com/uc?id=1mNufoynJ9-Zy1kJh2TA_lHm2squji0i9'\n-gdown 'https://drive.google.com/uc?id=1iO7um-HWoNoRKDtw27YUSgyeubn9uXqj'\n-tar -xvzf wmt16.en-de.deep-shallow.dist.tar.gz\n-tar -xvzf wmt16.en-de.deep-shallow.tar.gz\n-cp wmt16.en-de.deep-shallow/data-bin/dict.*.txt trans_ende_12-1_0.2\n-cp wmt16.en-de.deep-shallow.dist/data-bin/dict.*.txt trans_ende-dist_12-1_0.2\n-cp wmt16.en-de.deep-shallow.dist/data-bin/dict.*.txt trans_ende-dist_6-1_0.2\n-cp wmt16.en-de.deep-shallow/bpecodes trans_ende_12-1_0.2\n-cp wmt16.en-de.deep-shallow.dist/bpecodes trans_ende-dist_12-1_0.2\n-cp wmt16.en-de.deep-shallow.dist/bpecodes trans_ende-dist_6-1_0.2\n-cd -\n-\n-# run conversions and uploads\n-\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/trans_ende-dist_12-1_0.2/checkpoint_top5_average.pt --pytorch_dump_folder_path data/wmt16-en-de-dist-12-1\n-\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/trans_ende-dist_6-1_0.2/checkpoint_top5_average.pt --pytorch_dump_folder_path data/wmt16-en-de-dist-6-1\n-\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/trans_ende_12-1_0.2/checkpoint_top5_average.pt --pytorch_dump_folder_path data/wmt16-en-de-12-1\n-\n-\n-# upload\n-cd data\n-transformers-cli upload -y wmt16-en-de-dist-12-1\n-transformers-cli upload -y wmt16-en-de-dist-6-1\n-transformers-cli upload -y wmt16-en-de-12-1\n-cd -\n-\n-\n-# if updating just small files and not the large models, here is a script to generate the right commands:\n-perl -le 'for $f (@ARGV) { print qq[transformers-cli upload -y $_/$f --filename $_/$f] for (\"wmt16-en-de-dist-12-1\", \"wmt16-en-de-dist-6-1\", \"wmt16-en-de-12-1\")}' vocab-src.json vocab-tgt.json tokenizer_config.json config.json\n-# add/remove files as needed\n-"
        },
        {
            "sha": "ef8fa3d4186de1b25d841fc3f8d0a8c0b7c99995",
            "filename": "scripts/fsmt/convert-allenai-wmt19.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fconvert-allenai-wmt19.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fconvert-allenai-wmt19.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fconvert-allenai-wmt19.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,59 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script acquires data and converts it to fsmt model\n-# it covers:\n-# - allenai/wmt19-de-en-6-6-base\n-# - allenai/wmt19-de-en-6-6-big\n-\n-# this script needs to be run from the top level of the transformers repo\n-if [ ! -d \"src/transformers\" ]; then\n-    echo \"Error: This script needs to be run from the top of the transformers repo\"\n-    exit 1\n-fi\n-\n-mkdir data\n-\n-# get data (run once)\n-\n-cd data\n-gdown 'https://drive.google.com/uc?id=1j6z9fYdlUyOYsh7KJoumRlr1yHczxR5T'\n-gdown 'https://drive.google.com/uc?id=1yT7ZjqfvUYOBXvMjeY8uGRHQFWoSo8Q5'\n-gdown 'https://drive.google.com/uc?id=15gAzHeRUCs-QV8vHeTReMPEh1j8excNE'\n-tar -xvzf wmt19.de-en.tar.gz\n-tar -xvzf wmt19_deen_base_dr0.1_1.tar.gz\n-tar -xvzf wmt19_deen_big_dr0.1_2.tar.gz\n-cp wmt19.de-en/data-bin/dict.*.txt wmt19_deen_base_dr0.1_1\n-cp wmt19.de-en/data-bin/dict.*.txt wmt19_deen_big_dr0.1_2\n-cd -\n-\n-# run conversions and uploads\n-\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/wmt19_deen_base_dr0.1_1/checkpoint_last3_avg.pt --pytorch_dump_folder_path data/wmt19-de-en-6-6-base\n-\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/wmt19_deen_big_dr0.1_2/checkpoint_last3_avg.pt --pytorch_dump_folder_path data/wmt19-de-en-6-6-big\n-\n-\n-# upload\n-cd data\n-transformers-cli upload -y wmt19-de-en-6-6-base\n-transformers-cli upload -y wmt19-de-en-6-6-big\n-cd -\n-\n-\n-# if updating just small files and not the large models, here is a script to generate the right commands:\n-perl -le 'for $f (@ARGV) { print qq[transformers-cli upload -y $_/$f --filename $_/$f] for (\"wmt19-de-en-6-6-base\", \"wmt19-de-en-6-6-big\")}' vocab-src.json vocab-tgt.json tokenizer_config.json config.json\n-# add/remove files as needed\n-"
        },
        {
            "sha": "293522f0e881cdb1636ebc5d85f0f9a9f52c8466",
            "filename": "scripts/fsmt/convert-facebook-wmt19.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fconvert-facebook-wmt19.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fconvert-facebook-wmt19.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fconvert-facebook-wmt19.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,70 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script acquires data and converts it to fsmt model\n-# it covers:\n-# - facebook/wmt19-ru-en\n-# - facebook/wmt19-en-ru\n-# - facebook/wmt19-de-en\n-# - facebook/wmt19-en-de\n-\n-# this script needs to be run from the top level of the transformers repo\n-if [ ! -d \"src/transformers\" ]; then\n-    echo \"Error: This script needs to be run from the top of the transformers repo\"\n-    exit 1\n-fi\n-\n-mkdir data\n-\n-# get data (run once)\n-\n-cd data\n-wget https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-de.joined-dict.ensemble.tar.gz\n-wget https://dl.fbaipublicfiles.com/fairseq/models/wmt19.de-en.joined-dict.ensemble.tar.gz\n-wget https://dl.fbaipublicfiles.com/fairseq/models/wmt19.en-ru.ensemble.tar.gz\n-wget https://dl.fbaipublicfiles.com/fairseq/models/wmt19.ru-en.ensemble.tar.gz\n-tar -xvzf wmt19.en-de.joined-dict.ensemble.tar.gz\n-tar -xvzf wmt19.de-en.joined-dict.ensemble.tar.gz\n-tar -xvzf wmt19.en-ru.ensemble.tar.gz\n-tar -xvzf wmt19.ru-en.ensemble.tar.gz\n-cd -\n-\n-# run conversions and uploads\n-\n-export PAIR=ru-en\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/wmt19.$PAIR.ensemble/model4.pt --pytorch_dump_folder_path data/wmt19-$PAIR\n-\n-export PAIR=en-ru\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/wmt19.$PAIR.ensemble/model4.pt --pytorch_dump_folder_path data/wmt19-$PAIR\n-\n-export PAIR=de-en\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/wmt19.$PAIR.joined-dict.ensemble/model4.pt --pytorch_dump_folder_path data/wmt19-$PAIR\n-\n-export PAIR=en-de\n-PYTHONPATH=\"src\" python src/transformers/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py --fsmt_checkpoint_path data/wmt19.$PAIR.joined-dict.ensemble/model4.pt --pytorch_dump_folder_path data/wmt19-$PAIR\n-\n-\n-# upload\n-cd data\n-transformers-cli upload -y wmt19-ru-en\n-transformers-cli upload -y wmt19-en-ru\n-transformers-cli upload -y wmt19-de-en\n-transformers-cli upload -y wmt19-en-de\n-cd -\n-\n-# if updating just small files and not the large models, here is a script to generate the right commands:\n-perl -le 'for $f (@ARGV) { print qq[transformers-cli upload -y $_/$f --filename $_/$f] for map { \"wmt19-$_\" } (\"en-ru\", \"ru-en\", \"de-en\", \"en-de\")}' vocab-src.json vocab-tgt.json tokenizer_config.json config.json\n-# add/remove files as needed\n-"
        },
        {
            "sha": "3db46e17ce621ea205b9e0e45a7f5e2e83360f26",
            "filename": "scripts/fsmt/eval-allenai-wmt16.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 79,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Feval-allenai-wmt16.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Feval-allenai-wmt16.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Feval-allenai-wmt16.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,79 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script evals the following fsmt models\n-# it covers:\n-# - allenai/wmt16-en-de-dist-12-1\n-# - allenai/wmt16-en-de-dist-6-1\n-# - allenai/wmt16-en-de-12-1\n-\n-# this script needs to be run from the top level of the transformers repo\n-if [ ! -d \"src/transformers\" ]; then\n-    echo \"Error: This script needs to be run from the top of the transformers repo\"\n-    exit 1\n-fi\n-\n-# In these scripts you may have to lower BS if you get CUDA OOM (or increase it if you have a large GPU)\n-\n-### Normal eval ###\n-\n-export PAIR=en-de\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=64\n-export NUM_BEAMS=5\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-\n-MODEL_PATH=allenai/wmt16-en-de-dist-12-1\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-MODEL_PATH=allenai/wmt16-en-de-dist-6-1\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-MODEL_PATH=allenai/wmt16-en-de-12-1\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-\n-\n-### Searching hparams eval ###\n-\n-\n-export PAIR=en-de\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=32\n-export NUM_BEAMS=5\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-\n-MODEL_PATH=allenai/wmt16-en-de-dist-12-1\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:10:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1\"\n-\n-\n-MODEL_PATH=allenai/wmt16-en-de-dist-6-1\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:10:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1\"\n-\n-\n-MODEL_PATH=allenai/wmt16-en-de-12-1\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:10:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1\""
        },
        {
            "sha": "84740e2f5940d237f0aa40f5b4fbac7c1ff86454",
            "filename": "scripts/fsmt/eval-allenai-wmt19.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Feval-allenai-wmt19.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Feval-allenai-wmt19.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Feval-allenai-wmt19.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,67 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script evals the following fsmt models\n-# it covers:\n-# - allenai/wmt19-de-en-6-6-base\n-# - allenai/wmt19-de-en-6-6-big\n-\n-# this script needs to be run from the top level of the transformers repo\n-if [ ! -d \"src/transformers\" ]; then\n-    echo \"Error: This script needs to be run from the top of the transformers repo\"\n-    exit 1\n-fi\n-\n-# In these scripts you may have to lower BS if you get CUDA OOM (or increase it if you have a large GPU)\n-\n-### Normal eval ###\n-\n-export PAIR=de-en\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=64\n-export NUM_BEAMS=5\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-\n-MODEL_PATH=allenai/wmt19-de-en-6-6-base\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-MODEL_PATH=allenai/wmt19-de-en-6-6-big\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-\n-\n-### Searching hparams eval ###\n-\n-export PAIR=de-en\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=16\n-export NUM_BEAMS=5\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-\n-MODEL_PATH=allenai/wmt19-de-en-6-6-base\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:10:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1\"\n-\n-MODEL_PATH=allenai/wmt19-de-en-6-6-big\n-echo $PAIR $MODEL_PATH\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py $MODEL_PATH $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:10:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1\""
        },
        {
            "sha": "4578df1afa91b794c44cb16a85963fdff4cebc82",
            "filename": "scripts/fsmt/eval-facebook-wmt19.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 161,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Feval-facebook-wmt19.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Feval-facebook-wmt19.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Feval-facebook-wmt19.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,161 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script evals the following fsmt models\n-# it covers:\n-# - facebook/wmt19-ru-en\n-# - facebook/wmt19-en-ru\n-# - facebook/wmt19-de-en\n-# - facebook/wmt19-en-de\n-\n-\n-# this script needs to be run from the top level of the transformers repo\n-if [ ! -d \"src/transformers\" ]; then\n-    echo \"Error: This script needs to be run from the top of the transformers repo\"\n-    exit 1\n-fi\n-\n-\n-# In these scripts you may have to lower BS if you get CUDA OOM (or increase it if you have a large GPU)\n-\n-### a short estimate version for quick testing ###\n-\n-export PAIR=en-ru\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=8\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src | head -10 > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref | head -10 > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-\n-\n-### Normal eval ###\n-\n-# ru-en\n-\n-export PAIR=ru-en\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=50\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-\n-# (target BLEU: 41.3 http://matrix.statmt.org/matrix/output/1907?run_id=6937)\n-\n-\n-# en-ru\n-\n-export PAIR=en-ru\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=50\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-# (target BLEU: 36.4 http://matrix.statmt.org/matrix/output/1914?score_id=37605)\n-\n-\n-\n-# en-de\n-\n-export PAIR=en-de\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-# (target BLEU: 43.1 http://matrix.statmt.org/matrix/output/1909?run_id=6862)\n-\n-\n-# de-en\n-\n-export PAIR=de-en\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=50\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-\n-# (target BLEU: 42.3 http://matrix.statmt.org/matrix/output/1902?run_id=6750)\n-\n-\n-### Searching hparams eval ###\n-\n-# en-ru\n-\n-export PAIR=ru-en\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=32\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-CUDA_VISIBLE_DEVICES=\"0\" PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1\"\n-\n-\n-# en-ru\n-\n-export PAIR=en-ru\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=16\n-mkdir -p $DATA_DIR\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-CUDA_VISIBLE_DEVICES=\"0\" PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:8:11:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1 early_stopping=true:false\"\n-\n-# en-de\n-\n-export PAIR=en-de\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=16\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-CUDA_VISIBLE_DEVICES=\"1\" PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:8:11:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1 early_stopping=true:false\"\n-\n-# de-en\n-\n-export PAIR=de-en\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=16\n-mkdir -p $DATA_DIR\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-CUDA_VISIBLE_DEVICES=\"1\" PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval_search.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --search=\"num_beams=5:8:11:15 length_penalty=0.6:0.7:0.8:0.9:1.0:1.1 early_stopping=true:false\""
        },
        {
            "sha": "a70f40ee6ca4d68410ccd0ecb151e0eb8367703b",
            "filename": "scripts/fsmt/fsmt-make-super-tiny-model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Ffsmt-make-super-tiny-model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Ffsmt-make-super-tiny-model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Ffsmt-make-super-tiny-model.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,88 +0,0 @@\n-#!/usr/bin/env python\n-# coding: utf-8\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# This script creates a super tiny model that is useful inside tests, when we just want to test that\n-# the machinery works, without needing to the check the quality of the outcomes.\n-#\n-# This version creates a tiny vocab first, and then a tiny model - so the outcome is truly tiny -\n-# all files ~60KB. As compared to taking a full-size model, reducing to the minimum its layers and\n-# emb dimensions, but keeping the full vocab + merges files, leading to ~3MB in total for all files.\n-# The latter is done by `fsmt-make-super-tiny-model.py`.\n-#\n-# It will be used then as \"stas/tiny-wmt19-en-ru\"\n-\n-import json\n-import tempfile\n-from pathlib import Path\n-\n-from transformers import FSMTConfig, FSMTForConditionalGeneration, FSMTTokenizer\n-from transformers.models.fsmt.tokenization_fsmt import VOCAB_FILES_NAMES\n-\n-\n-mname_tiny = \"tiny-wmt19-en-ru\"\n-\n-# Build\n-\n-# borrowed from a test\n-vocab = [ \"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"i\", \"d\", \"n\", \"w</w>\", \"r</w>\", \"t</w>\", \"lo\", \"low\", \"er</w>\", \"low</w>\", \"lowest</w>\", \"newer</w>\", \"wider</w>\", \"<unk>\", ]\n-vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-merges = [\"l o 123\", \"lo w 1456\", \"e r</w> 1789\", \"\"]\n-\n-with tempfile.TemporaryDirectory() as tmpdirname:\n-    build_dir = Path(tmpdirname)\n-    src_vocab_file = build_dir / VOCAB_FILES_NAMES[\"src_vocab_file\"]\n-    tgt_vocab_file = build_dir / VOCAB_FILES_NAMES[\"tgt_vocab_file\"]\n-    merges_file = build_dir / VOCAB_FILES_NAMES[\"merges_file\"]\n-    with open(src_vocab_file, \"w\") as fp: fp.write(json.dumps(vocab_tokens))\n-    with open(tgt_vocab_file, \"w\") as fp: fp.write(json.dumps(vocab_tokens))\n-    with open(merges_file, \"w\") as fp   : fp.write(\"\\n\".join(merges))\n-\n-    tokenizer = FSMTTokenizer(\n-        langs=[\"en\", \"ru\"],\n-        src_vocab_size = len(vocab),\n-        tgt_vocab_size = len(vocab),\n-        src_vocab_file=src_vocab_file,\n-        tgt_vocab_file=tgt_vocab_file,\n-        merges_file=merges_file,\n-    )\n-\n-config = FSMTConfig(\n-    langs=['ru', 'en'],\n-    src_vocab_size=1000, tgt_vocab_size=1000,\n-    d_model=4,\n-    encoder_layers=1, decoder_layers=1,\n-    encoder_ffn_dim=4, decoder_ffn_dim=4,\n-    encoder_attention_heads=1, decoder_attention_heads=1,\n-)\n-\n-tiny_model = FSMTForConditionalGeneration(config)\n-print(f\"num of params {tiny_model.num_parameters()}\")\n-\n-# Test\n-batch = tokenizer([\"Making tiny model\"], return_tensors=\"pt\")\n-outputs = tiny_model(**batch)\n-\n-print(\"test output:\", len(outputs.logits[0]))\n-\n-# Save\n-tiny_model.half() # makes it smaller\n-tiny_model.save_pretrained(mname_tiny)\n-tokenizer.save_pretrained(mname_tiny)\n-\n-print(f\"Generated {mname_tiny}\")\n-\n-# Upload\n-# transformers-cli upload tiny-wmt19-en-ru"
        },
        {
            "sha": "b737cc61cea3c07eef4390186ea9a17fb66ac927",
            "filename": "scripts/fsmt/fsmt-make-tiny-model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 61,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Ffsmt-make-tiny-model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Ffsmt-make-tiny-model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Ffsmt-make-tiny-model.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,61 +0,0 @@\n-#!/usr/bin/env python\n-# coding: utf-8\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# This script creates a super tiny model that is useful inside tests, when we just want to test that\n-# the machinery works, without needing to the check the quality of the outcomes.\n-#\n-# This version creates a tiny model through reduction of a normal pre-trained model, but keeping the\n-# full vocab, merges file, and thus also resulting in a larger model due to a large vocab size.\n-# This gives ~3MB in total for all files.\n-#\n-# If you want a 50 times smaller than this see `fsmt-make-super-tiny-model.py`, which is slightly more complicated\n-#\n-#\n-# It will be used then as \"stas/tiny-wmt19-en-de\"\n-\n-# Build\n-from transformers import FSMTConfig, FSMTForConditionalGeneration, FSMTTokenizer\n-\n-\n-mname = \"facebook/wmt19-en-de\"\n-tokenizer = FSMTTokenizer.from_pretrained(mname)\n-# get the correct vocab sizes, etc. from the master model\n-config = FSMTConfig.from_pretrained(mname)\n-config.update({\n-    \"d_model\": 4,\n-    \"encoder_layers\": 1, \"decoder_layers\": 1,\n-    \"encoder_ffn_dim\": 4, \"decoder_ffn_dim\": 4,\n-    \"encoder_attention_heads\": 1, \"decoder_attention_heads\": 1})\n-\n-tiny_model = FSMTForConditionalGeneration(config)\n-print(f\"num of params {tiny_model.num_parameters()}\")\n-\n-# Test\n-batch = tokenizer([\"Making tiny model\"], return_tensors=\"pt\")\n-outputs = tiny_model(**batch)\n-\n-print(\"test output:\", len(outputs.logits[0]))\n-\n-# Save\n-mname_tiny = \"tiny-wmt19-en-de\"\n-tiny_model.half() # makes it smaller\n-tiny_model.save_pretrained(mname_tiny)\n-tokenizer.save_pretrained(mname_tiny)\n-\n-print(f\"Generated {mname_tiny}\")\n-\n-# Upload\n-# transformers-cli upload tiny-wmt19-en-de"
        },
        {
            "sha": "1b5fe1cda8b2ca293019142283874641903b4a08",
            "filename": "scripts/fsmt/gen-card-allenai-wmt16.py",
            "status": "removed",
            "additions": 0,
            "deletions": 156,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fgen-card-allenai-wmt16.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fgen-card-allenai-wmt16.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fgen-card-allenai-wmt16.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,156 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# Usage:\n-# ./gen-card-allenai-wmt16.py\n-\n-import os\n-from pathlib import Path\n-\n-\n-def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n-\n-    texts = {\n-        \"en\": \"Machine learning is great, isn't it?\",\n-        \"ru\": \"ÐœÐ°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ - ÑÑ‚Ð¾ Ð·Ð´Ð¾Ñ€Ð¾Ð²Ð¾, Ð½Ðµ Ñ‚Ð°Ðº Ð»Ð¸?\",\n-        \"de\": \"Maschinelles Lernen ist groÃŸartig, nicht wahr?\",\n-    }\n-\n-    # BLUE scores as follows:\n-    # \"pair\": [fairseq, transformers]\n-    scores = {\n-        \"wmt16-en-de-dist-12-1\": [28.3, 27.52],\n-        \"wmt16-en-de-dist-6-1\": [27.4, 27.11],\n-        \"wmt16-en-de-12-1\": [26.9, 25.75],\n-    }\n-    pair = f\"{src_lang}-{tgt_lang}\"\n-\n-    readme = f\"\"\"\n----\n-language:\n-- {src_lang}\n-- {tgt_lang}\n-thumbnail:\n-tags:\n-- translation\n-- wmt16\n-- allenai\n-license: apache-2.0\n-datasets:\n-- wmt16\n-metrics:\n-- bleu\n----\n-\n-# FSMT\n-\n-## Model description\n-\n-This is a ported version of fairseq-based [wmt16 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\n-\n-For more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\n-\n-All 3 models are available:\n-\n-* [wmt16-en-de-dist-12-1](https://huggingface.co/allenai/wmt16-en-de-dist-12-1)\n-* [wmt16-en-de-dist-6-1](https://huggingface.co/allenai/wmt16-en-de-dist-6-1)\n-* [wmt16-en-de-12-1](https://huggingface.co/allenai/wmt16-en-de-12-1)\n-\n-\n-## Intended uses & limitations\n-\n-#### How to use\n-\n-```python\n-from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n-mname = \"allenai/{model_name}\"\n-tokenizer = FSMTTokenizer.from_pretrained(mname)\n-model = FSMTForConditionalGeneration.from_pretrained(mname)\n-\n-input = \"{texts[src_lang]}\"\n-input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n-outputs = model.generate(input_ids)\n-decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n-print(decoded) # {texts[tgt_lang]}\n-\n-```\n-\n-#### Limitations and bias\n-\n-\n-## Training data\n-\n-Pretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\n-\n-## Eval results\n-\n-Here are the BLEU scores:\n-\n-model   | fairseq | transformers\n--------|---------|----------\n-{model_name}  | {scores[model_name][0]} | {scores[model_name][1]}\n-\n-The score is slightly below the score reported in the paper, as the researchers don't use `sacrebleu` and measure the score on tokenized outputs. `transformers` score was measured using `sacrebleu` on detokenized outputs.\n-\n-The score was calculated using this code:\n-\n-```bash\n-git clone https://github.com/huggingface/transformers\n-cd transformers\n-export PAIR={pair}\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=5\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt16 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt16 -l $PAIR --echo ref > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-```\n-\n-## Data Sources\n-\n-- [training, etc.](http://www.statmt.org/wmt16/)\n-- [test set](http://matrix.statmt.org/test_sets/newstest2016.tgz?1504722372)\n-\n-\n-### BibTeX entry and citation info\n-\n-```\n-@misc{{kasai2020deep,\n-    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\n-    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\n-    year={{2020}},\n-    eprint={{2006.10369}},\n-    archivePrefix={{arXiv}},\n-    primaryClass={{cs.CL}}\n-}}\n-```\n-\n-\"\"\"\n-    model_card_dir.mkdir(parents=True, exist_ok=True)\n-    path = os.path.join(model_card_dir, \"README.md\")\n-    print(f\"Generating {path}\")\n-    with open(path, \"w\", encoding=\"utf-8\") as f:\n-        f.write(readme)\n-\n-# make sure we are under the root of the project\n-repo_dir = Path(__file__).resolve().parent.parent.parent\n-model_cards_dir = repo_dir / \"model_cards\"\n-\n-for model_name in [\"wmt16-en-de-dist-12-1\", \"wmt16-en-de-dist-6-1\", \"wmt16-en-de-12-1\"]:\n-    model_card_dir = model_cards_dir / \"allenai\" / model_name\n-    write_model_card(model_card_dir, src_lang=\"en\", tgt_lang=\"de\", model_name=model_name)"
        },
        {
            "sha": "b7d727ff2a149f5a2912af28e0707fb974f9b267",
            "filename": "scripts/fsmt/gen-card-allenai-wmt19.py",
            "status": "removed",
            "additions": 0,
            "deletions": 153,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fgen-card-allenai-wmt19.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fgen-card-allenai-wmt19.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fgen-card-allenai-wmt19.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,153 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# Usage:\n-# ./gen-card-allenai-wmt19.py\n-\n-import os\n-from pathlib import Path\n-\n-\n-def write_model_card(model_card_dir, src_lang, tgt_lang, model_name):\n-\n-    texts = {\n-        \"en\": \"Machine learning is great, isn't it?\",\n-        \"ru\": \"ÐœÐ°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ - ÑÑ‚Ð¾ Ð·Ð´Ð¾Ñ€Ð¾Ð²Ð¾, Ð½Ðµ Ñ‚Ð°Ðº Ð»Ð¸?\",\n-        \"de\": \"Maschinelles Lernen ist groÃŸartig, nicht wahr?\",\n-    }\n-\n-    # BLUE scores as follows:\n-    # \"pair\": [fairseq, transformers]\n-    scores = {\n-        \"wmt19-de-en-6-6-base\": [0, 38.37],\n-        \"wmt19-de-en-6-6-big\": [0, 39.90],\n-    }\n-    pair = f\"{src_lang}-{tgt_lang}\"\n-\n-    readme = f\"\"\"\n----\n-\n-language:\n-- {src_lang}\n-- {tgt_lang}\n-thumbnail:\n-tags:\n-- translation\n-- wmt19\n-- allenai\n-license: apache-2.0\n-datasets:\n-- wmt19\n-metrics:\n-- bleu\n----\n-\n-# FSMT\n-\n-## Model description\n-\n-This is a ported version of fairseq-based [wmt19 transformer](https://github.com/jungokasai/deep-shallow/) for {src_lang}-{tgt_lang}.\n-\n-For more details, please, see [Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation](https://arxiv.org/abs/2006.10369).\n-\n-2 models are available:\n-\n-* [wmt19-de-en-6-6-big](https://huggingface.co/allenai/wmt19-de-en-6-6-big)\n-* [wmt19-de-en-6-6-base](https://huggingface.co/allenai/wmt19-de-en-6-6-base)\n-\n-\n-## Intended uses & limitations\n-\n-#### How to use\n-\n-```python\n-from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n-mname = \"allenai/{model_name}\"\n-tokenizer = FSMTTokenizer.from_pretrained(mname)\n-model = FSMTForConditionalGeneration.from_pretrained(mname)\n-\n-input = \"{texts[src_lang]}\"\n-input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n-outputs = model.generate(input_ids)\n-decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n-print(decoded) # {texts[tgt_lang]}\n-\n-```\n-\n-#### Limitations and bias\n-\n-\n-## Training data\n-\n-Pretrained weights were left identical to the original model released by allenai. For more details, please, see the [paper](https://arxiv.org/abs/2006.10369).\n-\n-## Eval results\n-\n-Here are the BLEU scores:\n-\n-model   |  transformers\n--------|---------\n-{model_name}  |  {scores[model_name][1]}\n-\n-The score was calculated using this code:\n-\n-```bash\n-git clone https://github.com/huggingface/transformers\n-cd transformers\n-export PAIR={pair}\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=5\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py allenai/{model_name} $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-```\n-\n-## Data Sources\n-\n-- [training, etc.](http://www.statmt.org/wmt19/)\n-- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\n-\n-\n-### BibTeX entry and citation info\n-\n-```\n-@misc{{kasai2020deep,\n-    title={{Deep Encoder, Shallow Decoder: Reevaluating the Speed-Quality Tradeoff in Machine Translation}},\n-    author={{Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah A. Smith}},\n-    year={{2020}},\n-    eprint={{2006.10369}},\n-    archivePrefix={{arXiv}},\n-    primaryClass={{cs.CL}}\n-}}\n-```\n-\n-\"\"\"\n-    model_card_dir.mkdir(parents=True, exist_ok=True)\n-    path = os.path.join(model_card_dir, \"README.md\")\n-    print(f\"Generating {path}\")\n-    with open(path, \"w\", encoding=\"utf-8\") as f:\n-        f.write(readme)\n-\n-# make sure we are under the root of the project\n-repo_dir = Path(__file__).resolve().parent.parent.parent\n-model_cards_dir = repo_dir / \"model_cards\"\n-\n-for model_name in [\"wmt19-de-en-6-6-base\", \"wmt19-de-en-6-6-big\"]:\n-    model_card_dir = model_cards_dir / \"allenai\" / model_name\n-    write_model_card(model_card_dir, src_lang=\"de\", tgt_lang=\"en\", model_name=model_name)"
        },
        {
            "sha": "58df676cbc9493dfd7d589b6b725f51ba2fc548c",
            "filename": "scripts/fsmt/gen-card-facebook-wmt19.py",
            "status": "removed",
            "additions": 0,
            "deletions": 165,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fgen-card-facebook-wmt19.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fgen-card-facebook-wmt19.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fgen-card-facebook-wmt19.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,165 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# Usage:\n-# ./gen-card-facebook-wmt19.py\n-\n-import os\n-from pathlib import Path\n-\n-\n-def write_model_card(model_card_dir, src_lang, tgt_lang):\n-\n-    texts = {\n-        \"en\": \"Machine learning is great, isn't it?\",\n-        \"ru\": \"ÐœÐ°ÑˆÐ¸Ð½Ð½Ð¾Ðµ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ - ÑÑ‚Ð¾ Ð·Ð´Ð¾Ñ€Ð¾Ð²Ð¾, Ð½Ðµ Ñ‚Ð°Ðº Ð»Ð¸?\",\n-        \"de\": \"Maschinelles Lernen ist groÃŸartig, oder?\",\n-    }\n-\n-    # BLUE scores as follows:\n-    # \"pair\": [fairseq, transformers]\n-    scores = {\n-        \"ru-en\": [\"[41.3](http://matrix.statmt.org/matrix/output/1907?run_id=6937)\", \"39.20\"],\n-        \"en-ru\": [\"[36.4](http://matrix.statmt.org/matrix/output/1914?run_id=6724)\", \"33.47\"],\n-        \"en-de\": [\"[43.1](http://matrix.statmt.org/matrix/output/1909?run_id=6862)\", \"42.83\"],\n-        \"de-en\": [\"[42.3](http://matrix.statmt.org/matrix/output/1902?run_id=6750)\", \"41.35\"],\n-    }\n-    pair = f\"{src_lang}-{tgt_lang}\"\n-\n-    readme = f\"\"\"\n----\n-language:\n-- {src_lang}\n-- {tgt_lang}\n-thumbnail:\n-tags:\n-- translation\n-- wmt19\n-- facebook\n-license: apache-2.0\n-datasets:\n-- wmt19\n-metrics:\n-- bleu\n----\n-\n-# FSMT\n-\n-## Model description\n-\n-This is a ported version of [fairseq wmt19 transformer](https://github.com/pytorch/fairseq/blob/master/examples/wmt19/README.md) for {src_lang}-{tgt_lang}.\n-\n-For more details, please see, [Facebook FAIR's WMT19 News Translation Task Submission](https://arxiv.org/abs/1907.06616).\n-\n-The abbreviation FSMT stands for FairSeqMachineTranslation\n-\n-All four models are available:\n-\n-* [wmt19-en-ru](https://huggingface.co/facebook/wmt19-en-ru)\n-* [wmt19-ru-en](https://huggingface.co/facebook/wmt19-ru-en)\n-* [wmt19-en-de](https://huggingface.co/facebook/wmt19-en-de)\n-* [wmt19-de-en](https://huggingface.co/facebook/wmt19-de-en)\n-\n-## Intended uses & limitations\n-\n-#### How to use\n-\n-```python\n-from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n-mname = \"facebook/wmt19-{src_lang}-{tgt_lang}\"\n-tokenizer = FSMTTokenizer.from_pretrained(mname)\n-model = FSMTForConditionalGeneration.from_pretrained(mname)\n-\n-input = \"{texts[src_lang]}\"\n-input_ids = tokenizer.encode(input, return_tensors=\"pt\")\n-outputs = model.generate(input_ids)\n-decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n-print(decoded) # {texts[tgt_lang]}\n-\n-```\n-\n-#### Limitations and bias\n-\n-- The original (and this ported model) doesn't seem to handle well inputs with repeated sub-phrases, [content gets truncated](https://discuss.huggingface.co/t/issues-with-translating-inputs-containing-repeated-phrases/981)\n-\n-## Training data\n-\n-Pretrained weights were left identical to the original model released by fairseq. For more details, please, see the [paper](https://arxiv.org/abs/1907.06616).\n-\n-## Eval results\n-\n-pair   | fairseq | transformers\n--------|---------|----------\n-{pair}  | {scores[pair][0]} | {scores[pair][1]}\n-\n-The score is slightly below the score reported by `fairseq`, since `transformers`` currently doesn't support:\n-- model ensemble, therefore the best performing checkpoint was ported (``model4.pt``).\n-- re-ranking\n-\n-The score was calculated using this code:\n-\n-```bash\n-git clone https://github.com/huggingface/transformers\n-cd transformers\n-export PAIR={pair}\n-export DATA_DIR=data/$PAIR\n-export SAVE_DIR=data/$PAIR\n-export BS=8\n-export NUM_BEAMS=15\n-mkdir -p $DATA_DIR\n-sacrebleu -t wmt19 -l $PAIR --echo src > $DATA_DIR/val.source\n-sacrebleu -t wmt19 -l $PAIR --echo ref > $DATA_DIR/val.target\n-echo $PAIR\n-PYTHONPATH=\"src:examples/seq2seq\" python examples/seq2seq/run_eval.py facebook/wmt19-$PAIR $DATA_DIR/val.source $SAVE_DIR/test_translations.txt --reference_path $DATA_DIR/val.target --score_path $SAVE_DIR/test_bleu.json --bs $BS --task translation --num_beams $NUM_BEAMS\n-```\n-note: fairseq reports using a beam of 50, so you should get a slightly higher score if re-run with `--num_beams 50`.\n-\n-## Data Sources\n-\n-- [training, etc.](http://www.statmt.org/wmt19/)\n-- [test set](http://matrix.statmt.org/test_sets/newstest2019.tgz?1556572561)\n-\n-\n-### BibTeX entry and citation info\n-\n-```bibtex\n-@inproceedings{{...,\n-  year={{2020}},\n-  title={{Facebook FAIR's WMT19 News Translation Task Submission}},\n-  author={{Ng, Nathan and Yee, Kyra and Baevski, Alexei and Ott, Myle and Auli, Michael and Edunov, Sergey}},\n-  booktitle={{Proc. of WMT}},\n-}}\n-```\n-\n-\n-## TODO\n-\n-- port model ensemble (fairseq uses 4 model checkpoints)\n-\n-\"\"\"\n-    os.makedirs(model_card_dir, exist_ok=True)\n-    path = os.path.join(model_card_dir, \"README.md\")\n-    print(f\"Generating {path}\")\n-    with open(path, \"w\", encoding=\"utf-8\") as f:\n-        f.write(readme)\n-\n-# make sure we are under the root of the project\n-repo_dir = Path(__file__).resolve().parent.parent.parent\n-model_cards_dir = repo_dir / \"model_cards\"\n-\n-for model_name in [\"wmt19-ru-en\", \"wmt19-en-ru\", \"wmt19-en-de\", \"wmt19-de-en\"]:\n-    base, src_lang, tgt_lang = model_name.split(\"-\")\n-    model_card_dir = model_cards_dir / \"facebook\" / model_name\n-    write_model_card(model_card_dir, src_lang=src_lang, tgt_lang=tgt_lang)"
        },
        {
            "sha": "1041ca25d8df4fc96408a868abecedeed8c8a048",
            "filename": "scripts/fsmt/s3-move.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fs3-move.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Fs3-move.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Fs3-move.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,116 +0,0 @@\n-\n-# this is the process of uploading the updated models to s3. As I can't upload them directly to the correct orgs, this script shows how this is done\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-1. upload updated models to my account\n-\n-transformers-cli upload -y wmt19-ru-en\n-transformers-cli upload -y wmt19-en-ru\n-transformers-cli upload -y wmt19-de-en\n-transformers-cli upload -y wmt19-en-de\n-transformers-cli upload -y wmt19-de-en-6-6-base\n-transformers-cli upload -y wmt19-de-en-6-6-big\n-transformers-cli upload -y wmt16-en-de-dist-12-1\n-transformers-cli upload -y wmt16-en-de-dist-6-1\n-transformers-cli upload -y wmt16-en-de-12-1\n-\n-\n-2. ask someone to move them to:\n-\n-* to facebook: \"wmt19-ru-en\", \"wmt19-en-ru\", \"wmt19-en-de\", \"wmt19-de-en\"\n-* to allenai: \"wmt16-en-de-dist-12-1\", \"wmt16-en-de-dist-6-1\", \"wmt16-en-de-12-1\", \"wmt19-de-en-6-6-base\", \"wmt19-de-en-6-6-big\"\n-\n-export b=\"s3://models.huggingface.co/bert\"\n-stas_to_fb () {\n-\tsrc=$1\n-\tshift\n-\taws s3 sync $b/stas/$src $b/facebook/$src $@\n-}\n-\n-stas_to_allenai () {\n-\tsrc=$1\n-\tshift\n-\taws s3 sync $b/stas/$src $b/allenai/$src $@\n-}\n-\n-stas_to_fb wmt19-en-ru\n-stas_to_fb wmt19-ru-en\n-stas_to_fb wmt19-en-de\n-stas_to_fb wmt19-de-en\n-\n-stas_to_allenai wmt16-en-de-dist-12-1\n-stas_to_allenai wmt16-en-de-dist-6-1\n-stas_to_allenai wmt16-en-de-6-1\n-stas_to_allenai wmt16-en-de-12-1\n-stas_to_allenai wmt19-de-en-6-6-base\n-stas_to_allenai wmt19-de-en-6-6-big\n-\n-\n-3. and then remove all these model files from my account\n-\n-transformers-cli s3 rm wmt16-en-de-12-1/config.json\n-transformers-cli s3 rm wmt16-en-de-12-1/merges.txt\n-transformers-cli s3 rm wmt16-en-de-12-1/pytorch_model.bin\n-transformers-cli s3 rm wmt16-en-de-12-1/tokenizer_config.json\n-transformers-cli s3 rm wmt16-en-de-12-1/vocab-src.json\n-transformers-cli s3 rm wmt16-en-de-12-1/vocab-tgt.json\n-transformers-cli s3 rm wmt16-en-de-dist-12-1/config.json\n-transformers-cli s3 rm wmt16-en-de-dist-12-1/merges.txt\n-transformers-cli s3 rm wmt16-en-de-dist-12-1/pytorch_model.bin\n-transformers-cli s3 rm wmt16-en-de-dist-12-1/tokenizer_config.json\n-transformers-cli s3 rm wmt16-en-de-dist-12-1/vocab-src.json\n-transformers-cli s3 rm wmt16-en-de-dist-12-1/vocab-tgt.json\n-transformers-cli s3 rm wmt16-en-de-dist-6-1/config.json\n-transformers-cli s3 rm wmt16-en-de-dist-6-1/merges.txt\n-transformers-cli s3 rm wmt16-en-de-dist-6-1/pytorch_model.bin\n-transformers-cli s3 rm wmt16-en-de-dist-6-1/tokenizer_config.json\n-transformers-cli s3 rm wmt16-en-de-dist-6-1/vocab-src.json\n-transformers-cli s3 rm wmt16-en-de-dist-6-1/vocab-tgt.json\n-transformers-cli s3 rm wmt19-de-en-6-6-base/config.json\n-transformers-cli s3 rm wmt19-de-en-6-6-base/merges.txt\n-transformers-cli s3 rm wmt19-de-en-6-6-base/pytorch_model.bin\n-transformers-cli s3 rm wmt19-de-en-6-6-base/tokenizer_config.json\n-transformers-cli s3 rm wmt19-de-en-6-6-base/vocab-src.json\n-transformers-cli s3 rm wmt19-de-en-6-6-base/vocab-tgt.json\n-transformers-cli s3 rm wmt19-de-en-6-6-big/config.json\n-transformers-cli s3 rm wmt19-de-en-6-6-big/merges.txt\n-transformers-cli s3 rm wmt19-de-en-6-6-big/pytorch_model.bin\n-transformers-cli s3 rm wmt19-de-en-6-6-big/tokenizer_config.json\n-transformers-cli s3 rm wmt19-de-en-6-6-big/vocab-src.json\n-transformers-cli s3 rm wmt19-de-en-6-6-big/vocab-tgt.json\n-transformers-cli s3 rm wmt19-de-en/config.json\n-transformers-cli s3 rm wmt19-de-en/merges.txt\n-transformers-cli s3 rm wmt19-de-en/pytorch_model.bin\n-transformers-cli s3 rm wmt19-de-en/tokenizer_config.json\n-transformers-cli s3 rm wmt19-de-en/vocab-src.json\n-transformers-cli s3 rm wmt19-de-en/vocab-tgt.json\n-transformers-cli s3 rm wmt19-en-de/config.json\n-transformers-cli s3 rm wmt19-en-de/merges.txt\n-transformers-cli s3 rm wmt19-en-de/pytorch_model.bin\n-transformers-cli s3 rm wmt19-en-de/tokenizer_config.json\n-transformers-cli s3 rm wmt19-en-de/vocab-src.json\n-transformers-cli s3 rm wmt19-en-de/vocab-tgt.json\n-transformers-cli s3 rm wmt19-en-ru/config.json\n-transformers-cli s3 rm wmt19-en-ru/merges.txt\n-transformers-cli s3 rm wmt19-en-ru/pytorch_model.bin\n-transformers-cli s3 rm wmt19-en-ru/tokenizer_config.json\n-transformers-cli s3 rm wmt19-en-ru/vocab-src.json\n-transformers-cli s3 rm wmt19-en-ru/vocab-tgt.json\n-transformers-cli s3 rm wmt19-ru-en/config.json\n-transformers-cli s3 rm wmt19-ru-en/merges.txt\n-transformers-cli s3 rm wmt19-ru-en/pytorch_model.bin\n-transformers-cli s3 rm wmt19-ru-en/tokenizer_config.json\n-transformers-cli s3 rm wmt19-ru-en/vocab-src.json\n-transformers-cli s3 rm wmt19-ru-en/vocab-tgt.json"
        },
        {
            "sha": "c4e08039ed10920950a2dd1d8de761dba7e58b55",
            "filename": "scripts/fsmt/tests-to-run.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Ftests-to-run.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ffsmt%2Ftests-to-run.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ffsmt%2Ftests-to-run.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,19 +0,0 @@\n-#!/usr/bin/env bash\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# these scripts need to be run before any changes to FSMT-related code - it should cover all bases\n-\n-CUDA_VISIBLE_DEVICES=\"\" RUN_SLOW=1 pytest --disable-warnings tests/test_tokenization_fsmt.py tests/test_configuration_auto.py tests/test_modeling_fsmt.py examples/seq2seq/test_fsmt_bleu_score.py\n-RUN_SLOW=1 pytest --disable-warnings tests/test_tokenization_fsmt.py tests/test_configuration_auto.py tests/test_modeling_fsmt.py examples/seq2seq/test_fsmt_bleu_score.py"
        },
        {
            "sha": "f223304a7717892d3ef422fa6fc44327f002b5b5",
            "filename": "scripts/pegasus/build_test_sample_spm_no_bos.py",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Fpegasus%2Fbuild_test_sample_spm_no_bos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Fpegasus%2Fbuild_test_sample_spm_no_bos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fpegasus%2Fbuild_test_sample_spm_no_bos.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,34 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# this script builds a small sample spm file tests/fixtures/test_sentencepiece_no_bos.model, with features needed by pegasus\n-\n-# 1. pip install sentencepiece\n-#\n-# 2. wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n-\n-# 3. build\n-import sentencepiece as spm\n-\n-\n-# pegasus:\n-# 1. no bos\n-# 2. eos_id is 1\n-# 3. unk_id is 2\n-# build a sample spm file accordingly\n-spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=test_sentencepiece_no_bos --bos_id=-1 --unk_id=2  --eos_id=1  --vocab_size=1000')\n-\n-# 4. now update the fixture\n-# mv test_sentencepiece_no_bos.model ../../tests/fixtures/"
        },
        {
            "sha": "b142039b246ee62ba0a2f01883112e85881c57f8",
            "filename": "scripts/tatoeba/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 72,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ftatoeba%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ftatoeba%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ftatoeba%2FREADME.md?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,72 +0,0 @@\n-<!---\n-Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-Setup transformers following instructions in README.md, (I would fork first).\n-```bash\n-git clone git@github.com:huggingface/transformers.git\n-cd transformers\n-pip install -e .\n-pip install pandas GitPython wget\n-```\n-\n-Get required metadata\n-```bash\n-curl https://cdn-datasets.huggingface.co/language_codes/language-codes-3b2.csv  > language-codes-3b2.csv\n-curl https://cdn-datasets.huggingface.co/language_codes/iso-639-3.csv > iso-639-3.csv\n-```\n-\n-Install Tatoeba-Challenge repo inside transformers\n-```bash\n-git clone git@github.com:Helsinki-NLP/Tatoeba-Challenge.git\n-```\n-\n-To convert a few models, call the conversion script from command line:\n-```bash\n-python src/transformers/models/marian/convert_marian_tatoeba_to_pytorch.py --models heb-eng eng-heb --save_dir converted\n-```\n-\n-To convert lots of models you can pass your list of Tatoeba model names to `resolver.convert_models` in a python client or script.\n-\n-```python\n-from transformers.convert_marian_tatoeba_to_pytorch import TatoebaConverter\n-resolver = TatoebaConverter(save_dir='converted')\n-resolver.convert_models(['heb-eng', 'eng-heb'])\n-```\n-\n-\n-### Upload converted models\n-Since version v3.5.0, the model sharing workflow is switched to git-based system . Refer to [model sharing doc](https://huggingface.co/transformers/main/model_sharing.html#model-sharing-and-uploading) for more details.\n-\n-To upload all converted models, \n-\n-1. Install [git-lfs](https://git-lfs.github.com/).\n-\n-2. Login to `huggingface-cli`\n-\n-```bash\n-huggingface-cli login\n-```\n-\n-3. Run the `upload_models` script\n-\n-```bash\n-./scripts/tatoeba/upload_models.sh\n-```\n-\n-\n-### Modifications\n-- To change naming logic, change the code near `os.rename`. The model card creation code may also need to change.\n-- To change model card content, you must modify `TatoebaCodeResolver.write_model_card`"
        },
        {
            "sha": "536eb5bc68c4c47cbf152a028af367003aff5349",
            "filename": "scripts/tatoeba/upload_models.sh",
            "status": "removed",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ftatoeba%2Fupload_models.sh",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/scripts%2Ftatoeba%2Fupload_models.sh",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Ftatoeba%2Fupload_models.sh?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -1,12 +0,0 @@\n-#!/bin/bash\n-\n-for FILE in converted/*; do \n-  model_name=`basename $FILE`\n-  huggingface-cli repo create $model_name -y\n-  git clone https://huggingface.co/Helsinki-NLP/$model_name\n-  mv $FILE/* $model_name/\n-  cd $model_name\n-  git add . && git commit -m \"initial commit\" \n-  git push\n-  cd ..\n-done"
        },
        {
            "sha": "40efb04b3535870a0ebf44398e9283d5f54d5130",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f8c34b0a0c69bda865089198c08d2a1be97d12a/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f8c34b0a0c69bda865089198c08d2a1be97d12a/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=0f8c34b0a0c69bda865089198c08d2a1be97d12a",
            "patch": "@@ -163,7 +163,6 @@\n     \"rjieba\",\n     \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n     \"ruff==0.11.2\",\n-    \"sacrebleu>=1.4.12,<2.0.0\",\n     \"sacremoses\",\n     \"safetensors>=0.4.3\",\n     \"sagemaker>=2.31.0\",\n@@ -344,7 +343,6 @@ def run(self):\n         \"evaluate\",\n         \"pytest-timeout\",\n         \"ruff\",\n-        \"sacrebleu\",\n         \"rouge-score\",\n         \"nltk\",\n         \"GitPython\","
        },
        {
            "sha": "84b29b50104897f9a0b603ef3b74e8f7e21f5000",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f8c34b0a0c69bda865089198c08d2a1be97d12a/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f8c34b0a0c69bda865089198c08d2a1be97d12a/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=0f8c34b0a0c69bda865089198c08d2a1be97d12a",
            "patch": "@@ -69,7 +69,6 @@\n     \"rjieba\": \"rjieba\",\n     \"rouge-score\": \"rouge-score!=0.0.7,!=0.0.8,!=0.1,!=0.1.1\",\n     \"ruff\": \"ruff==0.11.2\",\n-    \"sacrebleu\": \"sacrebleu>=1.4.12,<2.0.0\",\n     \"sacremoses\": \"sacremoses\",\n     \"safetensors\": \"safetensors>=0.4.3\",\n     \"sagemaker\": \"sagemaker>=2.31.0\","
        },
        {
            "sha": "fb483d6a512ab390d51c4a807673fda71929cf4a",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f8c34b0a0c69bda865089198c08d2a1be97d12a/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f8c34b0a0c69bda865089198c08d2a1be97d12a/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=0f8c34b0a0c69bda865089198c08d2a1be97d12a",
            "patch": "@@ -112,6 +112,7 @@\n \"\"\"\n \n Here is how to compare BLEU scores against fairseq implementation:\n+(don't forget to install sacrebleu: `pip install sacrebleu`)\n \n # en-ru\n "
        }
    ],
    "stats": {
        "total": 1920,
        "additions": 1,
        "deletions": 1919
    }
}