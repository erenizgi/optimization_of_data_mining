{
    "author": "SunMarc",
    "message": "Fix regression loading dtype (#34409)\n\n* fix regression\r\n\r\n* add test for torchao\r\n\r\n* expected output\r\n\r\n* better fix",
    "sha": "004530aa050efcdd489f1ac6809626fa578636ad",
    "files": [
        {
            "sha": "8481fa7df9cd96c496d24ebf90a1f2b6e5342903",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/004530aa050efcdd489f1ac6809626fa578636ad/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/004530aa050efcdd489f1ac6809626fa578636ad/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=004530aa050efcdd489f1ac6809626fa578636ad",
            "patch": "@@ -943,13 +943,14 @@ def _load_state_dict_into_meta_model(\n         old_param = model\n         splits = param_name.split(\".\")\n         for split in splits:\n-            old_param = getattr(old_param, split)\n-            # Not all the attributes of a module are Parameters/Tensor\n-            if not isinstance(old_param, (torch.nn.Parameter, torch.Tensor)):\n-                old_param = None\n+            # We shouldn't hit the default value unless for quant methods like hqq that modifies expected_keys.\n+            old_param = getattr(old_param, split, None)\n             if old_param is None:\n                 break\n \n+        if not isinstance(old_param, (torch.nn.Parameter, torch.Tensor)):\n+            old_param = None\n+\n         if old_param is not None:\n             if dtype is None:\n                 param = param.to(old_param.dtype)"
        },
        {
            "sha": "c7c701e49aec147314abf9725c17c109bbc2c0aa",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/004530aa050efcdd489f1ac6809626fa578636ad/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/004530aa050efcdd489f1ac6809626fa578636ad/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=004530aa050efcdd489f1ac6809626fa578636ad",
            "patch": "@@ -208,6 +208,26 @@ def test_int4wo_offload(self):\n \n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n+    def test_int8_dynamic_activation_int8_weight_quant(self):\n+        \"\"\"\n+        Simple LLM model testing int8_dynamic_activation_int8_weight\n+        \"\"\"\n+        quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n+\n+        # Note: we quantize the bfloat16 model on the fly to int4\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=torch_device,\n+            quantization_config=quant_config,\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+        self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n+\n \n if __name__ == \"__main__\":\n     unittest.main()"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 25,
        "deletions": 4
    }
}