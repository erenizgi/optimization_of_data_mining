{
    "author": "MekkCyber",
    "message": "[docs] Improve contribution guidelines for Quantization (#42870)\n\n* update\n\n* fix\n\n* nit\n\n* nit",
    "sha": "64c12fdf5f6a35704f2932db533c76973b1e9a30",
    "files": [
        {
            "sha": "1fb06dbab04b8052f3bd195530149c406def4d8c",
            "filename": "docs/source/en/quantization/contribute.md",
            "status": "modified",
            "additions": 95,
            "deletions": 16,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/64c12fdf5f6a35704f2932db533c76973b1e9a30/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/64c12fdf5f6a35704f2932db533c76973b1e9a30/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcontribute.md?ref=64c12fdf5f6a35704f2932db533c76973b1e9a30",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # Contribute\n \n-Transformers supports many quantization methods such as QLoRA, GPTQ, LLM.int8, and AWQ. However, there are still many more quantization approaches that haven't been integrated yet. To make adding and using these quantization methods with Transformers easier, use the [`~quantizers.HfQuantizer`] class.  [`~quantizers.HfQuantizer`] is designed to be an internal helper class for adding a quantization method instead of something applied to every PyTorch module.\n+Transformers supports many quantization methods such as QLoRA, GPTQ, LLM.int8, and AWQ. However, there are still many more quantization approaches that haven't been integrated yet. To make adding and using these quantization methods with Transformers easier, use the [`~quantizers.HfQuantizer`] class. [`~quantizers.HfQuantizer`] is designed to be an internal helper class for adding a quantization method instead of something applied to every PyTorch module.\n \n This guide will show you how to integrate a new quantization method with [`~quantizers.HfQuantizer`].\n \n@@ -28,16 +28,16 @@ Before integrating a new quantization method into Transformers, ensure the metho\n - The method can run on commonly-used hardware (CPU, GPU, etc.).\n - The method is wrapped in a [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) ([`~bitsandbytes.nn.Linear8bitLt`], [`~bitsandbytes.nn.Linear4bit`]), and the quantized linear layer should have the following definition.\n \n-    ```py\n-    class Linear4bit(nn.Module):\n-        def __init__(self, ...):\n-            ...\n-        \n-        def forward(self, x):\n-            return my_4bit_kernel(x, self.weight, self.bias)\n-    ```\n+  ```py\n+  class Linear4bit(nn.Module):\n+      def __init__(self, ...):\n+          ...\n \n-    This way, Transformers models are easily quantized by replacing instances of [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with a target class.\n+      def forward(self, x):\n+          return my_4bit_kernel(x, self.weight, self.bias)\n+  ```\n+\n+  This way, Transformers models are easily quantized by replacing instances of [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) with a target class.\n \n - The quantization method should be serializable. You can save the quantized weights locally or push them to the Hub.\n - Make sure the package containing the quantization kernels/primitive is stable (no frequent breaking changes).\n@@ -48,23 +48,23 @@ Some quantization methods may require \"pre-quantizing\" the model through data ca\n \n 0. The best starting point would be to have a look at another quantization method such as Finegrained Fp8. You will have to update or create three files in total: the [config file](https://github.com/huggingface/transformers/blob/main/src/transformers/utils/quantization_config.py), the [integration file](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/finegrained_fp8.py) and the [quantizer file](https://github.com/huggingface/transformers/blob/main/src/transformers/quantizers/quantizer_finegrained_fp8.py).\n \n-1. Create a new quantization config class inside [src/transformers/utils/quantization_config.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/utils/quantization_config.py). Add the new quantization config to the [_import_structure](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py#L1088) inside Transformers' [src/transformers/__init__.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py) file.\n+1. Create a new quantization config class inside [src/transformers/utils/quantization_config.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/utils/quantization_config.py). Add the new quantization config to the [\\_import_structure](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py#L1088) inside Transformers' [src/transformers/\\_\\_init\\_\\_.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/__init__.py) file.\n \n 2. Create a new file inside [src/transformers/quantizers/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers) named `quantizer_your_method.py`, and make it inherit from [`~quantizers.HfQuantizer]. Make sure to add the new quantizer and quantization config in the quantization auto-mapping in [src/transformers/quantizers/auto.py](https://github.com/huggingface/transformers/blob/abbffc4525566a48a9733639797c812301218b83/src/transformers/quantizers/auto.py).\n \n 3. Define the following class attributes and property methods for your quantization method:\n \n-    - `requires_calibration`: Whether the quantization method requires a data calibration process. If set to `True`, you can only support inference (with quantized weights) and not inference and quantization.\n-    - `is_serializable`: A property method to determine whether the method is serializable or not.\n-    - `is_trainable`:  A property method to determine whether you can fine-tune models on top of the quantization method (with or without PEFT approaches).\n+   - `requires_calibration`: Whether the quantization method requires a data calibration process. If set to `True`, you can only support inference (with quantized weights) and not inference and quantization.\n+   - `is_serializable`: A property method to determine whether the method is serializable or not.\n+   - `is_trainable`: A property method to determine whether you can fine-tune models on top of the quantization method (with or without PEFT approaches).\n \n 4. Write the `validate_environment` and `update_dtype` methods. These methods are called before creating the quantized model to ensure users use the right configuration. Refer to other quantizers for an example of it is implemented.\n \n 5. Write the `_process_model_before_weight_loading` method. In Transformers, the quantized models are initialized first on the `\"meta\"` device before loading the weights. This means the `_process_model_before_weight_loading` method takes care of manipulating the model skeleton to replace some modules ([nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)) with the target modules (quantization modules).\n \n-You can define module replacement logic or any other utility method by creating a new file in [transformers/src/integrations/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/integrations) and exposing the relevant methods in that folder's `__init__.py` file. \n+You can define module replacement logic or any other utility method by creating a new file in [transformers/src/integrations/](https://github.com/huggingface/transformers/tree/abbffc4525566a48a9733639797c812301218b83/src/transformers/integrations) and exposing the relevant methods in that folder's `__init__.py` file.\n \n-6. Add the `get_quantize_ops` method to the quantizer class if the quantization supports quantizing on the fly. In transformers, we materialize each tensor and apply a sequence of different operations on it. In our case, the quantization operation happens at the end. You need to create a `XXXQuantize`,  a subclass of `ConversionOps`, and add a `convert` method. In the `convert` method, you need to quantize the weights and return a dictionary of quantized params.\n+6. Add the `get_quantize_ops` method to the quantizer class if the quantization supports quantizing on the fly. In transformers, we materialize each tensor and apply a sequence of different operations on it. In our case, the quantization operation happens at the end. You need to create a `XXXQuantize`, a subclass of `ConversionOps`, and add a `convert` method. In the `convert` method, you need to quantize the weights and return a dictionary of quantized params.\n \n 7. Add the `get_weight_conversions` method to the quantizer class if the quantization supports loading pre-quantized weights. In transformers, we can collect multiple tensors and apply operations on them. This is particularly useful when we have tensors in the checkpoint that require to be regrouped to re-create the quantized tensors.\n \n@@ -73,3 +73,82 @@ You can define module replacement logic or any other utility method by creating\n 9. Document everything! Make sure your quantization method is documented by adding a new file under `docs/source/en/quantization`.\n \n 10. You should add tests by adding the package in our nightly Dockerfile inside `docker/transformers-quantization-latest-gpu` and then adding a new test file in `tests/quantization/xxx`. Feel free to check out existing quantization methods to see how it is implemented.\n+\n+## Files overview\n+\n+| File                                         | Purpose                                                                                          |\n+| -------------------------------------------- | ------------------------------------------------------------------------------------------------ |\n+| `utils/quantization_config.py`               | Define `YourMethodConfig` inheriting from `QuantizationConfigMixin`                              |\n+| `quantizers/quantizer_your_method.py`        | Implement `YourMethodHfQuantizer` inheriting from `HfQuantizer`                                  |\n+| `integrations/your_method.py`                | Implement `ConversionOps` subclasses and helper functions                                        |\n+| `quantizers/auto.py`                         | Register quantizer and config in `AUTO_QUANTIZER_MAPPING` and `AUTO_QUANTIZATION_CONFIG_MAPPING` |\n+| `docs/source/en/quantization/your_method.md` | Document usage for users                                                                         |\n+| `tests/quantization/your_method/`            | Add integration tests                                                                            |\n+\n+## Understanding `get_quantize_ops` vs `get_weight_conversions`\n+\n+These two methods handle different scenarios for loading weights. Understanding when to use each is essential.\n+\n+### `get_quantize_ops` — Quantize on the fly\n+\n+Use this when loading a **non-quantized checkpoint** (e.g., float16/bfloat16 weights) and quantizing during load.\n+\n+```\n+Checkpoint: model.safetensors (float16 weights for example)\n+     ↓\n+get_quantize_ops → YourQuantize.convert()\n+     ↓\n+Result: Quantized weights in memory\n+```\n+\n+The `convert` method receives one tensor at a time, quantizes it, and can return a dictionary of quantized params, for example:\n+\n+```py\n+class YourQuantize(ConversionOps):\n+    def convert(self, input_dict, model, full_layer_name, missing_keys, **kwargs):\n+        # input_dict = {\"layer.weight\": <float16 tensor>}\n+        value = list(input_dict.values())[0]\n+        module, tensor_name = get_module_from_name(model, full_layer_name)\n+\n+        # Quantize and assign\n+        quantized, scale, zero_point = your_quantize_fn(value)\n+        return {full_layer_name: quantized, full_layer_name + \".scale\": scale, full_layer_name + \".zero_point\": zero_point}\n+```\n+\n+### `get_weight_conversions` — Load pre-quantized checkpoints\n+\n+Use this when loading a **pre-quantized checkpoint** where the quantized weights are saved as several separate components (such as data, scale, and zero point), and these need to be combined into one tensor during loading. Not all quantization methods require this reconstruction step: for example, some methods like FP8 simply load weights and scales as-is, without combining them. Others, such as torchao, do require reassembling the quantized tensor from its multiple saved components.\n+\n+```\n+Checkpoint: model.safetensors (quantized components)\n+  - layer._weight_qdata\n+  - layer._weight_scale\n+  - layer._weight_zero_point\n+     ↓\n+get_weight_conversions → WeightConverter + YourDeserialize.convert()\n+     ↓\n+Result: Reconstructed quantized tensor → layer.weight\n+```\n+\n+The `WeightConverter` collects related tensors based on `source_patterns`, then passes them to your `convert` method:\n+\n+```py\n+def get_weight_conversions(self):\n+    if self.pre_quantized:\n+        return [\n+            WeightConverter(\n+                source_patterns=[\"_weight_qdata\", \"_weight_scale\", \"_weight_zero_point\"],\n+                target_patterns=\"weight\",\n+                operations=[YourDeserialize(self)],\n+            ),\n+        ]\n+    return []\n+\n+\n+class YourDeserialize(ConversionOps):\n+    def convert(self, input_dict, model, full_layer_name, **kwargs):\n+        # input_dict contains all collected tensors\n+        # Reconstruct the quantized tensor from components\n+        reconstructed_tensor = reconstruct_from_components(input_dict)\n+        return {full_layer_name: reconstructed_tensor}\n+```\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 111,
        "additions": 95,
        "deletions": 16
    }
}