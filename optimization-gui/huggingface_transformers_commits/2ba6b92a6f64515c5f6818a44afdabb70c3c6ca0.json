{
    "author": "zucchini-nlp",
    "message": "[VLMs] use only `xxx_token_id` for multimodal tokens (#37573)\n\n* use only `xxx_token_id` for multimodal tokens\n\n* update modeling files as well\n\n* fixup\n\n* why fixup doesn't fix modular docstring first?\n\n* janus, need to update configs in the hub still\n\n* last fixup",
    "sha": "2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
    "files": [
        {
            "sha": "f3faa60ca3d5920ad9f43596719c928c54d48b7b",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -258,6 +258,9 @@ class AriaConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"aria\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AriaTextConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "a95f3cda8349c45da1870720063e381f43d34438",
            "filename": "src/transformers/models/aria/convert_aria_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -106,7 +106,7 @@ def convert_aria_llama_to_hf(text_model_id, vision_model_id, output_hub_path, ol\n     config.vision_config.hidden_size = 1152\n     config.vision_config.attention_heads = 16\n     config.pad_token_id = 2\n-    config.image_token_index = 9\n+    config.image_token_id = 9\n     config.intermediate_size = config.moe_intermediate_size\n     config.auto_map = {\n         \"AutoConfig\": \"modeling_aria.AriaConfig\","
        },
        {
            "sha": "4dc9df7a515de71d4310c79119122d70d63dc27c",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -1507,11 +1507,11 @@ def forward(\n         if pixel_values is not None and inputs_embeds.shape[1] != 1:\n             if input_ids is None:\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n                 n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n             else:\n-                image_embeds = input_ids == self.config.image_token_index\n+                image_embeds = input_ids == self.config.image_token_id\n                 special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n                 n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n             image_features = self.get_image_features("
        },
        {
            "sha": "add5bdc16b764201a73717a3bc6a0981237d1b3f",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -266,6 +266,9 @@ class AriaConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"aria\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AriaTextConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n@@ -1546,11 +1549,11 @@ def forward(\n         if pixel_values is not None and inputs_embeds.shape[1] != 1:\n             if input_ids is None:\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n                 n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n             else:\n-                image_embeds = input_ids == self.config.image_token_index\n+                image_embeds = input_ids == self.config.image_token_id\n                 special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n                 n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n             image_features = self.get_image_features("
        },
        {
            "sha": "ad7fdfd319d183d454c482a76ec2ab9fa00572c6",
            "filename": "src/transformers/models/aya_vision/configuration_aya_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fconfiguration_aya_vision.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -52,6 +52,9 @@ class AyaVisionConfig(PretrainedConfig):\n     \"\"\"\n \n     model_type = \"aya_vision\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "45c2ab66e3e96dcfe2372ee1434bab1eebe52323",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -444,10 +444,10 @@ def forward(\n                 image_sizes=image_sizes,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "db55e39ab7319b7286237922568b925b7b85607a",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -273,6 +273,9 @@ class Blip2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"blip-2\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"qformer_config\": Blip2QFormerConfig, \"vision_config\": Blip2VisionConfig}\n \n     def __init__("
        },
        {
            "sha": "d6ec49505bc9ed4326433881e6c9f390060a768e",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -2283,10 +2283,10 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concating\n-        if getattr(self.config, \"image_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"image_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n@@ -2406,19 +2406,19 @@ def generate(\n \n         if input_ids is None:\n             start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"image_token_index\", None) is not None:\n-                start_tokens = [self.config.image_token_index] * self.config.num_query_tokens + start_tokens\n+            if getattr(self.config, \"image_token_id\", None) is not None:\n+                start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n             input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n         inputs_embeds = self.get_input_embeddings()(input_ids)\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"image_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n         else:\n             logger.warning_once("
        },
        {
            "sha": "6c0e4b9d8098eae3694c0d738a87f84eedd02984",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -285,6 +285,11 @@ class Gemma3Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"gemma3\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"boi_token_id\": \"boi_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n     sub_configs = {\n         \"text_config\": Gemma3TextConfig,\n         \"vision_config\": SiglipVisionConfig,"
        },
        {
            "sha": "316130ce9d161914437ed57f416d2e09d6b6bfab",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -1274,8 +1274,8 @@ def forward(\n         is_training = token_type_ids is not None and labels is not None\n \n         # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n-        if input_ids is not None and self.config.image_token_index >= self.vocab_size:\n-            special_image_mask = input_ids == self.config.image_token_index\n+        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n+            special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()\n             llm_input_ids[special_image_mask] = 0\n         else:\n@@ -1296,10 +1296,10 @@ def forward(\n \n             if input_ids is None:\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n                 special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():"
        },
        {
            "sha": "90e6a4be2ff3402c0330177a7e9d6b1025eb4acf",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -257,6 +257,11 @@ class Gemma3Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"gemma3\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"boi_token_id\": \"boi_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n     sub_configs = {\n         \"text_config\": Gemma3TextConfig,\n         \"vision_config\": SiglipVisionConfig,\n@@ -922,8 +927,8 @@ def forward(\n         is_training = token_type_ids is not None and labels is not None\n \n         # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n-        if input_ids is not None and self.config.image_token_index >= self.vocab_size:\n-            special_image_mask = input_ids == self.config.image_token_index\n+        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n+            special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()\n             llm_input_ids[special_image_mask] = 0\n         else:\n@@ -944,10 +949,10 @@ def forward(\n \n             if input_ids is None:\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n                 special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():"
        },
        {
            "sha": "3e7dd7561f6e5117b5b22dff9e7df6e0982a3a4b",
            "filename": "src/transformers/models/got_ocr2/configuration_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconfiguration_got_ocr2.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -153,6 +153,9 @@ class GotOcr2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"got_ocr2\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": GotOcr2VisionConfig}\n \n     def __init__("
        },
        {
            "sha": "d3a8a637edefe14d29d51de98fe4f9ff471001e6",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -813,13 +813,13 @@ def forward(\n \n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)"
        },
        {
            "sha": "aec8c5e17493ed481cd705c1741f6892fe83ed74",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -176,6 +176,9 @@ class GotOcr2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"got_ocr2\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": GotOcr2VisionConfig}\n \n     def __init__(\n@@ -477,13 +480,13 @@ def forward(\n \n         if pixel_values is not None:\n             image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)"
        },
        {
            "sha": "b3e7e388a1ac52753d403fe071cae257165735bd",
            "filename": "src/transformers/models/granite_speech/configuration_granite_speech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fconfiguration_granite_speech.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -147,6 +147,9 @@ class GraniteSpeechConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"granite_speech\"\n+    attribute_map = {\n+        \"audio_token_id\": \"audio_token_index\",\n+    }\n     sub_configs = {\n         \"text_config\": AutoConfig,\n         \"encoder_config\": GraniteSpeechEncoderConfig,"
        },
        {
            "sha": "55842e7426e1e9f8e7c2c656c10e13aaf47f5e49",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -515,7 +515,7 @@ def forward(\n             # Get the base embeddings; set all audio tokens to 0 index\n             # to avoid out of vocabulary issues with the LLM embedding.\n             # Audio features will be masked into is_audio_idx indices later.\n-            is_audio_idx = input_ids == self.config.audio_token_index\n+            is_audio_idx = input_ids == self.config.audio_token_id\n             llm_input_ids = input_ids.clone()\n             llm_input_ids[is_audio_idx] = 0\n             inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n@@ -624,7 +624,7 @@ def get_merged_audio_embeddings(\n             input_features_mask (`torch.Tensor`, *optional*, defaults to `None`)\n                 Mask to be applied to audio features prior to scattering into the language embeddings.\n         \"\"\"\n-        is_audio_index = input_ids == self.config.audio_token_index\n+        is_audio_index = input_ids == self.config.audio_token_id\n         llm_input_ids = torch.where(is_audio_index, 0, input_ids)\n         inputs_embeds = self.language_model.get_input_embeddings()(llm_input_ids)  # [bsz, # features, hidden size]\n "
        },
        {
            "sha": "04c32d552da128ee61aa80524c71b2e9c7268b8f",
            "filename": "src/transformers/models/instructblip/configuration_instructblip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fconfiguration_instructblip.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -268,6 +268,9 @@ class InstructBlipConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblip\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\n         \"text_config\": AutoConfig,\n         \"qformer_config\": InstructBlipQFormerConfig,"
        },
        {
            "sha": "cdfb59b5804d88b196fa223a948ee849c3e03d61",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -1467,10 +1467,10 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"image_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"image_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n         else:\n             logger.warning_once(\n@@ -1599,8 +1599,8 @@ def generate(\n \n         if input_ids is None:\n             start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"image_token_index\", None) is not None:\n-                start_tokens = [self.config.image_token_index] * self.config.num_query_tokens + start_tokens\n+            if getattr(self.config, \"image_token_id\", None) is not None:\n+                start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n             input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n@@ -1609,10 +1609,10 @@ def generate(\n \n         inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        # if the model already has \"image_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"image_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"image_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once("
        },
        {
            "sha": "d7611395968acda6b1efd552f4a1aac83f579e63",
            "filename": "src/transformers/models/instructblipvideo/configuration_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fconfiguration_instructblipvideo.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -274,6 +274,9 @@ class InstructBlipVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblipvideo\"\n+    attribute_map = {\n+        \"video_token_id\": \"video_token_index\",\n+    }\n     sub_configs = {\n         \"text_config\": AutoConfig,\n         \"qformer_config\": InstructBlipVideoQFormerConfig,"
        },
        {
            "sha": "e9d9e4938a93ae72e62890d436a2b9f214cc6616",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -1495,10 +1495,10 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"video_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"video_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n@@ -1635,8 +1635,8 @@ def generate(\n \n         if input_ids is None:\n             start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"video_token_index\", None) is not None:\n-                start_tokens = [self.config.video_token_index] * self.config.num_query_tokens * 4 + start_tokens\n+            if getattr(self.config, \"video_token_id\", None) is not None:\n+                start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n             input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n@@ -1645,10 +1645,10 @@ def generate(\n \n         inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        # if the model already has \"video_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"video_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once("
        },
        {
            "sha": "212050877a4c0aed92279b10756cf84e7b04795b",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -106,6 +106,9 @@ class InstructBlipVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"instructblipvideo\"\n+    attribute_map = {\n+        \"video_token_id\": \"video_token_index\",\n+    }\n     sub_configs = {\n         \"text_config\": AutoConfig,\n         \"qformer_config\": InstructBlipVideoQFormerConfig,\n@@ -315,10 +318,10 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        # if the model already has \"video_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"video_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once(\n@@ -455,8 +458,8 @@ def generate(\n \n         if input_ids is None:\n             start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"video_token_index\", None) is not None:\n-                start_tokens = [self.config.video_token_index] * self.config.num_query_tokens * 4 + start_tokens\n+            if getattr(self.config, \"video_token_id\", None) is not None:\n+                start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n             input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n             input_ids = input_ids.repeat(batch_size, 1)\n \n@@ -465,10 +468,10 @@ def generate(\n \n         inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        # if the model already has \"video_token_index\" then the input is expanded to account for image embeds\n+        # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n-        if getattr(self.config, \"video_token_index\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1).expand_as(inputs_embeds)\n+        if getattr(self.config, \"video_token_id\", None) is not None:\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n             inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n         else:\n             logger.warning_once("
        },
        {
            "sha": "d90f64b383174dac3d563f5c7256e5bd8cb884ae",
            "filename": "src/transformers/models/janus/configuration_janus.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconfiguration_janus.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -230,6 +230,8 @@ class JanusConfig(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         vq_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `JanusVQVAEConfig`):\n             The config object or dictionary of the VQVAE backbone.\n+        image_token_id (`int`, *optional*, defaults to 100581):\n+            Token index of a placeholder image token.\n \n     Example:\n \n@@ -262,7 +264,14 @@ class JanusConfig(PretrainedConfig):\n         \"vq_config\": JanusVQVAEConfig,\n     }\n \n-    def __init__(self, text_config=None, vision_config=None, vq_config=None, **kwargs):\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        vq_config=None,\n+        image_token_id=100581,\n+        **kwargs,\n+    ):\n         if isinstance(text_config, dict):\n             text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n@@ -307,7 +316,7 @@ def __init__(self, text_config=None, vision_config=None, vq_config=None, **kwarg\n         # This dimension is required when decoding discrete image tokens to continuous input.\n         self.vq_config.num_patches = self.vision_config.image_size // self.vision_config.patch_size\n         # The default is only the index for the 1B model, 7B uses a different one\n-        self.image_token_index = kwargs.get(\"image_token_index\", 100581)\n+        self.image_token_id = image_token_id\n         super().__init__(**kwargs)\n \n "
        },
        {
            "sha": "dc47f4ee8e5a05e10bcc4f6fb51c7bd67ea18594",
            "filename": "src/transformers/models/janus/convert_janus_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -390,7 +390,7 @@ def convert_model(\n         text_config=text_config,\n         vision_config=vision_config,\n         vq_config=vq_config,\n-        image_token_index=tokenizer.vocab.get(\"<image_placeholder>\"),\n+        image_token_id=tokenizer.vocab.get(\"<image_placeholder>\"),\n     )\n \n     # Save the config"
        },
        {
            "sha": "2a0af557f9071337212d9576bc091da4d355d260",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -1281,7 +1281,7 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values)\n-            image_attention_mask = input_ids == self.config.image_token_index\n+            image_attention_mask = input_ids == self.config.image_token_id\n \n             embed_dim = inputs_embeds.shape[-1]\n             image_features = image_embeds.reshape(-1, embed_dim)"
        },
        {
            "sha": "3a0efff5ae24cd7377d587e024ff54bde8d01c03",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -306,6 +306,8 @@ class JanusConfig(PretrainedConfig):\n             The config object or dictionary of the vision backbone.\n         vq_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `JanusVQVAEConfig`):\n             The config object or dictionary of the VQVAE backbone.\n+        image_token_id (`int`, *optional*, defaults to 100581):\n+            Token index of a placeholder image token.\n \n     Example:\n \n@@ -338,7 +340,14 @@ class JanusConfig(PretrainedConfig):\n         \"vq_config\": JanusVQVAEConfig,\n     }\n \n-    def __init__(self, text_config=None, vision_config=None, vq_config=None, **kwargs):\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        vq_config=None,\n+        image_token_id=100581,\n+        **kwargs,\n+    ):\n         if isinstance(text_config, dict):\n             text_config[\"model_type\"] = text_config.get(\"model_type\", \"llama\")\n             self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n@@ -383,7 +392,7 @@ def __init__(self, text_config=None, vision_config=None, vq_config=None, **kwarg\n         # This dimension is required when decoding discrete image tokens to continuous input.\n         self.vq_config.num_patches = self.vision_config.image_size // self.vision_config.patch_size\n         # The default is only the index for the 1B model, 7B uses a different one\n-        self.image_token_index = kwargs.get(\"image_token_index\", 100581)\n+        self.image_token_id = image_token_id\n         super().__init__(**kwargs)\n \n \n@@ -1081,7 +1090,7 @@ def forward(\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values)\n-            image_attention_mask = input_ids == self.config.image_token_index\n+            image_attention_mask = input_ids == self.config.image_token_id\n \n             embed_dim = inputs_embeds.shape[-1]\n             image_features = image_embeds.reshape(-1, embed_dim)"
        },
        {
            "sha": "e296d4e3c68d83365bc1cd20c5ee71d1fe7f3072",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -395,6 +395,11 @@ class Llama4Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llama4\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"boi_token_id\": \"boi_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n     sub_configs = {\"text_config\": Llama4TextConfig, \"vision_config\": Llama4VisionConfig}\n     base_model_tp_plan = {\n         \"multi_modal_projector.linear_1\": \"colwise_rep\","
        },
        {
            "sha": "985c0c0d2f4f766825f1abd86a5486df3f2a61b7",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -1745,7 +1745,7 @@ def forward(\n             vision_flat = image_features.view(-1, image_features.size(-1))\n             projected_vision_flat = self.multi_modal_projector(vision_flat)\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             final_mask = special_image_mask.to(inputs_embeds.device)\n             inputs_embeds = inputs_embeds.view(-1, inputs_embeds.size(-1))\n "
        },
        {
            "sha": "7b6ad8d0e0e12c6a4e66ec4c4c5417b85e566cfa",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -75,6 +75,9 @@ class LlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "dafbf8bf2f686091814f91276ba77eb8cceeb761",
            "filename": "src/transformers/models/llava/convert_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -129,13 +129,13 @@ def convert_llava_llama_to_hf(text_model_id, vision_model_id, output_hub_path, o\n \n     # llms-lab interleeave models do not use any selection startegy except for last hidden state\n     if \"Qwen\" in text_model_id:\n-        config.image_token_index = 151646\n+        config.image_token_id = 151646\n         if \"siglip\" in vision_model_id:\n             config.vision_feature_select_strategy = \"full\"\n             config.vision_feature_layer = -1\n     else:\n         config.pad_token_id = 32001\n-        config.image_token_index = 32000\n+        config.image_token_id = 32000\n \n     with torch.device(\"meta\"):\n         model = LlavaForConditionalGeneration(config)"
        },
        {
            "sha": "bc78b571d95d3b4e55908cbc00ac78f5e7696f0e",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -405,10 +405,10 @@ def forward(\n                 image_sizes=image_sizes,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "8e2ba1db75d1eb1563532d2cb9d72fce0a41199c",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -80,6 +80,9 @@ class LlavaNextConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_next\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "5d6e098d03c6341ade0703a3321366e9085a8d6c",
            "filename": "src/transformers/models/llava_next/convert_llava_next_weights_to_hf.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -102,25 +102,25 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n \n     if model_id == \"liuhaotian/llava-v1.6-mistral-7b\":\n         text_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n-        image_token_index = 32000\n+        image_token_id = 32000\n     elif model_id == \"liuhaotian/llava-v1.6-vicuna-7b\":\n         text_model_id = \"lmsys/vicuna-7b-v1.5\"\n-        image_token_index = 32000\n+        image_token_id = 32000\n     elif model_id == \"liuhaotian/llava-v1.6-vicuna-13b\":\n         text_model_id = \"lmsys/vicuna-13b-v1.5\"\n-        image_token_index = 32000\n+        image_token_id = 32000\n     elif model_id == \"liuhaotian/llava-v1.6-34b\":\n         text_model_id = \"NousResearch/Nous-Hermes-2-Yi-34B\"\n-        image_token_index = 64000\n+        image_token_id = 64000\n     elif model_id == \"lmms-lab/llama3-llava-next-8b\":\n         text_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n-        image_token_index = 128256\n+        image_token_id = 128256\n     elif model_id == \"lmms-lab/llava-next-72b\":\n         text_model_id = \"Qwen/Qwen1.5-72B-Chat\"\n-        image_token_index = 151646\n+        image_token_id = 151646\n     elif model_id == \"lmms-lab/llava-next-110b\":\n         text_model_id = \"Qwen/Qwen1.5-110B-Chat\"\n-        image_token_index = 151646\n+        image_token_id = 151646\n \n     vision_model_id = data[\"mm_vision_tower\"]\n \n@@ -142,7 +142,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         text_config=text_config.to_dict(),\n         image_grid_pinpoints=image_processor.image_grid_pinpoints,\n         use_image_newline_parameter=True,\n-        image_token_index=image_token_index,\n+        image_token_id=image_token_id,\n     )\n \n     with init_empty_weights():\n@@ -225,17 +225,17 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     if model_id == \"liuhaotian/llava-v1.6-mistral-7b\":\n         filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=\"llava_1_6_input_ids.pt\", repo_type=\"dataset\")\n         original_input_ids = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n-        # replace -200 by image_token_index (since we use token ID = 32000 for the image token)\n-        original_input_ids[original_input_ids == -200] = image_token_index\n+        # replace -200 by image_token_id (since we use token ID = 32000 for the image token)\n+        original_input_ids[original_input_ids == -200] = image_token_id\n         assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()\n \n     elif model_id == \"liuhaotian/llava-v1.6-34b\":\n         filepath = hf_hub_download(\n             repo_id=\"nielsr/test-image\", filename=\"llava_1_6_34b_input_ids.pt\", repo_type=\"dataset\"\n         )\n         original_input_ids = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n-        # replace -200 by image_token_index\n-        original_input_ids[original_input_ids == -200] = image_token_index\n+        # replace -200 by image_token_id\n+        original_input_ids[original_input_ids == -200] = image_token_id\n \n         assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()\n "
        },
        {
            "sha": "90cf7ea252b391e6914bfe9ab843c31555417fb1",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -624,10 +624,10 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "8a5f7fecee311153e871e63198809298399522cf",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -88,6 +88,10 @@ class LlavaNextVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_next_video\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "2877b2e9dd8a03407405fc71dda37155665e5d54",
            "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -159,18 +159,18 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n \n     if model_id == \"lmms-lab/LLaVA-NeXT-Video-7B-32K\":\n         text_model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n-        video_token_index = 32000\n-        image_token_index = 32001\n+        video_token_id = 32000\n+        image_token_id = 32001\n         overwrite_text_config = {}\n     elif model_id in [\"lmms-lab/LLaVA-NeXT-Video-7B\", \"lmms-lab/LLaVA-NeXT-Video-7B-DPO\"]:\n         text_model_id = \"lmsys/vicuna-7b-v1.5\"\n-        video_token_index = 32000\n-        image_token_index = 32001\n+        video_token_id = 32000\n+        image_token_id = 32001\n         overwrite_text_config = {\"factor\": 2.0, \"type\": \"linear\"}\n     elif model_id in [\"lmms-lab/LLaVA-NeXT-Video-34B\", \"lmms-lab/LLaVA-NeXT-Video-34B-DPO\"]:\n         text_model_id = \"NousResearch/Nous-Hermes-2-Yi-34B\"\n-        video_token_index = 64000\n-        image_token_index = 64001\n+        video_token_id = 64000\n+        image_token_id = 64001\n         overwrite_text_config = {}\n     else:\n         raise ValueError(\"Incorrect checkpoint referenced. Text model-id not identified!\")\n@@ -199,8 +199,8 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         text_config=text_config,\n         image_grid_pinpoints=image_processor.image_grid_pinpoints,\n         use_image_newline_parameter=True,\n-        video_token_index=video_token_index,\n-        image_token_index=image_token_index,\n+        video_token_id=video_token_id,\n+        image_token_id=image_token_id,\n     )\n \n     with init_empty_weights():"
        },
        {
            "sha": "c89e2d72ef8583fc2a5f616b52aa723ceb5a2cfb",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -703,10 +703,10 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -725,10 +725,10 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "b0b744c5b32ff18ae7d298cbe2779ccefa62ac1c",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -104,6 +104,10 @@ class LlavaNextVideoConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_next_video\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__(\n@@ -489,10 +493,10 @@ def forward(\n                 image_newline=self.image_newline,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -511,10 +515,10 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum().item()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "25f9c30d933bdd30f12a925bbc1299843a1d8612",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -85,6 +85,10 @@ class LlavaOnevisionConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava_onevision\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "dfd43643958fffbd9248f4a0be27295ce57bb6ea",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -700,10 +700,10 @@ def forward(\n                 vision_aspect_ratio=vision_aspect_ratio,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -724,10 +724,10 @@ def forward(\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n \n-            special_video_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n             special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "357ba2cddd4931c689c4f0aacd5a7bd945d72832",
            "filename": "src/transformers/models/mistral3/configuration_mistral3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconfiguration_mistral3.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -68,6 +68,9 @@ class Mistral3Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mistral3\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n     is_composition = True\n "
        },
        {
            "sha": "c8f9b64ab1f6022d9087e6e7a98172cb0d5558d3",
            "filename": "src/transformers/models/mistral3/convert_mistral3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -161,7 +161,7 @@ def convert_config(original_config: dict, max_position_embeddings: int = 131072)\n         vision_config=new_vision_config,\n         text_config=new_text_config,\n         multimodal_projector_bias=adapter_bias,\n-        image_token_index=image_token_id,\n+        image_token_id=image_token_id,\n         spatial_merge_size=spatial_merge_size,\n         vision_feature_layer=-1,\n     )"
        },
        {
            "sha": "5ce7763dd7c0b36edfc3e733962c6e2ea6b5d2f8",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -449,10 +449,10 @@ def forward(\n                 image_sizes=image_sizes,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "36fd45268385046d9ef4697a0dfca2819e323172",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -233,10 +233,10 @@ def forward(\n                 image_sizes=image_sizes,\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "b501e9efd35cc3d3eae072246f6b6aa37391bdca",
            "filename": "src/transformers/models/mllama/configuration_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconfiguration_mllama.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -337,6 +337,9 @@ class MllamaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mllama\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": MllamaTextConfig, \"vision_config\": MllamaVisionConfig}\n \n     def __init__("
        },
        {
            "sha": "4551b85bcd54811b0bc1ee44cfb675fb27553bfa",
            "filename": "src/transformers/models/paligemma/configuration_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -73,6 +73,9 @@ class PaliGemmaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"paligemma\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n "
        },
        {
            "sha": "3334e6f28fc32b4dd1d54dff35f4091e24a01602",
            "filename": "src/transformers/models/paligemma/convert_paligemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -80,7 +80,7 @@\n \n def get_paligemma2_config(variant: str, precision: str):\n     config = {\n-        \"image_token_index\": None,\n+        \"image_token_id\": None,\n         \"pad_token_id\": 0,\n         \"bos_token_id\": 2,\n         \"eos_token_id\": 1,\n@@ -93,7 +93,7 @@ def get_paligemma2_config(variant: str, precision: str):\n         patch_size = 14\n         num_image_tokens = (image_size**2) // (patch_size**2)\n         config[\"projection_dim\"] = variant_config[\"hidden_size\"]\n-        config[\"image_token_index\"] = 257152\n+        config[\"image_token_id\"] = 257152\n         config[\"num_hidden_layers\"] = variant_config[\"num_hidden_layers\"]  # For generate\n         text_config = Gemma2Config.from_pretrained(\"google/gemma-2-2b-it\").to_dict()\n         sup_text_config = {"
        },
        {
            "sha": "054872a799aca08befa064fecaa1034048af023b",
            "filename": "src/transformers/models/paligemma/convert_paligemma_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -45,7 +45,7 @@\n \n def get_paligemma_config(variant: str, precision: str):\n     config = {\n-        \"image_token_index\": None,\n+        \"image_token_id\": None,\n         \"pad_token_id\": 0,\n         \"bos_token_id\": 2,\n         \"eos_token_id\": 1,\n@@ -58,7 +58,7 @@ def get_paligemma_config(variant: str, precision: str):\n         patch_size = 14\n         num_image_tokens = (image_size**2) // (patch_size**2)\n \n-        config[\"image_token_index\"] = 257152 if variant != \"2b-test\" else 256000\n+        config[\"image_token_id\"] = 257152 if variant != \"2b-test\" else 256000\n         text_config = {\n             \"vocab_size\": 257152,\n             \"num_hidden_layers\": 18,"
        },
        {
            "sha": "ade69444f462814c001f862dc280ccaa55be41bf",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -473,8 +473,8 @@ def forward(\n         is_training = token_type_ids is not None and labels is not None\n \n         # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n-        if input_ids is not None and self.config.image_token_index >= self.vocab_size:\n-            special_image_mask = input_ids == self.config.image_token_index\n+        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n+            special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()\n             llm_input_ids[special_image_mask] = 0\n         else:\n@@ -498,10 +498,10 @@ def forward(\n \n             if input_ids is None:\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_index, dtype=torch.long, device=inputs_embeds.device)\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n                 special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():"
        },
        {
            "sha": "373aa6cb6e45fcb60e668fc0490a7fed16d7bf6f",
            "filename": "src/transformers/models/pixtral/convert_pixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -187,7 +187,7 @@ def convert_mistral_model(input_dir, output_dir):\n         vision_config,\n         text_config,\n         vision_feature_layer=-1,\n-        image_token_index=10,\n+        image_token_id=10,\n         vision_feature_select_strategy=\"full\",\n         image_seq_length=1,\n         multimodal_projector_bias=adapter_bias,"
        },
        {
            "sha": "22873e16d07e73d615ea7a2295be81450734bffd",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -458,6 +458,11 @@ class Qwen2_5OmniThinkerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_thinker\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+        \"audio_token_id\": \"audio_token_index\",\n+    }\n     sub_configs = {\n         \"audio_config\": Qwen2_5OmniAudioEncoderConfig,\n         \"vision_config\": Qwen2_5OmniVisionEncoderConfig,\n@@ -662,6 +667,11 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_talker\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+        \"audio_token_id\": \"audio_token_index\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "8accfc638e3eb5c080a43ae06fc9ac13bd36c55b",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -334,9 +334,9 @@ def get_rope_index(\n             mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)\n         \"\"\"\n         spatial_merge_size = self.spatial_merge_size\n-        image_token_id = self.config.image_token_index\n-        video_token_id = self.config.video_token_index\n-        audio_token_id = self.config.audio_token_index\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        audio_token_id = self.config.audio_token_id\n         vision_start_token_id = self.config.vision_start_token_id\n         audio_start_token_id = self.config.audio_start_token_id\n         position_id_per_seconds = self.config.position_id_per_seconds\n@@ -2450,7 +2450,7 @@ def forward(\n                 if audio_features.shape[0] != sum(audio_output_lengths.tolist()):\n                     raise ValueError(\"length of audio_features should match audio_output_lengths\")\n                 audio_mask = (\n-                    (input_ids == self.config.audio_token_index)\n+                    (input_ids == self.config.audio_token_id)\n                     .unsqueeze(-1)\n                     .expand_as(inputs_embeds)\n                     .to(inputs_embeds.device)\n@@ -2462,7 +2462,7 @@ def forward(\n                 pixel_values = pixel_values.type(self.visual.dtype)\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                 image_mask = (\n-                    (input_ids == self.config.image_token_index)\n+                    (input_ids == self.config.image_token_id)\n                     .unsqueeze(-1)\n                     .expand_as(inputs_embeds)\n                     .to(inputs_embeds.device)\n@@ -2474,7 +2474,7 @@ def forward(\n                 pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n                 video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n                 video_mask = (\n-                    (input_ids == self.config.video_token_index)\n+                    (input_ids == self.config.video_token_id)\n                     .unsqueeze(-1)\n                     .expand_as(inputs_embeds)\n                     .to(inputs_embeds.device)"
        },
        {
            "sha": "2524fd9186a63989c9303fe058a51c498e0d967c",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 16,
            "deletions": 6,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -443,6 +443,11 @@ class Qwen2_5OmniThinkerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_thinker\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+        \"audio_token_id\": \"audio_token_index\",\n+    }\n     sub_configs = {\n         \"audio_config\": Qwen2_5OmniAudioEncoderConfig,\n         \"vision_config\": Qwen2_5OmniVisionEncoderConfig,\n@@ -647,6 +652,11 @@ class Qwen2_5OmniTalkerConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_talker\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+        \"audio_token_id\": \"audio_token_index\",\n+    }\n \n     def __init__(\n         self,\n@@ -1225,9 +1235,9 @@ def get_rope_index(\n             mrope_position_deltas (`torch.Tensor` of shape `(batch_size)`)\n         \"\"\"\n         spatial_merge_size = self.spatial_merge_size\n-        image_token_id = self.config.image_token_index\n-        video_token_id = self.config.video_token_index\n-        audio_token_id = self.config.audio_token_index\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+        audio_token_id = self.config.audio_token_id\n         vision_start_token_id = self.config.vision_start_token_id\n         audio_start_token_id = self.config.audio_start_token_id\n         position_id_per_seconds = self.config.position_id_per_seconds\n@@ -2400,7 +2410,7 @@ def forward(\n                 if audio_features.shape[0] != sum(audio_output_lengths.tolist()):\n                     raise ValueError(\"length of audio_features should match audio_output_lengths\")\n                 audio_mask = (\n-                    (input_ids == self.config.audio_token_index)\n+                    (input_ids == self.config.audio_token_id)\n                     .unsqueeze(-1)\n                     .expand_as(inputs_embeds)\n                     .to(inputs_embeds.device)\n@@ -2412,7 +2422,7 @@ def forward(\n                 pixel_values = pixel_values.type(self.visual.dtype)\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                 image_mask = (\n-                    (input_ids == self.config.image_token_index)\n+                    (input_ids == self.config.image_token_id)\n                     .unsqueeze(-1)\n                     .expand_as(inputs_embeds)\n                     .to(inputs_embeds.device)\n@@ -2424,7 +2434,7 @@ def forward(\n                 pixel_values_videos = pixel_values_videos.type(self.visual.dtype)\n                 video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n                 video_mask = (\n-                    (input_ids == self.config.video_token_index)\n+                    (input_ids == self.config.video_token_id)\n                     .unsqueeze(-1)\n                     .expand_as(inputs_embeds)\n                     .to(inputs_embeds.device)"
        },
        {
            "sha": "a700a9928861f32bc90181ea3e25bc4e19bd10a6",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -156,6 +156,10 @@ class Qwen2_5_VLTextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n                 `high_freq_factor` (`float`, *optional*):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        image_token_id (`int`, *optional*):\n+            Token index used as placeholder for image embeddings.\n+        video_token_id (`int`, *optional*):\n+            Token index used as placeholder for video embeddings.\n \n     ```python\n     >>> from transformers import Qwen2_5_VLTextModel, Qwen2_5_VLConfig\n@@ -209,6 +213,8 @@ def __init__(\n         max_window_layers=80,\n         attention_dropout=0.0,\n         rope_scaling=None,\n+        image_token_id=None,\n+        video_token_id=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -244,6 +250,8 @@ def __init__(\n                 self.rope_scaling[\"type\"] = \"default\"\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        },
        {
            "sha": "23a1c699dd4c5e548bcf139095997d6fdf4fadca",
            "filename": "src/transformers/models/qwen2_audio/configuration_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fconfiguration_qwen2_audio.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -157,6 +157,9 @@ class Qwen2AudioConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"qwen2_audio\"\n+    attribute_map = {\n+        \"audio_token_id\": \"audio_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"audio_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "4496ef73b8cf8b7ec5ca2335d5854f043c2292d9",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -932,7 +932,7 @@ def _merge_input_ids_with_audio_features(\n                 raise ValueError(f\"both side of attention_mask has zero, invalid. {attention_mask}\")\n \n         # 1. Create a mask to know where special audio tokens are\n-        special_audio_token_mask = input_ids == self.config.audio_token_index\n+        special_audio_token_mask = input_ids == self.config.audio_token_id\n         num_special_audio_tokens = torch.sum(special_audio_token_mask, dim=-1)\n \n         # In case the Audio model or the Language model has been offloaded to CPU, we need to manually\n@@ -942,7 +942,7 @@ def _merge_input_ids_with_audio_features(\n         input_ids = input_ids.to(target_device)\n         num_audio_tokens = num_audio_tokens.to(target_device)\n         batch_indices, non_audio_indices = torch.where(\n-            (input_ids != self.config.audio_token_index) & (attention_mask == 1)\n+            (input_ids != self.config.audio_token_id) & (attention_mask == 1)\n         )\n \n         # 2. Compute the positions where text should be written\n@@ -1114,7 +1114,7 @@ def forward(\n                 audio_features = self.multi_modal_projector(selected_audio_feature)\n \n                 # if we have consecutive audio tokens, then it means we expanded input_ids in processing\n-                audio_tokens = input_ids == self.config.audio_token_index\n+                audio_tokens = input_ids == self.config.audio_token_id\n                 legacy_processing = (audio_tokens[:, :-1] & audio_tokens[:, 1:]).sum() == 0\n \n                 if legacy_processing:\n@@ -1130,14 +1130,14 @@ def forward(\n                     audio_features_mask = audio_features_mask < audio_output_lengths[:, None]\n                     audio_features = audio_features[audio_features_mask]\n \n-                    n_audio_tokens = (input_ids == self.config.audio_token_index).sum().item()\n+                    n_audio_tokens = (input_ids == self.config.audio_token_id).sum().item()\n                     n_audio_features = audio_features.shape[0]\n \n                     if n_audio_tokens != n_audio_features:\n                         raise ValueError(\n                             f\"Audio features and audio tokens do not match: tokens: {n_audio_tokens}, features {n_audio_features}\"\n                         )\n-                    special_audio_mask = (input_ids == self.config.audio_token_index).to(inputs_embeds.device)\n+                    special_audio_mask = (input_ids == self.config.audio_token_id).to(inputs_embeds.device)\n                     special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds)\n                     audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n                     inputs_embeds = inputs_embeds.masked_scatter(special_audio_mask, audio_features)"
        },
        {
            "sha": "5f4842ce6045c236d4783e596d46523fbe5ee4d8",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -145,6 +145,10 @@ class Qwen2VLTextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n                 `high_freq_factor` (`float`, *optional*):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        image_token_id (`int`, *optional*):\n+            Token index used as placeholder for image embeddings.\n+        video_token_id (`int`, *optional*):\n+            Token index used as placeholder for video embeddings.\n \n     ```python\n     >>> from transformers import Qwen2VLTextModel, Qwen2VLConfig\n@@ -198,6 +202,8 @@ def __init__(\n         max_window_layers=80,\n         attention_dropout=0.0,\n         rope_scaling=None,\n+        image_token_id=None,\n+        video_token_id=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -233,6 +239,8 @@ def __init__(\n                 self.rope_scaling[\"type\"] = \"default\"\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self, ignore_keys={\"mrope_section\"})\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n "
        },
        {
            "sha": "acb53924b2e5e2a2cc88c120eb6d8ccd416a9b67",
            "filename": "src/transformers/models/shieldgemma2/configuration_shieldgemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconfiguration_shieldgemma2.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -72,6 +72,11 @@ class ShieldGemma2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"shieldgemma2\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"boi_token_id\": \"boi_token_index\",\n+        \"eoi_token_id\": \"eoi_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "402d99467316030b71b8d30150c3a8cc875e0852",
            "filename": "src/transformers/models/video_llava/configuration_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -80,6 +80,10 @@ class VideoLlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"video_llava\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+        \"video_token_id\": \"video_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "70ebef344c3eea5bda5317ef0815a88fc6c3e10c",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -493,10 +493,10 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -509,10 +509,10 @@ def forward(\n                 pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n             )\n \n-            special_image_mask = (input_ids == self.config.video_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_index).sum()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_features.shape[0] * video_features.shape[1]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "f748201f7d7d616bce47e48e010d858f98f71f02",
            "filename": "src/transformers/models/vipllava/configuration_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fconfiguration_vipllava.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -70,6 +70,9 @@ class VipLlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"vipllava\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n \n     def __init__("
        },
        {
            "sha": "9243bbe9e2d278ce9066acea1bfdfd01e7d4ab6a",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -374,10 +374,10 @@ def forward(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1)\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n             special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_index).sum()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "5ae151131087c83b73acfb196d9b64600450d4de",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -224,12 +224,9 @@ def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         # to crash. On pretrained models this isn't a risk, as they are trained to not generate these tokens.\n         if config is not None:\n             for key in [\n-                \"image_token_index\",\n                 \"image_token_id\",\n-                \"video_token_index\",\n                 \"video_token_id\",\n                 \"vision_start_token_id\",\n-                \"audio_token_index\",\n                 \"audio_start_token_id\",\n                 \"audio_end_token_id\",\n                 \"vision_end_token_id\","
        },
        {
            "sha": "ca677f6549ba257dd008ca6411fe5121a24afb96",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=2ba6b92a6f64515c5f6818a44afdabb70c3c6ca0",
            "patch": "@@ -351,8 +351,8 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         \"pad_index\",\n         \"unk_index\",\n         \"mask_index\",\n-        \"image_token_index\",  # for VLMs\n-        \"video_token_index\",\n+        \"image_token_id\",  # for VLMs\n+        \"video_token_id\",\n         \"image_seq_length\",\n         \"video_seq_length\",\n         \"image_size\","
        }
    ],
    "stats": {
        "total": 420,
        "additions": 279,
        "deletions": 141
    }
}