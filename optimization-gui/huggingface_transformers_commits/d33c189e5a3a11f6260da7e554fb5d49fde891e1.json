{
    "author": "gante",
    "message": "[RoPE] run RoPE tests when the model uses RoPE (#40630)\n\n* enable rope tests\n\n* no manual rope test parameterization\n\n* Apply suggestions from code review\n\n* Update tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py\n\n* PR comment: use generalist torch code to find the rope layer",
    "sha": "d33c189e5a3a11f6260da7e554fb5d49fde891e1",
    "files": [
        {
            "sha": "6677b46f8ab2228fbb1283b2ca69a330ff2abc3c",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 50,
            "deletions": 15,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import set_seed\n+from transformers import PretrainedConfig, set_seed\n from transformers.testing_utils import (\n     is_flaky,\n     require_flash_attn,\n@@ -230,7 +230,6 @@ class CausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n     test_pruning = False\n     model_tester_class = None\n     all_model_classes = None\n-    rotary_embedding_layer = None  # Enables RoPE tests if set\n     pipeline_model_mapping = None\n \n     def setUp(self):\n@@ -319,21 +318,28 @@ def test_token_classification_model(self):\n \n     @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n-        if self.rotary_embedding_layer is None:\n-            self.skipTest(\"Rotary embedding layer not set\")\n+        \"\"\"\n+        Tests that we can initialize a model with RoPE scaling in the config, that it can run a forward pass, and\n+        that a few basic model output properties are honored.\n+        \"\"\"\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        if not _config_supports_rope_scaling(config):\n+            self.skipTest(\"This model does not support RoPE scaling\")\n+\n         short_input = ids_tensor([1, 10], config.vocab_size)\n         long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        config.rope_scaling = {\"rope_type\": \"default\"}\n         original_model = self.model_tester_class.base_model_class(config)\n         original_model.to(torch_device)\n         original_model.eval()\n         original_short_output = original_model(short_input).last_hidden_state\n         original_long_output = original_model(long_input).last_hidden_state\n \n         set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n-        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n+        config.rope_scaling = {\"rope_type\": scaling_type, \"factor\": 10.0}\n         scaled_model = self.model_tester_class.base_model_class(config)\n         scaled_model.to(torch_device)\n         scaled_model.eval()\n@@ -350,10 +356,26 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # The output should be different for long inputs\n         self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n \n-    def test_model_rope_scaling(self):\n-        if self.rotary_embedding_layer is None:\n-            self.skipTest(\"Rotary embedding layer not set\")\n+    def test_model_rope_scaling_frequencies(self):\n+        \"\"\"Tests the frequency properties of the different RoPE scaling types on the model RoPE layer.\"\"\"\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        if not _config_supports_rope_scaling(config):\n+            self.skipTest(\"This model does not support RoPE scaling\")\n+\n+        # Retrieves the RoPE layer class from the base model class. Uses `.named_modules()` to avoid hardcoding the\n+        # named location of the RoPE layer class.\n+        base_model = self.model_tester.base_model_class(config)\n+        possible_rope_attributes = [\n+            \"rotary_emb\",  # most common case\n+            \"global_rotary_emb\",\n+            \"local_rotary_emb\",\n+        ]\n+        for name, module in base_model.named_modules():\n+            if any(potential_name in name for potential_name in possible_rope_attributes):\n+                rope_class = type(module)\n+                break\n+\n         scaling_factor = 10\n         short_input_length = 10\n         long_input_length = int(config.max_position_embeddings * 1.5)\n@@ -368,16 +390,17 @@ def test_model_rope_scaling(self):\n         position_ids_long = position_ids_long.unsqueeze(0)\n \n         # Sanity check original RoPE\n-        original_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        config.rope_scaling = {\"rope_type\": \"default\"}\n+        original_rope = rope_class(config=config).to(torch_device)\n         original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n         original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n         torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n         torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        linear_scaling_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        config.rope_scaling = {\"rope_type\": \"linear\", \"factor\": scaling_factor}\n+        linear_scaling_rope = rope_class(config=config).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n@@ -390,8 +413,8 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n-        ntk_scaling_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        config.rope_scaling = {\"rope_type\": \"dynamic\", \"factor\": scaling_factor}\n+        ntk_scaling_rope = rope_class(config=config).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(ntk_cos_short, original_cos_short)\n@@ -404,8 +427,8 @@ def test_model_rope_scaling(self):\n \n         # Sanity check Yarn RoPE scaling\n         # Scaling should be over the entire input\n-        config.rope_scaling = {\"type\": \"yarn\", \"factor\": scaling_factor}\n-        yarn_scaling_rope = self.rotary_embedding_layer(config=config).to(torch_device)\n+        config.rope_scaling = {\"rope_type\": \"yarn\", \"factor\": scaling_factor}\n+        yarn_scaling_rope = rope_class(config=config).to(torch_device)\n         yarn_cos_short, yarn_sin_short = yarn_scaling_rope(x, position_ids_short)\n         yarn_cos_long, yarn_sin_long = yarn_scaling_rope(x, position_ids_long)\n         torch.testing.assert_close(yarn_cos_short, yarn_cos_long[:, :short_input_length, :])\n@@ -450,3 +473,15 @@ def test_flash_attn_2_equivalence(self):\n                 logits = outputs.hidden_states[-1]\n                 logits_fa = outputs_fa.hidden_states[-1]\n                 torch.testing.assert_close(logits_fa, logits, atol=3e-2, rtol=3e-2)\n+\n+\n+def _config_supports_rope_scaling(config: PretrainedConfig) -> bool:\n+    \"\"\"Returns whether a certain model config supports RoPE scaling parameterization.\"\"\"\n+    # Has rope_scaling -> model was designed with rope scaling in mind\n+    # Has rope_theta (and no rope_scaling) -> probably an older model, but should support rope scaling as well\n+    main_config_has_rope = hasattr(config, \"rope_scaling\") or hasattr(config, \"rope_theta\")\n+    sub_config_has_rope = any(\n+        hasattr(config[sub_config], \"rope_scaling\") or hasattr(config[sub_config], \"rope_theta\")\n+        for sub_config in config.sub_configs.keys()\n+    )\n+    return main_config_has_rope or sub_config_has_rope"
        },
        {
            "sha": "7c6096081ecd7ca0717232494d7fdce7a58bcdfc",
            "filename": "tests/models/arcee/test_modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Farcee%2Ftest_modeling_arcee.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -39,7 +39,6 @@\n         ArceeForTokenClassification,\n         ArceeModel,\n     )\n-    from transformers.models.arcee.modeling_arcee import ArceeRotaryEmbedding\n \n \n class ArceeModelTester(CausalLMModelTester):\n@@ -80,7 +79,6 @@ class ArceeModelTest(CausalLMModelTest, unittest.TestCase):\n     test_pruning = False\n     fx_compatible = False\n     model_tester_class = ArceeModelTester\n-    rotary_embedding_layer = ArceeRotaryEmbedding  # Enables RoPE tests if set\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer"
        },
        {
            "sha": "d194c74e7b435bbca93371fe38d09a2487c4ac0c",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import DbrxConfig, is_torch_available\n from transformers.testing_utils import require_torch, slow\n \n@@ -122,6 +124,15 @@ def test_disk_offload_safetensors(self):\n     def test_disk_offload_bin(self):\n         pass\n \n+    @unittest.skip(\"Dbrx doesn't have RoPE scaling implemented\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"Dbrx doesn't have RoPE scaling implemented\")\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        pass\n+\n \n @require_torch\n class DbrxModelIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "26bc36b9af1a066033cc6fdca0a66e3ce1fcf4a0",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -82,13 +82,16 @@ class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n     test_torchscript = False\n     test_all_params_have_gradient = False\n     model_tester_class = DeepseekV2ModelTester\n-    rotary_embedding_layer = DeepseekV2RotaryEmbedding\n     model_split_percents = [0.5, 0.7, 0.8]\n \n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = DeepseekV2ForCausalLM if is_torch_available() else None\n \n-    def test_model_rope_scaling(self):\n+    def test_model_rope_scaling_frequencies(self):\n+        \"\"\"\n+        Overwritten: DeepseekV2 implements RoPE in the complex domain, as opposed to in the real domain with\n+        `sin` and `cos`. Nevertheless, the checks are the same as in the original test.\n+        \"\"\"\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         scaling_factor = 10\n         short_input_length = 10\n@@ -109,7 +112,7 @@ def test_model_rope_scaling(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n+        config.rope_scaling = {\"rope_type\": \"linear\", \"factor\": scaling_factor}\n         linear_scaling_rope = DeepseekV2RotaryEmbedding(config=config).to(torch_device)\n         linear_freqs_cis_short = linear_scaling_rope(x, position_ids_short)\n         linear_freqs_cis_long = linear_scaling_rope(x, position_ids_long)\n@@ -118,7 +121,7 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        config.rope_scaling = {\"type\": \"dynamic\", \"factor\": scaling_factor}\n+        config.rope_scaling = {\"rope_type\": \"dynamic\", \"factor\": scaling_factor}\n         ntk_scaling_rope = DeepseekV2RotaryEmbedding(config=config).to(torch_device)\n         ntk_freqs_cis_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_freqs_cis_long = ntk_scaling_rope(x, position_ids_long)\n@@ -129,7 +132,7 @@ def test_model_rope_scaling(self):\n \n         # Sanity check Yarn RoPE scaling\n         # Scaling should be over the entire input\n-        config.rope_scaling = {\"type\": \"yarn\", \"factor\": scaling_factor}\n+        config.rope_scaling = {\"rope_type\": \"yarn\", \"factor\": scaling_factor}\n         yarn_scaling_rope = DeepseekV2RotaryEmbedding(config=config).to(torch_device)\n         yarn_freqs_cis_short = yarn_scaling_rope(x, position_ids_short)\n         yarn_freqs_cis_long = yarn_scaling_rope(x, position_ids_long)"
        },
        {
            "sha": "10c3287e5b801d77f628e8f64930a704826a6fce",
            "filename": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -37,7 +37,6 @@\n         Ernie4_5ForCausalLM,\n         Ernie4_5Model,\n     )\n-    from transformers.models.ernie4_5.modeling_ernie4_5 import Ernie4_5RotaryEmbedding\n \n \n class Ernie4_5ModelTester(CausalLMModelTester):\n@@ -69,7 +68,6 @@ class Ernie4_5ModelTest(CausalLMModelTest, unittest.TestCase):\n     test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = Ernie4_5ModelTester\n-    rotary_embedding_layer = Ernie4_5RotaryEmbedding  # Enables RoPE tests if set\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer"
        },
        {
            "sha": "2e27bfc9332af4ddcf27e81ae8fdd42af948183a",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -41,6 +41,7 @@\n         Ernie4_5_MoeForCausalLM,\n         Ernie4_5_MoeModel,\n     )\n+\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n "
        },
        {
            "sha": "d01a1022f342d775b78edd7aa401632bbc31c2a6",
            "filename": "tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import HunYuanDenseV1Config, is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n@@ -30,7 +32,6 @@\n         HunYuanDenseV1ForSequenceClassification,\n         HunYuanDenseV1Model,\n     )\n-\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n@@ -78,6 +79,15 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n+    @unittest.skip(\"HunYuanDenseV1's RoPE has custom parameterization\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"HunYuanDenseV1's RoPE has custom parameterization\")\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        pass\n+\n \n @require_torch\n class HunYuanDenseV1IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "3738ebee75d1109a81f081e3b69513a3d5e57e9d",
            "filename": "tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n import pytest\n+from parameterized import parameterized\n \n from transformers import HunYuanMoEV1Config, is_torch_available\n from transformers.testing_utils import (\n@@ -101,6 +102,15 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_generate_with_static_cache(self):\n         pass\n \n+    @unittest.skip(\"HunYuanMoEV1's RoPE has custom parameterization\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"HunYuanMoEV1's RoPE has custom parameterization\")\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        pass\n+\n \n @require_torch\n class HunYuanMoEV1IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "f285e11ef63e96124895906cd9be825c123a2ed5",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -46,7 +46,6 @@\n         LlamaModel,\n         LlamaTokenizer,\n     )\n-    from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n \n \n class LlamaModelTester(CausalLMModelTester):\n@@ -87,7 +86,6 @@ class LlamaModelTest(CausalLMModelTest, unittest.TestCase):\n     test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = LlamaModelTester\n-    rotary_embedding_layer = LlamaRotaryEmbedding  # Enables RoPE tests if set\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n     # This is because we are hitting edge cases with the causal_mask buffer"
        },
        {
            "sha": "7b13f3ae7a6f0a78334664e55cd9152912c92abc",
            "filename": "tests/models/minimax/test_modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fminimax%2Ftest_modeling_minimax.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -38,7 +38,6 @@\n         MiniMaxForTokenClassification,\n         MiniMaxModel,\n     )\n-\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n "
        },
        {
            "sha": "cd6459c170dc68b801b0645a2897e5225037d6cf",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -48,7 +48,6 @@\n         MistralForTokenClassification,\n         MistralModel,\n     )\n-\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n "
        },
        {
            "sha": "fc4ae6e913e2406519ea7f8170e538aea7670b00",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -16,6 +16,8 @@\n \n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import NemotronConfig, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n@@ -96,6 +98,15 @@ def setUp(self):\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass\n \n+    @unittest.skip(\"Nemotron has a hardcoded `rope_type`, so we can't apply RoPE scaling\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"Nemotron has a hardcoded `rope_type`, so we can't apply RoPE scaling\")\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        pass\n+\n \n @require_torch_accelerator\n class NemotronIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "80e4aa6565b43c5392d83092406c8eb8753a0ea4",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -36,7 +36,6 @@\n         PhiForTokenClassification,\n         PhiModel,\n     )\n-    from transformers.models.phi.modeling_phi import PhiRotaryEmbedding\n \n \n class PhiModelTester(CausalLMModelTester):\n@@ -69,7 +68,6 @@ class PhiModelTest(CausalLMModelTest, unittest.TestCase):\n     test_headmasking = False\n     test_pruning = False\n     model_tester_class = PhiModelTester\n-    rotary_embedding_layer = PhiRotaryEmbedding\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79292/workflows/fa2ba644-8953-44a6-8f67-ccd69ca6a476/jobs/1012905\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "7df2f7ec7418176824b205a53376a9eabe982402",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -40,7 +40,6 @@\n         Phi3ForTokenClassification,\n         Phi3Model,\n     )\n-    from transformers.models.phi3.modeling_phi3 import Phi3RotaryEmbedding\n \n     end_of_text_token = 32000\n \n@@ -116,7 +115,6 @@ class Phi3ModelTest(CausalLMModelTest, unittest.TestCase):\n     test_headmasking = False\n     test_pruning = False\n     model_tester_class = Phi3ModelTester\n-    rotary_embedding_layer = Phi3RotaryEmbedding\n \n \n @slow"
        },
        {
            "sha": "46714244a14bbb19ac7de136948d11f47af46d02",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -16,6 +16,8 @@\n \n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import PhimoeConfig, StaticCache, is_torch_available\n from transformers.testing_utils import (\n     require_torch,\n@@ -115,6 +117,15 @@ def is_pipeline_test_to_skip(\n     ):\n         return True\n \n+    @unittest.skip(\"PhiMoE's RoPE has custom parameterization\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"PhiMoE's RoPE has custom parameterization\")\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        pass\n+\n \n @slow\n @require_torch"
        },
        {
            "sha": "d22dea542e82a82bf3403581c7035e80cbb4c476",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -35,7 +35,6 @@\n \n     from transformers import RecurrentGemmaConfig, RecurrentGemmaForCausalLM, RecurrentGemmaModel\n \n-\n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n@@ -135,6 +134,15 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n     def test_model_outputs_equivalence(self):\n         pass\n \n+    @unittest.skip(\"RecurrentGemma doesn't have RoPE scaling implemented\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    @unittest.skip(\"RecurrentGemma doesn't have RoPE scaling implemented\")\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        pass\n+\n \n @require_torch_accelerator\n @slow"
        },
        {
            "sha": "d6695a68c4dc659ca5a60c9acf476403562e84f0",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d33c189e5a3a11f6260da7e554fb5d49fde891e1/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=d33c189e5a3a11f6260da7e554fb5d49fde891e1",
            "patch": "@@ -37,7 +37,6 @@\n         StableLmForTokenClassification,\n         StableLmModel,\n     )\n-    from transformers.models.stablelm.modeling_stablelm import StableLmRotaryEmbedding\n \n from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n@@ -78,7 +77,6 @@ class StableLmModelTest(CausalLMModelTest, unittest.TestCase):\n     test_pruning = False\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     model_tester_class = StableLmModelTester\n-    rotary_embedding_layer = StableLmRotaryEmbedding  # Enables RoPE tests if set\n \n \n @require_torch"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 122,
        "deletions": 36
    }
}