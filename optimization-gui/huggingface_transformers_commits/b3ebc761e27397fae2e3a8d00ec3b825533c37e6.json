{
    "author": "yonigozlan",
    "message": "[Fast image processors] Improve handling of image-like inputs other than images (segmentation_maps) (#39489)\n\n* improve handlike of other image-like inputs in fast image processors\n\n* fix issues with _prepare_images_structure\n\n* update sam image processor fast\n\n* use dict update",
    "sha": "b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
    "files": [
        {
            "sha": "55bd84cf2798abcc9cea688d48e5bf10e8c4652a",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 35,
            "deletions": 9,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -453,6 +453,7 @@ def filter_out_unused_kwargs(self, kwargs: dict):\n     def _prepare_images_structure(\n         self,\n         images: ImageInput,\n+        expected_ndims: int = 3,\n     ) -> ImageInput:\n         \"\"\"\n         Prepare the images structure for processing.\n@@ -464,7 +465,7 @@ def _prepare_images_structure(\n         Returns:\n             `ImageInput`: The images with a valid nesting.\n         \"\"\"\n-        return make_flat_list_of_images(images)\n+        return make_flat_list_of_images(images, expected_ndims=expected_ndims)\n \n     def _process_image(\n         self,\n@@ -486,6 +487,10 @@ def _process_image(\n             # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n             image = torch.from_numpy(image).contiguous()\n \n+        # If the image is 2D, we need to unsqueeze it to add a channel dimension for processing\n+        if image.ndim == 2:\n+            image = image.unsqueeze(0)\n+\n         # Infer the channel dimension format if not provided\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n@@ -500,32 +505,35 @@ def _process_image(\n \n         return image\n \n-    def _prepare_input_images(\n+    def _prepare_image_like_inputs(\n         self,\n         images: ImageInput,\n         do_convert_rgb: Optional[bool] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         device: Optional[\"torch.device\"] = None,\n+        expected_ndims: int = 3,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n-        Prepare the input images for processing.\n+        Prepare image-like inputs for processing.\n \n         Args:\n             images (`ImageInput`):\n-                The input images to process.\n+                The image-like inputs to process.\n             do_convert_rgb (`bool`, *optional*):\n                 Whether to convert the images to RGB.\n             input_data_format (`str` or `ChannelDimension`, *optional*):\n                 The input data format of the images.\n             device (`torch.device`, *optional*):\n                 The device to put the processed images on.\n+            expected_ndims (`int`, *optional*):\n+                The expected number of dimensions for the images. (can be 2 for segmentation maps etc.)\n \n         Returns:\n             List[`torch.Tensor`]: The processed images.\n         \"\"\"\n \n         # Get structured images (potentially nested)\n-        images = self._prepare_images_structure(images)\n+        images = self._prepare_images_structure(images, expected_ndims=expected_ndims)\n \n         process_image_partial = partial(\n             self._process_image, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n@@ -627,10 +635,6 @@ def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImag\n         do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n         input_data_format = kwargs.pop(\"input_data_format\")\n         device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n \n         # Update kwargs that need further processing before being validated\n         kwargs = self._further_process_kwargs(**kwargs)\n@@ -652,6 +656,28 @@ def preprocess(self, images: ImageInput, *args, **kwargs: Unpack[DefaultFastImag\n         kwargs.pop(\"default_to_square\")\n         kwargs.pop(\"data_format\")\n \n+        return self._preprocess_image_like_inputs(\n+            images, *args, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device, **kwargs\n+        )\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        *args,\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[DefaultFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        To be overriden by subclasses when image-like inputs other than images should be processed.\n+        It can be used for segmentation maps, depth maps, etc.\n+        \"\"\"\n+        # Prepare input images\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n         return self._preprocess(images, *args, **kwargs)\n \n     def _preprocess("
        },
        {
            "sha": "7e51bfeaec85b632ff24529bc5e253128ff57d52",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -213,13 +213,16 @@ def make_list_of_images(images, expected_ndims: int = 3) -> list[ImageInput]:\n \n def make_flat_list_of_images(\n     images: Union[list[ImageInput], ImageInput],\n+    expected_ndims: int = 3,\n ) -> ImageInput:\n     \"\"\"\n     Ensure that the output is a flat list of images. If the input is a single image, it is converted to a list of length 1.\n     If the input is a nested list of images, it is converted to a flat list of images.\n     Args:\n         images (`Union[list[ImageInput], ImageInput]`):\n             The input image.\n+        expected_ndims (`int`, *optional*, defaults to 3):\n+            The expected number of dimensions for a single input image.\n     Returns:\n         list: A list of images or a 4d array of images.\n     \"\"\"\n@@ -232,28 +235,31 @@ def make_flat_list_of_images(\n         return [img for img_list in images for img in img_list]\n \n     if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n-        if is_pil_image(images[0]) or images[0].ndim == 3:\n+        if is_pil_image(images[0]) or images[0].ndim == expected_ndims:\n             return images\n-        if images[0].ndim == 4:\n+        if images[0].ndim == expected_ndims + 1:\n             return [img for img_list in images for img in img_list]\n \n     if is_valid_image(images):\n-        if is_pil_image(images) or images.ndim == 3:\n+        if is_pil_image(images) or images.ndim == expected_ndims:\n             return [images]\n-        if images.ndim == 4:\n+        if images.ndim == expected_ndims + 1:\n             return list(images)\n \n     raise ValueError(f\"Could not make a flat list of images from {images}\")\n \n \n def make_nested_list_of_images(\n     images: Union[list[ImageInput], ImageInput],\n+    expected_ndims: int = 3,\n ) -> ImageInput:\n     \"\"\"\n     Ensure that the output is a nested list of images.\n     Args:\n         images (`Union[list[ImageInput], ImageInput]`):\n             The input image.\n+        expected_ndims (`int`, *optional*, defaults to 3):\n+            The expected number of dimensions for a single input image.\n     Returns:\n         list: A list of list of images or a list of 4d array of images.\n     \"\"\"\n@@ -267,16 +273,16 @@ def make_nested_list_of_images(\n \n     # If it's a list of images, it's a single batch, so convert it to a list of lists\n     if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n-        if is_pil_image(images[0]) or images[0].ndim == 3:\n+        if is_pil_image(images[0]) or images[0].ndim == expected_ndims:\n             return [images]\n-        if images[0].ndim == 4:\n+        if images[0].ndim == expected_ndims + 1:\n             return [list(image) for image in images]\n \n     # If it's a single image, convert it to a list of lists\n     if is_valid_image(images):\n-        if is_pil_image(images) or images.ndim == 3:\n+        if is_pil_image(images) or images.ndim == expected_ndims:\n             return [[images]]\n-        if images.ndim == 4:\n+        if images.ndim == expected_ndims + 1:\n             return [list(images)]\n \n     raise ValueError(\"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\")"
        },
        {
            "sha": "f2b94f3836e3f83b62a2ecea7889bb5ebe385ca2",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 50,
            "deletions": 101,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -34,9 +34,6 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    make_list_of_images,\n-    pil_torch_interpolation_mapping,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n@@ -91,6 +88,55 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n \n         return label\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[BeitFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[BeitFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        images_kwargs = kwargs.copy()\n+        images_kwargs[\"do_reduce_labels\"] = False\n+        batch_feature = self._preprocess(images, **images_kwargs)\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update({\"do_normalize\": False, \"do_rescale\": False})\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            ).pixel_values\n+            batch_feature[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return batch_feature\n+\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -136,105 +182,8 @@ def _preprocess(\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-        return processed_images\n-\n-    def _preprocess_images(\n-        self,\n-        images,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses images.\"\"\"\n-        kwargs[\"do_reduce_labels\"] = False\n-        processed_images = self._preprocess(images=images, **kwargs)\n-        return processed_images\n-\n-    def _preprocess_segmentation_maps(\n-        self,\n-        segmentation_maps,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses segmentation maps.\"\"\"\n-        processed_segmentation_maps = []\n-        for segmentation_map in segmentation_maps:\n-            segmentation_map = self._process_image(\n-                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n-            )\n-\n-            if segmentation_map.ndim == 2:\n-                segmentation_map = segmentation_map[None, ...]\n-\n-            processed_segmentation_maps.append(segmentation_map)\n-\n-        kwargs[\"do_normalize\"] = False\n-        kwargs[\"do_rescale\"] = False\n-        kwargs[\"input_data_format\"] = ChannelDimension.FIRST\n-        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n-        return processed_segmentation_maps\n-\n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[BeitFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        r\"\"\"\n-        segmentation_maps (`ImageInput`, *optional*):\n-            The segmentation maps to preprocess.\n-        \"\"\"\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-\n-        # Prepare segmentation maps\n-        if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n-\n-        images = self._preprocess_images(\n-            images=images,\n-            **kwargs,\n-        )\n-        data = {\"pixel_values\": images}\n-\n-        if segmentation_maps is not None:\n-            segmentation_maps = self._preprocess_segmentation_maps(\n-                segmentation_maps=segmentation_maps,\n-                **kwargs,\n-            )\n-            data[\"labels\"] = segmentation_maps\n \n-        return BatchFeature(data=data)\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\""
        },
        {
            "sha": "1d76b31c1b1a70251afe7321a293fdb73c03cd97",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 50,
            "deletions": 102,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -36,9 +36,6 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    make_list_of_images,\n-    pil_torch_interpolation_mapping,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -162,6 +159,55 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n \n         return label\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[DPTFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[DPTFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        images_kwargs = kwargs.copy()\n+        images_kwargs[\"do_reduce_labels\"] = False\n+        batch_feature = self._preprocess(images, **images_kwargs)\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update({\"do_normalize\": False, \"do_rescale\": False})\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            ).pixel_values\n+            batch_feature[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return batch_feature\n+\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -219,105 +265,7 @@ def _preprocess(\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-        return processed_images\n-\n-    def _preprocess_images(\n-        self,\n-        images,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses images.\"\"\"\n-        kwargs[\"do_reduce_labels\"] = False\n-        processed_images = self._preprocess(images=images, **kwargs)\n-        return processed_images\n-\n-    def _preprocess_segmentation_maps(\n-        self,\n-        segmentation_maps,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses segmentation maps.\"\"\"\n-        processed_segmentation_maps = []\n-        for segmentation_map in segmentation_maps:\n-            segmentation_map = self._process_image(\n-                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n-            )\n-\n-            if segmentation_map.ndim == 2:\n-                segmentation_map = segmentation_map[None, ...]\n-\n-            processed_segmentation_maps.append(segmentation_map)\n-\n-        kwargs[\"do_normalize\"] = False\n-        kwargs[\"do_rescale\"] = False\n-        kwargs[\"input_data_format\"] = ChannelDimension.FIRST\n-        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n-        return processed_segmentation_maps\n-\n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[DPTFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        r\"\"\"\n-        segmentation_maps (`ImageInput`, *optional*):\n-            The segmentation maps to preprocess.\n-        \"\"\"\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-\n-        # Prepare segmentation maps\n-        if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n-\n-        images = self._preprocess_images(\n-            images=images,\n-            **kwargs,\n-        )\n-        data = {\"pixel_values\": images}\n-\n-        if segmentation_maps is not None:\n-            segmentation_maps = self._preprocess_segmentation_maps(\n-                segmentation_maps=segmentation_maps,\n-                **kwargs,\n-            )\n-            data[\"labels\"] = segmentation_maps\n-\n-        return BatchFeature(data=data)\n+        return BatchFeature(data={\"pixel_values\": processed_images})\n \n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\""
        },
        {
            "sha": "46cefe530f1fd9eeff81394bc00d8e3fa4117d09",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -267,7 +267,7 @@ def _preprocess(\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-        return processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images})\n \n     def post_process_depth_estimation(\n         self,"
        },
        {
            "sha": "cab9221ec9d85ce9261fee5a1a9b2cca8ea15a56",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 85,
            "deletions": 119,
            "changes": 204,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -33,9 +33,7 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    make_list_of_images,\n     pil_torch_interpolation_mapping,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -161,6 +159,91 @@ def _pad(self, images: torch.Tensor, size: dict) -> torch.Tensor:\n         padded_images = torch.nn.functional.pad(images, padding, mode=\"constant\", value=0.0)\n         return padded_images\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[list[torch.Tensor]] = None,\n+        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+        **kwargs: Unpack[EomtImageProcessorFastKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess for corresponding images.\n+        instance_id_to_semantic_id (`list[dict[int, int]]` or `dict[int, int]`, *optional*):\n+            A mapping between object instance ids and class ids.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, instance_id_to_semantic_id, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        instance_id_to_semantic_id: Optional[dict[int, int]],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[EomtImageProcessorFastKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        ignore_index = kwargs.pop(\"ignore_index\", None)\n+        images_kwargs = kwargs.copy()\n+        processed_images, patch_offsets = self._preprocess(images, **images_kwargs)\n+        outputs = BatchFeature({\"pixel_values\": processed_images})\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                }\n+            )\n+\n+            processed_segmentation_maps, _ = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            )\n+            processed_segmentation_maps = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+            # Convert to list of binary masks and labels\n+            mask_labels, class_labels = [], []\n+            for idx, segmentation_map in enumerate(processed_segmentation_maps):\n+                if isinstance(instance_id_to_semantic_id, list):\n+                    instance_id = instance_id_to_semantic_id[idx]\n+                else:\n+                    instance_id = instance_id_to_semantic_id\n+                # Use instance2class_id mapping per image\n+                masks, classes = convert_segmentation_map_to_binary_masks(\n+                    segmentation_map,\n+                    instance_id,\n+                    ignore_index=ignore_index,\n+                )\n+\n+                mask_labels.append(torch.from_numpy(masks))\n+                class_labels.append(torch.from_numpy(classes))\n+\n+            # we cannot batch them since they don't share a common class size\n+            outputs[\"mask_labels\"] = mask_labels\n+            outputs[\"class_labels\"] = class_labels\n+\n+        if patch_offsets:\n+            outputs[\"patch_offsets\"] = [torch.tensor(offsets) for offsets in patch_offsets]\n+\n+        return outputs\n+\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -228,123 +311,6 @@ def _preprocess(\n \n         return processed_images, patch_offsets\n \n-    def _preprocess_images(self, images, **kwargs):\n-        \"\"\"Preprocesses the input images.\"\"\"\n-        return self._preprocess(images, **kwargs)\n-\n-    def _preprocess_masks(self, segmentation_maps: list[torch.Tensor], **kwargs):\n-        \"\"\"Preprocesses segmentation maps.\"\"\"\n-        processed_segmentation_maps = []\n-        for segmentation_map in segmentation_maps:\n-            segmentation_map = self._process_image(\n-                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n-            )\n-\n-            if segmentation_map.ndim == 2:\n-                segmentation_map = segmentation_map[None, ...]\n-\n-            processed_segmentation_maps.append(segmentation_map)\n-\n-        kwargs[\"do_normalize\"] = False\n-        kwargs[\"do_rescale\"] = False\n-        kwargs[\"input_data_format\"] = ChannelDimension.FIRST\n-\n-        # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n-\n-        processed_segmentation_maps, _ = self._preprocess(images=processed_segmentation_maps, **kwargs)\n-        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n-        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n-\n-        return processed_segmentation_maps\n-\n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        segmentation_maps: Optional[list[torch.Tensor]] = None,\n-        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n-        **kwargs: Unpack[EomtImageProcessorFastKwargs],\n-    ) -> BatchFeature:\n-        r\"\"\"\n-        segmentation_maps (`ImageInput`, *optional*):\n-            The segmentation maps to preprocess for corresponding images.\n-        instance_id_to_semantic_id (`list[dict[int, int]]` or `dict[int, int]`, *optional*):\n-            A mapping between object instance ids and class ids.\n-        \"\"\"\n-        # args are not validated, but their order in the `preprocess` and `_preprocess` signatures must be the same\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_kwargs_names)\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self._valid_kwargs_names:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-        # Prepare segmentation maps\n-        if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-\n-        # Check if resample is an int before checking if it's an instance of PILImageResampling\n-        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n-        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (int, PILImageResampling)) else resample\n-        )\n-\n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n-\n-        ignore_index = kwargs.pop(\"ignore_index\", None)\n-\n-        processed_images, patch_offsets = self._preprocess_images(images=images, **kwargs)\n-\n-        outputs = BatchFeature({\"pixel_values\": processed_images})\n-\n-        mask_labels, class_labels = [], []\n-        if segmentation_maps is not None:\n-            segmentation_maps = self._preprocess_masks(segmentation_maps=segmentation_maps, **kwargs)\n-            # Convert to list of binary masks and labels\n-            for idx, segmentation_map in enumerate(segmentation_maps):\n-                if isinstance(instance_id_to_semantic_id, list):\n-                    instance_id = instance_id_to_semantic_id[idx]\n-                else:\n-                    instance_id = instance_id_to_semantic_id\n-                # Use instance2class_id mapping per image\n-                masks, classes = convert_segmentation_map_to_binary_masks(\n-                    segmentation_map,\n-                    instance_id,\n-                    ignore_index=ignore_index,\n-                )\n-\n-                mask_labels.append(torch.from_numpy(masks))\n-                class_labels.append(torch.from_numpy(classes))\n-\n-            # we cannot batch them since they don't share a common class size\n-            outputs[\"mask_labels\"] = mask_labels\n-            outputs[\"class_labels\"] = class_labels\n-\n-        if patch_offsets:\n-            outputs[\"patch_offsets\"] = [torch.tensor(offsets) for offsets in patch_offsets]\n-\n-        return outputs\n-\n     def merge_image_patches(\n         self,\n         segmentation_logits: torch.Tensor,"
        },
        {
            "sha": "a22b95cfea970924427ec76a13fcada08fb0753d",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -157,14 +157,11 @@ def resize(\n         image = F.resize(image, size=new_size, interpolation=interpolation, **kwargs)\n         return image\n \n-    def _prepare_images_structure(\n-        self,\n-        images: ImageInput,\n-    ) -> ImageInput:\n+    def _prepare_images_structure(self, images: ImageInput, expected_ndims: int = 3) -> ImageInput:\n         \"\"\"\n         Prepare a nested images structure for processing.\n         \"\"\"\n-        return make_nested_list_of_images(images)\n+        return make_nested_list_of_images(images, expected_ndims=expected_ndims)\n \n     def split_images(\n         self,"
        },
        {
            "sha": "48e6e5b5c84f5b6d0f3a96eda95008734c8f6b85",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -205,14 +205,11 @@ class Idefics3ImageProcessorFast(BaseImageProcessorFast):\n     return_row_col_info = False\n     valid_kwargs = Idefics3FastImageProcessorKwargs\n \n-    def _prepare_images_structure(\n-        self,\n-        images: ImageInput,\n-    ) -> ImageInput:\n+    def _prepare_images_structure(self, images: ImageInput, expected_ndims: int = 3) -> ImageInput:\n         \"\"\"\n         Prepare a nested images structure for processing.\n         \"\"\"\n-        return make_nested_list_of_images(images)\n+        return make_nested_list_of_images(images, expected_ndims=expected_ndims)\n \n     def resize(\n         self,"
        },
        {
            "sha": "3dda73507006ce01dc24f7889b4bd8b7d7ea62b6",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -32,7 +32,6 @@\n     PILImageResampling,\n     SizeDict,\n     get_image_size,\n-    make_flat_list_of_images,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -95,22 +94,6 @@ def __init__(self, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]):\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaNextFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n-    def _prepare_images_structure(\n-        self,\n-        images: ImageInput,\n-    ) -> ImageInput:\n-        \"\"\"\n-        Prepare the images structure for processing.\n-\n-        Args:\n-            images (`ImageInput`):\n-                The input images to process.\n-\n-        Returns:\n-            `ImageInput`: The images with a valid nesting.\n-        \"\"\"\n-        return make_flat_list_of_images(images)\n-\n     def _resize_for_patching(\n         self,\n         image: \"torch.Tensor\","
        },
        {
            "sha": "bff75696d6c87923179e939bd5c90c8e6cbd7ba3",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -39,7 +39,6 @@\n     PILImageResampling,\n     SizeDict,\n     get_image_size,\n-    make_flat_list_of_images,\n )\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n@@ -100,22 +99,6 @@ def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImag\n         kwargs[\"batch_num_images\"] = batch_num_images\n         return super().preprocess(images, **kwargs)\n \n-    def _prepare_images_structure(\n-        self,\n-        images: ImageInput,\n-    ) -> ImageInput:\n-        \"\"\"\n-        Prepare the images structure for processing.\n-\n-        Args:\n-            images (`ImageInput`):\n-                The input images to process.\n-\n-        Returns:\n-            `ImageInput`: The images with a valid nesting.\n-        \"\"\"\n-        return make_flat_list_of_images(images)\n-\n     def _resize_for_patching(\n         self,\n         image: \"torch.Tensor\","
        },
        {
            "sha": "7b50e0cdaebcf5c3692bf2a759e8df93d383cb4e",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 58,
            "deletions": 100,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -31,9 +31,7 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    make_list_of_images,\n     pil_torch_interpolation_mapping,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -95,6 +93,63 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n \n         return label\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        images_kwargs = kwargs.copy()\n+        images_kwargs[\"do_reduce_labels\"] = False\n+        batch_feature = self._preprocess(images, **images_kwargs)\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                }\n+            )\n+\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            ).pixel_values\n+            batch_feature[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return batch_feature\n+\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -149,104 +204,7 @@ def _preprocess(\n         # Stack all processed images if return_tensors is specified\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n-        return processed_images\n-\n-    def _preprocess_images(\n-        self,\n-        images,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses images.\"\"\"\n-        kwargs[\"do_reduce_labels\"] = False\n-        processed_images = self._preprocess(images=images, **kwargs)\n-        return processed_images\n-\n-    def _preprocess_segmentation_maps(\n-        self,\n-        segmentation_maps,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses segmentation maps.\"\"\"\n-        processed_segmentation_maps = []\n-        for segmentation_map in segmentation_maps:\n-            segmentation_map = self._process_image(\n-                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n-            )\n-\n-            if segmentation_map.ndim == 2:\n-                segmentation_map = segmentation_map[None, ...]\n-\n-            processed_segmentation_maps.append(segmentation_map)\n-\n-        kwargs[\"do_normalize\"] = False\n-        kwargs[\"do_rescale\"] = False\n-        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n-        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n-        return processed_segmentation_maps\n-\n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[MobileNetV2FastImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        r\"\"\"\n-        segmentation_maps (`ImageInput`, *optional*):\n-            The segmentation maps to preprocess.\n-        \"\"\"\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-\n-        # Prepare segmentation maps\n-        if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n-\n-        images = self._preprocess_images(\n-            images=images,\n-            **kwargs,\n-        )\n-\n-        if segmentation_maps is not None:\n-            segmentation_maps = self._preprocess_segmentation_maps(\n-                segmentation_maps=segmentation_maps,\n-                **kwargs,\n-            )\n-            return BatchFeature(data={\"pixel_values\": images, \"labels\": segmentation_maps})\n-\n-        return BatchFeature(data={\"pixel_values\": images})\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit_fast.BeitImageProcessorFast.post_process_semantic_segmentation with Beit->MobileNetV2\n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):"
        },
        {
            "sha": "111fa367ffc6c15bd538ef288fe3fda48ea9b844",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 58,
            "deletions": 100,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -29,9 +29,7 @@\n     PILImageResampling,\n     SizeDict,\n     is_torch_tensor,\n-    make_list_of_images,\n     pil_torch_interpolation_mapping,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -96,6 +94,63 @@ def reduce_label(self, labels: list[\"torch.Tensor\"]):\n \n         return label\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n+        **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        images_kwargs = kwargs.copy()\n+        images_kwargs[\"do_reduce_labels\"] = False\n+        batch_feature = self._preprocess(images, **images_kwargs)\n+\n+        if segmentation_maps is not None:\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_rescale\": False,\n+                    \"do_flip_channel_order\": False,\n+                    # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                }\n+            )\n+\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            ).pixel_values\n+            batch_feature[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+\n+        return batch_feature\n+\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -154,104 +209,7 @@ def _preprocess(\n         # Stack all processed images if return_tensors is specified\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n-        return processed_images\n-\n-    def _preprocess_images(\n-        self,\n-        images,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses images.\"\"\"\n-        kwargs[\"do_reduce_labels\"] = False\n-        processed_images = self._preprocess(images=images, **kwargs)\n-        return processed_images\n-\n-    def _preprocess_segmentation_maps(\n-        self,\n-        segmentation_maps,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses segmentation maps.\"\"\"\n-        processed_segmentation_maps = []\n-        for segmentation_map in segmentation_maps:\n-            segmentation_map = self._process_image(\n-                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n-            )\n-\n-            if segmentation_map.ndim == 2:\n-                segmentation_map = segmentation_map[None, ...]\n-\n-            processed_segmentation_maps.append(segmentation_map)\n-\n-        kwargs[\"do_rescale\"] = False\n-        kwargs[\"do_flip_channel_order\"] = False\n-        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n-        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n-        return processed_segmentation_maps\n-\n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: ImageInput,\n-        segmentation_maps: Optional[ImageInput] = None,\n-        **kwargs: Unpack[MobileVitFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        r\"\"\"\n-        segmentation_maps (`ImageInput`, *optional*):\n-            The segmentation maps to preprocess.\n-        \"\"\"\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-\n-        # Prepare segmentation maps\n-        if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n-\n-        images = self._preprocess_images(\n-            images=images,\n-            **kwargs,\n-        )\n-\n-        if segmentation_maps is not None:\n-            segmentation_maps = self._preprocess_segmentation_maps(\n-                segmentation_maps=segmentation_maps,\n-                **kwargs,\n-            )\n-            return BatchFeature(data={\"pixel_values\": images, \"labels\": segmentation_maps})\n-\n-        return BatchFeature(data={\"pixel_values\": images})\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n         \"\"\""
        },
        {
            "sha": "5aa5dd88870d1108b7992997c66a5f31f7b7fe8b",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -181,7 +181,7 @@ def _preprocess(\n             device (`torch.device`, *optional*):\n                 The device to process the images on. If unset, the device is inferred from the input images.\n         \"\"\"\n-        images = self._prepare_input_images(\n+        images = self._prepare_image_like_inputs(\n             images=images,\n             do_convert_rgb=do_convert_rgb,\n             input_data_format=input_data_format,"
        },
        {
            "sha": "d4d264fdc9a7a7b32d06c29917d48c632c8a53d9",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 84,
            "deletions": 130,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -36,9 +36,7 @@\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    make_list_of_images,\n     pil_torch_interpolation_mapping,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -151,78 +149,6 @@ def resize(\n             image, size=SizeDict(height=output_height, width=output_width), interpolation=interpolation, **kwargs\n         )\n \n-    def _preprocess(\n-        self,\n-        images: list[\"torch.Tensor\"],\n-        do_resize: bool,\n-        size: SizeDict,\n-        interpolation: Optional[\"F.InterpolationMode\"],\n-        do_rescale: bool,\n-        rescale_factor: float,\n-        do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: bool,\n-        pad_size: SizeDict,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n-        **kwargs,\n-    ) -> BatchFeature:\n-        # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n-        resized_images_grouped = {}\n-        for shape, stacked_images in grouped_images.items():\n-            if do_resize:\n-                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n-            resized_images_grouped[shape] = stacked_images\n-        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n-\n-        # Group images by size for further processing\n-        # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n-        processed_images_grouped = {}\n-        for shape, stacked_images in grouped_images.items():\n-            # Fused rescale and normalize\n-            stacked_images = self.rescale_and_normalize(\n-                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n-            )\n-            if do_pad:\n-                stacked_images = self.pad_image(stacked_images, pad_size)\n-            processed_images_grouped[shape] = stacked_images\n-\n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-\n-        return processed_images\n-\n-    def _preprocess_segmentation_maps(\n-        self,\n-        segmentation_maps,\n-        **kwargs,\n-    ):\n-        \"\"\"Preprocesses segmentation maps.\"\"\"\n-        processed_segmentation_maps = []\n-        for segmentation_map in segmentation_maps:\n-            segmentation_map = self._process_image(\n-                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n-            )\n-\n-            if segmentation_map.ndim == 2:\n-                segmentation_map = segmentation_map[None, ...]\n-            processed_segmentation_maps.append(segmentation_map)\n-\n-        kwargs[\"do_rescale\"] = False\n-        kwargs[\"do_normalize\"] = False\n-        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n-        kwargs[\"size\"] = kwargs.pop(\"mask_size\")\n-        kwargs[\"pad_size\"] = kwargs.pop(\"mask_pad_size\")\n-        processed_segmentation_maps = self._preprocess(images=processed_segmentation_maps, **kwargs)\n-\n-        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)  # Remove channel dimension\n-\n-        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n-        return processed_segmentation_maps\n-\n     def _further_process_kwargs(\n         self,\n         size: Optional[SizeDict] = None,\n@@ -278,73 +204,101 @@ def preprocess(\n         segmentation_maps (`ImageInput`, *optional*):\n             The segmentation maps to preprocess.\n         \"\"\"\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-        # Prepare input images\n-        images = self._prepare_input_images(\n+        return super().preprocess(images, segmentation_maps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[ImageInput],\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[SamFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n             images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n         )\n+        original_sizes = [image.shape[-2:] for image in images]\n+        images_kwargs = kwargs.copy()\n+        pixel_values = self._preprocess(images, **images_kwargs)\n+        reshaped_input_sizes = [image.shape[-2:] for image in images]\n+        data = {\n+            \"pixel_values\": pixel_values,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n \n-        # Prepare segmentation maps\n         if segmentation_maps is not None:\n-            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # torch resize uses interpolation instead of resample\n-        resample = kwargs.pop(\"resample\")\n-        kwargs[\"interpolation\"] = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n+            processed_segmentation_maps = self._prepare_image_like_inputs(\n+                images=segmentation_maps,\n+                expected_ndims=2,\n+                do_convert_rgb=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n \n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n+            segmentation_maps_kwargs = kwargs.copy()\n+            segmentation_maps_kwargs.update(\n+                {\n+                    \"do_normalize\": False,\n+                    \"do_rescale\": False,\n+                    \"interpolation\": pil_torch_interpolation_mapping[PILImageResampling.NEAREST],\n+                    \"size\": segmentation_maps_kwargs.pop(\"mask_size\"),\n+                    \"pad_size\": segmentation_maps_kwargs.pop(\"mask_pad_size\"),\n+                }\n+            )\n+            processed_segmentation_maps = self._preprocess(\n+                images=processed_segmentation_maps, **segmentation_maps_kwargs\n+            )\n+            data[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n \n-        original_sizes = [image.shape[-2:] for image in images]\n+        return BatchFeature(data=data, tensor_type=kwargs[\"return_tensors\"])\n \n-        images = self._preprocess(\n-            images=images,\n-            **kwargs,\n-        )\n-        reshaped_input_sizes = [image.shape[-2:] for image in images]\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: bool,\n+        pad_size: SizeDict,\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> Union[\"torch.Tensor\", list[\"torch.Tensor\"]]:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n \n-        if segmentation_maps is not None:\n-            segmentation_maps = self._preprocess_segmentation_maps(\n-                segmentation_maps=segmentation_maps,\n-                **kwargs,\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n+            if do_pad:\n+                stacked_images = self.pad_image(stacked_images, pad_size)\n+            processed_images_grouped[shape] = stacked_images\n \n-            return BatchFeature(\n-                data={\n-                    \"pixel_values\": images,\n-                    \"labels\": segmentation_maps,\n-                    \"original_sizes\": original_sizes,\n-                    \"reshaped_input_sizes\": reshaped_input_sizes,\n-                },\n-                tensor_type=kwargs[\"return_tensors\"],\n-            )\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n-        return BatchFeature(\n-            data={\n-                \"pixel_values\": images,\n-                \"original_sizes\": original_sizes,\n-                \"reshaped_input_sizes\": reshaped_input_sizes,\n-            },\n-            tensor_type=kwargs[\"return_tensors\"],\n-        )\n+        return processed_images\n \n     def generate_crop_boxes(\n         self,"
        },
        {
            "sha": "c824e0a73630e22416866a7050873b8377b0d6ed",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -195,14 +195,11 @@ class SmolVLMImageProcessorFast(BaseImageProcessorFast):\n     return_row_col_info = False\n     valid_kwargs = SmolVLMFastImageProcessorKwargs\n \n-    def _prepare_images_structure(\n-        self,\n-        images: ImageInput,\n-    ) -> ImageInput:\n+    def _prepare_images_structure(self, images: ImageInput, expected_ndims: int = 3) -> ImageInput:\n         \"\"\"\n         Prepare a nested images structure for processing.\n         \"\"\"\n-        return make_nested_list_of_images(images)\n+        return make_nested_list_of_images(images, expected_ndims=expected_ndims)\n \n     def resize(\n         self,"
        },
        {
            "sha": "e2cd7d331253e02228de55ae99db347f547fba4f",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 32,
            "deletions": 83,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3ebc761e27397fae2e3a8d00ec3b825533c37e6/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=b3ebc761e27397fae2e3a8d00ec3b825533c37e6",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Fast Image processor class for ViTMatte.\"\"\"\n \n-from functools import partial\n from typing import Optional, Union\n \n from ...image_processing_utils import BatchFeature\n@@ -30,8 +29,6 @@\n     ChannelDimension,\n     ImageInput,\n     get_image_size,\n-    make_list_of_images,\n-    validate_kwargs,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -85,86 +82,6 @@ class VitMatteImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[VitMatteFastImageProcessorKwargs]) -> None:\n         super().__init__(**kwargs)\n \n-    @auto_docstring\n-    def preprocess(\n-        self,\n-        images: list[\"torch.Tensor\"],\n-        trimaps: list[\"torch.Tensor\"],\n-        **kwargs: Unpack[VitMatteFastImageProcessorKwargs],\n-    ) -> BatchFeature:\n-        r\"\"\"\n-        trimaps (`list[torch.Tensor]`):\n-            The trimaps to preprocess.\n-        \"\"\"\n-        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self.valid_kwargs.__annotations__.keys())\n-        # Set default kwargs from self. This ensures that if a kwarg is not provided\n-        # by the user, it gets its default value from the instance, or is set to None.\n-\n-        for kwarg_name in self.valid_kwargs.__annotations__:\n-            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n-\n-        # Extract parameters that are only used for preparing the input images\n-        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n-        input_data_format = kwargs.pop(\"input_data_format\")\n-        device = kwargs.pop(\"device\")\n-\n-        # Prepare input images\n-        images = self._prepare_input_images(\n-            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-        )\n-\n-        # Prepare input trimaps\n-        trimaps = self._prepare_input_trimaps(trimaps=trimaps, device=device)\n-\n-        # Update kwargs that need further processing before being validated\n-        kwargs = self._further_process_kwargs(**kwargs)\n-\n-        # Validate kwargs\n-        self._validate_preprocess_kwargs(**kwargs)\n-\n-        # Pop kwargs that are not needed in _preprocess\n-        kwargs.pop(\"resample\")\n-        kwargs.pop(\"default_to_square\")\n-        kwargs.pop(\"data_format\")\n-        kwargs.pop(\"do_resize\")\n-        kwargs.pop(\"do_center_crop\")\n-        kwargs.pop(\"size\")\n-        kwargs.pop(\"crop_size\")\n-\n-        return self._preprocess(images, trimaps, **kwargs)\n-\n-    def _prepare_input_trimaps(\n-        self, trimaps: ImageInput, device: Optional[\"torch.device\"] = None\n-    ) -> list[\"torch.Tensor\"]:\n-        \"\"\"\n-        Prepare input trimaps for processing,m this can not yet deal with nested list\n-\n-        Args:\n-            trimaps ('ImageInout):\n-                The input trimaps to be process, should not be nested\n-            device('Optional['torch.device'] defaults to 'self.device'):\n-                The device to process the trimaps on\n-\n-        Returns:\n-            list['torch.Tensor']:\n-                Input trimaps converted to a list of tensors\n-        \"\"\"\n-        # from batch or single image to list, and insert channel dimension\n-        trimaps = make_list_of_images(trimaps, expected_ndims=2)\n-\n-        # passing ChannelDimension.First achieves correct functionality on grayscale/single channel\n-        process_image_fn = partial(\n-            self._process_image,\n-            input_data_format=ChannelDimension.FIRST,\n-            device=device,\n-        )\n-\n-        processed_trimaps = []\n-        for trimap in trimaps:\n-            processed_trimaps.append(torch.unsqueeze(process_image_fn(trimap), dim=0))\n-\n-        return processed_trimaps\n-\n     def _pad_image(\n         self,\n         images: \"torch.tensor\",\n@@ -190,6 +107,38 @@ def _pad_image(\n \n         return images\n \n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        trimaps: list[\"torch.Tensor\"],\n+        **kwargs: Unpack[VitMatteFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        trimaps (`list[torch.Tensor]`):\n+            The trimaps to preprocess.\n+        \"\"\"\n+        return super().preprocess(images, trimaps, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        trimaps: ImageInput,\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[VitMatteFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        \"\"\"\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        trimaps = self._prepare_image_like_inputs(images=trimaps, expected_ndims=2, device=device)\n+\n+        return self._preprocess(images, trimaps, **kwargs)\n+\n     @filter_out_non_signature_kwargs()\n     def _preprocess(\n         self,"
        }
    ],
    "stats": {
        "total": 1277,
        "additions": 474,
        "deletions": 803
    }
}