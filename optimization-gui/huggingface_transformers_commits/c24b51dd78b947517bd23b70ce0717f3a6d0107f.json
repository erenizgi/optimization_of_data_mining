{
    "author": "jiqing-feng",
    "message": "Fix xpu output check for Ministral3 tests (#42761)\n\n* fix xpu output check for ministral3\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix xpu check\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* use Expectations\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* use asser eaqual\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "c24b51dd78b947517bd23b70ce0717f3a6d0107f",
    "files": [
        {
            "sha": "9fd7cd4a96e98663a79e04515f8e3817a731d916",
            "filename": "tests/models/ministral3/test_modeling_ministral3.py",
            "status": "modified",
            "additions": 21,
            "deletions": 4,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c24b51dd78b947517bd23b70ce0717f3a6d0107f/tests%2Fmodels%2Fministral3%2Ftest_modeling_ministral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c24b51dd78b947517bd23b70ce0717f3a6d0107f/tests%2Fmodels%2Fministral3%2Ftest_modeling_ministral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral3%2Ftest_modeling_ministral3.py?ref=c24b51dd78b947517bd23b70ce0717f3a6d0107f",
            "patch": "@@ -21,8 +21,10 @@\n \n from transformers import AutoTokenizer, Mistral3ForConditionalGeneration, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     backend_empty_cache,\n     cleanup,\n+    require_deterministic_for_xpu,\n     require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n@@ -89,18 +91,33 @@ def test_model_3b_logits(self):\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-1.1503, -1.9935, -0.4457, -1.0717, -1.9182, -1.1431, -0.9697, -1.7098]])\n+        # fmt: off\n+        EXPECTED_MEANS = Expectations(\n+            {\n+                (\"cuda\", None): torch.tensor([[-1.1503, -1.9935, -0.4457, -1.0717, -1.9182, -1.1431, -0.9697, -1.7098]]),\n+                (\"xpu\", None): torch.tensor([[-0.9800, -2.4773, -0.2386, -1.0664, -1.8994, -1.3792, -1.0531, -1.8832]]),\n+            }\n+        )\n+        # fmt: on\n+        EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n \n         del model\n         backend_empty_cache(torch_device)\n         gc.collect()\n \n     @slow\n+    @require_deterministic_for_xpu\n     def test_model_3b_generation(self):\n-        EXPECTED_TEXT_COMPLETION = (\n-            \"My favourite condiment is 100% pure olive oil. It's a staple in my kitchen and I use it in\"\n+        # fmt: off\n+        EXPECTED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", None): \"My favourite condiment is 100% pure olive oil. It's a staple in my kitchen and I use it in\",\n+                (\"xpu\", None): \"My favourite condiment is iced tea. I love the way it makes me feel. Itâ€™s like a little bubble bath for\",\n+            }\n         )\n+        # fmt: on\n+        EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         prompt = \"My favourite condiment is \"\n         tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Ministral-3-3B-Instruct-2512\")\n         model = Mistral3ForConditionalGeneration.from_pretrained(\n@@ -111,7 +128,7 @@ def test_model_3b_generation(self):\n         # greedy generation outputs\n         generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n-        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+        self.assertEqual(text, EXPECTED_TEXT)\n \n         del model\n         backend_empty_cache(torch_device)"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 21,
        "deletions": 4
    }
}