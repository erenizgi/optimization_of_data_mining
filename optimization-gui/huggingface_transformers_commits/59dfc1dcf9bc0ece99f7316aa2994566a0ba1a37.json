{
    "author": "Cyrilvallez",
    "message": "Fix device_map computation part 2 (#42290)\n\nfix",
    "sha": "59dfc1dcf9bc0ece99f7316aa2994566a0ba1a37",
    "files": [
        {
            "sha": "9696309ef221c7bd9553087a6df556a3e9149f07",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/59dfc1dcf9bc0ece99f7316aa2994566a0ba1a37/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/59dfc1dcf9bc0ece99f7316aa2994566a0ba1a37/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=59dfc1dcf9bc0ece99f7316aa2994566a0ba1a37",
            "patch": "@@ -204,13 +204,18 @@ def check_and_set_device_map(device_map: \"torch.device | int | str | dict | None\n \n \n def compute_module_sizes(\n-    model: \"PreTrainedModel\", hf_quantizer: \"HfQuantizer | None\" = None, buffers_only: bool = False\n+    model: \"PreTrainedModel\",\n+    hf_quantizer: \"HfQuantizer | None\" = None,\n+    buffers_only: bool = False,\n+    only_modules: bool = True,\n ) -> tuple[dict[str, int], dict[str, int]]:\n     \"\"\"\n     Compute the size of each submodule of a given model (in bytes).\n     Returns a tuple of 2 dicts, the fist one containing a mapping of all the modules and the corresponding size\n     in bytes, and the 2nd one containing a mapping from all leaf modules (modules containing parameters, the end of\n     the model graph) and the corresponding sizes.\n+    If `only_modules` is set to False, the first mapping will not only contain the size of all modules, but also\n+    the size of all parameters and buffers.\n     \"\"\"\n     all_module_sizes = defaultdict(int)\n     leaves_module_sizes = defaultdict(int)\n@@ -241,6 +246,9 @@ def all_tensors():\n             all_module_sizes[\".\".join(name_parts[:idx])] += size\n         if \".\" in name:\n             leaves_module_sizes[name.rsplit(\".\", 1)[0]] += size\n+        # If we want to also have the full leaves in `all_module_sizes`\n+        if not only_modules:\n+            all_module_sizes[name] += size\n \n     return all_module_sizes, leaves_module_sizes\n \n@@ -542,7 +550,7 @@ def _init_infer_auto_device_map(\n     else:\n         main_devices = [\"cpu\"]\n \n-    module_sizes, _ = compute_module_sizes(model, hf_quantizer)\n+    module_sizes, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n \n     if tied_parameters is None:\n         if len(model.all_tied_weights_keys) > 0:"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 10,
        "deletions": 2
    }
}