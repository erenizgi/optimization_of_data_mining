{
    "author": "itazap",
    "message": "refactor more tokenizers - v5 guide update (#42768)\n\n* refactor more tokenizers - v5 guide update\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update MIGRATION_GUIDE_V5.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* migration guide\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "6994c5ac32d0f930030a330d396b8e8d248161a8",
    "files": [
        {
            "sha": "dd66b4207d16ec3120365cbbf3092659d2c941db",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/6994c5ac32d0f930030a330d396b8e8d248161a8/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6994c5ac32d0f930030a330d396b8e8d248161a8/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=6994c5ac32d0f930030a330d396b8e8d248161a8",
            "patch": "@@ -165,6 +165,9 @@ If you want something even higher up the stack, then `PreTrainedTokenizerBase` i\n - `save_pretrained`\n - among a few others\n \n+**Note for implementing new tokenizers:** When creating a tokenizer class that loads from SentencePiece files, you can override the `convert_from_spm` class method in your converter to customize vocabulary structure, normalizers, regexes and anything that you would want to be passed to the tokenizers your are converting. \n+This is useful if the model requires specific token ordering or special split regex patterns. See existing converter classes in `convert_slow_tokenizer.py` for examples.\n+\n ### API Changes\n \n #### 1. Direct tokenizer initialization with vocab and merges\n@@ -192,7 +195,9 @@ tokenizer = LlamaTokenizer(vocab=vocab, merges=merges)\n \n This tokenizer will behave as a Llama-like tokenizer, with an updated vocabulary. This allows comparing different tokenizer classes with the same vocab; therefore enabling the comparison of different pre-tokenizers, normalizers, etc.\n \n-⚠️ The `vocab_file` (as in, a path towards a file containing the vocabulary) cannot be used to initialize the `LlamaTokenizer` as loading from files is reserved to the `from_pretrained` method.\n+**Simplified file loading:** Support is added for passing`vocab` and `merges` as file paths directly to tokenizer initialization. The tokenizer will automatically detect the format (SentencePiece `.model`, Tekken `tekken.json`, or plain vocab/merges files) for loading. For BPE tokenizers, if a vocab is provided but no merges, merges will be automatically generated (excluding special tokens).\n+\n+Note: Loading from file paths with `vocab=\"<path_to_a_file>\"`'s primary goal is to allow you to do some quick testing, but for `BPE` models for example we don't check wether you properly passed the merges or not. \n \n #### 2. Simplified decoding API\n \n@@ -243,6 +248,9 @@ We simplify the serialization of tokenization attributes:\n - `special_tokens_map.json` - special tokens are now stored in `tokenizer_config.json`.\n - `added_tokens.json` - added tokens are now stored in `tokenizer.json`.\n - `added_tokens_decoder` is only stored when there is no `tokenizer.json`.\n+- `add_bos_token` and `add_eos_token` - these are no longer saved in `tokenizer_config.json`. When a `tokenizer.json` file exists, these settings are defined in the tokenizer class or `tokenizer.json` itself.\n+\n+**Backend synchronization removed:** The automatic synchronization logic that updated backend tokenizer settings (like `add_prefix_space`, `do_lower_case`, `strip_accents`, `tokenize_chinese_chars`) after initialization has been removed. Tokenizer behavior is now fully determined by the `tokenizer.json` file or class definition at initialization time. \n \n When loading older tokenizers, these files are still read for backward compatibility, but new saves use the consolidated format. We're gradually moving towards consolidating attributes to fewer files so that other libraries and implementations may depend on them more reliably.\n \n@@ -256,7 +264,6 @@ Several models that had identical tokenizers now import from their base implemen\n - **LXMert** → uses BertTokenizer\n - **MT5** → uses T5Tokenizer\n - **MVP** → uses BartTokenizer\n-\n These modules will eventually be removed altogether.\n \n **Removed T5-specific workarounds**"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 9,
        "deletions": 2
    }
}