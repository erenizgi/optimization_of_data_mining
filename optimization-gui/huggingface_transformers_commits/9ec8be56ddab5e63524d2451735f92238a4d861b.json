{
    "author": "Rocketknight1",
    "message": "TransfoXL is deprecated, don't keep it in tested examples! (#37707)\n\n* TransfoXL is deprecated, so we should remove it from examples that get tested\n\n* Remove the tokenizer too\n\n* Trigger tests",
    "sha": "9ec8be56ddab5e63524d2451735f92238a4d861b",
    "files": [
        {
            "sha": "9943a4f54a1e81d3db3044f281f759ef4f26b4cd",
            "filename": "examples/pytorch/text-generation/run_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9ec8be56ddab5e63524d2451735f92238a4d861b/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9ec8be56ddab5e63524d2451735f92238a4d861b/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py?ref=9ec8be56ddab5e63524d2451735f92238a4d861b",
            "patch": "@@ -38,8 +38,6 @@\n     OpenAIGPTLMHeadModel,\n     OpenAIGPTTokenizer,\n     OPTForCausalLM,\n-    TransfoXLLMHeadModel,\n-    TransfoXLTokenizer,\n     XLMTokenizer,\n     XLMWithLMHeadModel,\n     XLNetLMHeadModel,\n@@ -62,7 +60,6 @@\n     \"ctrl\": (CTRLLMHeadModel, CTRLTokenizer),\n     \"openai-gpt\": (OpenAIGPTLMHeadModel, OpenAIGPTTokenizer),\n     \"xlnet\": (XLNetLMHeadModel, XLNetTokenizer),\n-    \"transfo-xl\": (TransfoXLLMHeadModel, TransfoXLTokenizer),\n     \"xlm\": (XLMWithLMHeadModel, XLMTokenizer),\n     \"gptj\": (GPTJForCausalLM, AutoTokenizer),\n     \"bloom\": (BloomForCausalLM, BloomTokenizerFast),\n@@ -368,10 +365,7 @@ def main():\n         prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\n         preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)\n \n-        if model.__class__.__name__ in [\"TransfoXLLMHeadModel\"]:\n-            tokenizer_kwargs = {\"add_space_before_punct_symbol\": True}\n-        else:\n-            tokenizer_kwargs = {}\n+        tokenizer_kwargs = {}\n \n         encoded_prompt = tokenizer.encode(\n             preprocessed_prompt_text, add_special_tokens=False, return_tensors=\"pt\", **tokenizer_kwargs"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 1,
        "deletions": 7
    }
}