{
    "author": "cyyever",
    "message": "Unify is_torchvision_v2_available with is_torchvision_available (#41259)\n\nFix is_torchvision_v2_available\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "f1b64c5b069c24494da845d2995dae0c83fef047",
    "files": [
        {
            "sha": "4dfa7f08b0dbb59ccc6d80f5c33004072ae13ca8",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -46,7 +46,6 @@\n     auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n-    is_torchvision_v2_available,\n     is_vision_available,\n     logging,\n )\n@@ -60,14 +59,13 @@\n     import torch\n \n if is_torchvision_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n     from .image_utils import pil_torch_interpolation_mapping\n+\n else:\n     pil_torch_interpolation_mapping = None\n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-elif is_torchvision_available():\n-    from torchvision.transforms import functional as F\n \n logger = logging.get_logger(__name__)\n "
        },
        {
            "sha": "36ed821e696a9902919fe43d6b13b09f80d07bac",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -28,7 +28,6 @@\n     is_torch_available,\n     is_torch_tensor,\n     is_torchvision_available,\n-    is_torchvision_v2_available,\n     is_vision_available,\n     logging,\n     requires_backends,\n@@ -54,9 +53,7 @@\n         from torchvision.transforms import InterpolationMode\n \n         pil_torch_interpolation_mapping = {\n-            PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else InterpolationMode.NEAREST,\n+            PILImageResampling.NEAREST: InterpolationMode.NEAREST_EXACT,\n             PILImageResampling.BOX: InterpolationMode.BOX,\n             PILImageResampling.BILINEAR: InterpolationMode.BILINEAR,\n             PILImageResampling.HAMMING: InterpolationMode.HAMMING,"
        },
        {
            "sha": "7ff894127ecd99f1b1f0094eae04e5d8ad25272d",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -38,16 +39,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class BeitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     r\"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):"
        },
        {
            "sha": "5be6f9f6c54b7bf6e973b9102179b63cbfe353d8",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -18,6 +18,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -31,13 +32,7 @@\n     reorder_images,\n )\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n-from ...utils import auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import auto_docstring\n \n \n def make_pixel_mask("
        },
        {
            "sha": "1d102614f7df3700845c00f9d8bfa217930c776b",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -19,17 +19,13 @@\n import numpy as np\n import PIL\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast\n from ...image_utils import ImageInput, PILImageResampling, SizeDict\n-from ...utils import auto_docstring, is_torchvision_v2_available, logging\n+from ...utils import auto_docstring, logging\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "322e98dbd0f592fd152a2f87c3c5b9849d4dee29",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -24,6 +24,7 @@\n \n import numpy as np\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -34,13 +35,7 @@\n )\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import TensorType, auto_docstring\n \n \n class Cohere2VisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):"
        },
        {
            "sha": "351d4fa1470f154afcc8f04d14aa3fcbd6fb43b9",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -10,6 +10,7 @@\n import torch\n from torch import nn\n from torchvision.io import read_image\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -33,7 +34,7 @@\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, logging\n+from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n from .image_processing_conditional_detr import (\n     compute_segments,\n@@ -43,12 +44,6 @@\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -433,13 +428,7 @@ def resize_annotation(\n             resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = (\n-            interpolation\n-            if interpolation is not None\n-            else F.InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else F.InterpolationMode.NEAREST\n-        )\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "3ab00c0fd091369715e636bac663fbbbdc9239a0",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -37,16 +38,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class ConvNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     crop_pct (`float`, *optional*):"
        },
        {
            "sha": "45f4fd2bdb93f4e19080e4a0f460f41aaa143383",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -38,12 +38,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import (\n-    TensorType,\n-    filter_out_non_signature_kwargs,\n-    is_vision_available,\n-    logging,\n-)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n if is_vision_available():"
        },
        {
            "sha": "ce884da8d08bb8680152bd2a58e537f876919f0f",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -29,11 +29,7 @@\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_deepseek_vl import DeepseekVLConfig\n "
        },
        {
            "sha": "8458d02d58a52635f38361e6da05c22294c0a0f6",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -9,6 +9,7 @@\n \n import torch\n from torchvision.io import read_image\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -32,17 +33,11 @@\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, logging\n+from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n from .image_processing_deformable_detr import get_size_with_aspect_ratio\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -427,13 +422,7 @@ def resize_annotation(\n             resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = (\n-            interpolation\n-            if interpolation is not None\n-            else F.InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else F.InterpolationMode.NEAREST\n-        )\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "bc621e0ffc261a7a404f24dc8f0d7293e5be3a46",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -30,7 +30,6 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n     requires_backends,\n )\n@@ -41,10 +40,7 @@\n     from .modeling_depth_pro import DepthProDepthEstimatorOutput\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from torchvision.transforms.v2 import functional as F\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "a2ac8d03eed353d522b6510155da4d2d49c0e8f8",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch import nn\n from torchvision.io import read_image\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -49,7 +50,6 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from ...utils.import_utils import requires\n@@ -61,12 +61,6 @@\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n@@ -450,13 +444,7 @@ def resize_annotation(\n             resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = (\n-            interpolation\n-            if interpolation is not None\n-            else F.InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else F.InterpolationMode.NEAREST\n-        )\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "7c080485ed008bc8bfa78e393e6b408fe86d172f",
            "filename": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,24 +17,19 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from transformers.image_processing_base import BatchFeature\n from transformers.image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n from transformers.image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling, SizeDict\n from transformers.utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from transformers.utils.import_utils import requires\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "29e06831b1b48d1bccc6945a49c675f9bcd01e9a",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import group_images_by_shape, reorder_images\n@@ -25,16 +26,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "1463ef405f37db1fad1f7d5435494197f5bef9f9",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -39,17 +39,13 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n if TYPE_CHECKING:\n     from .modeling_efficientloftr import KeypointMatchingOutput\n \n-if is_torchvision_v2_available():\n-    import torchvision.transforms.v2.functional as F\n-else:\n-    import torchvision.transforms.functional as F\n+import torchvision.transforms.v2.functional as F\n \n \n def _is_valid_image(image):"
        },
        {
            "sha": "77e787614a10f204e2ed1716595c38b7a7ba0a6f",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -18,6 +18,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import group_images_by_shape, reorder_images\n@@ -26,16 +27,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class EfficientNetFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     Args:"
        },
        {
            "sha": "ca80231d3a7646411e1134e8e97986100da8b025",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -19,6 +19,7 @@\n \n import numpy as np\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -40,7 +41,6 @@\n     TensorType,\n     auto_docstring,\n     filter_out_non_signature_kwargs,\n-    is_torchvision_v2_available,\n )\n from .image_processing_eomt import (\n     compute_segments,\n@@ -50,12 +50,6 @@\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class EomtImageProcessorFastKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     do_split_image (`bool`, *optional*, defaults to `False`):\n@@ -204,9 +198,7 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n-                    if is_torchvision_v2_available()\n-                    else F.InterpolationMode.NEAREST,\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT,\n                 }\n             )\n "
        },
        {
            "sha": "732d25e71f697e083e784199c1db1782298951ac",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -21,6 +21,7 @@\n from typing import Any, Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -34,7 +35,6 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n from .image_processing_flava import (\n     FLAVA_CODEBOOK_MEAN,\n@@ -45,12 +45,6 @@\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class FlavaMaskingGenerator:\n     def __init__(\n         self,"
        },
        {
            "sha": "c61152bc6b222b969bfcbe27f12742968277d6d7",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -19,6 +19,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -32,16 +33,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "8cdf31a437ae305fbfc399bd276476f9b453ed6f",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import (\n     BatchFeature,\n@@ -38,17 +39,11 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from .image_processing_glm4v import smart_resize\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "a47a1422a5dc5dcee6c9d75cb7436a1180c32829",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -30,17 +31,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n from .image_processing_got_ocr2 import get_optimal_tiled_canvas\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class GotOcr2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     crop_to_patches (`bool`, *optional*, defaults to `False`):"
        },
        {
            "sha": "744cb5f92923068601add8ea3890ea55a1b09396",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -9,6 +9,7 @@\n \n import torch\n from torchvision.io import read_image\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -32,7 +33,7 @@\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, logging\n+from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n from .image_processing_grounding_dino import get_size_with_aspect_ratio\n \n@@ -41,12 +42,6 @@\n     from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -459,13 +454,7 @@ def resize_annotation(\n             resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = (\n-            interpolation\n-            if interpolation is not None\n-            else F.InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else F.InterpolationMode.NEAREST\n-        )\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "7a6bcc53ae1acaefa1f4a923f7d5b422c7a62d8e",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -18,6 +18,7 @@\n \n import numpy as np\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -30,16 +31,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n def squared_euclidean_distance_torch(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n     \"\"\"\n     Compute squared Euclidean distances between all pixels and clusters."
        },
        {
            "sha": "d2fe3cc7f343b02c463fb39f12520da7423e4651",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -20,21 +20,16 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import TensorType, is_torchvision_v2_available\n+from ...utils import TensorType\n from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import group_videos_by_shape, reorder_videos\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class InstructBlipVideoVideoProcessorInitKwargs(VideosKwargs): ...\n \n "
        },
        {
            "sha": "96d7d3067f73f6b3af0b0f5155564dd6965f7e59",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,21 +17,16 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import TensorType, is_torchvision_v2_available\n+from ...utils import TensorType\n from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class InternVLVideoProcessorInitKwargs(VideosKwargs):\n     initial_shift: Union[bool, float, int]\n "
        },
        {
            "sha": "6cbca591626ecbc27ba7564ffcb7f351d14f9b72",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -36,16 +37,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class JanusFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     r\"\"\"\n     min_size (`int`, *optional*, defaults to 14):"
        },
        {
            "sha": "354bbe21c4dba00fcb1b373d916950e9f643b7ab",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n@@ -25,18 +26,12 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n     requires_backends,\n )\n from .image_processing_layoutlmv2 import apply_tesseract\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "caefa9b89660b8f7fe3253611fcc40eb1bbeb9e8",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n@@ -25,18 +26,12 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n     requires_backends,\n )\n from .image_processing_layoutlmv3 import apply_tesseract\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "ae30194288fa173067c344f6856eb604aa630251",
            "filename": "src/transformers/models/levit/image_processing_levit_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fimage_processing_levit_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,20 +17,15 @@\n from typing import Optional\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, SizeDict\n from ...image_transforms import (\n     ChannelDimension,\n     get_resize_output_image_size,\n )\n from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling\n-from ...utils import auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import auto_docstring\n \n \n @auto_docstring"
        },
        {
            "sha": "4081c86e108ac161bee3e0cf30a7c9cb52a46ec1",
            "filename": "src/transformers/models/lfm2_vl/image_processing_lfm2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -38,16 +39,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "6506d5749d946194c5d6757d6680ce50144bdd78",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -20,6 +20,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -33,16 +34,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n def get_factors(dividend: int) -> set[int]:\n     \"\"\"\n     Calculate all factors of a given number, i.e. a divisor that leaves"
        },
        {
            "sha": "5960700405494208325f970ee3a1495cfddc6a87",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -38,16 +39,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class LlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n \n "
        },
        {
            "sha": "df20e2b90e8323ff17ed2c80d6d5369aba85b428",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_processing_utils_fast import (\n@@ -39,16 +40,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class LlavaNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     image_grid_pinpoints (`list[list[int]]`, *optional*):"
        },
        {
            "sha": "58dbb09d63194a6af061f634d14169d9a1f9b164",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -23,6 +23,7 @@\n \n import torch\n from torch import nn\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -42,7 +43,7 @@\n     PILImageResampling,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, logging\n+from ...utils import TensorType, auto_docstring, logging\n from .image_processing_mask2former import (\n     compute_segments,\n     convert_segmentation_to_rle,\n@@ -51,11 +52,6 @@\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -348,9 +344,7 @@ def _preprocess(\n                         image=grouped_segmentation_maps[shape],\n                         size=size,\n                         size_divisor=size_divisor,\n-                        interpolation=F.InterpolationMode.NEAREST_EXACT\n-                        if is_torchvision_v2_available()\n-                        else F.InterpolationMode.NEAREST,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT,\n                     )\n             resized_images_grouped[shape] = stacked_images\n             if segmentation_maps is not None:"
        },
        {
            "sha": "9e15486cfa3524347f0fab1dc94193fd73729398",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -20,6 +20,7 @@\n \n import torch\n from torch import nn\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -42,7 +43,6 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from .image_processing_maskformer import (\n@@ -53,11 +53,6 @@\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -354,9 +349,7 @@ def _preprocess(\n                         image=grouped_segmentation_maps[shape],\n                         size=size,\n                         size_divisor=size_divisor,\n-                        interpolation=F.InterpolationMode.NEAREST_EXACT\n-                        if is_torchvision_v2_available()\n-                        else F.InterpolationMode.NEAREST,\n+                        interpolation=F.InterpolationMode.NEAREST_EXACT,\n                     )\n             resized_images_grouped[shape] = stacked_images\n             if segmentation_maps is not None:"
        },
        {
            "sha": "6c40fbf3f9b877d2d8c910b0473530442db7637d",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -38,16 +39,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class MobileNetV2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n@@ -134,9 +128,7 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n-                    if is_torchvision_v2_available()\n-                    else F.InterpolationMode.NEAREST,\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT,\n                 }\n             )\n "
        },
        {
            "sha": "fab16ecfdc878f3b6b9f3ecdae97eb474d8792d6",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -36,16 +37,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class MobileVitFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     do_flip_channel_order (`bool`, *optional*, defaults to `self.do_flip_channel_order`):\n@@ -135,9 +129,7 @@ def _preprocess_image_like_inputs(\n                     \"do_rescale\": False,\n                     \"do_flip_channel_order\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n-                    if is_torchvision_v2_available()\n-                    else F.InterpolationMode.NEAREST,\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT,\n                 }\n             )\n "
        },
        {
            "sha": "15cee9051082965f8db7bc62c16ede09e401517d",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -40,16 +41,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class NougatFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     Args:"
        },
        {
            "sha": "4a20a04e70f2babedd8f447befbb51fcc9ffa448",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -18,6 +18,7 @@\n \n import torch\n from torch import nn\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -39,17 +40,11 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from .image_processing_oneformer import load_metadata, prepare_metadata\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -453,11 +448,7 @@ def _preprocess(\n             for shape, stacked_segmentation_maps in grouped_segmentation_maps.items():\n                 if do_resize:\n                     stacked_segmentation_maps = self.resize(\n-                        stacked_segmentation_maps,\n-                        size=size,\n-                        interpolation=F.InterpolationMode.NEAREST_EXACT\n-                        if is_torchvision_v2_available()\n-                        else F.InterpolationMode.NEAREST,\n+                        stacked_segmentation_maps, size=size, interpolation=F.InterpolationMode.NEAREST_EXACT\n                     )\n                 processed_segmentation_maps_grouped[shape] = stacked_segmentation_maps\n             processed_segmentation_maps = reorder_images("
        },
        {
            "sha": "04b79299e9e14f4c69aa6a39c0d51d04e25e79f8",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -16,6 +16,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -35,17 +36,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n from .image_processing_ovis2 import get_min_tile_covering_grid, get_optimal_tiled_canvas\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class Ovis2ImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     Args:"
        },
        {
            "sha": "72cb17cd40cdf151a8e4182fd19e7337ddbbdb3f",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,23 +17,17 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature\n from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling, SizeDict\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n @auto_docstring\n class PerceiverImageProcessorFast(BaseImageProcessorFast):\n     resample = PILImageResampling.BICUBIC"
        },
        {
            "sha": "4bd9928daa94f0ec51413f889552c1de5dccafe8",
            "filename": "src/transformers/models/phi4_multimodal/image_processing_phi4_multimodal_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fimage_processing_phi4_multimodal_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -16,6 +16,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -27,16 +28,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "0893af3830f90d89a1e051519e1beebeef04eeb8",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -30,17 +31,11 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from .image_processing_pixtral import get_resize_output_image_size\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "62d5f276859f4ed754f36499316236eb962671b8",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import (\n@@ -38,16 +39,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class PoolFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     Args:"
        },
        {
            "sha": "06d6ed1564432a50071311a75459a90d8dbbb8a5",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -24,6 +24,7 @@\n if TYPE_CHECKING:\n     from ...modeling_outputs import DepthEstimatorOutput\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -42,17 +43,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     requires_backends,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n def _constrain_to_multiple_of(val, multiple, min_val=0, max_val=None):\n     \"\"\"Constrain a value to be a multiple of another value.\"\"\"\n     x = round(val / multiple) * multiple"
        },
        {
            "sha": "ec9878da32221a233b5f501a2b2f6f97871689cf",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -22,6 +22,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -42,18 +43,12 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from ...video_utils import VideoInput, make_batched_videos\n from .image_processing_qwen2_vl import smart_resize\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "84bcd827f02efd3798e3571e68785e8c14a6c16b",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -23,6 +23,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import (\n@@ -34,18 +35,12 @@\n     get_image_size,\n )\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import TensorType, add_start_docstrings, is_torchvision_v2_available\n+from ...utils import TensorType, add_start_docstrings\n from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n from .image_processing_qwen2_vl import smart_resize\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class Qwen2VLVideoProcessorInitKwargs(VideosKwargs):\n     min_pixels: Optional[int]\n     max_pixels: Optional[int]"
        },
        {
            "sha": "9aae271deaccbab9784dd7e02059eba29d32d5f2",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -32,7 +32,7 @@\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, requires_backends\n+from ...utils import TensorType, auto_docstring, requires_backends\n from ...utils.import_utils import requires\n from .image_processing_rt_detr import get_size_with_aspect_ratio\n \n@@ -242,13 +242,7 @@ def resize_annotation(\n             resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = (\n-            interpolation\n-            if interpolation is not None\n-            else F.InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else F.InterpolationMode.NEAREST\n-        )\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "7cb5f7b2bfc20be423acb66f23f169b92270bb86",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -23,6 +23,7 @@\n import torch\n from torch.nn import functional as F\n from torchvision.ops.boxes import batched_nms\n+from torchvision.transforms.v2 import functional as F_t\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -39,13 +40,7 @@\n     pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F_t\n-else:\n-    from torchvision.transforms import functional as F_t\n+from ...utils import auto_docstring\n \n \n class SamFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n@@ -223,9 +218,7 @@ def _preprocess_image_like_inputs(\n                 {\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n-                    \"interpolation\": F_t.InterpolationMode.NEAREST_EXACT\n-                    if is_torchvision_v2_available()\n-                    else F_t.InterpolationMode.NEAREST,\n+                    \"interpolation\": F_t.InterpolationMode.NEAREST_EXACT,\n                     \"size\": segmentation_maps_kwargs.pop(\"mask_size\"),\n                     \"pad_size\": segmentation_maps_kwargs.pop(\"mask_pad_size\"),\n                 }"
        },
        {
            "sha": "30e99980f4d1df3442d12e2eb7d28d607587b7fc",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -40,10 +40,7 @@\n     pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TensorType,\n-    auto_docstring,\n-)\n+from ...utils import TensorType, auto_docstring\n \n \n class Sam2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):"
        },
        {
            "sha": "fe42cc39cacf1a29b42f633fd3dcb4a744d8afc7",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -37,10 +37,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import compile_compatible_method_lru_cache\n-from ...utils import (\n-    ModelOutput,\n-    auto_docstring,\n-)\n+from ...utils import ModelOutput, auto_docstring\n from ...utils.generic import TransformersKwargs, check_model_inputs\n from ..auto import AutoModel\n from .configuration_sam2 import ("
        },
        {
            "sha": "45261fab2cd0e7195d05bc0d2a8dafc71b6d2e19",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -32,17 +33,11 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from .image_processing_siglip2 import get_image_size_for_max_num_patches\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "522a344b09b5508b5eaf04bc0630efd9221aa765",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,21 +17,16 @@\n \n import numpy as np\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling, SizeDict\n from ...processing_utils import Unpack, VideosKwargs\n-from ...utils import TensorType, is_torchvision_v2_available, logging\n+from ...utils import TensorType, logging\n from ...video_processing_utils import BaseVideoProcessor\n from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n DEFAULT_SYSTEM_MESSAGE = \"You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.\""
        },
        {
            "sha": "54f95fa75af617456c1bb2375fff65c741cad025",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -33,17 +33,13 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n if TYPE_CHECKING:\n     from .modeling_superpoint import SuperPointKeypointDescriptionOutput\n \n-if is_torchvision_v2_available():\n-    import torchvision.transforms.v2.functional as F\n-else:\n-    import torchvision.transforms.functional as F\n+import torchvision.transforms.v2.functional as F\n \n \n def is_grayscale("
        },
        {
            "sha": "82c9d733d367936bb63d3df988eedcef5a625816",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, ChannelDimension, get_image_size\n from ...image_processing_utils_fast import (\n@@ -30,17 +31,11 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "baa6276736f744497fb8191015a2da8177e949e3",
            "filename": "src/transformers/models/textnet/image_processing_textnet_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n@@ -37,16 +38,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class TextNetFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     size_divisor (`int`, *optional*, defaults to 32):"
        },
        {
            "sha": "5d74e6efb71fd447e0fce8cdad3ba01cddc63624",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -34,13 +35,7 @@\n     make_nested_list_of_images,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import TensorType, auto_docstring\n \n \n class TvpFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):"
        },
        {
            "sha": "6926b655ce45525e0c3c8d424aa74a3a280ca9f8",
            "filename": "src/transformers/models/vilt/image_processing_vilt_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -30,15 +31,9 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n # Set maximum size based on the typical aspect ratio of the COCO dataset\n MAX_LONGER_EDGE = 1333\n MAX_SHORTER_EDGE = 800"
        },
        {
            "sha": "c5a7256a612bc716fc6fde77d7b036e85b6ecb1a",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -37,16 +38,10 @@\n     TensorType,\n     auto_docstring,\n     filter_out_non_signature_kwargs,\n-    is_torchvision_v2_available,\n     logging,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "59bb3868e75e4ab13631105d1403dc20f2b6bf58",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -9,6 +9,7 @@\n \n import torch\n from torchvision.io import read_image\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n@@ -32,16 +33,10 @@\n     validate_annotations,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, logging\n+from ...utils import TensorType, auto_docstring, logging\n from ...utils.import_utils import requires\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -475,13 +470,7 @@ def resize_annotation(\n             resample (`InterpolationMode`, defaults to `F.InterpolationMode.NEAREST_EXACT`):\n                 The resampling filter to use when resizing the masks.\n         \"\"\"\n-        interpolation = (\n-            interpolation\n-            if interpolation is not None\n-            else F.InterpolationMode.NEAREST_EXACT\n-            if is_torchvision_v2_available()\n-            else F.InterpolationMode.NEAREST\n-        )\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST_EXACT\n         ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n \n         new_annotation = {}"
        },
        {
            "sha": "045dbfdacd4dbd0222714f8c9380bc3922363657",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -21,6 +21,7 @@\n \n import numpy as np\n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import (\n     BatchFeature,\n@@ -44,20 +45,13 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     logging,\n     requires_backends,\n )\n from .image_processing_zoedepth import get_resize_output_image_size\n from .modeling_zoedepth import ZoeDepthDepthEstimatorOutput\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "0bc81bf8eb28f4d668077ac3574ebf14e6f3fefe",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f1b64c5b069c24494da845d2995dae0c83fef047/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=f1b64c5b069c24494da845d2995dae0c83fef047",
            "patch": "@@ -46,7 +46,6 @@\n     is_remote_url,\n     is_torch_available,\n     is_torchcodec_available,\n-    is_torchvision_available,\n     is_torchvision_v2_available,\n     logging,\n )\n@@ -70,8 +69,6 @@\n \n if is_torchvision_v2_available():\n     from torchvision.transforms.v2 import functional as F\n-elif is_torchvision_available():\n-    from torchvision.transforms import functional as F\n \n \n logger = logging.get_logger(__name__)"
        }
    ],
    "stats": {
        "total": 550,
        "additions": 92,
        "deletions": 458
    }
}