{
    "author": "yao-matrix",
    "message": "make mistral3 pass on xpu (#37882)\n\n* enabled mistral3 test cases on XPU\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* calibrate A100 expectation\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n---------\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "1dfad4beb2273b82e91c45a9cb2511028d4e0e24",
    "files": [
        {
            "sha": "dd1c940938ca01d545c470a1556334e41ca1bab9",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 83,
            "deletions": 42,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dfad4beb2273b82e91c45a9cb2511028d4e0e24/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dfad4beb2273b82e91c45a9cb2511028d4e0e24/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=1dfad4beb2273b82e91c45a9cb2511028d4e0e24",
            "patch": "@@ -15,18 +15,20 @@\n \n import unittest\n \n+import accelerate\n+\n from transformers import (\n     AutoProcessor,\n     Mistral3Config,\n-    is_bitsandbytes_available,\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n-    require_bitsandbytes,\n+    require_deterministic_for_xpu,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -46,10 +48,6 @@\n     )\n \n \n-if is_bitsandbytes_available():\n-    from transformers import BitsAndBytesConfig\n-\n-\n class Mistral3VisionText2TextModelTester:\n     def __init__(\n         self,\n@@ -292,20 +290,23 @@ def test_flex_attention_with_grads(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n class Mistral3IntegrationTest(unittest.TestCase):\n+    @require_read_token\n     def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n         self.model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+        self.model = Mistral3ForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, torch_dtype=torch.bfloat16\n+        )\n+        accelerate.cpu_offload(self.model, execution_device=torch_device)\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @require_read_token\n     def test_mistral3_integration_generate_text_only(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = Mistral3ForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n-        )\n \n         messages = [\n             {\n@@ -321,19 +322,23 @@ def test_mistral3_integration_generate_text_only(self):\n         ).to(torch_device, dtype=torch.bfloat16)\n \n         with torch.no_grad():\n-            generate_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n+            generate_ids = self.model.generate(**inputs, max_new_tokens=200, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"Sure, here's a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\"\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Sure, here is a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\",\n+                (\"cuda\", 7): \"Sure, here is a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\",\n+                (\"cuda\", 8): \"Sure, here is a haiku for you:\\n\\nWhispers of the breeze,\\nCherry blossoms softly fall,\\nSpring's gentle embrace.\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_read_token\n     def test_mistral3_integration_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = Mistral3ForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n-        )\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -348,25 +353,32 @@ def test_mistral3_integration_generate(self):\n             messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n         ).to(torch_device, dtype=torch.bfloat16)\n         with torch.no_grad():\n-            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            generate_ids = self.model.generate(**inputs, max_new_tokens=20, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        expected_output = \"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an\"\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n+                (\"cuda\", 7): \"The image features two cats resting on a pink blanket. The cat on the left is a kitten\",\n+                (\"cuda\", 8): \"The image features two cats resting on a pink blanket. The cat on the left is a small kit\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(decoded_output, expected_output)\n \n     @require_read_token\n+    @require_deterministic_for_xpu\n     def test_mistral3_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = Mistral3ForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n-        )\n         messages = [\n             [\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                        {\"type\": \"image\", \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/view.jpg\"},\n                         {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n                     ],\n                 },\n@@ -384,44 +396,57 @@ def test_mistral3_integration_batched_generate(self):\n \n         inputs = processor.apply_chat_template(\n             messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        ).to(model.device, dtype=torch.bfloat16)\n+        ).to(torch_device, dtype=torch.bfloat16)\n+\n+        output = self.model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n-        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+        gen_tokens = output[:, inputs[\"input_ids\"].shape[1] :]\n \n         # Check first output\n-        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n-        expected_output = \"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's mirror gleams,\\nWhispering pines\"\n+        decoded_output = processor.decode(gen_tokens[0], skip_special_tokens=True)\n+\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Calm lake's mirror gleams,\\nWhispering pines stand in silence,\\nPath to peace begins.\",\n+                (\"cuda\", 7): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n+                (\"cuda\", 8): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n \n         # Check second output\n-        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = \"Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese\"\n+        decoded_output = processor.decode(gen_tokens[1], skip_special_tokens=True)\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"The image depicts a vibrant urban scene in what appears to be Chinatown. The focal point is a traditional Chinese archway\",\n+                (\"cuda\", 7): 'The image depicts a vibrant street scene in Chinatown, likely in a major city. The focal point is a traditional Chinese',\n+                (\"cuda\", 8): 'The image depicts a vibrant street scene in what appears to be Chinatown in a major city. The focal point is a',\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n \n     @require_read_token\n-    @require_bitsandbytes\n+    @require_deterministic_for_xpu\n     def test_mistral3_integration_batched_generate_multi_image(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n-        model = Mistral3ForConditionalGeneration.from_pretrained(\n-            self.model_checkpoint, quantization_config=quantization_config\n-        )\n \n         # Prepare inputs\n         messages = [\n             [\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                        {\"type\": \"image\", \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/view.jpg\"},\n                         {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n                     ],\n                 },\n@@ -432,11 +457,11 @@ def test_mistral3_integration_batched_generate_multi_image(self):\n                     \"content\": [\n                         {\n                             \"type\": \"image\",\n-                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                            \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n                         },\n                         {\n                             \"type\": \"image\",\n-                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                            \"url\": \"https://huggingface.co/ydshieh/kosmos-2.5/resolve/main/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n                         },\n                         {\n                             \"type\": \"text\",\n@@ -448,22 +473,38 @@ def test_mistral3_integration_batched_generate_multi_image(self):\n         ]\n         inputs = processor.apply_chat_template(\n             messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        ).to(model.device, dtype=torch.float16)\n+        ).to(torch_device, dtype=torch.bfloat16)\n \n-        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+        output = self.model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+        gen_tokens = output[:, inputs[\"input_ids\"].shape[1] :]\n \n         # Check first output\n-        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n-        expected_output = \"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's wooden path\\nSilent forest stands guard\\n\"\n+        decoded_output = processor.decode(gen_tokens[0], skip_special_tokens=True)\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Still lake reflects skies,\\nWooden path to nature's heart,\\nSilence speaks volumes.\",\n+                (\"cuda\", 7): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n+                (\"cuda\", 8): \"Calm waters reflect\\nWhispering pines stand in silence\\nPath to peace begins\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n \n         # Check second output\n-        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = \"These images depict two different landmarks. Can you identify them?Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\"\n+        decoded_output = processor.decode(gen_tokens[1], skip_special_tokens=True)\n+        expected_outputs = Expectations(\n+            {\n+                (\"xpu\", 3): \"Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\",\n+                (\"cuda\", 7): \"Certainly! The images depict the following landmarks:\\n\\n1. The first image shows the Statue of Liberty and the New York City\",\n+                (\"cuda\", 8): \"Certainly! The images depict the following landmarks:\\n\\n1. The first image shows the Statue of Liberty and the New York City\",\n+            }\n+        )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n+\n         self.assertEqual(\n             decoded_output,\n             expected_output,"
        }
    ],
    "stats": {
        "total": 125,
        "additions": 83,
        "deletions": 42
    }
}