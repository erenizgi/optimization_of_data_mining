{
    "author": "Isotr0py",
    "message": "Fix regression in mllama vision encoder (#40083)\n\nfix mllama vision encoder\n\nSigned-off-by: Isotr0py <2037008807@qq.com>",
    "sha": "f7cbd5f3ef3ce800028cfe3bb570cba681a1070e",
    "files": [
        {
            "sha": "5cc9f0f3da533ac1e20a925bcf725a9fd6920fef",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f7cbd5f3ef3ce800028cfe3bb570cba681a1070e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f7cbd5f3ef3ce800028cfe3bb570cba681a1070e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=f7cbd5f3ef3ce800028cfe3bb570cba681a1070e",
            "patch": "@@ -351,13 +351,15 @@ def forward(\n                 [What are attention masks?](../glossary#attention-mask)\n \n         \"\"\"\n+        encoder_states = ()\n         for encoder_layer in self.layers:\n             hidden_states = encoder_layer(\n                 hidden_state=hidden_states,\n                 attention_mask=attention_mask,\n             )\n+            encoder_states = encoder_states + (hidden_states,)\n \n-        return BaseModelOutput(last_hidden_state=hidden_states)\n+        return BaseModelOutput(last_hidden_state=hidden_states, hidden_states=encoder_states)\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->MllamaText\n@@ -1113,7 +1115,7 @@ def forward(\n         hidden_state = hidden_state.reshape(batch_size, num_concurrent_media, num_tiles, num_patches, dim)\n \n         # Collect intermediate layer outputs from encoder output\n-        all_intermediate_hidden_states = [output.last_hidden_state for _ in self.intermediate_layers_indices]\n+        all_intermediate_hidden_states = [output.hidden_states[i] for i in self.intermediate_layers_indices]\n         intermediate_hidden_states = torch.stack(all_intermediate_hidden_states, dim=-1)\n \n         # Remove padding from intermediate hidden states"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}