{
    "author": "yonigozlan",
    "message": "Add InternVL (2.5 MPO) (#35968)\n\n* initial commit\n\n* add convert internvl\n\n* add first end-to-end working internvl\n\n* nit prompt and image proc\n\n* add working chat template\n\n* add conversion llama-based models\n\n* add tests\n\n* pass all tests\n\n* fix isort\n\n* fix modular after main merge\n\n* add video processing for internvl\n\n* add support for interlaced images and videos\n\n* Remove processing and config from modular, add more tests\n\n* add llama model tests\n\n* Modify processor for compatibility with refactored got ocr image processor\n\n* add comments in processor\n\n* Add docs and nits\n\n* change video processing to use custom sample_indices_fn\n\n* rebase and fix tests\n\n* add processor tests\n\n* Add changes Raushan review\n\n* Use the new attention interface for the vision model\n\n* nits\n\n* add support for custom video_load_backend\n\n* remove mention to InternVLTokenizer\n\n* refactor vision model to simplify logic\n\n* refactor processor for better readibility\n\n* fix copies\n\n* fix require av processor test\n\n* refactor internVL vision\n\n* Update processor and fix processing tests\n\n* fix docstring\n\n* update convert_weights for internvl3\n\n* change image processor to fast by default\n\n* remove do_center_crop=True in convert_weights\n\n* force use_cache to True\n\n* push_to_hub before reloading\n\n* fix internVLVision for larger models\n\n* update convert weight for qk norm\n\n* fix convert_weights\n\n* fix eos_token_id in convert\n\n* update docs and integration tests\n\n* make modifs after review\n\n* fix wrong k_norm and reduce modular\n\n* change image_token_index to image_token_id\n\n* change checkpoint to OpenGVLab org\n\n* last nits\n\n* explicitely del self.num_key_value_groups\n\n* add extra special tokens",
    "sha": "a245011252d18ba319d181f23b657a3b4862ff3c",
    "files": [
        {
            "sha": "6aae6d9f90cec7784b99fb21c87d01342839c5a3",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -953,6 +953,8 @@\n         title: InstructBLIP\n       - local: model_doc/instructblipvideo\n         title: InstructBlipVideo\n+      - local: model_doc/internvl\n+        title: InternVL\n       - local: model_doc/janus\n         title: Janus\n       - local: model_doc/kosmos-2"
        },
        {
            "sha": "fa03a0e70de8e0f24463a9172d1bce520c5329b8",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "added",
            "additions": 349,
            "deletions": 0,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,349 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    </div>\n+</div>\n+\n+# InternVL\n+\n+The InternVL3 family of Visual Language Models was introduced in [InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models](https://huggingface.co/papers/2504.10479).\n+\n+The abstract from the paper is the following:\n+\n+*We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.*\n+\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/internvl_architecture.png\" alt=\"drawing\" width=\"600\"/>\n+\n+<small> Overview of InternVL3 models architecture, which is the same as InternVL2.5. Taken from the <a href=\"https://huggingface.co/OpenGVLab/InternVL3-1B\">original checkpoint.</a> </small>\n+\n+\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/internvl_overview_performance.png\" alt=\"drawing\" width=\"600\"/>\n+\n+<small> Comparison of InternVL3 performance on OpenCompass against other SOTA VLLMs. Taken from the <a href=\"https://huggingface.co/OpenGVLab/InternVL3-1B\">original checkpoint.</a> </small>\n+\n+\n+\n+This model was contributed by [yonigozlan](https://huggingface.co/yonigozlan).\n+The original code can be found [here](https://github.com/OpenGVLab/InternVL).\n+\n+## Usage example\n+\n+### Inference with Pipeline\n+\n+Here is how you can use the `image-text-to-text` pipeline to perform inference with the `InternVL3` models in just a few lines of code:\n+\n+```python\n+>>> from transformers import pipeline\n+\n+>>> messages = [\n+...     {\n+...         \"role\": \"user\",\n+...         \"content\": [\n+...             {\n+...                 \"type\": \"image\",\n+...                 \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n+...             },\n+...             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+...         ],\n+...     },\n+... ]\n+\n+>>> pipe = pipeline(\"image-text-to-text\", model=\"OpenGVLab/InternVL3-1B-hf\")\n+>>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)\n+>>> outputs[0][\"generated_text\"]\n+'The image showcases a vibrant scene of nature, featuring several flowers and a bee. \\n\\n1. **Foreground Flowers**: \\n   - The primary focus is on a large, pink cosmos flower with a prominent yellow center. The petals are soft and slightly r'\n+```\n+### Inference on a single image\n+\n+This example demonstrates how to perform inference on a single image with the InternVL models using chat templates.\n+\n+> [!NOTE]\n+> Note that the model has been trained with a specific prompt format for chatting. Use `processor.apply_chat_template(my_conversation_dict)` to correctly format your prompts.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     {\n+...         \"role\": \"user\",\n+...         \"content\": [\n+...             {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+...             {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n+...         ],\n+...     }\n+... ]\n+\n+>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+>>> generate_ids = model.generate(**inputs, max_new_tokens=50)\n+>>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+\n+>>> decoded_output\n+'The image shows two cats lying on a pink blanket. The cat on the left is a tabby with a mix of brown, black, and white fur, and it appears to be sleeping with its head resting on the blanket. The cat on the'\n+```\n+\n+### Text-only generation\n+This example shows how to generate text using the InternVL model without providing any image input.\n+\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     {\n+...         \"role\": \"user\",\n+...         \"content\": [\n+...             {\"type\": \"text\", \"text\": \"Write a haiku\"},\n+...         ],\n+...     }\n+... ]\n+\n+>>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+\n+>>> generate_ids = model.generate(**inputs, max_new_tokens=50)\n+>>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+\n+>>> print(decoded_output)\n+\"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\"\n+```\n+\n+### Batched image and text inputs\n+InternVL models also support batched image and text inputs.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+...             ],\n+...         },\n+...     ],\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"Describe this image\"},\n+...             ],\n+...         },\n+...     ],\n+... ]\n+\n+\n+>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+>>> output = model.generate(**inputs, max_new_tokens=25)\n+\n+>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n+>>> decoded_outputs\n+[\"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\",\n+ 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of']\n+```\n+\n+### Batched multi-image input\n+This implementation of the InternVL models supports batched text-images inputs with different number of images for each text.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+...             ],\n+...         },\n+...     ],\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n+...                 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n+...             ],\n+...         },\n+...     ],\n+>>> ]\n+\n+>>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+>>> output = model.generate(**inputs, max_new_tokens=25)\n+\n+>>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n+>>> decoded_outputs\n+[\"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\",\n+ 'user\\n\\n\\nThese images depict two different landmarks. Can you identify them?\\nassistant\\nYes, these images depict the Statue of Liberty and the Golden Gate Bridge.']\n+```\n+\n+### Video input\n+InternVL models can also handle video inputs. Here is an example of how to perform inference on a video input using chat templates.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n+\n+>>> model_checkpoint = \"OpenGVLab/InternVL3-8B-hf\"\n+>>> quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, quantization_config=quantization_config)\n+\n+>>> messages = [\n+...     {\n+...         \"role\": \"user\",\n+...         \"content\": [\n+...             {\n+...                 \"type\": \"video\",\n+...                 \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+...             },\n+...             {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+...         ],\n+...     }\n+>>> ]\n+>>> inputs = processor.apply_chat_template(\n+...     messages,\n+...     return_tensors=\"pt\",\n+...     add_generation_prompt=True,\n+...     tokenize=True,\n+...     return_dict=True,\n+>>> ).to(model.device, dtype=torch.float16)\n+\n+>>> output = model.generate(**inputs, max_new_tokens=25)\n+\n+>>> decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+>>> decoded_output\n+'The man is performing a forehand shot.'\n+```\n+\n+### Interleaved image and video inputs\n+This example showcases how to handle a batch of chat conversations with interleaved image and video inputs using chat template.\n+\n+```python\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n+>>> import torch\n+\n+>>> torch_device = \"cuda\"\n+>>> model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n+>>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n+>>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+>>> messages = [\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n+...                 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n+...             ],\n+...         },\n+...     ],\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"video\", \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"},\n+...                 {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+...             ],\n+...         },\n+...     ],\n+...     [\n+...         {\n+...             \"role\": \"user\",\n+...             \"content\": [\n+...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+...             ],\n+...         },\n+...     ],\n+>>> ]\n+>>> inputs = processor.apply_chat_template(\n+...     messages,\n+...     padding=True,\n+...     add_generation_prompt=True,\n+...     tokenize=True,\n+...     return_dict=True,\n+...     return_tensors=\"pt\",\n+>>> ).to(model.device, dtype=torch.bfloat16)\n+\n+>>> outputs = model.generate(**inputs, max_new_tokens=25)\n+\n+>>> decoded_outputs = processor.batch_decode(outputs, skip_special_tokens=True)\n+>>> decoded_outputs\n+['user\\n\\n\\nThese images depict two different landmarks. Can you identify them?\\nassistant\\nThe images depict the Statue of Liberty and the Golden Gate Bridge.',\n+ 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot',\n+ \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"]\n+```\n+\n+## InternVLVisionConfig\n+\n+[[autodoc]] InternVLVisionConfig\n+\n+## InternVLConfig\n+\n+[[autodoc]] InternVLConfig\n+\n+## InternVLVisionModel\n+\n+[[autodoc]] InternVLVisionModel\n+    - forward\n+\n+## InternVLForConditionalGeneration\n+\n+[[autodoc]] InternVLForConditionalGeneration\n+    - forward\n+\n+## InternVLProcessor\n+\n+[[autodoc]] InternVLProcessor"
        },
        {
            "sha": "35297d332caf32c7099e7c7b486d683d55314707",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -244,7 +244,7 @@ model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\", device_m\n \n ### Benchmarks\n \n-FlashAttention2 speeds up inference considerably especially for inputs with long sequences. However, since FlashAttention2 doesn't support computing attention scores with padding tokens, you must manually pad and unpad the attention scores for batched inference if a sequence contains padding tokens. The downside is batched generation is slower with padding tokens. \n+FlashAttention2 speeds up inference considerably especially for inputs with long sequences. However, since FlashAttention2 doesn't support computing attention scores with padding tokens, you must manually pad and unpad the attention scores for batched inference if a sequence contains padding tokens. The downside is batched generation is slower with padding tokens.\n \n <hfoptions id=\"padded\">\n <hfoption id=\"short sequence length\">"
        },
        {
            "sha": "9aaba242f29553ad85977149776de646a1a7247e",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -18,7 +18,7 @@\n from contextlib import redirect_stdout\n from dataclasses import dataclass\n from io import BytesIO\n-from typing import TYPE_CHECKING, Callable, Optional, Union\n+from typing import Callable, Optional, Union\n from urllib.parse import urlparse\n \n import numpy as np\n@@ -77,9 +77,8 @@\n         pil_torch_interpolation_mapping = {}\n \n \n-if TYPE_CHECKING:\n-    if is_torch_available():\n-        import torch\n+if is_torch_available():\n+    import torch\n \n \n logger = logging.get_logger(__name__)\n@@ -162,6 +161,15 @@ def is_valid_list_of_images(images: list):\n     return images and all(is_valid_image(image) for image in images)\n \n \n+def concatenate_list(input_list):\n+    if isinstance(input_list[0], list):\n+        return [item for sublist in input_list for item in sublist]\n+    elif isinstance(input_list[0], np.ndarray):\n+        return np.concatenate(input_list, axis=0)\n+    elif isinstance(input_list[0], torch.Tensor):\n+        return torch.cat(input_list, dim=0)\n+\n+\n def valid_images(imgs):\n     # If we have an list of images, make sure every image is valid\n     if isinstance(imgs, (list, tuple)):"
        },
        {
            "sha": "26e6e0a97994aa1db773ecc59cdd5e4b48555a14",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -143,6 +143,7 @@\n     from .informer import *\n     from .instructblip import *\n     from .instructblipvideo import *\n+    from .internvl import *\n     from .jamba import *\n     from .janus import *\n     from .jetmoe import *"
        },
        {
            "sha": "7d13bc788d466caa36c0f46338f4159783715618",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -162,6 +162,8 @@\n         (\"informer\", \"InformerConfig\"),\n         (\"instructblip\", \"InstructBlipConfig\"),\n         (\"instructblipvideo\", \"InstructBlipVideoConfig\"),\n+        (\"internvl\", \"InternVLConfig\"),\n+        (\"internvl_vision\", \"InternVLVisionConfig\"),\n         (\"jamba\", \"JambaConfig\"),\n         (\"janus\", \"JanusConfig\"),\n         (\"jetmoe\", \"JetMoeConfig\"),\n@@ -519,6 +521,8 @@\n         (\"informer\", \"Informer\"),\n         (\"instructblip\", \"InstructBLIP\"),\n         (\"instructblipvideo\", \"InstructBlipVideo\"),\n+        (\"internvl\", \"InternVL\"),\n+        (\"internvl_vision\", \"InternVLVision\"),\n         (\"jamba\", \"Jamba\"),\n         (\"janus\", \"Janus\"),\n         (\"jetmoe\", \"JetMoe\"),\n@@ -797,6 +801,7 @@\n         (\"chinese_clip_vision_model\", \"chinese_clip\"),\n         (\"rt_detr_resnet\", \"rt_detr\"),\n         (\"granitevision\", \"llava_next\"),\n+        (\"internvl_vision\", \"internvl\"),\n         (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n         (\"qwen2_vl_text\", \"qwen2_vl\"),\n         (\"sam_vision_model\", \"sam\"),"
        },
        {
            "sha": "a7271d04f6071cc2450861fc19655a5e3e1a6a76",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -151,6 +151,7 @@\n         (\"ijepa\", \"IJepaModel\"),\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"informer\", \"InformerModel\"),\n+        (\"internvl_vision\", \"InternVLVisionModel\"),\n         (\"jamba\", \"JambaModel\"),\n         (\"janus\", \"JanusModel\"),\n         (\"jetmoe\", \"JetMoeModel\"),\n@@ -862,6 +863,7 @@\n         (\"idefics2\", \"Idefics2ForConditionalGeneration\"),\n         (\"idefics3\", \"Idefics3ForConditionalGeneration\"),\n         (\"instructblip\", \"InstructBlipForConditionalGeneration\"),\n+        (\"internvl\", \"InternVLForConditionalGeneration\"),\n         (\"janus\", \"JanusForConditionalGeneration\"),\n         (\"kosmos-2\", \"Kosmos2ForConditionalGeneration\"),\n         (\"llama4\", \"Llama4ForConditionalGeneration\"),"
        },
        {
            "sha": "7060b6125dfb49ef0c91193321f717556c10e5da",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -75,6 +75,7 @@\n         (\"idefics3\", \"Idefics3Processor\"),\n         (\"instructblip\", \"InstructBlipProcessor\"),\n         (\"instructblipvideo\", \"InstructBlipVideoProcessor\"),\n+        (\"internvl\", \"InternVLProcessor\"),\n         (\"janus\", \"JanusProcessor\"),\n         (\"kosmos-2\", \"Kosmos2Processor\"),\n         (\"layoutlmv2\", \"LayoutLMv2Processor\"),"
        },
        {
            "sha": "5da5fb73f4d6a620c7d621fe221ae130b5f7714f",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -258,6 +258,7 @@\n             (\"idefics3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"instructblip\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n             (\"instructblipvideo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"internvl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n             (\n                 \"jamba\",\n                 ("
        },
        {
            "sha": "2651425082749e735fd3ab30464c15b639984428",
            "filename": "src/transformers/models/internvl/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2F__init__.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_internvl import *\n+    from .modeling_internvl import *\n+    from .processing_internvl import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "a9fe4db5f31be850506e71dd70aa66b89bddba0f",
            "filename": "src/transformers/models/internvl/configuration_internvl.py",
            "status": "added",
            "additions": 225,
            "deletions": 0,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconfiguration_internvl.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,225 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class InternVLVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`InternVLVisionModel`]. It is used to instantiate an InternVLVisionModel\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the defaults will yield\n+    a similar configuration to that of the InternVL3-1B.\n+    e.g. [OpenGVLab/InternVL3-1B-hf](https://huggingface.co/OpenGVLab/InternVL3-1B-hf)\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to add a bias to the queries, keys and values.\n+        use_qk_norm (`bool`, *optional*, defaults to `False`):\n+            Whether to apply normalization to the queries and keys before the attention operation.\n+        intermediate_size (`int`, *optional*, defaults to 4096):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for attention weights.\n+        projection_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for the projection layer.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        norm_type (`str`, *optional*, defaults to `\"layer_norm\"`):\n+            The type of normalization to use in the encoder. Can be `\"layer_norm\"` or `\"rms_norm\"`.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int` or `list[int]`, *optional*, defaults to `[448, 448]`):\n+            The size (resolution) of each image.\n+        patch_size (`int` or `list[int]`, *optional*, defaults to `[14, 14]`):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        use_mask_token (`bool`, *optional*, defaults to `False`):\n+            Whether to use a mask token for masked image modeling.\n+        use_absolute_position_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to use BERT-style absolute position embeddings.\n+        layer_scale_init_value (`float`, *optional*, defaults to 0.1):\n+            Scale to use in the self-attention layers. 0.1 for base, 1e-5 for large. Set 0 to disable layer scale.\n+        use_mean_pooling (`bool`, *optional*, defaults to `True`):\n+            Whether to mean pool the final hidden states of the patches instead of using the final hidden state of the\n+            CLS token, before applying the classification head.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import InternVLVisionConfig, InternVLVisionModel\n+\n+    >>> # Initializing a InternVLVisionModel OpenGVLab/InternVL3-1B-hf style configuration\n+    >>> configuration = InternVLVisionConfig()\n+\n+    >>> # Initializing a model (with random weights) from the OpenGVLab/InternVL3-1B-hf configuration\n+    >>> model = InternVLVisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"internvl_vision\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        attention_bias=False,\n+        use_qk_norm=False,\n+        intermediate_size=4096,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_dropout=0.0,\n+        projection_dropout=0.0,\n+        initializer_range=0.02,\n+        norm_type=\"layer_norm\",\n+        layer_norm_eps=1e-06,\n+        image_size=[448, 448],\n+        patch_size=[14, 14],\n+        num_channels=3,\n+        use_mask_token=False,\n+        use_absolute_position_embeddings=True,\n+        layer_scale_init_value=0.1,\n+        use_mean_pooling=True,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_bias = attention_bias\n+        self.use_qk_norm = use_qk_norm\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_dropout = attention_dropout\n+        self.projection_dropout = projection_dropout\n+        self.initializer_range = initializer_range\n+        self.norm_type = norm_type\n+        self.layer_norm_eps = layer_norm_eps\n+\n+        image_size = image_size if isinstance(image_size, (list, tuple)) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, (list, tuple)) else (patch_size, patch_size)\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+\n+        self.num_channels = num_channels\n+        self.use_mask_token = use_mask_token\n+        self.use_absolute_position_embeddings = use_absolute_position_embeddings\n+        self.layer_scale_init_value = layer_scale_init_value\n+        self.use_mean_pooling = use_mean_pooling\n+\n+\n+class InternVLConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`InternVLForConditionalGeneration`]. It is used to instantiate a\n+    InternVL model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of InternVL3-1B.\n+    e.g. [OpenGVLab/InternVL3-1B-hf](https://huggingface.co/OpenGVLab/InternVL3-1B-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `InternVisonConfig`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        image_token_id (`int`, *optional*, defaults to 151667):\n+            The image token index to encode the image prompt.\n+        image_seq_length (`int`, *optional*, defaults to 256):\n+            Number of image tokens to use per image patch.\n+        downsample_ratio (`float`, *optional*, defaults to 0.5):\n+            Factor by which to downsample the image.\n+        projector_hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the projector.\n+        vision_feature_layer (`int`, *optional*, defaults to -1):\n+            The index of the layer to use as the image features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`.\n+\n+    ```python\n+    >>> from transformers import InternVLForConditionalGeneration, InternVLConfig\n+\n+    >>> # Initializing a InternVL style configuration\n+    >>> configuration = InternVLConfig()\n+\n+    >>> # Initializing a model (with random weights) from the OpenGVLab/InternVL3-1B-hf configuration\n+    >>> model = InternVLForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"internvl\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": InternVLVisionConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_id=151667,\n+        image_seq_length=256,\n+        downsample_ratio=0.5,\n+        projector_hidden_act=\"gelu\",\n+        vision_feature_layer=-1,\n+        vision_feature_select_strategy=\"default\",\n+        **kwargs,\n+    ):\n+        self.image_token_id = image_token_id\n+        self.image_seq_length = image_seq_length\n+        self.downsample_ratio = downsample_ratio\n+        self.projector_hidden_act = projector_hidden_act\n+        self.vision_feature_layer = vision_feature_layer\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+\n+        if isinstance(vision_config, dict):\n+            self.vision_config = InternVLVisionConfig(**vision_config)\n+        elif isinstance(vision_config, InternVLVisionConfig):\n+            self.vision_config = vision_config\n+        elif vision_config is None:\n+            self.vision_config = InternVLVisionConfig()\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"qwen2\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"]()\n+\n+        self.text_config = text_config\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"InternVLVisionConfig\", \"InternVLConfig\"]"
        },
        {
            "sha": "52a7b464f97889e362d5d911d5d6b5c73c837e9b",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "added",
            "additions": 417,
            "deletions": 0,
            "changes": 417,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,417 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import os\n+import re\n+\n+import torch\n+from einops import rearrange\n+\n+from transformers import (\n+    AutoModel,\n+    AutoTokenizer,\n+    GenerationConfig,\n+    GotOcr2ImageProcessorFast,\n+    InternVLConfig,\n+    InternVLForConditionalGeneration,\n+    InternVLProcessor,\n+    InternVLVisionConfig,\n+    LlamaConfig,\n+    Qwen2Config,\n+)\n+\n+\n+LM_TYPE_CORRESPONDENCE = {\n+    \"OpenGVLab/InternVL2_5-1B-MPO\": \"qwen2\",\n+    \"OpenGVLab/InternVL2_5-2B-MPO\": \"llama\",\n+    \"OpenGVLab/InternVL2_5-4B-MPO\": \"qwen2\",\n+    \"OpenGVLab/InternVL2_5-8B-MPO\": \"llama\",\n+    \"OpenGVLab/InternVL2_5-26B-MPO\": \"llama\",\n+    \"OpenGVLab/InternVL2_5-38B-MPO\": \"qwen2\",\n+    \"OpenGVLab/InternVL2_5-78B-MPO\": \"qwen2\",\n+    \"OpenGVLab/InternVL3-1B\": \"qwen2\",\n+    \"OpenGVLab/InternVL3-2B\": \"qwen2\",\n+    \"OpenGVLab/InternVL3-8B\": \"qwen2\",\n+    \"OpenGVLab/InternVL3-9B\": \"llama\",\n+    \"OpenGVLab/InternVL3-14B\": \"qwen2\",\n+    \"OpenGVLab/InternVL3-38B\": \"qwen2\",\n+    \"OpenGVLab/InternVL3-78B\": \"qwen2\",\n+}\n+\n+UNNECESSARY_CONFIG_KEYS = [ \"_name_or_path\", \"_attn_implementation_autoset\", \"auto_map\", \"use_bfloat16\", \"use_flash_attn\", \"bias\", \"laux_allreduce\", \"moe_coeff_ratio\", \"moe_intermediate_size\", \"moe_output_scale\", \"noisy_gate_policy\", \"shared_expert_intermediate_size\", \"use_residual\", \"use_moe\", \"use_rts\", \"use_weighted_residual\", \"moe_config\", \"num_experts\", \"num_routed_experts\", \"num_shared_experts\", \"capacity_factor\", \"eval_capacity_factor\", \"drop_path_rate\"]  # fmt: skip\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING_VISION = {\n+    # Vision encoder mapping\n+    r\"vision_model\":                                r\"vision_tower\",\n+    r\"layers\":                                      r\"layer\",\n+    r\"class_embedding\":                             r\"cls_token\",\n+    r\"position_embedding\":                          r\"position_embeddings\",\n+    r\"patch_embedding\":                             r\"patch_embeddings.projection\",\n+    r\"ls(\\d+)\":                                     r\"lambda_\\1\",\n+    r\"attn.proj\":                                   r\"attention.projection_layer\",\n+    r\"attn.dropout\":                                r\"attention.projection_dropout\",\n+    r\"attn\":                                        r\"attention\",\n+    r\"norm1\":                                       r\"layernorm_before\",\n+    r\"norm2\":                                       r\"layernorm_after\",\n+\n+}\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_LLAMA = {\n+    # Vision encoder mapping\n+    r\"tok_embeddings\":                              r\"embed_tokens\",\n+    r\"attention.wo\":                                r\"self_attn.o_proj\",\n+    r\"feed_forward.w1\":                             r\"mlp.gate_proj\",\n+    r\"feed_forward.w2\":                             r\"mlp.down_proj\",\n+    r\"feed_forward.w3\":                             r\"mlp.up_proj\",\n+    r\"attention_norm\":                              r\"input_layernorm\",\n+    r\"ffn_norm\":                                    r\"post_attention_layernorm\",\n+    r\"output\":                                      r\"lm_head\",\n+}\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING_MULTI = {\n+    # Vision encoder mapping\n+    r\"mlp1.0\":                                 r\"multi_modal_projector.layer_norm\",\n+    r\"mlp1.1\":                                 r\"multi_modal_projector.linear_1\",\n+    r\"mlp1.3\":                                 r\"multi_modal_projector.linear_2\",\n+}\n+\n+\n+chat_template = (\n+    \"{% for message in messages %}\"\n+        \"{{'<|im_start|>' + message['role'] + '\\n'}}\"\n+        \"{% if message['content'] is string %}\"\n+            \"{{ message['content'] }}\"\n+        \"{% else %}\"\n+            \"{% for content in message['content'] %}\"\n+                \"{% if content['type'] == 'image' %}\"\n+                    \"{{ '<image>\\n' }}\"\n+                \"{% elif content['type'] == 'video' %}\"\n+                    \"{{ '<video>\\n' }}\"\n+                \"{% elif content['type'] == 'text' %}\"\n+                    \"{{ content['text'] }}\"\n+                \"{% endif %}\"\n+            \"{% endfor %}\"\n+        \"{% endif %}\"\n+        \"{{'<|im_end|>\\n'}}\"\n+    \"{% endfor %}\"\n+    \"{% if add_generation_prompt %}\"\n+        \"{{'<|im_start|>assistant\\n' }}\"\n+    \"{% endif %}\"\n+)\n+# fmt: on\n+\n+CONTEXT_LENGTH = 8192\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None, path: str = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text_vision = \"\\n\".join([key for key in state_dict_keys if key.startswith(\"vision_model\")])\n+        new_text = old_text_vision\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING_VISION.items():\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text_vision.split(\"\\n\"), new_text.split(\"\\n\")))\n+        old_text_language = \"\\n\".join([key for key in state_dict_keys if key.startswith(\"language_model\")])\n+        new_text = old_text_language\n+        if LM_TYPE_CORRESPONDENCE[path] == \"llama\":\n+            for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_LLAMA.items():\n+                new_text = re.sub(pattern, replacement, new_text)\n+        output_dict.update(dict(zip(old_text_language.split(\"\\n\"), new_text.split(\"\\n\"))))\n+        old_text_multi = \"\\n\".join(\n+            [\n+                key\n+                for key in state_dict_keys\n+                if not (key.startswith(\"language_model\") or key.startswith(\"vision_model\"))\n+            ]\n+        )\n+        new_text = old_text_multi\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING_MULTI.items():\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict.update(dict(zip(old_text_multi.split(\"\\n\"), new_text.split(\"\\n\"))))\n+\n+    return output_dict\n+\n+\n+def load_original_state_dict(input_base_path):\n+    model = AutoModel.from_pretrained(\n+        input_base_path,\n+        torch_dtype=torch.bfloat16,\n+        low_cpu_mem_usage=True,\n+        use_flash_attn=False,\n+        trust_remote_code=True,\n+    ).eval()\n+\n+    return model.state_dict()\n+\n+\n+def get_internvl_config(input_base_path):\n+    base_config = AutoModel.from_pretrained(input_base_path, trust_remote_code=True).config\n+    llm_config = base_config.llm_config.to_dict()\n+    vision_config = base_config.vision_config.to_dict()\n+    vision_config[\"use_absolute_position_embeddings\"] = True\n+    if LM_TYPE_CORRESPONDENCE[input_base_path] == \"qwen2\":\n+        image_token_id = 151667\n+        language_config_class = Qwen2Config\n+    else:\n+        image_token_id = 92546\n+        language_config_class = LlamaConfig\n+\n+    llm_config = {k: v for k, v in llm_config.items() if k not in UNNECESSARY_CONFIG_KEYS}\n+    # Force use_cache to True\n+    llm_config[\"use_cache\"] = True\n+    # Force correct eos_token_id for InternVL3\n+    if \"InternVL3\" in input_base_path and LM_TYPE_CORRESPONDENCE[input_base_path] == \"qwen2\":\n+        llm_config[\"eos_token_id\"] = 151645\n+\n+    vision_config = {k: v for k, v in vision_config.items() if k not in UNNECESSARY_CONFIG_KEYS}\n+    if \"attention_probs_dropout_prob\" in vision_config:\n+        attention_dropout = vision_config.pop(\"attention_probs_dropout_prob\")\n+        vision_config[\"attention_dropout\"] = attention_dropout\n+        vision_config[\"projection_dropout\"] = attention_dropout\n+    if \"qk_normalization\" in vision_config:\n+        use_qk_norm = vision_config.pop(\"qk_normalization\")\n+        vision_config[\"use_qk_norm\"] = use_qk_norm\n+    if \"qkv_bias\" in vision_config:\n+        attention_bias = vision_config.pop(\"qkv_bias\")\n+        vision_config[\"attention_bias\"] = attention_bias\n+\n+    return InternVLConfig(\n+        text_config=language_config_class(**llm_config),\n+        vision_config=InternVLVisionConfig(**vision_config),\n+        image_token_id=image_token_id,\n+    )\n+\n+\n+def write_model(\n+    model_path,\n+    input_base_path,\n+    push_to_hub=False,\n+    hub_dir=None,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+\n+    config = get_internvl_config(input_base_path)\n+    config.architectures = [\"InternVLForConditionalGeneration\"]\n+    config.save_pretrained(model_path)\n+    if push_to_hub:\n+        config.push_to_hub(hub_dir, use_temp_dir=True)\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(f\"Fetching all parameters from the checkpoint at {input_base_path}...\")\n+    state_dict_old = load_original_state_dict(input_base_path)\n+    print(\"Converting model...\")\n+    all_keys = list(state_dict_old.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys, path=input_base_path)\n+    lm_dim = config.text_config.hidden_size\n+    dim = config.vision_config.hidden_size\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        if \"attn.qkv\" in key:\n+            new_key_query = new_key.replace(\"attention.qkv\", \"attention.q_proj\")\n+            state_dict[new_key_query] = state_dict_old[key][:dim]\n+\n+            new_key_key = new_key.replace(\"attention.qkv\", \"attention.k_proj\")\n+            state_dict[new_key_key] = state_dict_old[key][dim : 2 * dim]\n+\n+            new_key_value = new_key.replace(\"attention.qkv\", \"attention.v_proj\")\n+            state_dict[new_key_value] = state_dict_old[key][-dim:]\n+        elif \"attention.wqkv\" in key:\n+            num_key_value_groups = config.text_config.num_attention_heads // config.text_config.num_key_value_heads\n+            head_dim = config.text_config.head_dim\n+            wqkv_weights = state_dict_old[key]\n+\n+            qkv_vecs = rearrange(wqkv_weights, \"(h gs d) z -> h gs d z\", gs=2 + num_key_value_groups, d=head_dim)\n+            q_proj = qkv_vecs[:, :num_key_value_groups, ...].reshape(-1, lm_dim).contiguous()\n+            k_proj = qkv_vecs[:, -2, ...].reshape(-1, lm_dim).contiguous()\n+            v_proj = qkv_vecs[:, -1, ...].reshape(-1, lm_dim).contiguous()\n+\n+            new_key_query = new_key.replace(\"attention.wqkv\", \"self_attn.q_proj\")\n+            state_dict[new_key_query] = q_proj\n+\n+            new_key_key = new_key.replace(\"attention.wqkv\", \"self_attn.k_proj\")\n+            state_dict[new_key_key] = k_proj\n+\n+            new_key_value = new_key.replace(\"attention.wqkv\", \"self_attn.v_proj\")\n+            state_dict[new_key_value] = v_proj\n+        else:\n+            state_dict[new_key] = state_dict_old[key]\n+\n+    del state_dict_old\n+    gc.collect()\n+\n+    print(\"Loading the checkpoint in a InternVLForConditionalGeneration model.\")\n+    model = InternVLForConditionalGeneration(config)\n+    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n+    model = model.to(torch.bfloat16)\n+    print(\"model dtype:\", model.dtype)\n+    print(\"Missing keys:\", missing_keys)\n+    print(\"Unexpected keys:\", unexpected_keys)\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(model_path)\n+    if push_to_hub:\n+        model.push_to_hub(hub_dir, use_temp_dir=True)\n+\n+    image_processor = GotOcr2ImageProcessorFast.from_pretrained(model_path)\n+    tokenizer = AutoTokenizer.from_pretrained(model_path)\n+    processor = InternVLProcessor(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n+    processor.save_pretrained(model_path)\n+    if push_to_hub:\n+        processor.push_to_hub(hub_dir, use_temp_dir=True)\n+\n+    # generation config\n+    if LM_TYPE_CORRESPONDENCE[input_base_path] == \"llama\":\n+        print(\"Saving generation config...\")\n+        # in the original model, eos_token is not the same in the text_config and the generation_config\n+        # (\"</s>\" - 2 in the text_config and \"<|im_end|>\" - 92542 in the generation_config)\n+        generation_config = GenerationConfig(\n+            eos_token_id=92542,\n+        )\n+        generation_config.save_pretrained(model_path)\n+        if push_to_hub:\n+            generation_config.push_to_hub(hub_dir, use_temp_dir=True)\n+\n+    # del state_dict, model\n+\n+    # # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = InternVLForConditionalGeneration.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+    print(\"Model reloaded successfully.\")\n+    del model\n+\n+\n+def write_tokenizer(save_dir: str, push_to_hub: bool = False, path: str = None, hub_dir: str = None):\n+    if LM_TYPE_CORRESPONDENCE[path] == \"qwen2\":\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            \"Qwen/Qwen2.5-VL-7B-Instruct\",\n+            return_token_type_ids=False,\n+            extra_special_tokens={\n+                \"start_image_token\": \"<img>\",\n+                \"end_image_token\": \"</img>\",\n+                \"context_image_token\": \"<IMG_CONTEXT>\",\n+            },\n+        )\n+        tokenizer.model_max_length = CONTEXT_LENGTH\n+        tokenizer.add_special_tokens(\n+            {\n+                \"additional_special_tokens\": [\n+                    \"<img>\",\n+                    \"</img>\",\n+                    \"<IMG_CONTEXT>\",\n+                    \"<quad>\",\n+                    \"</quad>\",\n+                    \"<ref>\",\n+                    \"</ref>\",\n+                    \"<box>\",\n+                    \"</box>\",\n+                ]\n+            },\n+            replace_additional_special_tokens=False,\n+        )\n+    else:\n+        # Obtained with:\n+        # tokenizer_llama_fast = LlamaTokenizerFast.from_pretrained(\n+        #     \"OpenGVLab/InternVL2_5-2B-MPO\", pad_token=\"</s>\", legacy=False, from_slow=True\n+        # )\n+        # tokenizer_llama_fast._tokenizer.pre_tokenizer.prepend_scheme = \"never\"\n+        # Then manually modifying `added_tokens_decoder` indices to match the original tokenizer\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            \"./intern_vl_hf_implem/tokenizer_internvl_llama_fast\",\n+            return_token_type_ids=False,\n+            extra_special_tokens={\n+                \"start_image_token\": \"<img>\",\n+                \"end_image_token\": \"</img>\",\n+                \"context_image_token\": \"<IMG_CONTEXT>\",\n+            },\n+        )\n+\n+    tokenizer.chat_template = chat_template\n+    tokenizer.save_pretrained(save_dir)\n+    if push_to_hub:\n+        tokenizer.push_to_hub(hub_dir, use_temp_dir=True)\n+\n+\n+def write_image_processor(save_dir: str, push_to_hub: bool = False, hub_dir: str = None):\n+    image_processor = GotOcr2ImageProcessorFast(\n+        do_resize=True,\n+        size={\"height\": 448, \"width\": 448},\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=[0.485, 0.456, 0.406],\n+        image_std=[0.229, 0.224, 0.225],\n+        do_convert_rgb=True,\n+    )\n+\n+    image_processor.save_pretrained(save_dir)\n+    if push_to_hub:\n+        image_processor.push_to_hub(hub_dir, use_temp_dir=True)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--input_dir\",\n+        default=\"OpenGVLab/InternVL3-1B\",\n+        help=\"Location of original InternVL model\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\"InternVL3-1B-hf\",\n+        help=\"Location to write HF model and processors\",\n+    )\n+    parser.add_argument(\n+        \"--hub_dir\",\n+        default=\"OpenGVLab/InternVL3-1B-hf\",\n+        help=\"Location to write HF model and processors\",\n+    )\n+\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the 🤗 hub.\"\n+    )\n+    args = parser.parse_args()\n+    write_tokenizer(\n+        save_dir=args.output_dir,\n+        push_to_hub=args.push_to_hub,\n+        path=args.input_dir,\n+        hub_dir=args.hub_dir,\n+    )\n+\n+    write_image_processor(\n+        save_dir=args.output_dir,\n+        push_to_hub=args.push_to_hub,\n+        hub_dir=args.hub_dir,\n+    )\n+    write_model(\n+        model_path=args.output_dir,\n+        input_base_path=args.input_dir,\n+        push_to_hub=args.push_to_hub,\n+        hub_dir=args.hub_dir,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "af043badd1df0dc9c4175864b1f1fd8488a5536e",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "added",
            "additions": 1094,
            "deletions": 0,
            "changes": 1094,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,1094 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/internvl/modular_internvl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_internvl.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import collections.abc\n+from dataclasses import dataclass\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    ModelOutput,\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torchdynamo_compiling,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_CHECKPOINT_FOR_DOC = \"OpenGVLab/InternVL3-1B-hf\"\n+\n+\n+_CONFIG_FOR_DOC = \"InternVLConfig\"\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class InternVLVisionRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        InternVLVisionRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = key\n+    value_states = value\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # No upcasting of the attention weights to float32 in this implementation\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class InternVLVisionAttention(nn.Module):\n+    \"\"\"Attention Class for InternVL Vision Encoder\"\"\"\n+\n+    def __init__(self, config: InternVLVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        proj_dropout = config.projection_dropout\n+        qk_norm = config.use_qk_norm\n+\n+        self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.v_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n+        self.projection_layer = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.projection_dropout = nn.Dropout(proj_dropout) if proj_dropout > 0 else nn.Identity()\n+\n+        self.q_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n+        self.k_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n+\n+        # Needed for flash attention\n+        self.is_causal = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ):\n+        batch_size, seq_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n+        query_states = self.q_norm(query_states)\n+\n+        key_states = key_states.reshape(-1, self.num_heads, self.head_dim)\n+        key_states = self.k_norm(key_states)\n+\n+        query_states = query_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            is_causal=False,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n+\n+        output = self.projection_layer(attn_output)\n+        output = self.projection_dropout(output)\n+\n+        outputs = (output, attn_weights) if output_attentions else (output, None)\n+        return outputs\n+\n+\n+class InternVLVisionPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = InternVLVisionConfig\n+    base_model_prefix = \"internvl_vision\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"InternVLVisionLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, InternVLVisionEmbeddings):\n+            module.cls_token.data.zero_()\n+            if module.mask_token is not None:\n+                module.mask_token.data.zero_()\n+            if module.position_embeddings is not None:\n+                module.position_embeddings.data.zero_()\n+        elif isinstance(module, InternVLVisionLayer):\n+            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n+            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+\n+\n+@dataclass\n+class InternVLVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n+    \"\"\"\n+    Class for outputs of [`InternVLVisionModel`].\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+            Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n+            *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n+            will be returned.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+\n+class InternVLVisionPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        patch_shape = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+        self.patch_shape = patch_shape\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+            )\n+\n+        embeddings = self.projection(pixel_values)\n+        patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n+        embeddings = embeddings.flatten(2).transpose(1, 2)\n+\n+        return embeddings, (patch_height, patch_width)\n+\n+\n+# Based on timm implementation, which can be found here:\n+# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n+class InternVLVisionEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, position and patch embeddings. Optionally, also the mask token.\n+\n+    \"\"\"\n+\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__()\n+\n+        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        if config.use_mask_token:\n+            self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        else:\n+            self.mask_token = None\n+        self.patch_embeddings = InternVLVisionPatchEmbeddings(config)\n+        self.patch_size = config.patch_size\n+        self.image_size = (\n+            config.image_size\n+            if isinstance(config.image_size, collections.abc.Iterable)\n+            else (config.image_size, config.image_size)\n+        )\n+        num_patches = self.patch_embeddings.num_patches\n+        if config.use_absolute_position_embeddings:\n+            self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n+        else:\n+            self.position_embeddings = None\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+    ) -> torch.Tensor:\n+        _, _, height, width = pixel_values.shape\n+        embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n+        batch_size, seq_len, _ = embeddings.size()\n+\n+        if bool_masked_pos is not None:\n+            mask_tokens = self.mask_token.expand(batch_size, seq_len, -1)\n+            # replace the masked visual tokens by mask_tokens\n+            w = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n+            embeddings = embeddings * (1 - w) + mask_tokens * w\n+\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n+\n+        if self.position_embeddings is not None:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings, (patch_height, patch_width)\n+\n+\n+class InternVLVisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+NORM2FN = {\"layer_norm\": nn.LayerNorm, \"rms_norm\": InternVLVisionRMSNorm}\n+\n+\n+class InternVLVisionLayer(nn.Module):\n+    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n+\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__()\n+        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n+        self.seq_len_dim = 1\n+        self.attention = InternVLVisionAttention(config)\n+        self.mlp = InternVLVisionMLP(config)\n+        # InternVL uses different layernorm implementations for different models\n+        self.layernorm_before = NORM2FN[config.norm_type](config.hidden_size, eps=config.layer_norm_eps)\n+        self.layernorm_after = NORM2FN[config.norm_type](config.hidden_size, eps=config.layer_norm_eps)\n+\n+        init_values = config.layer_scale_init_value\n+        self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+        self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        attention_output, attention_weights = self.attention(\n+            self.layernorm_before(hidden_states),  # in InternVLVision, layernorm is applied before self-attention\n+            output_attentions=output_attentions,\n+        )\n+\n+        attention_output = self.lambda_1 * attention_output\n+\n+        # first residual connection\n+        hidden_states = attention_output + hidden_states\n+\n+        # in InternVLVision, layernorm is also applied after self-attention\n+        layer_output = self.layernorm_after(hidden_states)\n+\n+        layer_output = self.mlp(layer_output)\n+        layer_output = self.dropout(layer_output)\n+\n+        if self.lambda_2 is not None:\n+            layer_output = self.lambda_2 * layer_output\n+\n+        # second residual connection\n+        layer_output = layer_output + hidden_states\n+\n+        return layer_output, attention_weights\n+\n+\n+class InternVLVisionEncoder(nn.Module):\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([InternVLVisionLayer(config) for i in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> Union[tuple, BaseModelOutput]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__, hidden_states, output_attentions\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n+\n+_CONFIG_VISION_FOR_DOC = \"InternVLVisionConfig\"\n+\n+\n+INTERNVL_VISION_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`InternVLVisionConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+INTERNVL_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`InternVLVisionImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare InternVLVision Model transformer outputting raw hidden-states without any specific head on top.\",\n+    INTERNVL_VISION_START_DOCSTRING,\n+)\n+class InternVLVisionModel(InternVLVisionPreTrainedModel):\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = InternVLVisionEmbeddings(config)\n+        self.encoder = InternVLVisionEncoder(config)\n+\n+        self.layernorm = (\n+            nn.Identity() if config.use_mean_pooling else nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(INTERNVL_VISION_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=InternVLVisionModelOutputWithPooling,\n+        config_class=_CONFIG_VISION_FOR_DOC,\n+        modality=\"vision\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Union[tuple, InternVLVisionModelOutputWithPooling]:\n+        r\"\"\"\n+        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        sequence_output = self.layernorm(sequence_output)\n+\n+        return InternVLVisionModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+INTERNVL_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`InternVLConfig`] or [`InternVLVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    INTERNVL_START_DOCSTRING,\n+)\n+class InternVLPreTrainedModel(PreTrainedModel):\n+    config_class = InternVLConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"InternVLVisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+class InternVLMultiModalProjector(nn.Module):\n+    def __init__(self, config: InternVLConfig):\n+        super().__init__()\n+        self.layer_norm = nn.LayerNorm(config.vision_config.hidden_size * int(1 / config.downsample_ratio) ** 2)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * int(1 / config.downsample_ratio) ** 2, config.text_config.hidden_size\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size)\n+\n+    def forward(self, image_features):\n+        hidden_states = self.layer_norm(image_features)\n+        hidden_states = self.linear_1(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@dataclass\n+class InternVLCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for InternVL causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+INTERNVL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`InternVLProcessor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The INTERNVL model which consists of a vision backbone and a language model.\"\"\",\n+    INTERNVL_START_DOCSTRING,\n+)\n+class InternVLForConditionalGeneration(InternVLPreTrainedModel, GenerationMixin):\n+    def __init__(self, config: InternVLConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+\n+        self.multi_modal_projector = InternVLMultiModalProjector(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`int` or `List[int]`):\n+                Layer index or list of layer indices to extract features from.\n+        Returns:\n+            vision_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`.\n+        \"\"\"\n+        downsample_ratio = self.config.downsample_ratio\n+        if vision_feature_layer == -1:\n+            vision_features = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n+        else:\n+            vision_features = self.vision_model(pixel_values=pixel_values).hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            vision_features = vision_features[:, 1:, :]\n+\n+        # Calculate dimensions based on vision features\n+        channels = vision_features.shape[1]\n+        feature_size = int(channels**0.5)\n+        batch_size = vision_features.shape[0]\n+\n+        # Reshape tensor to spatial dimensions\n+        vision_features = vision_features.reshape(batch_size, feature_size, feature_size, -1)\n+\n+        # Apply downsampling using pixel shuffle\n+        vision_features = self.pixel_shuffle(vision_features, scale_factor=downsample_ratio)\n+\n+        # Reshape tensor to prepare for projection\n+        vision_features = vision_features.reshape(batch_size, -1, vision_features.shape[-1])\n+\n+        # Project features through multi-modal projector\n+        vision_features = self.multi_modal_projector(vision_features)\n+\n+        return vision_features\n+\n+    @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=InternVLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[int] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, InternVLCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+        >>> torch_device = \"cuda\"\n+        >>> processor = AutoProcessor.from_pretrained(\"OpenGVLab/InternVL3-1B-hf\")\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\n+        ...     \"OpenGVLab/InternVL3-1B-hf\", torch_dtype=torch.bfloat16, device_map=torch_device\n+        ... )\n+\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\n+        ...                 \"type\": \"image\",\n+        ...                 \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+        ...             },\n+        ...             {\n+        ...                 \"type\": \"image\",\n+        ...                 \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+        ...             },\n+        ...             {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(torch_device)\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=200)\n+        >>> print(processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True))\n+        The images depict the Statue of Liberty and the Golden Gate Bridge.\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        logits = outputs[0]\n+\n+        loss = None\n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            if attention_mask is not None:\n+                # we use the input attention mask to shift the logits and labels, because it is 2D.\n+                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n+                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n+                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n+                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n+            else:\n+                shift_logits = logits[..., :-1, :].contiguous()\n+                shift_labels = labels[..., 1:].contiguous()\n+            # Flatten the tokens\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(\n+                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return InternVLCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = self.language_model.prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+    def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n+        \"\"\"Perform pixel shuffle downsampling on vision features.\n+\n+        Args:\n+            vision_features (`torch.Tensor`):\n+                Input tensor of shape (batch_size, width, height, channels).\n+            scale_factor (`float`, *optional*, defaults to `0.5`):\n+                Factor by which to downsample. Default is 0.5, which halves the dimensions.\n+\n+        Returns:\n+            vision_features (`torch.Tensor`):\n+                Downsampled tensor of shape (batch_size, height*scale_factor, width*scale_factor, channels/(scale_factor^2)).\n+        \"\"\"\n+        batch_size, width, height, channels = vision_features.size()\n+\n+        if height % scale_factor != 0 or width % scale_factor != 0:\n+            raise ValueError(\"Height and width must be divisible by scale_factor for proper downsampling.\")\n+\n+        # Reshape to allow downsampling\n+        vision_features = vision_features.view(\n+            batch_size, width, int(height * scale_factor), int(channels / scale_factor)\n+        )\n+        # Permute dimensions to align downsampled axis correctly\n+        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+\n+        # Reshape to achieve final downsampled dimensions\n+        vision_features = vision_features.view(\n+            batch_size, int(height * scale_factor), int(width * scale_factor), int(channels / (scale_factor**2))\n+        )\n+\n+        # Swap height and width back for proper orientation\n+        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+\n+        return vision_features\n+\n+\n+__all__ = [\n+    \"InternVLVisionPreTrainedModel\",\n+    \"InternVLVisionModel\",\n+    \"InternVLPreTrainedModel\",\n+    \"InternVLForConditionalGeneration\",\n+]"
        },
        {
            "sha": "e219e7ec0e14c6b63cb1f68fd898fcf45fbeecef",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "added",
            "additions": 708,
            "deletions": 0,
            "changes": 708,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,708 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import collections.abc\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.utils.checkpoint\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n+from ..clip.modeling_clip import CLIPMLP\n+from ..janus.modeling_janus import JanusVisionAttention\n+from ..llama.modeling_llama import LlamaRMSNorm\n+from ..llava.modeling_llava import LlavaCausalLMOutputWithPast, LlavaForConditionalGeneration, LlavaPreTrainedModel\n+from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+_CHECKPOINT_FOR_DOC = \"OpenGVLab/InternVL3-1B-hf\"\n+\n+_CONFIG_VISION_FOR_DOC = \"InternVLVisionConfig\"\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = key\n+    value_states = value\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    # No upcasting of the attention weights to float32 in this implementation\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class InternVLVisionRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class InternVLVisionAttention(JanusVisionAttention):\n+    def __init__(self, config: InternVLVisionConfig):\n+        super().__init__()\n+        del self.num_key_value_groups\n+\n+        # Needed for flash attention\n+        self.is_causal = False\n+        qk_norm = config.use_qk_norm\n+\n+        self.q_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n+        self.k_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n+\n+\n+class InternVLVisionPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = InternVLVisionConfig\n+    base_model_prefix = \"internvl_vision\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"InternVLVisionLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, InternVLVisionEmbeddings):\n+            module.cls_token.data.zero_()\n+            if module.mask_token is not None:\n+                module.mask_token.data.zero_()\n+            if module.position_embeddings is not None:\n+                module.position_embeddings.data.zero_()\n+        elif isinstance(module, InternVLVisionLayer):\n+            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n+            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+\n+\n+@dataclass\n+class InternVLVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n+    \"\"\"\n+    Class for outputs of [`InternVLVisionModel`].\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+            Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n+            *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n+            will be returned.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+            shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+\n+class InternVLVisionPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        patch_shape = (image_size[0] // patch_size[0], image_size[1] // patch_size[1])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+        self.patch_shape = patch_shape\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+            )\n+\n+        embeddings = self.projection(pixel_values)\n+        patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n+        embeddings = embeddings.flatten(2).transpose(1, 2)\n+\n+        return embeddings, (patch_height, patch_width)\n+\n+\n+# Based on timm implementation, which can be found here:\n+# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n+class InternVLVisionEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, position and patch embeddings. Optionally, also the mask token.\n+\n+    \"\"\"\n+\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__()\n+\n+        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        if config.use_mask_token:\n+            self.mask_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        else:\n+            self.mask_token = None\n+        self.patch_embeddings = InternVLVisionPatchEmbeddings(config)\n+        self.patch_size = config.patch_size\n+        self.image_size = (\n+            config.image_size\n+            if isinstance(config.image_size, collections.abc.Iterable)\n+            else (config.image_size, config.image_size)\n+        )\n+        num_patches = self.patch_embeddings.num_patches\n+        if config.use_absolute_position_embeddings:\n+            self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n+        else:\n+            self.position_embeddings = None\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+    ) -> torch.Tensor:\n+        _, _, height, width = pixel_values.shape\n+        embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n+        batch_size, seq_len, _ = embeddings.size()\n+\n+        if bool_masked_pos is not None:\n+            mask_tokens = self.mask_token.expand(batch_size, seq_len, -1)\n+            # replace the masked visual tokens by mask_tokens\n+            w = bool_masked_pos.unsqueeze(-1).type_as(mask_tokens)\n+            embeddings = embeddings * (1 - w) + mask_tokens * w\n+\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n+\n+        if self.position_embeddings is not None:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings, (patch_height, patch_width)\n+\n+\n+class InternVLVisionMLP(CLIPMLP):\n+    pass\n+\n+\n+NORM2FN = {\"layer_norm\": nn.LayerNorm, \"rms_norm\": InternVLVisionRMSNorm}\n+\n+\n+class InternVLVisionLayer(nn.Module):\n+    \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n+\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__()\n+        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n+        self.seq_len_dim = 1\n+        self.attention = InternVLVisionAttention(config)\n+        self.mlp = InternVLVisionMLP(config)\n+        # InternVL uses different layernorm implementations for different models\n+        self.layernorm_before = NORM2FN[config.norm_type](config.hidden_size, eps=config.layer_norm_eps)\n+        self.layernorm_after = NORM2FN[config.norm_type](config.hidden_size, eps=config.layer_norm_eps)\n+\n+        init_values = config.layer_scale_init_value\n+        self.lambda_1 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+        self.lambda_2 = nn.Parameter(init_values * torch.ones((config.hidden_size)), requires_grad=True)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n+        attention_output, attention_weights = self.attention(\n+            self.layernorm_before(hidden_states),  # in InternVLVision, layernorm is applied before self-attention\n+            output_attentions=output_attentions,\n+        )\n+\n+        attention_output = self.lambda_1 * attention_output\n+\n+        # first residual connection\n+        hidden_states = attention_output + hidden_states\n+\n+        # in InternVLVision, layernorm is also applied after self-attention\n+        layer_output = self.layernorm_after(hidden_states)\n+\n+        layer_output = self.mlp(layer_output)\n+        layer_output = self.dropout(layer_output)\n+\n+        if self.lambda_2 is not None:\n+            layer_output = self.lambda_2 * layer_output\n+\n+        # second residual connection\n+        layer_output = layer_output + hidden_states\n+\n+        return layer_output, attention_weights\n+\n+\n+class InternVLVisionEncoder(nn.Module):\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([InternVLVisionLayer(config) for i in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> Union[tuple, BaseModelOutput]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__, hidden_states, output_attentions\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n+\n+_CONFIG_VISION_FOR_DOC = \"InternVLVisionConfig\"\n+\n+\n+INTERNVL_VISION_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`InternVLVisionConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+INTERNVL_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`InternVLVisionImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare InternVLVision Model transformer outputting raw hidden-states without any specific head on top.\",\n+    INTERNVL_VISION_START_DOCSTRING,\n+)\n+class InternVLVisionModel(InternVLVisionPreTrainedModel):\n+    def __init__(self, config: InternVLVisionConfig) -> None:\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = InternVLVisionEmbeddings(config)\n+        self.encoder = InternVLVisionEncoder(config)\n+\n+        self.layernorm = (\n+            nn.Identity() if config.use_mean_pooling else nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(INTERNVL_VISION_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=InternVLVisionModelOutputWithPooling,\n+        config_class=_CONFIG_VISION_FOR_DOC,\n+        modality=\"vision\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        bool_masked_pos: Optional[torch.BoolTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Union[tuple, InternVLVisionModelOutputWithPooling]:\n+        r\"\"\"\n+        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n+            Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        sequence_output = self.layernorm(sequence_output)\n+\n+        return InternVLVisionModelOutputWithPooling(\n+            last_hidden_state=sequence_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+_CONFIG_FOR_DOC = \"InternVLConfig\"\n+\n+\n+class InternVLPreTrainedModel(LlavaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+INTERNVL_INPUTS_DOCSTRING = None\n+\n+\n+class InternVLMultiModalProjector(nn.Module):\n+    def __init__(self, config: InternVLConfig):\n+        super().__init__()\n+        self.layer_norm = nn.LayerNorm(config.vision_config.hidden_size * int(1 / config.downsample_ratio) ** 2)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * int(1 / config.downsample_ratio) ** 2, config.text_config.hidden_size\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size)\n+\n+    def forward(self, image_features):\n+        hidden_states = self.layer_norm(image_features)\n+        hidden_states = self.linear_1(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+class InternVLCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+class InternVLForConditionalGeneration(LlavaForConditionalGeneration):\n+    def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n+        \"\"\"Perform pixel shuffle downsampling on vision features.\n+\n+        Args:\n+            vision_features (`torch.Tensor`):\n+                Input tensor of shape (batch_size, width, height, channels).\n+            scale_factor (`float`, *optional*, defaults to `0.5`):\n+                Factor by which to downsample. Default is 0.5, which halves the dimensions.\n+\n+        Returns:\n+            vision_features (`torch.Tensor`):\n+                Downsampled tensor of shape (batch_size, height*scale_factor, width*scale_factor, channels/(scale_factor^2)).\n+        \"\"\"\n+        batch_size, width, height, channels = vision_features.size()\n+\n+        if height % scale_factor != 0 or width % scale_factor != 0:\n+            raise ValueError(\"Height and width must be divisible by scale_factor for proper downsampling.\")\n+\n+        # Reshape to allow downsampling\n+        vision_features = vision_features.view(\n+            batch_size, width, int(height * scale_factor), int(channels / scale_factor)\n+        )\n+        # Permute dimensions to align downsampled axis correctly\n+        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+\n+        # Reshape to achieve final downsampled dimensions\n+        vision_features = vision_features.view(\n+            batch_size, int(height * scale_factor), int(width * scale_factor), int(channels / (scale_factor**2))\n+        )\n+\n+        # Swap height and width back for proper orientation\n+        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+\n+        return vision_features\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`int` or `List[int]`):\n+                Layer index or list of layer indices to extract features from.\n+        Returns:\n+            vision_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`.\n+        \"\"\"\n+        downsample_ratio = self.config.downsample_ratio\n+        if vision_feature_layer == -1:\n+            vision_features = self.vision_tower(pixel_values=pixel_values).last_hidden_state\n+        else:\n+            vision_features = self.vision_model(pixel_values=pixel_values).hidden_states[vision_feature_layer]\n+        if vision_feature_select_strategy == \"default\":\n+            vision_features = vision_features[:, 1:, :]\n+\n+        # Calculate dimensions based on vision features\n+        channels = vision_features.shape[1]\n+        feature_size = int(channels**0.5)\n+        batch_size = vision_features.shape[0]\n+\n+        # Reshape tensor to spatial dimensions\n+        vision_features = vision_features.reshape(batch_size, feature_size, feature_size, -1)\n+\n+        # Apply downsampling using pixel shuffle\n+        vision_features = self.pixel_shuffle(vision_features, scale_factor=downsample_ratio)\n+\n+        # Reshape tensor to prepare for projection\n+        vision_features = vision_features.reshape(batch_size, -1, vision_features.shape[-1])\n+\n+        # Project features through multi-modal projector\n+        vision_features = self.multi_modal_projector(vision_features)\n+\n+        return vision_features\n+\n+    @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=InternVLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[int] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, InternVLCausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+        >>> torch_device = \"cuda\"\n+        >>> processor = AutoProcessor.from_pretrained(\"OpenGVLab/InternVL3-1B-hf\")\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\n+        ...     \"OpenGVLab/InternVL3-1B-hf\", torch_dtype=torch.bfloat16, device_map=torch_device\n+        ... )\n+\n+        >>> messages = [\n+        ...     {\n+        ...         \"role\": \"user\",\n+        ...         \"content\": [\n+        ...             {\n+        ...                 \"type\": \"image\",\n+        ...                 \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+        ...             },\n+        ...             {\n+        ...                 \"type\": \"image\",\n+        ...                 \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+        ...             },\n+        ...             {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+\n+        >>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(torch_device)\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=200)\n+        >>> print(processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True))\n+        The images depict the Statue of Liberty and the Golden Gate Bridge.\n+        ```\"\"\"\n+        super().forward(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            labels=labels,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n+            **lm_kwargs,\n+        )\n+\n+\n+__all__ = [\n+    \"InternVLVisionPreTrainedModel\",\n+    \"InternVLVisionModel\",\n+    \"InternVLPreTrainedModel\",\n+    \"InternVLForConditionalGeneration\",\n+]"
        },
        {
            "sha": "e371c9b601d56206429cee1c0410aa34ff2ff986",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "added",
            "additions": 378,
            "deletions": 0,
            "changes": 378,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,378 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from functools import partial\n+from typing import Dict, List, Optional, Union\n+\n+import numpy as np\n+\n+from transformers.processing_utils import (\n+    AllKwargsForChatTemplate,\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+)\n+from transformers.tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import (\n+    ImageInput,\n+    VideoInput,\n+    VideoMetadata,\n+    concatenate_list,\n+    make_batched_videos,\n+    make_flat_list_of_images,\n+)\n+\n+\n+class InternVLImagesKwargs(ImagesKwargs, total=False):\n+    crop_to_patches: Optional[bool]\n+    min_patches: Optional[int]\n+    max_patches: Optional[int]\n+\n+\n+class InternVLProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: InternVLImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding_side\": \"left\",\n+        },\n+        \"images_kwargs\": {\n+            \"crop_to_patches\": True,\n+        },\n+        \"videos_kwargs\": {\n+            \"crop_to_patches\": False,\n+        },\n+    }\n+\n+\n+class InternVLProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a InternVL processor which wraps a [`AutoImageProcessor`] and\n+    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n+    tokenizer functionalities. See the [`~InternVLProcessor.__call__`] and [`~InternVLProcessor.decode`] for more information.\n+    Args:\n+        image_processor ([`AutoImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        image_seq_length (`int`, *optional*, defaults to 256):\n+            The number of image token to use per image patch. it should be set so that:\n+            image_seq_length = (config.image_size // config.patch_size) ** 2 * (config.scale_factor**2)\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        fake_image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            The token to use for the image placeholder in the text. This token will be replaced by the\n+            appropriate image tokens when processing the text with images.\n+        fake_video_token (`str`, *optional*, defaults to `\"<video>\"`):\n+            The token to use for the video placeholder in the text. This token will be replaced by the\n+            appropriate image tokens when processing the text with videos.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"image_seq_length\",\n+        \"fake_image_token\",\n+        \"fake_video_token\",\n+    ]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        image_seq_length: int = 256,\n+        chat_template=None,\n+        fake_image_token=\"<image>\",\n+        fake_video_token=\"<video>\",\n+        **kwargs,\n+    ):\n+        self.image_seq_length = image_seq_length\n+        self.fake_image_token = fake_image_token\n+        self.fake_video_token = fake_video_token\n+        self.start_image_token = tokenizer.start_image_token\n+        self.end_image_token = tokenizer.end_image_token\n+        self.context_image_token = tokenizer.context_image_token\n+\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n+\n+    def _insert_media_placeholders(\n+        self,\n+        text: list[str],\n+        image_pixel_values,\n+        video_pixel_values,\n+        image_num_patches: list[int],\n+        video_num_patches: list[int],\n+        image_num_patches_indices: np.ndarray,\n+        video_num_patches_indices: np.ndarray,\n+        video_patch_indices: np.ndarray,\n+    ):\n+        \"\"\"\n+        Processes interleaved text with <image> and <video> placeholders, replacing them with appropriate\n+        image and video tokens while keeping track of the patches used.\n+        \"\"\"\n+        image_index = 0\n+        video_index = 0\n+        processed_text = []\n+        image_video_patches = []\n+        # Support interleaved image and video in prompts:\n+        # Processed patches of images and videos are inserted in `image_video_patches` in the order they appear in the prompts\n+        for prompt in text:\n+            new_prompt = prompt\n+            while self.fake_image_token in new_prompt or self.fake_video_token in new_prompt:\n+                if self.fake_image_token in new_prompt and (\n+                    self.fake_video_token not in new_prompt\n+                    or new_prompt.index(self.fake_image_token) < new_prompt.index(self.fake_video_token)\n+                ):\n+                    # Get the slice of patches corresponding to the current image\n+                    start_index = image_num_patches_indices[image_index - 1] if image_index > 0 else 0\n+                    end_index = image_num_patches_indices[image_index]\n+                    image_video_patches.append(image_pixel_values[start_index:end_index])\n+                    # Replace the corresponding image placeholder with the correct number of image tokens\n+                    new_prompt = new_prompt.replace(\n+                        self.fake_image_token,\n+                        f\"{self.start_image_token}{self.context_image_token * self.image_seq_length * image_num_patches[image_index]}{self.end_image_token}\",\n+                        1,\n+                    )\n+                    image_index += 1\n+                else:\n+                    # Get the slice of patches corresponding to the current video\n+                    # Here we need to account for both the multiple video frames and the potential multiple patches per frame\n+                    # As of now, InternVL only supports one patch per frame, but we keep the code flexible for future updates\n+                    current_patch_index = video_patch_indices[video_index - 1] if video_index > 0 else 0\n+                    end_patch_index = video_patch_indices[video_index]\n+                    start_index = video_num_patches_indices[current_patch_index] if video_index > 0 else 0\n+                    end_index = video_num_patches_indices[end_patch_index - 1]\n+                    image_video_patches.append(video_pixel_values[start_index:end_index])\n+                    # Get the number of patches per frame and replace the video placeholder with the correct number of image tokens\n+                    num_patches = list(video_num_patches[current_patch_index:end_patch_index])\n+                    video_prompt = \"\\n\".join(\n+                        f\"Frame{i + 1}: {self.start_image_token}{self.context_image_token * self.image_seq_length * num_patches[i]}{self.end_image_token}\"\n+                        for i in range(len(num_patches))\n+                    )\n+                    new_prompt = new_prompt.replace(self.fake_video_token, video_prompt, 1)\n+                    video_index += 1\n+            processed_text.append(new_prompt)\n+\n+        return processed_text, image_video_patches, image_index, video_index\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos: Optional[VideoInput] = None,\n+        **kwargs: Unpack[InternVLProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n+        is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n+        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwrags` arguments to\n+        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        if text is None:\n+            raise ValueError(\"You have to specify text.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            InternVLProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if not isinstance(text, (list, tuple)):\n+            text = [text]\n+\n+        # Process images and videos separately, as videos don't support crop_to_patches\n+        image_num_patches = []\n+        video_num_patches = []\n+        image_videos_inputs = {}\n+        image_pixel_values = None\n+        video_pixel_values = None\n+        image_num_patches_indices = np.array([0])\n+        video_patch_indices = np.array([0])\n+        video_num_patches_indices = np.array([0])\n+        if images is not None:\n+            images = make_flat_list_of_images(images)\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_num_patches = image_inputs.pop(\"num_patches\")\n+            image_pixel_values = image_inputs.pop(\"pixel_values\")\n+            image_num_patches_indices = np.cumsum(image_num_patches)\n+        if videos is not None:\n+            videos = make_batched_videos(videos)\n+            num_frames_per_video = [len(video) for video in videos]\n+            video_patch_indices = np.cumsum(num_frames_per_video)\n+            output_kwargs[\"images_kwargs\"][\"crop_to_patches\"] = False\n+            video_inputs = self.image_processor(images=videos, **output_kwargs[\"videos_kwargs\"])\n+            video_num_patches = video_inputs.pop(\"num_patches\")\n+            video_pixel_values = video_inputs.pop(\"pixel_values\")\n+            video_num_patches_indices = np.cumsum(video_num_patches)\n+\n+        if images is not None or videos is not None:\n+            text, image_video_patches, image_index, video_index = self._insert_media_placeholders(\n+                text,\n+                image_pixel_values,\n+                video_pixel_values,\n+                image_num_patches,\n+                video_num_patches,\n+                image_num_patches_indices,\n+                video_num_patches_indices,\n+                video_patch_indices,\n+            )\n+            if images is not None and image_index != len(images):\n+                raise ValueError(\"Number of image placeholders in the prompt does not match the number of images.\")\n+            if videos is not None and video_index != len(videos):\n+                raise ValueError(\"Number of video placeholders in the prompt does not match the number of videos.\")\n+\n+            # Concatenate the interleaved image and video patches (function agnostic to the patches type (list, numpy array, torch tensor))\n+            image_videos_inputs = {\"pixel_values\": concatenate_list(image_video_patches)}\n+\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+\n+        return BatchFeature(data={**text_inputs, **image_videos_inputs})\n+\n+    def sample_indices_fn(\n+        self, metadata: VideoMetadata, num_frames: int = None, initial_shift: Union[bool, float, int] = True\n+    ):\n+        \"\"\"\n+        The function to generate indices of frames to sample from a video.\n+\n+        Args:\n+            metadata (`VideoMetadata`):\n+                `VideoMetadata` object containing metadat about the video, such as \"total_num_frames\" or \"fps\".\n+            num_frames (`int`, *optional*):\n+                Number of frames to sample uniformly. If None, all frames are sampled.\n+            initial_shift (`bool`, `float` or `int`, defaults to `0`):\n+                The initial shift to apply when sampling frames. If `True`, the shift is set so that frames are sampled from the middle of the video.\n+\n+        Returns:\n+            `np.ndarray`: Array of frame indices to sample.\n+        \"\"\"\n+        if initial_shift is True:\n+            initial_shift = metadata.total_num_frames / num_frames / 2\n+        if num_frames is not None:\n+            indices = np.arange(\n+                initial_shift, metadata.total_num_frames, metadata.total_num_frames / num_frames\n+            ).astype(int)\n+        else:\n+            indices = np.arange(initial_shift, metadata.total_num_frames).astype(int)\n+\n+        return indices\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(tokenizer_input_names) + list(image_processor_input_names)\n+\n+    # Add model-specific video sampling method when applying the template\n+    def apply_chat_template(\n+        self,\n+        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]],\n+        chat_template: Optional[str] = None,\n+        num_frames: int = 8,\n+        initial_shift: Union[bool, float, int] = True,\n+        video_load_backend=\"pyav\",\n+        **kwargs: Unpack[AllKwargsForChatTemplate],\n+    ):\n+        \"\"\"\n+        Similar to the `apply_chat_template` method on tokenizers, this method applies a Jinja template to input\n+        conversations to turn them into a single tokenizable string.\n+\n+        The input is expected to be in the following format, where each message content is a list consisting of text and\n+        optionally image or video inputs. One can also provide an image, video, URL or local path which will be used to form\n+        `pixel_values` when `return_dict=True`. If not provided, one will get only the formatted text, optionally tokenized text.\n+\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"image\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe this image in detail.\"},\n+                ],\n+            },\n+        ]\n+\n+        Args:\n+            conversation (`Union[List[Dict, [str, str]], List[List[Dict[str, str]]]]`):\n+                The conversation to format.\n+            chat_template (`Optional[str]`, *optional*):\n+                The Jinja template to use for formatting the conversation. If not provided, the tokenizer's\n+                chat template is used.\n+            num_frames (`int`, *optional*, defaults to 8):\n+                Number of frames to sample from a video when using the default `sample_indices_fn`.\n+            initial_shift (`bool`, `float` or `int`, defaults to `0`):\n+                The initial shift to apply when sampling frames using the default `sample_indices_fn`.\n+                If `True`, the shift is set so that frames are sampled from the middle of the video.\n+        \"\"\"\n+        sample_indices_fn = kwargs.pop(\n+            \"sample_indices_fn\", partial(self.sample_indices_fn, num_frames=num_frames, initial_shift=initial_shift)\n+        )\n+\n+        return super().apply_chat_template(\n+            conversation,\n+            chat_template,\n+            video_load_backend=video_load_backend,\n+            num_frames=num_frames,\n+            sample_indices_fn=sample_indices_fn,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"InternVLProcessor\"]"
        },
        {
            "sha": "c5794f52a09897fa351b1353dff7f1142936243a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -130,6 +130,7 @@\n     \"gemma3\",\n     \"mistral3\",\n     \"chameleon\",\n+    \"internvl\",\n     \"qwen2_5_omni\",\n ]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/internvl/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fmodels%2Finternvl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fmodels%2Finternvl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2F__init__.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c"
        },
        {
            "sha": "3c46ca7eda7f9539956fd9fdb2c2b66068e332f6",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "added",
            "additions": 894,
            "deletions": 0,
            "changes": 894,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,894 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch InternVL model.\"\"\"\n+\n+import unittest\n+from io import BytesIO\n+\n+import requests\n+\n+from transformers import (\n+    AutoProcessor,\n+    BitsAndBytesConfig,\n+    InternVLConfig,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_av,\n+    require_bitsandbytes,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        InternVLForConditionalGeneration,\n+    )\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class InternVLVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        image_seq_length=64,\n+        vision_feature_layer=-1,\n+        ignore_index=-100,\n+        bos_token_id=0,\n+        eos_token_id=0,\n+        pad_token_id=0,\n+        image_token_id=1,\n+        num_channels=3,\n+        image_size=64,\n+        model_type=\"internvl\",\n+        is_training=True,\n+        text_config={\n+            \"model_type\": \"qwen2\",\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 128,\n+            \"intermediate_size\": 37,\n+            \"num_hidden_layers\": 4,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"output_channels\": 64,\n+            \"hidden_act\": \"silu\",\n+            \"max_position_embeddings\": 512,\n+            \"rope_theta\": 10000,\n+            \"mlp_ratio\": 4,\n+            \"tie_word_embeddings\": True,\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 0,\n+            \"pad_token_id\": 0,\n+        },\n+        vision_config={\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 128,\n+            \"image_size\": 64,\n+            \"patch_size\": 4,\n+            \"num_channels\": 3,\n+            \"hidden_act\": \"quick_gelu\",\n+            \"use_absolute_position_embeddings\": True,\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.bos_token_id = bos_token_id\n+        self.eos_token_id = eos_token_id\n+        self.pad_token_id = pad_token_id\n+        self.image_token_id = image_token_id\n+        self.model_type = model_type\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.batch_size = batch_size\n+        self.vision_feature_layer = vision_feature_layer\n+        self.is_training = is_training\n+        self.image_seq_length = image_seq_length\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.seq_length = seq_length + image_seq_length\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+\n+    def get_config(self):\n+        return InternVLConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            model_type=self.model_type,\n+            bos_token_id=self.bos_token_id,\n+            eos_token_id=self.eos_token_id,\n+            pad_token_id=self.pad_token_id,\n+            image_token_id=self.image_token_id,\n+            image_seq_length=self.image_seq_length,\n+            vision_feature_layer=self.vision_feature_layer,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        # input_ids[:, -1] = self.pad_token_id\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n+        input_ids[:, : self.image_seq_length] = self.image_token_id\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def create_and_check_model_fp16_forward(self, config, input_ids, pixel_values, attention_mask):\n+        model = InternVLForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.half()\n+        model.eval()\n+        logits = model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            pixel_values=pixel_values.to(torch.bfloat16),\n+            return_dict=True,\n+        )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+    def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_values, attention_mask):\n+        config.torch_dtype = torch.float16\n+        model = InternVLForConditionalGeneration(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n+            logits = model(\n+                input_ids=input_ids,\n+                attention_mask=attention_mask,\n+                pixel_values=pixel_values.to(torch.bfloat16),\n+                return_dict=True,\n+            )[\"logits\"]\n+        self.parent.assertFalse(torch.isnan(logits).any().item())\n+\n+\n+@require_torch\n+class InternVLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (InternVLForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (InternVLForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"image-text-to-text\": InternVLForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+\n+    def setUp(self):\n+        self.model_tester = InternVLVisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=InternVLConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    self.assertIn(\n+                        ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                        [0.0, 1.0],\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\"Qwen2 flash attention does not support right padding\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+\n+@slow\n+@require_torch_gpu\n+class InternVLQwen2IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.small_model_checkpoint = \"OpenGVLab/InternVL3-1B-hf\"\n+        self.medium_model_checkpoint = \"OpenGVLab/InternVL3-2B-hf\"\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_qwen2_small_model_integration_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+\n+        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"The image shows two cats lying on a pink blanket. The cat on the left is a tabby\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_qwen2_small_model_integration_forward(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+\n+        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+\n+        # Forward\n+        with torch.inference_mode():\n+            output = model(**inputs)\n+\n+        actual_logits = output.logits[0, -1, :5].cpu()\n+        expected_logits = torch.tensor([11.9375, 14.8750, 14.0625, 10.7500, 6.9062], dtype=torch.bfloat16)\n+        self.assertTrue(\n+            torch.allclose(actual_logits, expected_logits, atol=0.1),\n+            f\"Actual logits: {actual_logits}\"\n+            f\"\\nExpected logits: {expected_logits}\"\n+            f\"\\nDifference: {torch.abs(actual_logits - expected_logits)}\",\n+        )\n+\n+    def test_qwen2_small_model_integration_generate_text_only(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        prompt = \"<|im_start|>user\\nWrite a haiku<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"Whispers of dawn,\\nSilent whispers of the night,\\nNew day's light begins.\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_qwen2_small_model_integration_generate_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"The image shows two cats lying on a pink blanket. The cat on the left is a tabby\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_qwen2_small_model_integration_batched_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        # Prepare inputs\n+        prompt = [\n+            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<image>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+\n+        inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese archway, known as a \"Chinese Gate\" or \"Chinese Gate of'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    def test_qwen2_small_model_integration_batched_generate_multi_image(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        # Prepare inputs\n+        prompt = [\n+            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<image><image>\\nWhat are the differences between these two images?<|im_end|>\\n<|im_start|>assistant\\n\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+                ).content\n+            )\n+        )\n+        image3 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n+                ).content\n+            )\n+        )\n+\n+        inputs = processor(text=prompt, images=[[image1], [image2, image3]], padding=True, return_tensors=\"pt\").to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n+        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = 'user\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images show the Statue of Liberty and the Golden Gate Bridge from different angles. Here are the differences:\\n\\n1. **Angle'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    @require_av\n+    @require_bitsandbytes\n+    def test_qwen2_medium_model_integration_video(self):\n+        processor = AutoProcessor.from_pretrained(self.medium_model_checkpoint)\n+        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.medium_model_checkpoint, quantization_config=quantization_config\n+        )\n+        # Prepare inputs\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+                ],\n+            }\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        ).to(torch_device, dtype=torch.float16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+        expected_output = 'The man is performing a forehand shot.'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    @require_av\n+    def test_qwen2_small_model_integration_interleaved_images_videos(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, torch_dtype=torch.bfloat16, device_map=torch_device\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What are the differences between these two images?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://llava-vl.github.io/static/images/view.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, dtype=torch.bfloat16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n+        expected_output = 'user\\n\\n\\nWhat are the differences between these two images?\\nassistant\\nThe images depict two distinct scenes:\\n\\n1. **Left Image**: This shows the Statue of Liberty on Liberty Island, with the'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nA forehand shot'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check third output\n+        decoded_output = processor.decode(output[2], skip_special_tokens=True)\n+        expected_output = \"user\\n\\nWrite a haiku for this image\\nassistant\\nSilky lake,  \\nWooden pier,  \\nNature's peace.\"  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+\n+@slow\n+@require_torch_gpu\n+class InternVLLlamaIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.small_model_checkpoint = \"OpenGVLab/InternVL2_5-2B-MPO-hf\"\n+        self.medium_model_checkpoint = \"OpenGVLab/InternVL2_5-8B-MPO-hf\"\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_llama_small_model_integration_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+\n+        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"The image shows two cats sleeping on a pink couch. They are lying side by side, with their\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_llama_small_model_integration_forward(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+\n+        prompt = \"<|im_start|>user\\n<image>\\nPlease describe the image explicitly.<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+\n+        # Forward\n+        with torch.inference_mode():\n+            output = model(**inputs)\n+\n+        actual_logits = output.logits[0, -1, :5].cpu()\n+        expected_logits = torch.tensor([-9.8750, -0.4258, 1.4844, -10.3125, -10.3125], dtype=torch.bfloat16)\n+        # The original implementation and the transformers implementation do not match exactly, hence the higher tolerance.\n+        # The difference is likely due to the different implementations of the attention mechanism (different order of operations)\n+        # between the transformers Llama model and the original InternLM model.\n+        # The difference has almost no effect on the output tokens, but it does affect the logits a lot more.\n+        self.assertTrue(\n+            torch.allclose(actual_logits, expected_logits, atol=1),\n+            f\"Actual logits: {actual_logits}\"\n+            f\"\\nExpected logits: {expected_logits}\"\n+            f\"\\nDifference: {torch.abs(actual_logits - expected_logits)}\",\n+        )\n+\n+    def test_llama_small_model_integration_generate_text_only(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        prompt = \"<|im_start|>user\\nWrite a haiku<|im_end|>\\n<|im_start|>assistant\\n\"\n+        inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=200, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"Autumn leaves fall,\\nNature's breath, a season's sigh,\\nSilent woods awake.\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_llama_small_model_integration_generate_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe the image explicitly.\"},\n+                ],\n+            }\n+        ]\n+\n+        inputs = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device, dtype=torch.bfloat16)\n+        with torch.no_grad():\n+            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            decoded_output = processor.decode(\n+                generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n+            )\n+        expected_output = \"The image shows two cats sleeping on a pink couch. They are lying side by side, with their\"\n+        self.assertEqual(decoded_output, expected_output)\n+\n+    def test_llama_small_model_integration_batched_generate(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        # Prepare inputs\n+        prompt = [\n+            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<image>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+\n+        inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        expected_output = 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nWooden dock stretches to the sea,\\nSilent water mirrors.'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = 'user\\n\\nDescribe this image\\nassistant\\nThe image shows a street scene with a traditional Chinese gate in the background, adorned with red and gold colors and Chinese characters'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    def test_llama_small_model_integration_batched_generate_multi_image(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        # Prepare inputs\n+        prompt = [\n+            \"<|im_start|>user\\n<image>\\nWrite a haiku for this image<|im_end|>\\n<|im_start|>assistant\\n\",\n+            \"<|im_start|>user\\n<image><image>\\nWhat are the difference between these two images?<|im_end|>\\n<|im_start|>assistant\\n\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n+                ).content\n+            )\n+        )\n+        image3 = Image.open(\n+            BytesIO(\n+                requests.get(\n+                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n+                ).content\n+            )\n+        )\n+\n+        inputs = processor(text=prompt, images=[[image1], [image2, image3]], padding=True, return_tensors=\"pt\").to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        # Check first output\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n+        expected_output = 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, still waters.'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = 'user\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. After closely examining the images again, I can see that there are several differences'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    @require_av\n+    @require_bitsandbytes\n+    def test_llama_medium_model_integration_video(self):\n+        processor = AutoProcessor.from_pretrained(self.medium_model_checkpoint)\n+        quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.medium_model_checkpoint, quantization_config=quantization_config\n+        )\n+        # Prepare inputs\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+                ],\n+            }\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        ).to(torch_device, dtype=torch.float16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+        expected_output = \"The man is performing a forehand shot.\"\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+    @require_av\n+    def test_llama_small_model_integration_interleaved_images_videos(self):\n+        processor = AutoProcessor.from_pretrained(self.small_model_checkpoint)\n+        model = InternVLForConditionalGeneration.from_pretrained(\n+            self.small_model_checkpoint, torch_dtype=torch.bfloat16, device_map=torch_device\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What are the difference between these two images?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://llava-vl.github.io/static/images/view.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device, dtype=torch.bfloat16)\n+\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+\n+        decoded_output = processor.decode(output[0], skip_special_tokens=True)\n+        # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n+        expected_output = 'user\\n\\n\\nWhat are the difference between these two images?\\nassistant\\nI apologize for the confusion in my previous response. Upon closer inspection, the differences between the two images are:\\n\\n1. **'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check second output\n+        decoded_output = processor.decode(output[1], skip_special_tokens=True)\n+        expected_output = 'user\\nFrame1: \\nFrame2: \\nFrame3: \\nFrame4: \\nFrame5: \\nFrame6: \\nFrame7: \\nFrame8: \\nWhat type of shot is the man performing?\\nassistant\\nThe man is performing a forehand shot. This is a common shot in tennis where the player swings the racket across their'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )\n+\n+        # Check third output\n+        decoded_output = processor.decode(output[2], skip_special_tokens=True)\n+        expected_output = 'user\\n\\nWrite a haiku for this image\\nassistant\\nMajestic snow-capped peaks,\\nA wooden path leads to the sea,\\nPeaceful, untouched dreams.'  # fmt: skip\n+        self.assertEqual(\n+            decoded_output,\n+            expected_output,\n+            f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n+        )"
        },
        {
            "sha": "e00804373fab83fd7988a46c65eb27ca402e5adc",
            "filename": "tests/models/internvl/test_processor_internvl.py",
            "status": "added",
            "additions": 327,
            "deletions": 0,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -0,0 +1,327 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import shutil\n+import tempfile\n+import unittest\n+\n+from huggingface_hub import hf_hub_download\n+\n+from transformers import AutoProcessor, AutoTokenizer, InternVLProcessor\n+from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from transformers import GotOcr2ImageProcessor\n+\n+\n+@require_vision\n+class InternVLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = InternVLProcessor\n+    videos_input_name = \"pixel_values\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = GotOcr2ImageProcessor(\n+            do_resize=True,\n+            size={\"height\": 20, \"width\": 20},\n+            max_patches=2,\n+            do_rescale=True,\n+            rescale_factor=1 / 255,\n+            do_normalize=True,\n+            do_center_crop=True,\n+            image_mean=[0.485, 0.456, 0.406],\n+            image_std=[0.229, 0.224, 0.225],\n+            do_convert_rgb=True,\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"OpenGVLab/InternVL3-1B-hf\", padding_side=\"left\")\n+        processor_kwargs = cls.prepare_processor_dict()\n+        processor = InternVLProcessor.from_pretrained(\n+            \"OpenGVLab/InternVL3-1B-hf\",\n+            image_processor=image_processor,\n+            tokenizer=tokenizer,\n+            **processor_kwargs,\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.fake_image_token\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\"image_seq_length\": 10}\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    @require_av\n+    @require_torch\n+    def test_process_interleaved_images_videos(self):\n+        processor = self.get_processor()\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n+                        },\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What are the differences between these two images?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What type of shot is the man performing?\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://llava-vl.github.io/static/images/view.jpg\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+\n+        inputs_batched = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        )\n+\n+        # Process non batched inputs to check if the pixel_values and input_ids are reconstructed in the correct order when batched together\n+        images_patches_index = 0\n+        for i, message in enumerate(messages):\n+            inputs = processor.apply_chat_template(\n+                message,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                return_tensors=\"pt\",\n+                padding=True,\n+            )\n+            # We slice with [-inputs[\"input_ids\"].shape[1] :] as the input_ids are left padded\n+            torch.testing.assert_close(\n+                inputs[\"input_ids\"][0], inputs_batched[\"input_ids\"][i][-inputs[\"input_ids\"].shape[1] :]\n+            )\n+            torch.testing.assert_close(\n+                inputs[\"pixel_values\"],\n+                inputs_batched[\"pixel_values\"][\n+                    images_patches_index : images_patches_index + inputs[\"pixel_values\"].shape[0]\n+                ],\n+            )\n+            images_patches_index += inputs[\"pixel_values\"].shape[0]\n+\n+    # Override video chat_template tests as InternVLProcessor returns flattened video features\n+    @require_av\n+    def test_apply_chat_template_video_special_processing(self):\n+        \"\"\"\n+        Tests that models can use their own preprocessing to preprocess conversations.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\", \"path\": video_file_path},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def _process_messages_for_chat_template(\n+            conversation,\n+            batch_images,\n+            batch_videos,\n+            batch_video_metadata,\n+            **chat_template_kwargs,\n+        ):\n+            # Let us just always return a dummy prompt\n+            new_msg = [\n+                [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": [\n+                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n+                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n+                        ],\n+                    },\n+                ]\n+            ]\n+            return new_msg\n+\n+        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"np\",\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+\n+        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n+        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n+        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n+        # Difference with common tests, InternVLProcessor returns flattened video features, and uses 8 frames by default\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8)\n+\n+    def test_apply_chat_template_video_frame_sampling(self):\n+        processor = self.get_processor()\n+\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        num_frames = 3\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            num_frames=num_frames,\n+            return_tensors=\"np\",\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), num_frames)\n+\n+        # Load with `video_fps` arg\n+        video_fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            video_fps=video_fps,\n+            num_frames=None,  # force to use default num_frames\n+            return_tensors=\"np\",\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), video_fps * 10)\n+\n+        # Load with `video_fps` and `num_frames` args, should raise an error\n+        with self.assertRaises(ValueError):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                video_fps=video_fps,\n+                num_frames=num_frames,\n+            )\n+\n+        # Load without any arg should use the default loading method\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        # Difference with common tests, InternVLProcessor returns flattened video features, and uses 8 frames by default\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 8)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 2)"
        },
        {
            "sha": "4681e5b45ce9b0500cdbac59fa71646f3b565322",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a245011252d18ba319d181f23b657a3b4862ff3c/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a245011252d18ba319d181f23b657a3b4862ff3c/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=a245011252d18ba319d181f23b657a3b4862ff3c",
            "patch": "@@ -156,6 +156,7 @@\n         \"Llama4VisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Emu3VQVAE\",  # Building part of bigger (tested) model\n         \"Emu3TextModel\",  # Building part of bigger (tested) model\n+        \"InternVLVisionModel\",  # Building part of bigger (tested) model\n         \"JanusVisionModel\",  # Building part of bigger (tested) model\n         \"TimesFmModel\",  # Building part of bigger (tested) model\n     ]"
        }
    ],
    "stats": {
        "total": 4452,
        "additions": 4447,
        "deletions": 5
    }
}