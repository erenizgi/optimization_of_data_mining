{
    "author": "gante",
    "message": "[generate] revert change in Aria: the maximum cache length must match `max_length` (#36120)\n\n* revert inputs_embeds len\r\n\r\n* Update test_utils.py\r\n\r\n* make fixup",
    "sha": "636ee57489b3f25bc9dcafc3e412aa0753ddd8b2",
    "files": [
        {
            "sha": "b4f845b424ee1d2d21d06ddc3f4785b6f42df161",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/636ee57489b3f25bc9dcafc3e412aa0753ddd8b2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/636ee57489b3f25bc9dcafc3e412aa0753ddd8b2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=636ee57489b3f25bc9dcafc3e412aa0753ddd8b2",
            "patch": "@@ -1470,7 +1470,6 @@ def _prepare_generated_length(\n         elif (\n             model_input_name == \"inputs_embeds\"\n             and input_ids_length != inputs_tensor.shape[1]\n-            and input_ids_length != 0\n             and not self.config.is_encoder_decoder\n         ):\n             generation_config.max_length -= inputs_tensor.shape[1]"
        },
        {
            "sha": "6c8a5e1285f80389807b7bd0410ece0e58a3db96",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/636ee57489b3f25bc9dcafc3e412aa0753ddd8b2/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/636ee57489b3f25bc9dcafc3e412aa0753ddd8b2/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=636ee57489b3f25bc9dcafc3e412aa0753ddd8b2",
            "patch": "@@ -1786,12 +1786,12 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             model.config.use_cache = True\n             model.config.is_decoder = True\n             batch_size = input_ids.shape[0]\n-            max_cache_len = 30\n+            max_length = 30\n \n             # here we force to not stop at eos and go until max-length\n             model.generation_config.eos_token_id = model.config.get_text_config().eos_token_id = -1\n             generation_kwargs = {\n-                \"max_length\": max_cache_len,\n+                \"max_length\": max_length,\n                 \"cache_implementation\": \"static\",\n                 \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n             }\n@@ -1810,11 +1810,11 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             num_hidden_layers = text_config.num_hidden_layers\n \n             inputs_embeds = model.get_input_embeddings()(input_ids)\n-            max_cache_len += inputs_embeds.shape[1] - 1  # the last generated token has no cache\n             outputs = model.generate(inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict)\n \n-            # we should get `max_length` in shape, not `max_length - embeds_length`\n-            cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n+            # we should get `max_length - 1` in shape, not `max_length - embeds_length`.\n+            # -1 because the last generated token isn't yet in the cache.\n+            cache_shape = (batch_size, num_key_value_heads, max_length - 1, head_dim)\n             self.assertTrue(isinstance(outputs.past_key_values, StaticCache))\n             self.assertTrue(len(outputs.past_key_values.key_cache) == num_hidden_layers)\n             self.assertTrue(outputs.past_key_values.key_cache[0].shape == cache_shape)"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 5,
        "deletions": 6
    }
}