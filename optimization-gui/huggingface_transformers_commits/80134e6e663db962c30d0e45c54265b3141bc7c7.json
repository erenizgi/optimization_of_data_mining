{
    "author": "jcaip",
    "message": "Update transformers to support `FqnToConfig` (#41894)\n\n* Update transformers to support `FqnToConfig`\n\nSummary:\n\nTest Plan:\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* add case for modulefqn\n\n* remove comment\n\n* update tests\n\n* cleanup\n\n* update\n\n* wip\n\n* wip\n\n* update quantizer_torchao for module default\n\n* fix underscore\n\n* update tests\n\n* update\n\n* fix import error\n\n* fix import\n\n* import change not included in previous commit\n\n* Apply suggestion from @MekkCyber\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update src/transformers/quantizers/quantizer_torchao.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* update tests and add comment\n\n* fix test\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "80134e6e663db962c30d0e45c54265b3141bc7c7",
    "files": [
        {
            "sha": "1900dabd6e5e16939d9fd3d18f9fdd9d01f61cd7",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/80134e6e663db962c30d0e45c54265b3141bc7c7/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/80134e6e663db962c30d0e45c54265b3141bc7c7/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=80134e6e663db962c30d0e45c54265b3141bc7c7",
            "patch": "@@ -422,19 +422,19 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n #### 1. Skip quantization for certain layers\n \n-With `ModuleFqnToConfig` we can specify a default configuration for all layers while skipping quantization for certain layers.\n+With `FqnToConfig` we can specify a default configuration for all layers while skipping quantization for certain layers.\n \n ```py\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n \n model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n \n-from torchao.quantization import Int4WeightOnlyConfig, ModuleFqnToConfig\n+from torchao.quantization import Int4WeightOnlyConfig, FqnToConfig\n config = Int4WeightOnlyConfig(group_size=128)\n \n # set default to int4 (for linears), and skip quantizing `model.layers.0.self_attn.q_proj`\n-quant_config = ModuleFqnToConfig({\"_default\": config, \"model.layers.0.self_attn.q_proj\": None})\n+quant_config = FqnToConfig({\"_default\": config, \"model.layers.0.self_attn.q_proj\": None})\n quantization_config = TorchAoConfig(quant_type=quant_config)\n quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", dtype=torch.bfloat16, quantization_config=quantization_config)\n # lm_head is not quantized and model.layers.0.self_attn.q_proj is not quantized\n@@ -459,7 +459,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n \n model_id = \"facebook/opt-125m\"\n \n-from torchao.quantization import Int4WeightOnlyConfig, ModuleFqnToConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType\n+from torchao.quantization import Int4WeightOnlyConfig, FqnToConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType\n \n weight_dtype = torch.int8\n granularity = PerAxis(0)\n@@ -470,7 +470,7 @@ embedding_config = IntxWeightOnlyConfig(\n     mapping_type=mapping_type,\n )\n linear_config = Int8DynamicActivationInt4WeightConfig(group_size=128)\n-quant_config = ModuleFqnToConfig({\"_default\": linear_config, \"model.decoder.embed_tokens\": embedding_config, \"model.decoder.embed_positions\": None})\n+quant_config = FqnToConfig({\"_default\": linear_config, \"model.decoder.embed_tokens\": embedding_config, \"model.decoder.embed_positions\": None})\n # set `include_embedding` to True in order to include embedding in quantization\n # when `include_embedding` is True, we'll remove input embedding from `modules_not_to_convert` as well\n quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True)\n@@ -521,7 +521,7 @@ from torchao.quantization import (\n     IntxWeightOnlyConfig,\n     PerRow,\n     PerAxis,\n-    ModuleFqnToConfig,\n+    FqnToConfig,\n     Float8Tensor,\n     Int4TilePackedTo4dTensor,\n     IntxUnpackedToInt8Tensor,\n@@ -550,7 +550,7 @@ qconfig_dict = {\n \n     \"_default\": intxwo,\n }\n-quant_config = ModuleFqnToConfig(qconfig_dict)\n+quant_config = FqnToConfig(qconfig_dict)\n quantization_config = TorchAoConfig(quant_type=quant_config)\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     model_id,"
        },
        {
            "sha": "72364db59a138a8de8fdf9355500e355e8d7565f",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 64,
            "deletions": 2,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/80134e6e663db962c30d0e45c54265b3141bc7c7/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80134e6e663db962c30d0e45c54265b3141bc7c7/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=80134e6e663db962c30d0e45c54265b3141bc7c7",
            "patch": "@@ -251,6 +251,23 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n             _QUANTIZABLE = [torch.nn.Linear]\n             if self.quantization_config.include_input_output_embeddings:\n                 _QUANTIZABLE.append(torch.nn.Embedding)\n+\n+            # Handle FqnToConfig, introduced in torchao 0.15.0+\n+            if self.quantization_config._get_ao_version() >= version.parse(\"0.15.0\"):\n+                from torchao.quantization import FqnToConfig, fqn_matches_fqn_config\n+\n+                if isinstance(self.quantization_config.quant_type, FqnToConfig):\n+                    module_fqn, param_name_fqn = param_name.rsplit(\".\", 1)\n+                    if (\n+                        fqn_matches_fqn_config(module_fqn, self.quantization_config.quant_type)\n+                        or fqn_matches_fqn_config(param_name, self.quantization_config.quant_type)\n+                        or (\n+                            \"_default\" in self.quantization_config.quant_type.fqn_to_config\n+                            and isinstance(module, tuple(_QUANTIZABLE))\n+                        )\n+                    ):\n+                        return True\n+\n             return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n \n     def create_quantized_param(\n@@ -319,8 +336,54 @@ def create_quantized_param(\n                 model.tie_weights()\n                 setattr(model.config.get_text_config(decoder=True), \"tie_word_embeddings\", False)\n \n+            # handle FqnToConfig, introduced in torchao 0.15.0+\n+            if self.quantization_config._get_ao_version() >= version.Version(\"0.15.0\"):\n+                from torchao.quantization import FqnToConfig\n+\n+                config = self.quantization_config.get_apply_tensor_subclass()\n+                if isinstance(config, FqnToConfig):\n+                    module_fqn, top_level_param_name = param_name.rsplit(\".\", 1)\n+                    c = None\n+                    if param_name in config.fqn_to_config:\n+                        assert not module_fqn.startswith(\"re:\"), (\n+                            \"param fqn should not start with`re:`, which is used for specifying regex\"\n+                        )\n+                        c = config.module_fqn_to_config[param_name]\n+                    elif module_fqn in config.fqn_to_config:\n+                        assert not module_fqn.startswith(\"re:\"), (\n+                            \"module fqn should not start with`re:`, which is used for specifying regex\"\n+                        )\n+                        c = config.module_fqn_to_config[module_fqn]\n+                    # regex match module and param\n+                    else:\n+                        for maybe_module_fqn_pattern in config.fqn_to_config:\n+                            # if key doesn't start with re, it is an exact fqn key, so we don't regex match\n+                            if not maybe_module_fqn_pattern.startswith(\"re:\"):\n+                                continue\n+                            # see if param matches first\n+                            elif re.fullmatch(maybe_module_fqn_pattern[3:], param_name):\n+                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n+                                break\n+                            elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n+                                # we'll apply the config for first fully matched pattern\n+                                c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n+                                break\n+                        else:\n+                            c = config.module_fqn_to_config.get(\"_default\", None)\n+\n+                    if c is not None:\n+                        if top_level_param_name == \"weight\":\n+                            # we can apply the module config directly\n+                            quantize_(module, c, (lambda x, fqn: True))\n+                        else:\n+                            # need to apply to custom param name\n+                            custom_param_fqn_config = FqnToConfig({top_level_param_name: c})\n+                            quantize_(module, custom_param_fqn_config, filter_fn=None)\n+                    return\n+\n             # handle ModuleFqnToConfig, introduced in torchao 0.12.0+\n-            if self.quantization_config._get_ao_version() >= version.Version(\"0.12.0\"):\n+            # TODO deprecate this when we deprecate ModuleFqnToConfig\n+            elif self.quantization_config._get_ao_version() >= version.Version(\"0.12.0\"):\n                 from torchao.quantization import ModuleFqnToConfig\n \n                 config = self.quantization_config.get_apply_tensor_subclass()\n@@ -342,7 +405,6 @@ def create_quantized_param(\n                                 break\n                         else:\n                             c = config.module_fqn_to_config.get(\"_default\", None)\n-\n                     if c is not None:\n                         # filter_fn: not filtering out any modules\n                         quantize_(module, c, filter_fn=lambda x, fqn: True)"
        },
        {
            "sha": "d682cc57a3862d894d04563814253d6f4d80a0cf",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 154,
            "deletions": 4,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/80134e6e663db962c30d0e45c54265b3141bc7c7/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/80134e6e663db962c30d0e45c54265b3141bc7c7/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=80134e6e663db962c30d0e45c54265b3141bc7c7",
            "patch": "@@ -62,6 +62,8 @@\n         from torchao.dtypes import Int4CPULayout\n     if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.11.0\"):\n         from torchao.dtypes import Int4XPULayout\n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.15.0\"):\n+        from torchao.quantization import FqnToConfig\n \n \n def check_torchao_int4_wo_quantized(test_module, qlayer):\n@@ -378,6 +380,154 @@ def test_module_fqn_to_config_regex_precedence(self):\n         ]\n         self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n \n+    @require_torchao_version_greater_or_equal(\"0.15.0\")\n+    def test_fqn_to_config_regex_precedence(self):\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        config = FqnToConfig(\n+            {\n+                r\"re:model\\.layers\\..+\\.self_attn\\.q_proj.weight\": None,\n+                \"model.layers.3.self_attn.q_proj.weight\": linear2_config,\n+                \"_default\": linear1_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, Float8Tensor))\n+        self.assertTrue(not isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.k_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.15.0\")\n+    def test_fqn_to_config_param_over_module_regex_precedence(self):\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        config = FqnToConfig(\n+            {\n+                r\"re:model\\.layers\\..+\\.self_attn\\.q_proj.weight\": None,\n+                r\"re:model\\.layers\\..+\\.self_attn\\.q_proj\": linear2_config,\n+                \"_default\": linear1_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        self.assertTrue(not isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.k_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.15.0\")\n+    def test_fqn_to_config_param_over_module_precedence(self):\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        config = FqnToConfig(\n+            {\n+                \"model.layers.3.self_attn.q_proj.weight\": None,\n+                \"model.layers.3.self_attn.q_proj\": linear2_config,\n+                \"_default\": linear1_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        self.assertTrue(not isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.k_proj.weight, AffineQuantizedTensor))\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.15.0\")\n+    def test_fqn_to_config_exact_over_regex_precedence(self):\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        config = FqnToConfig(\n+            {\n+                \"model.layers.3.self_attn.q_proj.weight\": None,\n+                \"model.layers.1.self_attn.q_proj\": linear1_config,\n+                r\"re:model\\.layers\\..+\\.self_attn\\.q_proj.weight\": linear2_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_name,\n+            device_map=self.device,\n+            quantization_config=quant_config,\n+        )\n+        self.assertTrue(not isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n+        self.assertTrue(isinstance(quantized_model.model.layers[2].self_attn.q_proj.weight, Float8Tensor))\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n+\n+        input_ids = tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n+\n+        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n+        EXPECTED_OUTPUT = [\n+            \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n+            \"What are we having for dinner?\\n\\nJess: (smiling) I\",\n+        ]\n+        self.assertTrue(tokenizer.decode(output[0], skip_special_tokens=True) in EXPECTED_OUTPUT)\n+\n+    @require_torchao_version_greater_or_equal(\"0.15.0\")\n+    def test_fqn_to_config_non_weight_param(self):\n+        linear1_config = Int8WeightOnlyConfig()\n+        linear2_config = Float8WeightOnlyConfig()\n+        config = FqnToConfig(\n+            {\n+                r\"re:.*gate_up_proj\": linear2_config,\n+                \"model.layers.0.feed_forward.experts.gate_up_proj\": None,\n+                \"_default\": linear1_config,\n+            }\n+        )\n+        quant_config = TorchAoConfig(quant_type=config)\n+        quantized_model = AutoModelForCausalLM.from_pretrained(\n+            \"jcaip/Llama-4-Scout-17B-two-layers-only-testing\",\n+            device_map=\"auto\",\n+            dtype=torch.bfloat16,\n+            quantization_config=quant_config,\n+        )\n+\n+        self.assertTrue(isinstance(quantized_model.model.layers[1].feed_forward.experts.gate_up_proj, Float8Tensor))\n+        self.assertTrue(\n+            not isinstance(quantized_model.model.layers[0].feed_forward.experts.gate_up_proj, Float8Tensor)\n+        )\n+        self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n+\n \n @require_torch_accelerator\n class TorchAoAcceleratorTest(TorchAoTest):\n@@ -580,6 +730,8 @@ class TorchAoSafeSerializationTest(TorchAoSerializationTest):\n     def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n+        # placeholder\n+        cls.quant_scheme = torchao.quantization.Float8WeightOnlyConfig()\n \n     def tearDown(self):\n         gc.collect()\n@@ -710,11 +862,10 @@ def setUpClass(cls):\n \n         from torchao.quantization import Float8WeightOnlyConfig\n \n+        super().setUpClass()\n         cls.quant_scheme = Float8WeightOnlyConfig()\n         cls.quant_scheme_kwargs = {}\n \n-        super().setUpClass()\n-\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n \n@@ -732,11 +883,10 @@ def setUpClass(cls):\n \n         from torchao.quantization import Int8DynamicActivationInt4WeightConfig\n \n+        super().setUpClass()\n         cls.quant_scheme = Int8DynamicActivationInt4WeightConfig()\n         cls.quant_scheme_kwargs = {}\n \n-        super().setUpClass()\n-\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n "
        }
    ],
    "stats": {
        "total": 238,
        "additions": 225,
        "deletions": 13
    }
}