{
    "author": "yonigozlan",
    "message": "Fix issue with from pretrained and kwargs in image processors (#41997)\n\n* accept kwargs in image proc from_pretrained\n\n* only use kwargs that are in cls.valid_kwargs\n\n* remove specific logic for _from_auto\n\n* add image_seq_length to Images_kwargs for backward compatibility\n\n* fix missing image kwargs in pix2struct",
    "sha": "900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
    "files": [
        {
            "sha": "60774390bf23f7e303188707f974d9dcfb5fb7ba",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 4,
            "deletions": 16,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
            "patch": "@@ -362,25 +362,13 @@ def from_dict(cls, image_processor_dict: dict[str, Any], **kwargs):\n         \"\"\"\n         image_processor_dict = image_processor_dict.copy()\n         return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n-\n-        # The `size` parameter is a dict and was previously an int or tuple in feature extractors.\n-        # We set `size` here directly to the `image_processor_dict` so that it is converted to the appropriate\n-        # dict within the image processor and isn't overwritten if `size` is passed in as a kwarg.\n-        if \"size\" in kwargs and \"size\" in image_processor_dict:\n-            image_processor_dict[\"size\"] = kwargs.pop(\"size\")\n-        if \"crop_size\" in kwargs and \"crop_size\" in image_processor_dict:\n-            image_processor_dict[\"crop_size\"] = kwargs.pop(\"crop_size\")\n-\n+        image_processor_dict.update({k: v for k, v in kwargs.items() if k in cls.valid_kwargs.__annotations__})\n         image_processor = cls(**image_processor_dict)\n \n-        # Update image_processor with kwargs if needed\n-        to_remove = []\n-        for key, value in kwargs.items():\n+        # Remove kwargs that are used to initialize the image processor attributes\n+        for key in list(kwargs):\n             if hasattr(image_processor, key):\n-                setattr(image_processor, key, value)\n-                to_remove.append(key)\n-        for key in to_remove:\n-            kwargs.pop(key, None)\n+                kwargs.pop(key)\n \n         logger.info(f\"Image processor {image_processor}\")\n         if return_unused_kwargs:"
        },
        {
            "sha": "45741efd951772cfaac55dbe92ec5bd0ff14af11",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
            "patch": "@@ -185,6 +185,7 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     input_data_format = None\n     device = None\n     model_input_names = [\"pixel_values\"]\n+    image_seq_length = None\n     valid_kwargs = ImagesKwargs\n     unused_kwargs = None\n "
        },
        {
            "sha": "ec5645ee4bb9abeb7897b57996eef75a58f3739d",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
            "patch": "@@ -53,11 +53,18 @@ class Pix2StructImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     max_patches (`int`, *optional*):\n         Maximum number of patches to extract.\n+    patch_size (`dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+        The patch size to use for the image. According to Pix2Struct paper and code, the patch size is 16x16.\n+    is_vqa (`bool`, *optional*, defaults to `False`):\n+        Whether or not the image processor is for the VQA task. If `True` and `header_text` is passed in, text is\n+        rendered onto the input images.\n     header_text (`Union[list[str], str]`, *optional*):\n         Text to render as a header. Only has an effect if `image_processor.is_vqa` is `True`.\n     \"\"\"\n \n     max_patches: int\n+    patch_size: dict[str, int]\n+    is_vqa: bool\n     header_text: Optional[Union[list[str], str]]\n \n "
        },
        {
            "sha": "7a368697eda88d54dfc5ee2efad8d188aa4473f4",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
            "patch": "@@ -219,6 +219,9 @@ class methods and docstrings.\n             - `'np'`: Return NumPy `np.ndarray` objects.\n         disable_grouping (`bool`, *optional*):\n             Whether to group images by shapes when processing or not, only relevant for fast image processing.\n+        image_seq_length (`int`, *optional*):\n+            The number of image tokens to be used for each image in the input.\n+            Added for backward compatibility but this should be set as a processor attribute in future models.\n     \"\"\"\n \n     do_convert_rgb: Optional[bool]\n@@ -239,6 +242,7 @@ class methods and docstrings.\n     device: Annotated[Optional[str], device_validator()]\n     return_tensors: Annotated[Optional[Union[str, TensorType]], tensor_type_validator()]\n     disable_grouping: Optional[bool]\n+    image_seq_length: Optional[int]\n \n \n class VideosKwargs(TypedDict, total=False):"
        },
        {
            "sha": "e4c8d5d63abd20853e7e7e4100083ecbcdb89737",
            "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/900cf9d33bc091f3e47f8e598cba464f8b93bdd7/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py?ref=900cf9d33bc091f3e47f8e598cba464f8b93bdd7",
            "patch": "@@ -172,6 +172,7 @@ def test_image_processor_defaults_preserved_by_image_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n         image_processor = self.get_component(\"image_processor\", max_patches=1024, patch_size={\"height\": 8, \"width\": 8})\n+        print(\"image_processor\", image_processor)\n         tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)"
        }
    ],
    "stats": {
        "total": 33,
        "additions": 17,
        "deletions": 16
    }
}