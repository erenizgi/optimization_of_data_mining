{
    "author": "amyeroberts",
    "message": "Trainer - deprecate tokenizer for processing_class (#32385)\n\n* Trainer - deprecate tokenizer for processing_class\r\n\r\n* Extend chage across Seq2Seq trainer and docs\r\n\r\n* Add tests\r\n\r\n* Update to FutureWarning and add deprecation version",
    "sha": "b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
    "files": [
        {
            "sha": "49dde04fe6069480e8daceae2bfa5f3649403d9f",
            "filename": "docs/source/en/hpo_train.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fhpo_train.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -15,7 +15,7 @@ rendered properly in your Markdown viewer.\n \n # Hyperparameter Search using Trainer API\n \n-ü§ó Transformers provides a [`Trainer`] class optimized for training ü§ó Transformers models, making it easier to start training without manually writing your own training loop. The [`Trainer`] provides API for hyperparameter search. This doc shows how to enable it in example. \n+ü§ó Transformers provides a [`Trainer`] class optimized for training ü§ó Transformers models, making it easier to start training without manually writing your own training loop. The [`Trainer`] provides API for hyperparameter search. This doc shows how to enable it in example.\n \n ## Hyperparameter Search backend\n \n@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n you should install them before using them as the hyperparameter search backend\n ```bash\n-pip install optuna/sigopt/wandb/ray[tune] \n+pip install optuna/sigopt/wandb/ray[tune]\n ```\n \n ## How to enable Hyperparameter search in example\n@@ -112,7 +112,7 @@ Create a [`Trainer`] with your `model_init` function, training arguments, traini\n ...     train_dataset=small_train_dataset,\n ...     eval_dataset=small_eval_dataset,\n ...     compute_metrics=compute_metrics,\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     model_init=model_init,\n ...     data_collator=data_collator,\n ... )"
        },
        {
            "sha": "317948331eb10294a03bb505f7f7597a2e25e8a6",
            "filename": "docs/source/en/model_doc/mamba.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmamba.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -39,8 +39,8 @@ The original code can be found [here](https://github.com/state-spaces/mamba).\n \n # Usage\n \n-### A simple generation example: \n-```python \n+### A simple generation example:\n+```python\n from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n import torch\n \n@@ -55,7 +55,7 @@ print(tokenizer.batch_decode(out))\n ### Peft finetuning\n The slow version is not very stable for training, and the fast one needs `float32`!\n \n-```python \n+```python\n from datasets import load_dataset\n from trl import SFTTrainer\n from peft import LoraConfig\n@@ -80,7 +80,7 @@ lora_config =  LoraConfig(\n )\n trainer = SFTTrainer(\n     model=model,\n-    tokenizer=tokenizer,\n+    processing_class=tokenizer,\n     args=training_args,\n     peft_config=lora_config,\n     train_dataset=dataset,"
        },
        {
            "sha": "f6fc66f4b6cb78c6bde20e1a1d1ba1f5a13042c7",
            "filename": "docs/source/en/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquicktour.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -111,7 +111,7 @@ Load an audio dataset (see the ü§ó Datasets [Quick Start](https://huggingface.c\n >>> dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")  # doctest: +IGNORE_RESULT\n ```\n \n-You need to make sure the sampling rate of the dataset matches the sampling \n+You need to make sure the sampling rate of the dataset matches the sampling\n rate [`facebook/wav2vec2-base-960h`](https://huggingface.co/facebook/wav2vec2-base-960h) was trained on:\n \n ```py\n@@ -174,7 +174,7 @@ If you can't find a model for your use-case, you'll need to finetune a pretraine\n \n <Youtube id=\"AhChOFRegn4\"/>\n \n-Under the hood, the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] classes work together to power the [`pipeline`] you used above. An [AutoClass](./model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate `AutoClass` for your task and it's associated preprocessing class. \n+Under the hood, the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] classes work together to power the [`pipeline`] you used above. An [AutoClass](./model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate `AutoClass` for your task and it's associated preprocessing class.\n \n Let's return to the example from the previous section and see how you can use the `AutoClass` to replicate the results of the [`pipeline`].\n \n@@ -485,7 +485,7 @@ Now gather all these classes in [`Trainer`]:\n ...     args=training_args,\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )  # doctest: +SKIP\n ```\n@@ -502,7 +502,7 @@ For tasks - like translation or summarization - that use a sequence-to-sequence\n \n </Tip>\n \n-You can customize the training loop behavior by subclassing the methods inside [`Trainer`]. This allows you to customize features such as the loss function, optimizer, and scheduler. Take a look at the [`Trainer`] reference for which methods can be subclassed. \n+You can customize the training loop behavior by subclassing the methods inside [`Trainer`]. This allows you to customize features such as the loss function, optimizer, and scheduler. Take a look at the [`Trainer`] reference for which methods can be subclassed.\n \n The other way to customize the training loop is by using [Callbacks](./main_classes/callback). You can use callbacks to integrate with other libraries and inspect the training loop to report on progress or stop the training early. Callbacks do not modify anything in the training loop itself. To customize something like the loss function, you need to subclass the [`Trainer`] instead.\n "
        },
        {
            "sha": "f3e068444ca5568a674e9fcd53d170d0b547a14a",
            "filename": "docs/source/en/tasks/asr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fasr.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -281,7 +281,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )\n@@ -368,4 +368,4 @@ Get the predicted `input_ids` with the highest probability, and use the processo\n ['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']\n ```\n </pt>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "59d6a175da82bae4e971c641f6fec2f0b6fa027f",
            "filename": "docs/source/en/tasks/audio_classification.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Faudio_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -98,8 +98,8 @@ Take a look at an example now:\n \n There are two fields:\n \n-- `audio`: a 1-dimensional `array` of the speech signal that must be called to load and resample the audio file. \n-- `intent_class`: represents the class id of the speaker's intent. \n+- `audio`: a 1-dimensional `array` of the speech signal that must be called to load and resample the audio file.\n+- `intent_class`: represents the class id of the speaker's intent.\n \n To make it easier for the model to get the label name from the label id, create a dictionary that maps the label name to an integer and vice versa:\n \n@@ -235,7 +235,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=feature_extractor,\n+...     processing_class=feature_extractor,\n ...     compute_metrics=compute_metrics,\n ... )\n \n@@ -321,4 +321,4 @@ Get the class with the highest probability, and use the model's `id2label` mappi\n 'cash_deposit'\n ```\n </pt>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "d83e025c409019330f762b551f17e944a50de23b",
            "filename": "docs/source/en/tasks/document_question_answering.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fdocument_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fdocument_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fdocument_question_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -420,7 +420,7 @@ Finally, bring everything together, and call [`~Trainer.train`]:\n ...     data_collator=data_collator,\n ...     train_dataset=encoded_train_dataset,\n ...     eval_dataset=encoded_test_dataset,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n \n >>> trainer.train()\n@@ -489,4 +489,4 @@ which token is at the end of the answer. Both have shape (batch_size, sequence_l\n \n >>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])\n 'lee a. waller'\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "514ec3fbfe0b93007c6fd7811cd935bc166f502c",
            "filename": "docs/source/en/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -317,7 +317,7 @@ At this point, only three steps remain:\n ...     data_collator=data_collator,\n ...     train_dataset=food[\"train\"],\n ...     eval_dataset=food[\"test\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     compute_metrics=compute_metrics,\n ... )\n "
        },
        {
            "sha": "530e92d81f5c0d432fcb3694b3ca47b13e1d5aaf",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -19,25 +19,25 @@ rendered properly in your Markdown viewer.\n \n Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between it's outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in [Distilling the Knowledge in a Neural Network by Hinton et al](https://arxiv.org/abs/1503.02531). In this guide, we will do task-specific knowledge distillation. We will use the [beans dataset](https://huggingface.co/datasets/beans) for this.\n \n-This guide demonstrates how you can distill a [fine-tuned ViT model](https://huggingface.co/merve/vit-mobilenet-beans-224) (teacher model) to a [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224) (student model) using the [Trainer¬†API](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) of ü§ó Transformers. \n+This guide demonstrates how you can distill a [fine-tuned ViT model](https://huggingface.co/merve/vit-mobilenet-beans-224) (teacher model) to a [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224) (student model) using the [Trainer¬†API](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) of ü§ó Transformers.\n \n-Let's install the libraries needed for distillation and evaluating the process. \n+Let's install the libraries needed for distillation and evaluating the process.\n \n ```bash\n pip install transformers datasets accelerate tensorboard evaluate --upgrade\n ```\n \n In this example, we are using the `merve/beans-vit-224` model as teacher model. It's an image classification model, based on `google/vit-base-patch16-224-in21k` fine-tuned on beans dataset. We will distill this model to a randomly initialized MobileNetV2.\n \n-We will now load the dataset. \n+We will now load the dataset.\n \n ```python\n from datasets import load_dataset\n \n dataset = load_dataset(\"beans\")\n ```\n \n-We can use an image processor from either of the models, as in this case they return the same output with same resolution. We will use the `map()` method of `dataset` to apply the preprocessing to every split of the dataset. \n+We can use an image processor from either of the models, as in this case they return the same output with same resolution. We will use the `map()` method of `dataset` to apply the preprocessing to every split of the dataset.\n \n ```python\n from transformers import AutoImageProcessor\n@@ -93,15 +93,15 @@ class ImageDistilTrainer(Trainer):\n         return (loss, student_output) if return_outputs else loss\n ```\n \n-We will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through the `Trainer`. \n+We will now login to Hugging Face Hub so we can push our model to the Hugging Face Hub through the `Trainer`.\n \n ```python\n from huggingface_hub import notebook_login\n \n notebook_login()\n ```\n \n-Let's set the `TrainingArguments`, the teacher model and the student model. \n+Let's set the `TrainingArguments`, the teacher model and the student model.\n \n ```python\n from transformers import AutoModelForImageClassification, MobileNetV2Config, MobileNetV2ForImageClassification\n@@ -164,7 +164,7 @@ trainer = ImageDistilTrainer(\n     train_dataset=processed_datasets[\"train\"],\n     eval_dataset=processed_datasets[\"validation\"],\n     data_collator=data_collator,\n-    tokenizer=teacher_processor,\n+    processing_class=teacher_processor,\n     compute_metrics=compute_metrics,\n     temperature=5,\n     lambda_param=0.5"
        },
        {
            "sha": "06eb45eda99150409e3943111784771727bdf1d6",
            "filename": "docs/source/en/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -270,7 +270,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "fdc81896bc1924f2b42668ca8c16f0c595a677a8",
            "filename": "docs/source/en/tasks/object_detection.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -340,15 +340,15 @@ with `pixel_values`, a tensor with `pixel_mask`, and `labels`.\n           [ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741],\n           [ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741],\n           [ 0.0741,  0.0741,  0.0741,  ...,  0.0741,  0.0741,  0.0741]],\n-  \n+\n           [[ 1.6232,  1.6408,  1.6583,  ...,  0.8704,  1.0105,  1.1331],\n           [ 1.6408,  1.6583,  1.6758,  ...,  0.8529,  0.9930,  1.0980],\n           [ 1.6933,  1.6933,  1.7108,  ...,  0.8179,  0.9580,  1.0630],\n           ...,\n           [ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052],\n           [ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052],\n           [ 0.2052,  0.2052,  0.2052,  ...,  0.2052,  0.2052,  0.2052]],\n-  \n+\n           [[ 1.8905,  1.9080,  1.9428,  ..., -0.1487, -0.0964, -0.0615],\n           [ 1.9254,  1.9428,  1.9603,  ..., -0.1661, -0.1138, -0.0790],\n           [ 1.9777,  1.9777,  1.9951,  ..., -0.2010, -0.1138, -0.0790],\n@@ -569,7 +569,7 @@ Finally, bring everything together, and call [`~transformers.Trainer.train`]:\n ...     args=training_args,\n ...     train_dataset=cppe5[\"train\"],\n ...     eval_dataset=cppe5[\"validation\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     data_collator=collate_fn,\n ...     compute_metrics=eval_compute_metrics_fn,\n ... )"
        },
        {
            "sha": "998010e67ca95fbe195c13a854194540692c6116",
            "filename": "docs/source/en/tasks/question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fquestion_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -225,7 +225,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=tokenized_squad[\"train\"],\n ...     eval_dataset=tokenized_squad[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n "
        },
        {
            "sha": "27516ace1cc345bd5caaf00774020749e32a2b84",
            "filename": "docs/source/en/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsequence_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -190,7 +190,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=tokenized_imdb[\"train\"],\n ...     eval_dataset=tokenized_imdb[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "7d7ecf1fbab6db03a25cb8f01476bf790aa1f80a",
            "filename": "docs/source/en/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -214,7 +214,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=tokenized_billsum[\"train\"],\n ...     eval_dataset=tokenized_billsum[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "188d4ea5f9ee68b4132aaec2ddfac9b71d4c34cc",
            "filename": "docs/source/en/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 85,
            "deletions": 85,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -18,13 +18,13 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Text-to-speech (TTS) is the task of creating natural-sounding speech from text, where the speech can be generated in multiple \n-languages and for multiple speakers. Several text-to-speech models are currently available in ü§ó Transformers, such as \n-[Bark](../model_doc/bark), [MMS](../model_doc/mms), [VITS](../model_doc/vits) and [SpeechT5](../model_doc/speecht5). \n+Text-to-speech (TTS) is the task of creating natural-sounding speech from text, where the speech can be generated in multiple\n+languages and for multiple speakers. Several text-to-speech models are currently available in ü§ó Transformers, such as\n+[Bark](../model_doc/bark), [MMS](../model_doc/mms), [VITS](../model_doc/vits) and [SpeechT5](../model_doc/speecht5).\n \n-You can easily generate audio using the `\"text-to-audio\"` pipeline (or its alias - `\"text-to-speech\"`). Some models, like Bark, \n+You can easily generate audio using the `\"text-to-audio\"` pipeline (or its alias - `\"text-to-speech\"`). Some models, like Bark,\n can also be conditioned to generate non-verbal communications such as laughing, sighing and crying, or even add music.\n-Here's an example of how you would use the `\"text-to-speech\"` pipeline with Bark: \n+Here's an example of how you would use the `\"text-to-speech\"` pipeline with Bark:\n \n ```py\n >>> from transformers import pipeline\n@@ -34,18 +34,18 @@ Here's an example of how you would use the `\"text-to-speech\"` pipeline with Bark\n >>> output = pipe(text)\n ```\n \n-Here's a code snippet you can use to listen to the resulting audio in a notebook: \n+Here's a code snippet you can use to listen to the resulting audio in a notebook:\n \n ```python\n >>> from IPython.display import Audio\n >>> Audio(output[\"audio\"], rate=output[\"sampling_rate\"])\n ```\n \n-For more examples on what Bark and other pretrained TTS models can do, refer to our \n-[Audio course](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models). \n+For more examples on what Bark and other pretrained TTS models can do, refer to our\n+[Audio course](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models).\n \n-If you are looking to fine-tune a TTS model, the only text-to-speech models currently available in ü§ó Transformers \n-are [SpeechT5](model_doc/speecht5) and [FastSpeech2Conformer](model_doc/fastspeech2_conformer), though more will be added in the future. SpeechT5 is pre-trained on a combination of speech-to-text and text-to-speech data, allowing it to learn a unified space of hidden representations shared by both text and speech. This means that the same pre-trained model can be fine-tuned for different tasks. Furthermore, SpeechT5 supports multiple speakers through x-vector speaker embeddings. \n+If you are looking to fine-tune a TTS model, the only text-to-speech models currently available in ü§ó Transformers\n+are [SpeechT5](model_doc/speecht5) and [FastSpeech2Conformer](model_doc/fastspeech2_conformer), though more will be added in the future. SpeechT5 is pre-trained on a combination of speech-to-text and text-to-speech data, allowing it to learn a unified space of hidden representations shared by both text and speech. This means that the same pre-trained model can be fine-tuned for different tasks. Furthermore, SpeechT5 supports multiple speakers through x-vector speaker embeddings.\n \n The remainder of this guide illustrates how to:\n \n@@ -66,7 +66,7 @@ pip install git+https://github.com/huggingface/transformers.git\n \n <Tip>\n \n-To follow this guide you will need a GPU. If you're working in a notebook, run the following line to check if a GPU is available: \n+To follow this guide you will need a GPU. If you're working in a notebook, run the following line to check if a GPU is available:\n \n ```bash\n !nvidia-smi\n@@ -90,13 +90,13 @@ We encourage you to log in to your Hugging Face account to upload and share your\n \n ## Load the dataset\n \n-[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) is a large-scale multilingual speech corpus consisting of \n-data sourced from 2009-2020 European Parliament event recordings. It contains labelled audio-transcription data for 15 \n-European languages. In this guide, we are using the Dutch language subset, feel free to pick another subset. \n+[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) is a large-scale multilingual speech corpus consisting of\n+data sourced from 2009-2020 European Parliament event recordings. It contains labelled audio-transcription data for 15\n+European languages. In this guide, we are using the Dutch language subset, feel free to pick another subset.\n \n-Note that VoxPopuli or any other automated speech recognition (ASR) dataset may not be the most suitable \n-option for training TTS models. The features that make it beneficial for ASR, such as excessive background noise, are \n-typically undesirable in TTS. However, finding top-quality, multilingual, and multi-speaker TTS datasets can be quite \n+Note that VoxPopuli or any other automated speech recognition (ASR) dataset may not be the most suitable\n+option for training TTS models. The features that make it beneficial for ASR, such as excessive background noise, are\n+typically undesirable in TTS. However, finding top-quality, multilingual, and multi-speaker TTS datasets can be quite\n challenging.\n \n Let's load the data:\n@@ -109,7 +109,7 @@ Let's load the data:\n 20968\n ```\n \n-20968 examples should be sufficient for fine-tuning. SpeechT5 expects audio data to have a sampling rate of 16 kHz, so \n+20968 examples should be sufficient for fine-tuning. SpeechT5 expects audio data to have a sampling rate of 16 kHz, so\n make sure the examples in the dataset meet this requirement:\n \n ```py\n@@ -118,7 +118,7 @@ dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n \n ## Preprocess the data\n \n-Let's begin by defining the model checkpoint to use and loading the appropriate processor: \n+Let's begin by defining the model checkpoint to use and loading the appropriate processor:\n \n ```py\n >>> from transformers import SpeechT5Processor\n@@ -127,26 +127,26 @@ Let's begin by defining the model checkpoint to use and loading the appropriate\n >>> processor = SpeechT5Processor.from_pretrained(checkpoint)\n ```\n \n-### Text cleanup for SpeechT5 tokenization \n+### Text cleanup for SpeechT5 tokenization\n \n Start by cleaning up the text data. You'll need the tokenizer part of the processor to process the text:\n \n ```py\n >>> tokenizer = processor.tokenizer\n ```\n \n-The dataset examples contain `raw_text` and `normalized_text` features. When deciding which feature to use as the text input, \n-consider that the SpeechT5 tokenizer doesn't have any tokens for numbers. In `normalized_text` the numbers are written \n+The dataset examples contain `raw_text` and `normalized_text` features. When deciding which feature to use as the text input,\n+consider that the SpeechT5 tokenizer doesn't have any tokens for numbers. In `normalized_text` the numbers are written\n out as text. Thus, it is a better fit, and we recommend using    `normalized_text` as input text.\n \n-Because SpeechT5 was trained on the English language, it may not recognize certain characters in the Dutch dataset. If \n-left as is, these characters will be converted to `<unk>` tokens. However, in Dutch, certain characters like `√†` are \n+Because SpeechT5 was trained on the English language, it may not recognize certain characters in the Dutch dataset. If\n+left as is, these characters will be converted to `<unk>` tokens. However, in Dutch, certain characters like `√†` are\n used to stress syllables. In order to preserve the meaning of the text, we can replace this character with a regular `a`.\n \n-To identify unsupported tokens, extract all unique characters in the dataset using the `SpeechT5Tokenizer` which \n-works with characters as tokens. To do this, write the `extract_all_chars` mapping function that concatenates \n-the transcriptions from all examples into one string and converts it to a set of characters. \n-Make sure to set `batched=True` and `batch_size=-1` in `dataset.map()` so that all transcriptions are available at once for \n+To identify unsupported tokens, extract all unique characters in the dataset using the `SpeechT5Tokenizer` which\n+works with characters as tokens. To do this, write the `extract_all_chars` mapping function that concatenates\n+the transcriptions from all examples into one string and converts it to a set of characters.\n+Make sure to set `batched=True` and `batch_size=-1` in `dataset.map()` so that all transcriptions are available at once for\n the mapping function.\n \n ```py\n@@ -168,16 +168,16 @@ the mapping function.\n >>> tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}\n ```\n \n-Now you have two sets of characters: one with the vocabulary from the dataset and one with the vocabulary from the tokenizer. \n-To identify any unsupported characters in the dataset, you can take the difference between these two sets. The resulting \n+Now you have two sets of characters: one with the vocabulary from the dataset and one with the vocabulary from the tokenizer.\n+To identify any unsupported characters in the dataset, you can take the difference between these two sets. The resulting\n set will contain the characters that are in the dataset but not in the tokenizer.\n \n ```py\n >>> dataset_vocab - tokenizer_vocab\n {' ', '√†', '√ß', '√®', '√´', '√≠', '√Ø', '√∂', '√º'}\n ```\n \n-To handle the unsupported characters identified in the previous step, define a function that maps these characters to \n+To handle the unsupported characters identified in the previous step, define a function that maps these characters to\n valid tokens. Note that spaces are already replaced by `‚ñÅ` in the tokenizer and don't need to be handled separately.\n \n ```py\n@@ -206,9 +206,9 @@ Now that you have dealt with special characters in the text, it's time to shift\n \n ### Speakers\n \n-The VoxPopuli dataset includes speech from multiple speakers, but how many speakers are represented in the dataset? To \n-determine this, we can count the number of unique speakers and the number of examples each speaker contributes to the dataset. \n-With a total of 20,968 examples in the dataset, this information will give us a better understanding of the distribution of \n+The VoxPopuli dataset includes speech from multiple speakers, but how many speakers are represented in the dataset? To\n+determine this, we can count the number of unique speakers and the number of examples each speaker contributes to the dataset.\n+With a total of 20,968 examples in the dataset, this information will give us a better understanding of the distribution of\n speakers and examples in the data.\n \n ```py\n@@ -236,9 +236,9 @@ By plotting a histogram you can get a sense of how much data there is for each s\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_speakers_histogram.png\" alt=\"Speakers histogram\"/>\n </div>\n \n-The histogram reveals that approximately one-third of the speakers in the dataset have fewer than 100 examples, while \n-around ten speakers have more than 500 examples. To improve training efficiency and balance the dataset, we can limit \n-the data to speakers with between 100 and 400 examples. \n+The histogram reveals that approximately one-third of the speakers in the dataset have fewer than 100 examples, while\n+around ten speakers have more than 500 examples. To improve training efficiency and balance the dataset, we can limit\n+the data to speakers with between 100 and 400 examples.\n \n ```py\n >>> def select_speaker(speaker_id):\n@@ -248,14 +248,14 @@ the data to speakers with between 100 and 400 examples.\n >>> dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])\n ```\n \n-Let's check how many speakers remain: \n+Let's check how many speakers remain:\n \n ```py\n >>> len(set(dataset[\"speaker_id\"]))\n 42\n ```\n \n-Let's see how many examples are left: \n+Let's see how many examples are left:\n \n ```py\n >>> len(dataset)\n@@ -264,18 +264,18 @@ Let's see how many examples are left:\n \n You are left with just under 10,000 examples from approximately 40 unique speakers, which should be sufficient.\n \n-Note that some speakers with few examples may actually have more audio available if the examples are long. However, \n-determining the total amount of audio for each speaker requires scanning through the entire dataset, which is a \n+Note that some speakers with few examples may actually have more audio available if the examples are long. However,\n+determining the total amount of audio for each speaker requires scanning through the entire dataset, which is a\n time-consuming process that involves loading and decoding each audio file. As such, we have chosen to skip this step here.\n \n ### Speaker embeddings\n \n-To enable the TTS model to differentiate between multiple speakers, you'll need to create a speaker embedding for each example. \n+To enable the TTS model to differentiate between multiple speakers, you'll need to create a speaker embedding for each example.\n The speaker embedding is an additional input into the model that captures a particular speaker's voice characteristics.\n-To generate these speaker embeddings, use the pre-trained [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb) \n-model from SpeechBrain. \n+To generate these speaker embeddings, use the pre-trained [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)\n+model from SpeechBrain.\n \n-Create a function `create_speaker_embedding()` that takes an input audio waveform and outputs a 512-element vector \n+Create a function `create_speaker_embedding()` that takes an input audio waveform and outputs a 512-element vector\n containing the corresponding speaker embedding.\n \n ```py\n@@ -301,17 +301,17 @@ containing the corresponding speaker embedding.\n ...     return speaker_embeddings\n ```\n \n-It's important to note that the `speechbrain/spkrec-xvect-voxceleb` model was trained on English speech from the VoxCeleb \n-dataset, whereas the training examples in this guide are in Dutch. While we believe that this model will still generate \n+It's important to note that the `speechbrain/spkrec-xvect-voxceleb` model was trained on English speech from the VoxCeleb\n+dataset, whereas the training examples in this guide are in Dutch. While we believe that this model will still generate\n reasonable speaker embeddings for our Dutch dataset, this assumption may not hold true in all cases.\n \n-For optimal results, we recommend training an X-vector model on the target speech first. This will ensure that the model \n+For optimal results, we recommend training an X-vector model on the target speech first. This will ensure that the model\n is better able to capture the unique voice characteristics present in the Dutch language.\n \n ### Processing the dataset\n \n-Finally, let's process the data into the format the model expects. Create a `prepare_dataset` function that takes in a \n-single example and uses the `SpeechT5Processor` object to tokenize the input text and load the target audio into a log-mel spectrogram. \n+Finally, let's process the data into the format the model expects. Create a `prepare_dataset` function that takes in a\n+single example and uses the `SpeechT5Processor` object to tokenize the input text and load the target audio into a log-mel spectrogram.\n It should also add the speaker embeddings as an additional input.\n \n ```py\n@@ -363,8 +363,8 @@ The labels should be a log-mel spectrogram with 80 mel bins.\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/tts_logmelspectrogram_1.png\" alt=\"Log-mel spectrogram with 80 mel bins\"/>\n </div>\n \n-Side note: If you find this spectrogram confusing, it may be due to your familiarity with the convention of placing low frequencies \n-at the bottom and high frequencies at the top of a plot. However, when plotting spectrograms as an image using the matplotlib library, \n+Side note: If you find this spectrogram confusing, it may be due to your familiarity with the convention of placing low frequencies\n+at the bottom and high frequencies at the top of a plot. However, when plotting spectrograms as an image using the matplotlib library,\n the y-axis is flipped and the spectrograms appear upside down.\n \n Now apply the processing function to the entire dataset. This will take between 5 and 10 minutes.\n@@ -373,7 +373,7 @@ Now apply the processing function to the entire dataset. This will take between\n >>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)\n ```\n \n-You'll see a warning saying that some examples in the dataset are longer than the maximum input length the model can handle (600 tokens). \n+You'll see a warning saying that some examples in the dataset are longer than the maximum input length the model can handle (600 tokens).\n Remove those examples from the dataset. Here we go even further and to allow for larger batch sizes we remove anything over 200 tokens.\n \n ```py\n@@ -387,16 +387,16 @@ Remove those examples from the dataset. Here we go even further and to allow for\n 8259\n ```\n \n-Next, create a basic train/test split: \n+Next, create a basic train/test split:\n \n ```py\n >>> dataset = dataset.train_test_split(test_size=0.1)\n ```\n \n ### Data collator\n \n-In order to combine multiple examples into a batch, you need to define a custom data collator. This collator will pad shorter sequences with padding \n-tokens, ensuring that all examples have the same length. For the spectrogram labels, the padded portions are replaced with the special value `-100`. This special value \n+In order to combine multiple examples into a batch, you need to define a custom data collator. This collator will pad shorter sequences with padding\n+tokens, ensuring that all examples have the same length. For the spectrogram labels, the padded portions are replaced with the special value `-100`. This special value\n instructs the model to ignore that part of the spectrogram when calculating the spectrogram loss.\n \n ```py\n@@ -437,18 +437,18 @@ instructs the model to ignore that part of the spectrogram when calculating the\n ...         return batch\n ```\n \n-In SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other words, it throws away every \n-other timestep from the target sequence. The decoder then predicts a sequence that is twice as long. Since the original \n-target sequence length may be odd, the data collator makes sure to round the maximum length of the batch down to be a \n+In SpeechT5, the input to the decoder part of the model is reduced by a factor 2. In other words, it throws away every\n+other timestep from the target sequence. The decoder then predicts a sequence that is twice as long. Since the original\n+target sequence length may be odd, the data collator makes sure to round the maximum length of the batch down to be a\n multiple of 2.\n \n-```py \n+```py\n >>> data_collator = TTSDataCollatorWithPadding(processor=processor)\n ```\n \n ## Train the model\n \n-Load the pre-trained model from the same checkpoint as you used for loading the processor: \n+Load the pre-trained model from the same checkpoint as you used for loading the processor:\n \n ```py\n >>> from transformers import SpeechT5ForTextToSpeech\n@@ -458,11 +458,11 @@ Load the pre-trained model from the same checkpoint as you used for loading the\n \n The `use_cache=True` option is incompatible with gradient checkpointing. Disable it for training.\n \n-```py \n+```py\n >>> model.config.use_cache = False\n ```\n \n-Define the training arguments. Here we are not computing any evaluation metrics during the training process. Instead, we'll \n+Define the training arguments. Here we are not computing any evaluation metrics during the training process. Instead, we'll\n only look at the loss:\n \n ```python\n@@ -501,19 +501,19 @@ Instantiate the `Trainer` object  and pass the model, dataset, and data collator\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n ...     data_collator=data_collator,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n ```\n \n-And with that, you're ready to start training! Training will take several hours. Depending on your GPU, \n-it is possible that you will encounter a CUDA \"out-of-memory\" error when you start training. In this case, you can reduce \n+And with that, you're ready to start training! Training will take several hours. Depending on your GPU,\n+it is possible that you will encounter a CUDA \"out-of-memory\" error when you start training. In this case, you can reduce\n the `per_device_train_batch_size` incrementally by factors of 2 and increase `gradient_accumulation_steps` by 2x to compensate.\n \n ```py\n >>> trainer.train()\n ```\n \n-To be able to use your checkpoint with a pipeline, make sure to save the processor with the checkpoint: \n+To be able to use your checkpoint with a pipeline, make sure to save the processor with the checkpoint:\n \n ```py\n >>> processor.save_pretrained(\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")\n@@ -530,8 +530,8 @@ Push the final model to the ü§ó Hub:\n ### Inference with a pipeline\n \n Great, now that you've fine-tuned a model, you can use it for inference!\n-First, let's see how you can use it with a corresponding pipeline. Let's create a `\"text-to-speech\"` pipeline with your \n-checkpoint: \n+First, let's see how you can use it with a corresponding pipeline. Let's create a `\"text-to-speech\"` pipeline with your\n+checkpoint:\n \n ```py\n >>> from transformers import pipeline\n@@ -545,14 +545,14 @@ Pick a piece of text in Dutch you'd like narrated, e.g.:\n >>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n ```\n \n-To use SpeechT5 with the pipeline, you'll need a speaker embedding. Let's get it from an example in the test dataset: \n+To use SpeechT5 with the pipeline, you'll need a speaker embedding. Let's get it from an example in the test dataset:\n \n ```py\n >>> example = dataset[\"test\"][304]\n >>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n ```\n \n-Now you can pass the text and speaker embeddings to the pipeline, and it will take care of the rest: \n+Now you can pass the text and speaker embeddings to the pipeline, and it will take care of the rest:\n \n ```py\n >>> forward_params = {\"speaker_embeddings\": speaker_embeddings}\n@@ -567,40 +567,40 @@ You can then listen to the result:\n \n ```py\n >>> from IPython.display import Audio\n->>> Audio(output['audio'], rate=output['sampling_rate']) \n+>>> Audio(output['audio'], rate=output['sampling_rate'])\n ```\n \n ### Run inference manually\n \n-You can achieve the same inference results without using the pipeline, however, more steps will be required. \n+You can achieve the same inference results without using the pipeline, however, more steps will be required.\n \n-Load the model from the ü§ó Hub: \n+Load the model from the ü§ó Hub:\n \n ```py\n >>> model = SpeechT5ForTextToSpeech.from_pretrained(\"YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl\")\n ```\n \n-Pick an example from the test dataset to obtain a speaker embedding. \n+Pick an example from the test dataset obtain a speaker embedding.\n \n-```py \n+```py\n >>> example = dataset[\"test\"][304]\n >>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n ```\n \n Define the input text and tokenize it.\n \n-```py \n+```py\n >>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n >>> inputs = processor(text=text, return_tensors=\"pt\")\n ```\n \n-Create a spectrogram with your model: \n+Create a spectrogram with your model:\n \n ```py\n >>> spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)\n ```\n \n-Visualize the spectrogram, if you'd like to: \n+Visualize the spectrogram, if you'd like to:\n \n ```py\n >>> plt.figure()\n@@ -623,15 +623,15 @@ Finally, use the vocoder to turn the spectrogram into sound.\n >>> Audio(speech.numpy(), rate=16000)\n ```\n \n-In our experience, obtaining satisfactory results from this model can be challenging. The quality of the speaker \n-embeddings appears to be a significant factor. Since SpeechT5 was pre-trained with English x-vectors, it performs best \n+In our experience, obtaining satisfactory results from this model can be challenging. The quality of the speaker\n+embeddings appears to be a significant factor. Since SpeechT5 was pre-trained with English x-vectors, it performs best\n when using English speaker embeddings. If the synthesized speech sounds poor, try using a different speaker embedding.\n \n-Increasing the training duration is also likely to enhance the quality of the results. Even so, the speech clearly is Dutch instead of English, and it does \n+Increasing the training duration is also likely to enhance the quality of the results. Even so, the speech clearly is Dutch instead of English, and it does\n capture the voice characteristics of the speaker (compare to the original audio in the example).\n-Another thing to experiment with is the model's configuration. For example, try using `config.reduction_factor = 1` to \n+Another thing to experiment with is the model's configuration. For example, try using `config.reduction_factor = 1` to\n see if this improves the results.\n \n-Finally, it is essential to consider ethical considerations. Although TTS technology has numerous useful applications, it \n-may also be used for malicious purposes, such as impersonating someone's voice without their knowledge or consent. Please \n+Finally, it is essential to consider ethical considerations. Although TTS technology has numerous useful applications, it\n+may also be used for malicious purposes, such as impersonating someone's voice without their knowledge or consent. Please\n use TTS judiciously and responsibly."
        },
        {
            "sha": "b93dd0cbe26d975ffe14f9f684b829163f13db65",
            "filename": "docs/source/en/tasks/token_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftoken_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -296,7 +296,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=tokenized_wnut[\"train\"],\n ...     eval_dataset=tokenized_wnut[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "426ba1c340fb81b0baf317668b671a8c9d3dab58",
            "filename": "docs/source/en/tasks/translation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -221,7 +221,7 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     train_dataset=tokenized_books[\"train\"],\n ...     eval_dataset=tokenized_books[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "c268de1786bdc5453b38f899d89d39f5ddd6e453",
            "filename": "docs/source/en/tasks/video_classification.md",
            "status": "modified",
            "additions": 30,
            "deletions": 30,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -61,7 +61,7 @@ Start by loading a subset of the [UCF-101 dataset](https://www.crcv.ucf.edu/data\n \n After the subset has been downloaded, you need to extract the compressed archive:\n \n-```py \n+```py\n >>> import tarfile\n \n >>> with tarfile.open(file_path) as t:\n@@ -106,21 +106,21 @@ UCF101_subset/\n \n You can then count the number of total videos.\n \n-```py \n+```py\n >>> import pathlib\n >>> dataset_root_path = \"UCF101_subset\"\n >>> dataset_root_path = pathlib.Path(dataset_root_path)\n ```\n \n-```py \n+```py\n >>> video_count_train = len(list(dataset_root_path.glob(\"train/*/*.avi\")))\n >>> video_count_val = len(list(dataset_root_path.glob(\"val/*/*.avi\")))\n >>> video_count_test = len(list(dataset_root_path.glob(\"test/*/*.avi\")))\n >>> video_total = video_count_train + video_count_val + video_count_test\n >>> print(f\"Total videos: {video_total}\")\n ```\n \n-```py \n+```py\n >>> all_video_file_paths = (\n ...     list(dataset_root_path.glob(\"train/*/*.avi\"))\n ...     + list(dataset_root_path.glob(\"val/*/*.avi\"))\n@@ -148,9 +148,9 @@ For the validation and evaluation splits, you wouldn't want to have video clips\n Next up, you will derive the set of labels present in the dataset. Also, create two dictionaries that'll be helpful when initializing the model:\n \n * `label2id`: maps the class names to integers.\n-* `id2label`: maps the integers to class names. \n+* `id2label`: maps the integers to class names.\n \n-```py \n+```py\n >>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n >>> label2id = {label: i for i, label in enumerate(class_labels)}\n >>> id2label = {i: label for label, i in label2id.items()}\n@@ -166,7 +166,7 @@ There are 10 unique classes. For each class, there are 30 videos in the training\n \n Instantiate a video classification model from a pretrained checkpoint and its associated image processor. The model's encoder comes with pre-trained parameters, and the classification head is randomly initialized. The image processor will come in handy when writing the preprocessing pipeline for our dataset.\n \n-```py \n+```py\n >>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n \n >>> model_ckpt = \"MCG-NJU/videomae-base\"\n@@ -191,13 +191,13 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n The warning is telling us we are throwing away some weights (e.g. the weights and bias of the `classifier` layer) and randomly initializing some others (the weights and bias of a new `classifier` layer). This is expected in this case, because we are adding a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do.\n \n-**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained by fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`.  \n+**Note** that [this checkpoint](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics) leads to better performance on this task as the checkpoint was obtained fine-tuning on a similar downstream task having considerable domain overlap. You can check out [this checkpoint](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset) which was obtained by fine-tuning `MCG-NJU/videomae-base-finetuned-kinetics`.\n \n ## Prepare the datasets for training\n \n-For preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/). Start by importing the dependencies we need. \n+For preprocessing the videos, you will leverage the [PyTorchVideo library](https://pytorchvideo.org/). Start by importing the dependencies we need.\n \n-```py \n+```py\n >>> import pytorchvideo.data\n \n >>> from pytorchvideo.transforms import (\n@@ -218,7 +218,7 @@ For preprocessing the videos, you will leverage the [PyTorchVideo library](https\n ... )\n ```\n \n-For the training dataset transformations, use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, keep the same transformation chain except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorchVideo](https://pytorchvideo.org).  \n+For the training dataset transformations, use a combination of uniform temporal subsampling, pixel normalization, random cropping, and random horizontal flipping. For the validation and evaluation dataset transformations, keep the same transformation chain except for random cropping and horizontal flipping. To learn more about the details of these transformations check out the [official documentation of PyTorchVideo](https://pytorchvideo.org).\n \n Use the `image_processor` associated with the pre-trained model to obtain the following information:\n \n@@ -243,9 +243,9 @@ Start by defining some constants.\n >>> clip_duration = num_frames_to_sample * sample_rate / fps\n ```\n \n-Now, define the dataset-specific transformations and the datasets respectively. Starting with the training set: \n+Now, define the dataset-specific transformations and the datasets respectively. Starting with the training set:\n \n-```py \n+```py\n >>> train_transform = Compose(\n ...     [\n ...         ApplyTransformToKey(\n@@ -272,9 +272,9 @@ Now, define the dataset-specific transformations and the datasets respectively.\n ... )\n ```\n \n-The same sequence of workflow can be applied to the validation and evaluation sets: \n+The same sequence of workflow can be applied to the validation and evaluation sets:\n \n-```py \n+```py\n >>> val_transform = Compose(\n ...     [\n ...         ApplyTransformToKey(\n@@ -306,7 +306,7 @@ The same sequence of workflow can be applied to the validation and evaluation se\n ... )\n ```\n \n-**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorchVideo dataset. So, if you want to use a custom dataset not supported off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine. \n+**Note**: The above dataset pipelines are taken from the [official PyTorchVideo example](https://pytorchvideo.org/docs/tutorial_classification#dataset). We're using the [`pytorchvideo.data.Ucf101()`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101) function because it's tailored for the UCF-101 dataset. Under the hood, it returns a [`pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset`](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset) object. `LabeledVideoDataset` class is the base class for all things video in the PyTorchVideo dataset. So, if you want to use a custom dataset not supported off-the-shelf by PyTorchVideo, you can extend the `LabeledVideoDataset` class accordingly. Refer to the `data` API [documentation to](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html) learn more. Also, if your dataset follows a similar structure (as shown above), then using the `pytorchvideo.data.Ucf101()` should work just fine.\n \n You can access the `num_videos` argument to know the number of videos in the dataset.\n \n@@ -315,9 +315,9 @@ You can access the `num_videos` argument to know the number of videos in the dat\n # (300, 30, 75)\n ```\n \n-## Visualize the preprocessed video for better debugging \n+## Visualize the preprocessed video for better debugging\n \n-```py \n+```py\n >>> import imageio\n >>> import numpy as np\n >>> from IPython.display import Image\n@@ -330,7 +330,7 @@ You can access the `num_videos` argument to know the number of videos in the dat\n \n >>> def create_gif(video_tensor, filename=\"sample.gif\"):\n ...     \"\"\"Prepares a GIF from a video tensor.\n-...     \n+...\n ...     The video tensor is expected to have the following shape:\n ...     (num_frames, num_channels, height, width).\n ...     \"\"\"\n@@ -357,14 +357,14 @@ You can access the `num_videos` argument to know the number of videos in the dat\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n </div>\n \n-## Train the model \n+## Train the model\n \n Leverage [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) from  ü§ó Transformers for training the model. To instantiate a `Trainer`, you need to define the training configuration and an evaluation metric. The most important is the [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments), which is a class that contains all the attributes to configure the training. It requires an output folder name, which will be used to save the checkpoints of the model. It also helps sync all the information in the model repository on ü§ó Hub.\n \n Most of the training arguments are self-explanatory, but one that is quite important here is `remove_unused_columns=False`. This one will drop any features not used by the model's call function. By default it's `True` because usually it's ideal to drop unused feature columns, making it easier to unpack inputs into the model's call function. But, in this case, you need the unused features ('video' in particular) in order to create `pixel_values` (which is a mandatory key our model expects in its inputs).\n \n \n-```py \n+```py\n >>> from transformers import TrainingArguments, Trainer\n \n >>> model_name = model_ckpt.split(\"/\")[-1]\n@@ -388,7 +388,7 @@ Most of the training arguments are self-explanatory, but one that is quite impor\n ... )\n ```\n \n-The dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As such, we must define `max_steps` when instantiating `TrainingArguments`. \n+The dataset returned by `pytorchvideo.data.Ucf101()` doesn't implement the `__len__` method. As such, we must define `max_steps` when instantiating `TrainingArguments`.\n \n Next, you need to define a function to compute the metrics from the predictions, which will use the `metric` you'll load now. The only preprocessing you have to do is to take the argmax of our predicted logits:\n \n@@ -409,7 +409,7 @@ In the [VideoMAE paper](https://arxiv.org/abs/2203.12602), the authors use the f\n \n Also, define a `collate_fn`, which will be used to batch examples together. Each batch consists of 2 keys, namely `pixel_values` and `labels`.\n \n-```py \n+```py\n >>> def collate_fn(examples):\n ...     # permute to (num_frames, num_channels, height, width)\n ...     pixel_values = torch.stack(\n@@ -421,13 +421,13 @@ Also, define a `collate_fn`, which will be used to batch examples together. Each\n \n Then you just pass all of this along with the datasets to `Trainer`:\n \n-```py \n+```py\n >>> trainer = Trainer(\n ...     model,\n ...     args,\n ...     train_dataset=train_dataset,\n ...     eval_dataset=val_dataset,\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     compute_metrics=compute_metrics,\n ...     data_collator=collate_fn,\n ... )\n@@ -437,7 +437,7 @@ You might wonder why you passed along the `image_processor` as a tokenizer when\n \n Now fine-tune our model by calling the `train` method:\n \n-```py \n+```py\n >>> train_results = trainer.train()\n ```\n \n@@ -453,7 +453,7 @@ Great, now that you have fine-tuned a model, you can use it for inference!\n \n Load a video for inference:\n \n-```py \n+```py\n >>> sample_test_video = next(iter(test_dataset))\n ```\n \n@@ -507,10 +507,10 @@ Now, pass your input to the model and return the `logits`:\n >>> logits = run_inference(trained_model, sample_test_video[\"video\"])\n ```\n \n-Decoding the `logits`, we get: \n+Decoding the `logits`, we get:\n \n-```py \n+```py\n >>> predicted_class_idx = logits.argmax(-1).item()\n >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n # Predicted class: BasketballDunk\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "7083d8c98b932ebb73660489bd27b52ed3710500",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 54,
            "deletions": 55,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -18,14 +18,14 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. \n-The input to models supporting this task is typically a combination of an image and a question, and the output is an \n+Visual Question Answering (VQA) is the task of answering open-ended questions based on an image.\n+The input to models supporting this task is typically a combination of an image and a question, and the output is an\n answer expressed in natural language.\n \n Some noteworthy use case examples for VQA include:\n * Accessibility applications for visually impaired individuals.\n * Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.\n-* Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products. \n+* Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products.\n * Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask \"Is there a dog?\" to find all images with dogs from a set of images.\n \n In this guide you'll learn how to:\n@@ -36,15 +36,15 @@ In this guide you'll learn how to:\n \n ## Fine-tuning ViLT\n \n-ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for \n-Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier \n-head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. \n+ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for\n+Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier\n+head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized.\n Visual Question Answering is thus treated as a **classification problem**.\n \n-More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we \n-illustrate how to use them for zero-shot VQA inference. \n+More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we\n+illustrate how to use them for zero-shot VQA inference.\n \n-Before you begin, make sure you have all the necessary libraries installed. \n+Before you begin, make sure you have all the necessary libraries installed.\n \n ```bash\n pip install -q transformers datasets\n@@ -67,15 +67,15 @@ Let's define the model checkpoint as a global variable.\n \n ## Load the data\n \n-For illustration purposes, in this guide we use a very small sample of the annotated visual question answering `Graphcore/vqa` dataset. \n+For illustration purposes, in this guide we use a very small sample of the annotated visual question answering `Graphcore/vqa` dataset.\n You can find the full dataset on [ü§ó Hub](https://huggingface.co/datasets/Graphcore/vqa).\n \n-As an alternative to the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa), you can download the \n-same data manually from the official [VQA dataset page](https://visualqa.org/download.html). If you prefer to follow the \n+As an alternative to the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa), you can download the\n+same data manually from the official [VQA dataset page](https://visualqa.org/download.html). If you prefer to follow the\n tutorial with your custom data, check out how to [Create an image dataset](https://huggingface.co/docs/datasets/image_dataset#loading-script)\n-guide in the ü§ó Datasets documentation.  \n+guide in the ü§ó Datasets documentation.\n \n-Let's load the first 200 examples from the validation split and explore the dataset's features:  \n+Let's load the first 200 examples from the validation split and explore the dataset's features:\n \n ```python\n >>> from datasets import load_dataset\n@@ -104,20 +104,20 @@ Let's take a look at an example to understand the dataset's features:\n    0.30000001192092896]}}\n ```\n \n-The features relevant to the task include: \n+The features relevant to the task include:\n * `question`: the question to be answered from the image\n * `image_id`: the path to the image the question refers to\n * `label`: the annotations\n \n-We can remove the rest of the features as they won't be necessary: \n+We can remove the rest of the features as they won't be necessary:\n \n-```py \n+```py\n >>> dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])\n ```\n \n-As you can see, the `label` feature contains several answers to the same question (called `ids` here) collected by different human annotators. \n-This is because the answer to a question can be subjective. In this case, the question is \"where is he looking?\". Some people \n-annotated this with \"down\", others with \"at table\", another one with \"skateboard\", etc. \n+As you can see, the `label` feature contains several answers to the same question (called `ids` here) collected by different human annotators.\n+This is because the answer to a question can be subjective. In this case, the question is \"where is he looking?\". Some people\n+annotated this with \"down\", others with \"at table\", another one with \"skateboard\", etc.\n \n Take a look at the image and consider which answer would you give:\n \n@@ -132,14 +132,14 @@ Take a look at the image and consider which answer would you give:\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/vqa-example.png\" alt=\"VQA Image Example\"/>\n </div>\n \n-Due to the questions' and answers' ambiguity, datasets like this are treated as a multi-label classification problem (as \n-multiple answers are possibly valid). Moreover, rather than just creating a one-hot encoded vector, one creates a \n+Due to the questions' and answers' ambiguity, datasets like this are treated as a multi-label classification problem (as\n+multiple answers are possibly valid). Moreover, rather than just creating a one-hot encoded vector, one creates a\n soft encoding, based on the number of times a certain answer appeared in the annotations.\n \n-For instance, in the example above, because the answer \"down\" is selected way more often than other answers, it has a \n-score (called `weight` in the dataset) of 1.0, and the rest of the answers have scores < 1.0. \n+For instance, in the example above, because the answer \"down\" is selected way more often than other answers, it has a\n+score (called `weight` in the dataset) of 1.0, and the rest of the answers have scores < 1.0.\n \n-To later instantiate the model with an appropriate classification head, let's create two dictionaries: one that maps \n+To later instantiate the model with an appropriate classification head, let's create two dictionaries: one that maps\n the label name to an integer and vice versa:\n \n ```py\n@@ -150,10 +150,10 @@ the label name to an integer and vice versa:\n >>> unique_labels = list(set(flattened_labels))\n \n >>> label2id = {label: idx for idx, label in enumerate(unique_labels)}\n->>> id2label = {idx: label for label, idx in label2id.items()} \n+>>> id2label = {idx: label for label, idx in label2id.items()}\n ```\n \n-Now that we have the mappings, we can replace the string answers with their ids, and flatten the dataset for a more convenient further preprocessing. \n+Now that we have the mappings, we can replace the string answers with their ids, and flatten the dataset for a more convenient further preprocessing.\n \n ```python\n >>> def replace_ids(inputs):\n@@ -172,21 +172,21 @@ Now that we have the mappings, we can replace the string answers with their ids,\n \n ## Preprocessing data\n \n-The next step is to load a ViLT processor to prepare the image and text data for the model. \n+The next step is to load a ViLT processor to prepare the image and text data for the model.\n [`ViltProcessor`] wraps a BERT tokenizer and ViLT image processor into a convenient single processor:\n \n-```py \n+```py\n >>> from transformers import ViltProcessor\n \n >>> processor = ViltProcessor.from_pretrained(model_checkpoint)\n ```\n \n-To preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. The processor will use \n-the [`BertTokenizerFast`] to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids` for the text data. \n+To preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. The processor will use\n+the [`BertTokenizerFast`] to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids` for the text data.\n As for images, the processor will leverage [`ViltImageProcessor`] to resize and normalize the image, and create `pixel_values` and `pixel_mask`.\n \n-All these preprocessing steps are done under the hood, we only need to call the `processor`. However, we still need to \n-prepare the target labels. In this representation, each element corresponds to a possible answer (label). For correct answers, the element holds \n+All these preprocessing steps are done under the hood, we only need to call the `processor`. However, we still need to\n+prepare the target labels. In this representation, each element corresponds to a possible answer (label). For correct answers, the element holds\n their respective score (weight), while the remaining elements are set to zero.\n \n The following function applies the `processor` to the images and questions and formats the labels as described above:\n@@ -197,29 +197,29 @@ The following function applies the `processor` to the images and questions and f\n >>> def preprocess_data(examples):\n ...     image_paths = examples['image_id']\n ...     images = [Image.open(image_path) for image_path in image_paths]\n-...     texts = examples['question']    \n+...     texts = examples['question']\n \n ...     encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n \n ...     for k, v in encoding.items():\n ...           encoding[k] = v.squeeze()\n-    \n+\n ...     targets = []\n \n ...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n ...         target = torch.zeros(len(id2label))\n \n ...         for label, score in zip(labels, scores):\n ...             target[label] = score\n-      \n+\n ...         targets.append(target)\n \n ...     encoding[\"labels\"] = targets\n-    \n+\n ...     return encoding\n ```\n \n-To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.map`] function. You can speed up `map` by \n+To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.map`] function. You can speed up `map` by\n setting `batched=True` to process multiple elements of the dataset at once. At this point, feel free to remove the columns you don't need.\n \n ```py\n@@ -241,7 +241,7 @@ As a final step, create a batch of examples using [`DefaultDataCollator`]:\n \n ## Train the model\n \n-You‚Äôre ready to start training your model now! Load ViLT with [`ViltForQuestionAnswering`]. Specify the number of labels \n+You‚Äôre ready to start training your model now! Load ViLT with [`ViltForQuestionAnswering`]. Specify the number of labels\n along with the label mappings:\n \n ```py\n@@ -282,14 +282,14 @@ At this point, only three steps remain:\n ...     args=training_args,\n ...     data_collator=data_collator,\n ...     train_dataset=processed_dataset,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n ```\n \n 3. Call [`~Trainer.train`] to finetune your model.\n \n ```py\n->>> trainer.train() \n+>>> trainer.train()\n ```\n \n Once training is completed, share your model to the Hub with the [`~Trainer.push_to_hub`] method to share your final model on the ü§ó Hub:\n@@ -309,7 +309,7 @@ way to try out your fine-tuned model for inference is to use it in a [`Pipeline`\n >>> pipe = pipeline(\"visual-question-answering\", model=\"MariaK/vilt_finetuned_200\")\n ```\n \n-The model in this guide has only been trained on 200 examples, so don't expect a lot from it. Let's see if it at least \n+The model in this guide has only been trained on 200 examples, so don't expect a lot from it. Let's see if it at least\n learned something from the data and take the first example from the dataset to illustrate inference:\n \n ```py\n@@ -352,13 +352,13 @@ Predicted answer: down\n \n ## Zero-shot VQA\n \n-The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach \n-VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training \n-paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). \n-This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. \n+The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach\n+VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training\n+paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)).\n+This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering.\n \n-Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a \n-GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: \n+Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a\n+GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically:\n \n ```py\n >>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n@@ -370,9 +370,9 @@ GPU, if available, which we didn't need to do earlier when training, as [`Traine\n >>> model.to(device)\n ```\n \n-The model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset: \n+The model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset:\n \n-```py \n+```py\n >>> example = dataset[0]\n >>> image = Image.open(example['image_id'])\n >>> question = example['question']\n@@ -381,7 +381,7 @@ The model takes image and text as input, so let's use the exact same image/quest\n To use BLIP-2 for visual question answering task, the textual prompt has to follow a specific format: `Question: {} Answer:`.\n \n ```py\n->>> prompt = f\"Question: {question} Answer:\" \n+>>> prompt = f\"Question: {question} Answer:\"\n ```\n \n Now we need to preprocess the image/prompt with the model's processor, pass the processed input through the model, and decode the output:\n@@ -392,10 +392,9 @@ Now we need to preprocess the image/prompt with the model's processor, pass the\n >>> generated_ids = model.generate(**inputs, max_new_tokens=10)\n >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n >>> print(generated_text)\n-\"He is looking at the crowd\" \n+\"He is looking at the crowd\"\n ```\n \n-As you can see, the model recognized the crowd, and the direction of the face (looking down), however, it seems to miss \n-the fact the crowd is behind the skater. Still, in cases where acquiring human-annotated datasets is not feasible, this \n+As you can see, the model recognized the crowd, and the direction of the face (looking down), however, it seems to miss\n+the fact the crowd is behind the skater. Still, in cases where acquiring human-annotated datasets is not feasible, this\n approach can quickly produce useful results.\n- "
        },
        {
            "sha": "f9ea3337699444d6274386290967e62ea4362e9a",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -81,7 +81,7 @@ trainer = Trainer(\n     args=training_args,\n     train_dataset=dataset[\"train\"],\n     eval_dataset=dataset[\"test\"],\n-    tokenizer=tokenizer,\n+    processing_class=tokenizer,\n     data_collator=data_collator,\n     compute_metrics=compute_metrics,\n )\n@@ -153,7 +153,7 @@ from transformers import TrainerCallback\n class EarlyStoppingCallback(TrainerCallback):\n     def __init__(self, num_steps=10):\n         self.num_steps = num_steps\n-    \n+\n     def on_step_end(self, args, state, control, **kwargs):\n         if state.global_step >= self.num_steps:\n             return {\"should_training_stop\": True}\n@@ -171,7 +171,7 @@ trainer = Trainer(\n     args=training_args,\n     train_dataset=dataset[\"train\"],\n     eval_dataset=dataset[\"test\"],\n-    tokenizer=tokenizer,\n+    processing_class=tokenizer,\n     data_collator=data_collator,\n     compute_metrics=compute_metrics,\n     callback=[EarlyStoppingCallback()],\n@@ -289,7 +289,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n model = AutoModelForCausalLM.from_config(config).to(0)\n \n trainer = trl.SFTTrainer(\n-    model=model, \n+    model=model,\n     args=args,\n     train_dataset=train_dataset,\n     dataset_text_field='text',\n@@ -327,7 +327,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n model = AutoModelForCausalLM.from_config(config).to(0)\n \n trainer = trl.SFTTrainer(\n-    model=model, \n+    model=model,\n     args=args,\n     train_dataset=train_dataset,\n     dataset_text_field='text',\n@@ -370,7 +370,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n model = AutoModelForCausalLM.from_config(config).to(0)\n \n trainer = trl.SFTTrainer(\n-    model=model, \n+    model=model,\n     args=args,\n     train_dataset=train_dataset,\n     dataset_text_field='text',\n@@ -419,8 +419,8 @@ The kernel supports the Llama, Gemma, Mistral, and Mixtral model architectures.\n \n ## LOMO optimizer\n \n-The LOMO optimizers have been introduced in [Full Parameter Fine-Tuning for Large Language Models with Limited Resources](https://hf.co/papers/2306.09782) and [AdaLomo: Low-memory Optimization with Adaptive Learning Rate](https://hf.co/papers/2310.10195). \n-They both consist of an efficient full-parameter fine-tuning method. These optimizers fuse the gradient computation and the parameter update in one step to reduce memory usage. Supported optimizers for LOMO are `\"lomo\"` and `\"adalomo\"`. First either install LOMO from pypi `pip install lomo-optim` or install it from source with `pip install git+https://github.com/OpenLMLab/LOMO.git`. \n+The LOMO optimizers have been introduced in [Full Parameter Fine-Tuning for Large Language Models with Limited Resources](https://hf.co/papers/2306.09782) and [AdaLomo: Low-memory Optimization with Adaptive Learning Rate](https://hf.co/papers/2310.10195).\n+They both consist of an efficient full-parameter fine-tuning method. These optimizers fuse the gradient computation and the parameter update in one step to reduce memory usage. Supported optimizers for LOMO are `\"lomo\"` and `\"adalomo\"`. First either install LOMO from pypi `pip install lomo-optim` or install it from source with `pip install git+https://github.com/OpenLMLab/LOMO.git`.\n \n <Tip>\n \n@@ -457,7 +457,7 @@ tokenizer = AutoTokenizer.from_pretrained(model_id)\n model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n \n trainer = trl.SFTTrainer(\n-    model=model, \n+    model=model,\n     args=args,\n     train_dataset=train_dataset,\n     dataset_text_field='text',\n@@ -579,8 +579,8 @@ To use Accelerate with [`Trainer`], run the [`accelerate.config`](https://huggin\n <hfoption id=\"DistributedDataParallel\">\n \n ```yml\n-compute_environment: LOCAL_MACHINE                                                                                             \n-distributed_type: MULTI_GPU                                                                                                    \n+compute_environment: LOCAL_MACHINE\n+distributed_type: MULTI_GPU\n downcast_bf16: 'no'\n gpu_ids: all\n machine_rank: 0 #change rank as per the node\n@@ -654,8 +654,8 @@ use_cpu: false\n <hfoption id=\"DeepSpeed with Accelerate plugin\">\n \n ```yml\n-compute_environment: LOCAL_MACHINE                                                                                             \n-deepspeed_config:                                                                                                              \n+compute_environment: LOCAL_MACHINE\n+deepspeed_config:\n   gradient_accumulation_steps: 1\n   gradient_clipping: 0.7\n   offload_optimizer_device: cpu"
        },
        {
            "sha": "41e9f82e35f7f1932e729b8224f1d89fbb5f4d72",
            "filename": "docs/source/es/tasks/asr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fasr.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -276,7 +276,7 @@ En este punto, solo quedan tres pasos:\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=processor.feature_extractor,\n+...     processing_class=processor.feature_extractor,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "1bea46884202fe4ac5e49f50a2ffa4d56bdffc5c",
            "filename": "docs/source/es/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fimage_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -160,7 +160,7 @@ Al llegar a este punto, solo quedan tres pasos:\n ...     data_collator=data_collator,\n ...     train_dataset=food[\"train\"],\n ...     eval_dataset=food[\"test\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ... )\n \n >>> trainer.train()"
        },
        {
            "sha": "32df3401d737dea3a95026df707b55984a566bc6",
            "filename": "docs/source/es/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -225,7 +225,7 @@ En este punto, solo quedan tres pasos:\n ...     args=training_args,\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n ... )\n "
        },
        {
            "sha": "42a6e4b6e1bc4ee3af2d6e760054b398db9b5d48",
            "filename": "docs/source/es/tasks/question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fquestion_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -195,7 +195,7 @@ En este punto, solo quedan tres pasos:\n ...     args=training_args,\n ...     train_dataset=tokenized_squad[\"train\"],\n ...     eval_dataset=tokenized_squad[\"validation\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n "
        },
        {
            "sha": "c9060cba6b771db41ee71be6e29af77345f14fc9",
            "filename": "docs/source/es/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fsummarization.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -155,7 +155,7 @@ En este punto, solo faltan tres pasos:\n ...     args=training_args,\n ...     train_dataset=tokenized_billsum[\"train\"],\n ...     eval_dataset=tokenized_billsum[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n "
        },
        {
            "sha": "dab83e9a9d9ebcb5b55b33a7cc79952a4403bbe5",
            "filename": "docs/source/es/trainer.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftrainer.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -14,7 +14,7 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# El Trainer \n+# El Trainer\n \n El [`Trainer`] es un bucle completo de entrenamiento y evaluaci√≥n para modelos de PyTorch implementado en la biblioteca Transformers. Solo necesitas pasarle las piezas necesarias para el entrenamiento (modelo, tokenizador, conjunto de datos, funci√≥n de evaluaci√≥n, hiperpar√°metros de entrenamiento, etc.), y la clase [`Trainer`] se encarga del resto. Esto facilita comenzar a entrenar m√°s r√°pido sin tener que escribir manualmente tu propio bucle de entrenamiento. Pero al mismo tiempo, [`Trainer`] es muy personalizable y ofrece una gran cantidad de opciones de entrenamiento para que puedas adaptarlo a tus necesidades exactas de entrenamiento.\n \n@@ -79,7 +79,7 @@ trainer = Trainer(\n     args=training_args,\n     train_dataset=dataset[\"train\"],\n     eval_dataset=dataset[\"test\"],\n-    tokenizer=tokenizer,\n+    processing_class=tokenizer,\n     data_collator=data_collator,\n     compute_metrics=compute_metrics,\n )\n@@ -151,7 +151,7 @@ from transformers import TrainerCallback\n class EarlyStoppingCallback(TrainerCallback):\n     def __init__(self, num_steps=10):\n         self.num_steps = num_steps\n-    \n+\n     def on_step_end(self, args, state, control, **kwargs):\n         if state.global_step >= self.num_steps:\n             return {\"should_training_stop\": True}\n@@ -169,7 +169,7 @@ trainer = Trainer(\n     args=training_args,\n     train_dataset=dataset[\"train\"],\n     eval_dataset=dataset[\"test\"],\n-    tokenizer=tokenizer,\n+    processing_class=tokenizer,\n     data_collator=data_collator,\n     compute_metrics=compute_metrics,\n     callback=[EarlyStoppingCallback()],\n@@ -265,8 +265,8 @@ Para usar Accelerate con [`Trainer`], ejecuta el comando [`accelerate.config`](h\n <hfoption id=\"DistributedDataParallel\">\n \n ```yml\n-compute_environment: LOCAL_MACHINE                                                                                             \n-distributed_type: MULTI_GPU                                                                                                    \n+compute_environment: LOCAL_MACHINE\n+distributed_type: MULTI_GPU\n downcast_bf16: 'no'\n gpu_ids: all\n machine_rank: 0 #change rank as per the node\n@@ -337,8 +337,8 @@ use_cpu: false\n <hfoption id=\"DeepSpeed with Accelerate plugin\">\n \n ```yml\n-compute_environment: LOCAL_MACHINE                                                                                             \n-deepspeed_config:                                                                                                              \n+compute_environment: LOCAL_MACHINE\n+deepspeed_config:\n   gradient_accumulation_steps: 1\n   gradient_clipping: 0.7\n   offload_optimizer_device: cpu\n@@ -406,4 +406,4 @@ accelerate launch --num_processes=2 \\\n     --overwrite_output_dir\n ```\n \n-Consulta el tutorial [Lanzamiento de tus scripts con Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch) para obtener m√°s informaci√≥n sobre `accelerate_launch` y las configuraciones personalizadas.\n\\ No newline at end of file\n+Consulta el tutorial [Lanzamiento de tus scripts con Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch) para obtener m√°s informaci√≥n sobre `accelerate_launch` y las configuraciones personalizadas."
        },
        {
            "sha": "3cc2a8c5faac762ff9879859902d1b79a553d474",
            "filename": "docs/source/fr/quicktour.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Ffr%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Ffr%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Fquicktour.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -169,7 +169,7 @@ Si vous ne parvenez pas √† trouver un mod√®le adapt√© √† votre cas d'utilisation\n \n <Youtube id=\"AhChOFRegn4\"/>\n \n-Les classes [`AutoModelForSequenceClassification`] et [`AutoTokenizer`] fonctionnent ensemble pour cr√©er un [`pipeline`] comme celui que vous avez utilis√© ci-dessus. Une [AutoClass](./model_doc/auto) est un raccourci qui r√©cup√®re automatiquement l'architecture d'un mod√®le pr√©-entra√Æn√© √† partir de son nom ou de son emplacement. Il vous suffit de s√©lectionner l'`AutoClass` appropri√©e √† votre t√¢che et la classe de pr√©traitement qui lui est associ√©e. \n+Les classes [`AutoModelForSequenceClassification`] et [`AutoTokenizer`] fonctionnent ensemble pour cr√©er un [`pipeline`] comme celui que vous avez utilis√© ci-dessus. Une [AutoClass](./model_doc/auto) est un raccourci qui r√©cup√®re automatiquement l'architecture d'un mod√®le pr√©-entra√Æn√© √† partir de son nom ou de son emplacement. Il vous suffit de s√©lectionner l'`AutoClass` appropri√©e √† votre t√¢che et la classe de pr√©traitement qui lui est associ√©e.\n \n Reprenons l'exemple de la section pr√©c√©dente et voyons comment vous pouvez utiliser l'`AutoClass` pour reproduire les r√©sultats du [`pipeline`].\n \n@@ -479,7 +479,7 @@ Maintenant, rassemblez tous ces √©l√©ments dans un [`Trainer`] :\n ...     args=training_args,\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )  # doctest: +SKIP\n ```\n@@ -496,7 +496,7 @@ Pour les t√¢ches - comme la traduction ou la g√©n√©ration de r√©sum√© - qui util\n \n </Tip>\n \n-Vous pouvez personnaliser le comportement de la boucle d'apprentissage en red√©finissant les m√©thodes √† l'int√©rieur de [`Trainer`]. Cela vous permet de personnaliser des caract√©ristiques telles que la fonction de perte, l'optimiseur et le planificateur. Consultez la documentation de [`Trainer`] pour savoir quelles m√©thodes peuvent √™tre red√©finies. \n+Vous pouvez personnaliser le comportement de la boucle d'apprentissage en red√©finissant les m√©thodes √† l'int√©rieur de [`Trainer`]. Cela vous permet de personnaliser des caract√©ristiques telles que la fonction de perte, l'optimiseur et le planificateur. Consultez la documentation de [`Trainer`] pour savoir quelles m√©thodes peuvent √™tre red√©finies.\n \n L'autre moyen de personnaliser la boucle d'apprentissage est d'utiliser les [Callbacks](./main_classes/callback). Vous pouvez utiliser les callbacks pour int√©grer d'autres biblioth√®ques et inspecter la boucle d'apprentissage afin de suivre la progression ou d'arr√™ter l'apprentissage plus t√¥t. Les callbacks ne modifient rien dans la boucle d'apprentissage elle-m√™me. Pour personnaliser quelque chose comme la fonction de perte, vous devez red√©finir le [`Trainer`] √† la place.\n "
        },
        {
            "sha": "90591daf8b204ff72cd62fb997eba74e7c744e4c",
            "filename": "docs/source/ja/hpo_train.md",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fhpo_train.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n „Åì„Çå„Çâ„Çí‰ΩøÁî®„Åô„ÇãÂâç„Å´„ÄÅ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉºÊ§úÁ¥¢„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n ```bash\n-pip install optuna/sigopt/wandb/ray[tune] \n+pip install optuna/sigopt/wandb/ray[tune]\n ```\n \n ## How to enable Hyperparameter search in example\n@@ -119,7 +119,7 @@ Wandb„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ[object_parameter](https://docs.wandb.ai/guides/sweeps/co\n ...     train_dataset=small_train_dataset,\n ...     eval_dataset=small_eval_dataset,\n ...     compute_metrics=compute_metrics,\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     model_init=model_init,\n ...     data_collator=data_collator,\n ... )\n@@ -142,9 +142,3 @@ Wandb„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ[object_parameter](https://docs.wandb.ai/guides/sweeps/co\n \n ## Hyperparameter search For DDP finetune\n ÁèæÂú®„ÄÅDDPÔºàDistributed Data ParallelÔºâ„ÅÆ„Åü„ÇÅ„ÅÆ„Éè„Ç§„Éë„Éº„Éë„É©„É°„Éº„Çø„ÉºÊ§úÁ¥¢„ÅØ„ÄÅOptuna „Å® SigOpt „Å´ÂØæ„Åó„Å¶ÊúâÂäπ„Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ„É©„É≥„ÇØ„Çº„É≠„Éó„É≠„Çª„Çπ„ÅÆ„Åø„ÅåÊ§úÁ¥¢„Éà„É©„Ç§„Ç¢„É´„ÇíÁîüÊàê„Åó„ÄÅ‰ªñ„ÅÆ„É©„É≥„ÇØ„Å´ÂºïÊï∞„ÇíÊ∏°„Åó„Åæ„Åô„ÄÇ\n-\n-\n-\n-\n-\n-"
        },
        {
            "sha": "e03dea33cbd189a0cd2e5ca3d391fe9176689355",
            "filename": "docs/source/ja/quicktour.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fquicktour.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -516,7 +516,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ...     args=training_args,\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )  # doctest: +SKIP\n ```"
        },
        {
            "sha": "ebefeba831a03e823ae57fd4f0c3f3aa127d5c64",
            "filename": "docs/source/ja/tasks/asr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fasr.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -148,7 +148,7 @@ MInDS-14 „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞ „É¨„Éº„Éà„ÅØ 8000kHz „Åß„Åô (\n ...     return batch\n ```\n \n-„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰Ωì„Å´ÂâçÂá¶ÁêÜÈñ¢Êï∞„ÇíÈÅ©Áî®„Åô„Çã„Å´„ÅØ„ÄÅü§ó Datasets [`~datasets.Dataset.map`] Èñ¢Êï∞„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ `num_proc` „Éë„É©„É°„Éº„Çø„Çí‰ΩøÁî®„Åó„Å¶„Éó„É≠„Çª„Çπ„ÅÆÊï∞„ÇíÂ¢ó„ÇÑ„Åô„Åì„Å®„Åß„ÄÅ`map` „ÇíÈ´òÈÄüÂåñ„Åß„Åç„Åæ„Åô„ÄÇ [`~datasets.Dataset.remove_columns`] „É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ‰∏çË¶Å„Å™Âàó„ÇíÂâäÈô§„Åó„Åæ„Åô„ÄÇ \n+„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂÖ®‰Ωì„Å´ÂâçÂá¶ÁêÜÈñ¢Êï∞„ÇíÈÅ©Áî®„Åô„Çã„Å´„ÅØ„ÄÅü§ó Datasets [`~datasets.Dataset.map`] Èñ¢Êï∞„Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ `num_proc` „Éë„É©„É°„Éº„Çø„Çí‰ΩøÁî®„Åó„Å¶„Éó„É≠„Çª„Çπ„ÅÆÊï∞„ÇíÂ¢ó„ÇÑ„Åô„Åì„Å®„Åß„ÄÅ`map` „ÇíÈ´òÈÄüÂåñ„Åß„Åç„Åæ„Åô„ÄÇ [`~datasets.Dataset.remove_columns`] „É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®„Åó„Å¶„ÄÅ‰∏çË¶Å„Å™Âàó„ÇíÂâäÈô§„Åó„Åæ„Åô„ÄÇ\n \n ```py\n >>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[\"train\"], num_proc=4)\n@@ -281,7 +281,7 @@ MInDS-14 „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞ „É¨„Éº„Éà„ÅØ 8000kHz „Åß„Åô (\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "aa38d12d4ef0cfd97ab14c2cb6a49e3f6eae86b7",
            "filename": "docs/source/ja/tasks/audio_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Faudio_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -233,7 +233,7 @@ MInDS-14 „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞ „É¨„Éº„Éà„ÅØ 8000khz „Åß„Åô (\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=feature_extractor,\n+...     processing_class=feature_extractor,\n ...     compute_metrics=compute_metrics,\n ... )\n \n@@ -320,4 +320,4 @@ MInDS-14 „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆ„Çµ„É≥„Éó„É™„É≥„Ç∞ „É¨„Éº„Éà„ÅØ 8000khz „Åß„Åô (\n 'cash_deposit'\n ```\n </pt>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "f07cc6dff28eae20f29a950e6389591a390b9d79",
            "filename": "docs/source/ja/tasks/document_question_answering.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fdocument_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fdocument_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fdocument_question_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -364,7 +364,7 @@ end_index 18\n Ëá™ÂàÜ„ÅßÂÆüË£Ö„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅ[Ë≥™ÂïèÂøúÁ≠î„ÅÆÁ´†](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) „ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n „Ç§„É≥„Çπ„Éî„É¨„Éº„Ç∑„Éß„É≥„ÇíÂæó„Çã„Åü„ÇÅ„Å´„Éè„Ç∞„Éï„Çß„Ç§„Çπ„Ç≥„Éº„Çπ„ÅÆ„ÄÇ\n \n-## Train \n+## Train\n \n „Åä„ÇÅ„Åß„Å®„ÅÜÔºÅ„Åì„ÅÆ„Ç¨„Ç§„Éâ„ÅÆÊúÄ„ÇÇÈõ£„Åó„ÅÑÈÉ®ÂàÜ„ÇíÁÑ°‰∫ã„Å´„Éä„Éì„Ç≤„Éº„Éà„Åß„Åç„Åü„ÅÆ„Åß„ÄÅÁã¨Ëá™„ÅÆ„É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„Åô„ÇãÊ∫ñÂÇô„ÅåÊï¥„ÅÑ„Åæ„Åó„Åü„ÄÇ\n „Éà„É¨„Éº„Éã„É≥„Ç∞„Å´„ÅØÊ¨°„ÅÆÊâãÈ†Ü„ÅåÂê´„Åæ„Çå„Åæ„Åô„ÄÇ\n@@ -423,7 +423,7 @@ end_index 18\n ...     data_collator=data_collator,\n ...     train_dataset=encoded_train_dataset,\n ...     eval_dataset=encoded_test_dataset,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n \n >>> trainer.train()"
        },
        {
            "sha": "013dfc286dce63bd71f5cc934bd5427d4d371efa",
            "filename": "docs/source/ja/tasks/image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -323,7 +323,7 @@ food[\"test\"].set_transform(preprocess_val)\n ...     data_collator=data_collator,\n ...     train_dataset=food[\"train\"],\n ...     eval_dataset=food[\"test\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     compute_metrics=compute_metrics,\n ... )\n \n@@ -551,4 +551,3 @@ Epoch 5/5\n \n </tf>\n </frameworkcontent>\n-"
        },
        {
            "sha": "1079121c6062bceb7b53e1e71625f6744c49f6eb",
            "filename": "docs/source/ja/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -165,7 +165,7 @@ trainer = ImageDistilTrainer(\n     train_dataset=processed_datasets[\"train\"],\n     eval_dataset=processed_datasets[\"validation\"],\n     data_collator=data_collator,\n-    tokenizer=teacher_extractor,\n+    processing_class=teacher_extractor,\n     compute_metrics=compute_metrics,\n     temperature=5,\n     lambda_param=0.5"
        },
        {
            "sha": "075a7a2cb764556b49f7833e676dbd4f7357965e",
            "filename": "docs/source/ja/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -271,7 +271,7 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n ...     args=training_args,\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "31e8effa54b224deb50a1fbc06e90f2ce972e5b4",
            "filename": "docs/source/ja/tasks/object_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fobject_detection.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -371,7 +371,7 @@ DETR „É¢„Éá„É´„Çí„Éà„É¨„Éº„Éã„É≥„Ç∞„Åß„Åç„Çã„Äå„É©„Éô„É´„Äç„ÄÇÁîªÂÉè„Éó„É≠„Çª„ÉÉ\n ...     args=training_args,\n ...     data_collator=collate_fn,\n ...     train_dataset=cppe5[\"train\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ... )\n \n >>> trainer.train()"
        },
        {
            "sha": "9217c211e6f973ddfe2d9ca6c3e3ad36bf6cf743",
            "filename": "docs/source/ja/tasks/question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fquestion_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -227,7 +227,7 @@ pip install transformers datasets evaluate\n ...     args=training_args,\n ...     train_dataset=tokenized_squad[\"train\"],\n ...     eval_dataset=tokenized_squad[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n "
        },
        {
            "sha": "6784696e6c95a3bdc3abe661236ad540b3304bfc",
            "filename": "docs/source/ja/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fsummarization.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -216,7 +216,7 @@ pip install transformers datasets evaluate rouge_score\n ...     args=training_args,\n ...     train_dataset=tokenized_billsum[\"train\"],\n ...     eval_dataset=tokenized_billsum[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "669d15730e24f892e27103a9ad48733b5e7fb34d",
            "filename": "docs/source/ja/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Ftext-to-speech.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -125,7 +125,7 @@ dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n >>> processor = SpeechT5Processor.from_pretrained(checkpoint)\n ```\n \n-### Text cleanup for SpeechT5 tokenization \n+### Text cleanup for SpeechT5 tokenization\n \n \n „Åæ„Åö„ÅØ„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„Åô„Çã„Åì„Å®„Åã„ÇâÂßã„ÇÅ„Åæ„Åô„ÄÇ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂá¶ÁêÜ„Åô„Çã„Å´„ÅØ„ÄÅ„Éó„É≠„Çª„ÉÉ„Çµ„ÅÆ„Éà„Éº„ÇØ„Éä„Ç§„Ç∂„ÉºÈÉ®ÂàÜ„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ\n@@ -442,7 +442,7 @@ SpeechT5 „Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆ„Éá„Ç≥„Éº„ÉÄÈÉ®ÂàÜ„Å∏„ÅÆÂÖ•Âäõ„Åå 2 ÂàÜ„ÅÆ 1 „Å´\n „Çø„Éº„Ç≤„ÉÉ„Éà „Ç∑„Éº„Ç±„É≥„Çπ„ÅÆÈï∑„Åï„ÅåÂ•áÊï∞„Åß„ÅÇ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Éá„Éº„ÇøÁÖßÂêàÊ©üËÉΩ„ÅØ„Éê„ÉÉ„ÉÅ„ÅÆÊúÄÂ§ßÈï∑„ÇíÂàá„ÇäÊç®„Å¶„Å¶„ÄÅ\n 2„ÅÆÂÄçÊï∞„ÄÇ\n \n-```py \n+```py\n >>> data_collator = TTSDataCollatorWithPadding(processor=processor)\n ```\n \n@@ -458,7 +458,7 @@ SpeechT5 „Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆ„Éá„Ç≥„Éº„ÉÄÈÉ®ÂàÜ„Å∏„ÅÆÂÖ•Âäõ„Åå 2 ÂàÜ„ÅÆ 1 „Å´\n \n `use_cache=True`„Ç™„Éó„Ç∑„Éß„É≥„ÅØ„ÄÅÂãæÈÖç„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Å®‰∫íÊèõÊÄß„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Éà„É¨„Éº„Éã„É≥„Ç∞„ÅÆ„Åü„ÇÅ„Å´ÁÑ°Âäπ„Å´„Åó„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> model.config.use_cache = False\n ```\n \n@@ -501,7 +501,7 @@ SpeechT5 „Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆ„Éá„Ç≥„Éº„ÉÄÈÉ®ÂàÜ„Å∏„ÅÆÂÖ•Âäõ„Åå 2 ÂàÜ„ÅÆ 1 „Å´\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n ...     data_collator=data_collator,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n ```\n „Åì„Çå„Åß„ÄÅ„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÈñãÂßã„Åô„ÇãÊ∫ñÂÇô„ÅåÊï¥„ÅÑ„Åæ„Åó„Åü„ÄÇ„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´„ÅØÊï∞ÊôÇÈñì„Åã„Åã„Çä„Åæ„Åô„ÄÇ GPU „Å´Âøú„Åò„Å¶„ÄÅ\n@@ -567,7 +567,7 @@ SpeechT5 „Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆ„Éá„Ç≥„Éº„ÉÄÈÉ®ÂàÜ„Å∏„ÅÆÂÖ•Âäõ„Åå 2 ÂàÜ„ÅÆ 1 „Å´\n \n ```py\n >>> from IPython.display import Audio\n->>> Audio(output['audio'], rate=output['sampling_rate']) \n+>>> Audio(output['audio'], rate=output['sampling_rate'])\n ```\n \n ### Run inference manually\n@@ -583,14 +583,14 @@ SpeechT5 „Åß„ÅØ„ÄÅ„É¢„Éá„É´„ÅÆ„Éá„Ç≥„Éº„ÉÄÈÉ®ÂàÜ„Å∏„ÅÆÂÖ•Âäõ„Åå 2 ÂàÜ„ÅÆ 1 „Å´\n \n „ÉÜ„Çπ„Éà „Éá„Éº„Çø„Çª„ÉÉ„Éà„Åã„Çâ‰æã„ÇíÈÅ∏Êäû„Åó„Å¶„ÄÅ„Çπ„Éî„Éº„Ç´„Éº„ÅÆÂüã„ÇÅËæº„Åø„ÇíÂèñÂæó„Åó„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> example = dataset[\"test\"][304]\n >>> speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)\n ```\n \n ÂÖ•Âäõ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂÆöÁæ©„Åó„ÄÅ„Éà„Éº„ÇØ„É≥Âåñ„Åó„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n >>> inputs = processor(text=text, return_tensors=\"pt\")\n ```"
        },
        {
            "sha": "4389aeacb5644b7f519b2c7bc7c4ef2aa3f7b5e9",
            "filename": "docs/source/ja/tasks/token_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Ftoken_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -295,7 +295,7 @@ pip install transformers datasets evaluate seqeval\n ...     args=training_args,\n ...     train_dataset=tokenized_wnut[\"train\"],\n ...     eval_dataset=tokenized_wnut[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "7fa45eac9cdb682e662bbb976a6be3b94b6c3cd0",
            "filename": "docs/source/ja/tasks/translation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Ftranslation.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -220,7 +220,7 @@ pip install transformers datasets evaluate sacrebleu\n ...     args=training_args,\n ...     train_dataset=tokenized_books[\"train\"],\n ...     eval_dataset=tokenized_books[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "741356a6f5789e5f447d1c360e835fa917d6bf3f",
            "filename": "docs/source/ja/tasks/video_classification.md",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fvideo_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -61,7 +61,7 @@ pip install -q pytorchvideo transformers evaluate\n \n „Çµ„Éñ„Çª„ÉÉ„Éà„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Åó„ÅüÂæå„ÄÅÂúßÁ∏Æ„Ç¢„Éº„Ç´„Ç§„Éñ„ÇíÊäΩÂá∫„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> import tarfile\n \n >>> with tarfile.open(file_path) as t:\n@@ -127,7 +127,7 @@ UCF101_subset/\n * `id2label`: Êï¥Êï∞„Çí„ÇØ„É©„ÇπÂêç„Å´„Éû„ÉÉ„Éî„É≥„Ç∞„Åó„Åæ„Åô„ÄÇ\n \n \n-```py \n+```py\n >>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n >>> label2id = {label: i for i, label in enumerate(class_labels)}\n >>> id2label = {i: label for label, i in label2id.items()}\n@@ -143,7 +143,7 @@ UCF101_subset/\n \n ‰∫ãÂâç„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„Åü„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Å®„Åù„Çå„Å´Èñ¢ÈÄ£„Åô„ÇãÁîªÂÉè„Éó„É≠„Çª„ÉÉ„Çµ„Åã„Çâ„Éì„Éá„Ç™ÂàÜÈ°û„É¢„Éá„É´„Çí„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ„Åó„Åæ„Åô„ÄÇ„É¢„Éá„É´„ÅÆ„Ç®„É≥„Ç≥„Éº„ÉÄ„Éº„Å´„ÅØ‰∫ãÂâç„Éà„É¨„Éº„Éã„É≥„Ç∞„Åï„Çå„Åü„Éë„É©„É°„Éº„Çø„Éº„Åå‰ªòÂ±û„Åó„Å¶„Åä„Çä„ÄÅÂàÜÈ°û„Éò„ÉÉ„Éâ„ÅØ„É©„É≥„ÉÄ„É†„Å´ÂàùÊúüÂåñ„Åï„Çå„Åæ„Åô„ÄÇÁîªÂÉè„Éó„É≠„Çª„ÉÉ„Çµ„ÅØ„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÂâçÂá¶ÁêÜ„Éë„Ç§„Éó„É©„Ç§„É≥„Çí‰ΩúÊàê„Åô„Çã„Å®„Åç„Å´ÂΩπÁ´ã„Å°„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n \n >>> model_ckpt = \"MCG-NJU/videomae-base\"\n@@ -175,7 +175,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n „Éì„Éá„Ç™„ÅÆÂâçÂá¶ÁêÜ„Å´„ÅØ„ÄÅ[PyTorchVideo „É©„Ç§„Éñ„É©„É™](https://pytorchvideo.org/) „ÇíÂà©Áî®„Åó„Åæ„Åô„ÄÇ„Åæ„Åö„ÄÅÂøÖË¶Å„Å™‰æùÂ≠òÈñ¢‰øÇ„Çí„Ç§„É≥„Éù„Éº„Éà„Åó„Åæ„Åô„ÄÇ\n \n \n-```py \n+```py\n >>> import pytorchvideo.data\n \n >>> from pytorchvideo.transforms import (\n@@ -224,7 +224,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n Ê¨°„Å´„ÄÅ„Éá„Éº„Çø„Çª„ÉÉ„ÉàÂõ∫Êúâ„ÅÆÂ§âÊèõ„Å®„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Åù„Çå„Åû„ÇåÂÆöÁæ©„Åó„Åæ„Åô„ÄÇ„Éà„É¨„Éº„Éã„É≥„Ç∞„Çª„ÉÉ„Éà„Åã„ÇâÂßã„ÇÅ„Åæ„Åô:\n \n \n-```py \n+```py\n >>> train_transform = Compose(\n ...     [\n ...         ApplyTransformToKey(\n@@ -254,7 +254,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n Âêå„Åò‰∏ÄÈÄ£„ÅÆ„ÉØ„Éº„ÇØ„Éï„É≠„Éº„ÇíÊ§úË®º„Çª„ÉÉ„Éà„Å®Ë©ï‰æ°„Çª„ÉÉ„Éà„Å´ÈÅ©Áî®„Åß„Åç„Åæ„Åô„ÄÇ\n \n \n-```py \n+```py\n >>> val_transform = Compose(\n ...     [\n ...         ApplyTransformToKey(\n@@ -297,9 +297,9 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n # (300, 30, 75)\n ```\n \n-## Visualize the preprocessed video for better debugging \n+## Visualize the preprocessed video for better debugging\n \n-```py \n+```py\n >>> import imageio\n >>> import numpy as np\n >>> from IPython.display import Image\n@@ -312,7 +312,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n >>> def create_gif(video_tensor, filename=\"sample.gif\"):\n ...     \"\"\"Prepares a GIF from a video tensor.\n-...     \n+...\n ...     The video tensor is expected to have the following shape:\n ...     (num_frames, num_channels, height, width).\n ...     \"\"\"\n@@ -339,13 +339,13 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n </div>\n \n-## Train the model \n+## Train the model\n \n ü§ó Transformers „ÅÆ [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) „Çí„É¢„Éá„É´„ÅÆ„Éà„É¨„Éº„Éã„É≥„Ç∞„Å´Âà©Áî®„Åó„Åæ„Åô„ÄÇ `Trainer`„Çí„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ„Åô„Çã„Å´„ÅØ„ÄÅ„Éà„É¨„Éº„Éã„É≥„Ç∞ÊßãÊàê„Å®Ë©ï‰æ°„É°„Éà„É™„ÇØ„Çπ„ÇíÂÆöÁæ©„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÊúÄ„ÇÇÈáçË¶Å„Å™„ÅÆ„ÅØ [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments) „Åß„ÄÅ„Åì„Çå„ÅØ„Éà„É¨„Éº„Éã„É≥„Ç∞„ÇíÊßãÊàê„Åô„Çã„Åü„ÇÅ„ÅÆ„Åô„Åπ„Å¶„ÅÆÂ±ûÊÄß„ÇíÂê´„ÇÄ„ÇØ„É©„Çπ„Åß„Åô„ÄÇ„É¢„Éá„É´„ÅÆ„ÉÅ„Çß„ÉÉ„ÇØ„Éù„Ç§„É≥„Éà„Çí‰øùÂ≠ò„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åï„Çå„ÇãÂá∫Âäõ„Éï„Ç©„É´„ÉÄ„ÉºÂêç„ÅåÂøÖË¶Å„Åß„Åô„ÄÇ„Åæ„Åü„ÄÅü§ó Hub ‰∏ä„ÅÆ„É¢„Éá„É´ „É™„Éù„Ç∏„Éà„É™ÂÜÖ„ÅÆ„Åô„Åπ„Å¶„ÅÆÊÉÖÂ†±„ÇíÂêåÊúü„Åô„Çã„ÅÆ„Å´„ÇÇÂΩπÁ´ã„Å°„Åæ„Åô„ÄÇ\n \n „Éà„É¨„Éº„Éã„É≥„Ç∞ÂºïÊï∞„ÅÆ„Åª„Å®„Çì„Å©„ÅØ‰∏ÄÁõÆÁû≠ÁÑ∂„Åß„Åô„Åå„ÄÅ„Åì„Åì„ÅßÈùûÂ∏∏„Å´ÈáçË¶Å„Å™„ÅÆ„ÅØ`remove_unused_columns=False`„Åß„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ„É¢„Éá„É´„ÅÆÂëº„Å≥Âá∫„ÅóÈñ¢Êï∞„Åß‰ΩøÁî®„Åï„Çå„Å™„ÅÑÊ©üËÉΩ„ÅåÂâäÈô§„Åï„Çå„Åæ„Åô„ÄÇ„Éá„Éï„Ç©„É´„Éà„Åß„ÅØ`True`„Åß„Åô„ÄÇ„Åì„Çå„ÅØ„ÄÅÈÄöÂ∏∏„ÄÅÊú™‰ΩøÁî®„ÅÆÁâπÂæ¥Âàó„ÇíÂâäÈô§„Åó„ÄÅ„É¢„Éá„É´„ÅÆÂëº„Å≥Âá∫„ÅóÈñ¢Êï∞„Å∏„ÅÆÂÖ•Âäõ„ÇíËß£Âáç„Åó„ÇÑ„Åô„Åè„Åô„Çã„Åì„Å®„ÅåÁêÜÊÉ≥ÁöÑ„Åß„ÅÇ„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ„Åü„Å†„Åó„ÄÅ„Åì„ÅÆÂ†¥Âêà„ÄÅ`pixel_values` („É¢„Éá„É´„ÅåÂÖ•Âäõ„ÅßÊúüÂæÖ„Åô„ÇãÂøÖÈ†à„Ç≠„Éº„Åß„Åô) „Çí‰ΩúÊàê„Åô„Çã„Å´„ÅØ„ÄÅÊú™‰ΩøÁî®„ÅÆÊ©üËÉΩ (Áâπ„Å´`video`) „ÅåÂøÖË¶Å„Åß„Åô„ÄÇ\n \n-```py \n+```py\n >>> from transformers import TrainingArguments, Trainer\n \n >>> model_name = model_ckpt.split(\"/\")[-1]\n@@ -391,7 +391,7 @@ def compute_metrics(eval_pred):\n „Åæ„Åü„ÄÅ„Çµ„É≥„Éó„É´„Çí„Åæ„Å®„ÇÅ„Å¶„Éê„ÉÉ„ÉÅÂá¶ÁêÜ„Åô„Çã„Åü„ÇÅ„Å´‰ΩøÁî®„Åï„Çå„Çã `collat‚Äã‚Äãe_fn` „ÇíÂÆöÁæ©„Åó„Åæ„Åô„ÄÇÂêÑ„Éê„ÉÉ„ÉÅ„ÅØ„ÄÅ`pixel_values` „Å® `labels` „Å®„ÅÑ„ÅÜ 2 „Å§„ÅÆ„Ç≠„Éº„ÅßÊßãÊàê„Åï„Çå„Åæ„Åô„ÄÇ\n \n \n-```py \n+```py\n >>> def collate_fn(examples):\n ...     # permute to (num_frames, num_channels, height, width)\n ...     pixel_values = torch.stack(\n@@ -403,13 +403,13 @@ def compute_metrics(eval_pred):\n \n Ê¨°„Å´„ÄÅ„Åì„Çå„Çâ„Åô„Åπ„Å¶„Çí„Éá„Éº„Çø„Çª„ÉÉ„Éà„Å®„Å®„ÇÇ„Å´`Trainer`„Å´Ê∏°„Åô„Å†„Åë„Åß„Åô„ÄÇ\n \n-```py \n+```py\n >>> trainer = Trainer(\n ...     model,\n ...     args,\n ...     train_dataset=train_dataset,\n ...     eval_dataset=val_dataset,\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     compute_metrics=compute_metrics,\n ...     data_collator=collate_fn,\n ... )\n@@ -419,7 +419,7 @@ def compute_metrics(eval_pred):\n \n Ê¨°„Å´„ÄÅ`train` „É°„ÇΩ„ÉÉ„Éâ„ÇíÂëº„Å≥Âá∫„Åó„Å¶„É¢„Éá„É´„ÇíÂæÆË™øÊï¥„Åó„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> train_results = trainer.train()\n ```\n \n@@ -435,7 +435,7 @@ def compute_metrics(eval_pred):\n \n Êé®Ë´ñ„ÅÆ„Åü„ÇÅ„Å´„Éì„Éá„Ç™„Çí„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> sample_test_video = next(iter(test_dataset))\n ```\n \n@@ -491,7 +491,7 @@ def compute_metrics(eval_pred):\n \n `logits` „Çí„Éá„Ç≥„Éº„Éâ„Åô„Çã„Å®„ÄÅÊ¨°„ÅÆ„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> predicted_class_idx = logits.argmax(-1).item()\n >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n # Predicted class: BasketballDunk"
        },
        {
            "sha": "3231cba5e3af718e47adab6c3f48847a5d3761f7",
            "filename": "docs/source/ja/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fja%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fvisual_question_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -110,7 +110,7 @@ Dataset({\n ÊÆã„Çä„ÅÆÊ©üËÉΩ„ÅØÂøÖË¶Å„Å™„ÅÑ„ÅÆ„ÅßÂâäÈô§„Åß„Åç„Åæ„Åô„ÄÇ\n \n \n-```py \n+```py\n >>> dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])\n ```\n \n@@ -150,7 +150,7 @@ Dataset({\n >>> unique_labels = list(set(flattened_labels))\n \n >>> label2id = {label: idx for idx, label in enumerate(unique_labels)}\n->>> id2label = {idx: label for label, idx in label2id.items()} \n+>>> id2label = {idx: label for label, idx in label2id.items()}\n ```\n \n „Éû„ÉÉ„Éî„É≥„Ç∞„Åå„Åß„Åç„Åü„ÅÆ„Åß„ÄÅÊñáÂ≠óÂàó„ÅÆÂõûÁ≠î„Çí„Åù„ÅÆ ID „Å´ÁΩÆ„ÅçÊèõ„Åà„ÄÅ„Åï„Çâ„Å´ÂâçÂá¶ÁêÜ„Çí„Çà„Çä‰æøÂà©„Å´„Åô„Çã„Åü„ÇÅ„Å´„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí„Éï„É©„ÉÉ„ÉàÂåñ„Åô„Çã„Åì„Å®„Åå„Åß„Åç„Åæ„Åô„ÄÇ\n@@ -175,7 +175,7 @@ Dataset({\n Ê¨°„ÅÆ„Çπ„ÉÜ„ÉÉ„Éó„Åß„ÅØ„ÄÅViLT „Éó„É≠„Çª„ÉÉ„Çµ„Çí„É≠„Éº„Éâ„Åó„Å¶„ÄÅ„É¢„Éá„É´„ÅÆÁîªÂÉè„Éá„Éº„Çø„Å®„ÉÜ„Ç≠„Çπ„Éà „Éá„Éº„Çø„ÇíÊ∫ñÂÇô„Åó„Åæ„Åô„ÄÇ\n [`ViltProcessor`] „ÅØ„ÄÅBERT „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„Å® ViLT ÁîªÂÉè„Éó„É≠„Çª„ÉÉ„Çµ„Çí‰æøÂà©„Å™Âçò‰∏Ä„Éó„É≠„Çª„ÉÉ„Çµ„Å´„É©„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ\n \n-```py \n+```py\n >>> from transformers import ViltProcessor\n \n >>> processor = ViltProcessor.from_pretrained(model_checkpoint)\n@@ -197,25 +197,25 @@ Dataset({\n >>> def preprocess_data(examples):\n ...     image_paths = examples['image_id']\n ...     images = [Image.open(image_path) for image_path in image_paths]\n-...     texts = examples['question']    \n+...     texts = examples['question']\n \n ...     encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n \n ...     for k, v in encoding.items():\n ...           encoding[k] = v.squeeze()\n-    \n+\n ...     targets = []\n \n ...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n ...         target = torch.zeros(len(id2label))\n \n ...         for label, score in zip(labels, scores):\n ...             target[label] = score\n-      \n+\n ...         targets.append(target)\n \n ...     encoding[\"labels\"] = targets\n-    \n+\n ...     return encoding\n ```\n \n@@ -284,14 +284,14 @@ Dataset({\n ...     args=training_args,\n ...     data_collator=data_collator,\n ...     train_dataset=processed_dataset,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n ```\n \n 3. [`~Trainer.train`] „ÇíÂëº„Å≥Âá∫„Åó„Å¶„É¢„Éá„É´„ÇíÂæÆË™øÊï¥„Åó„Åæ„Åô„ÄÇ\n \n ```py\n->>> trainer.train() \n+>>> trainer.train()\n ```\n \n „Éà„É¨„Éº„Éã„É≥„Ç∞„ÅåÂÆå‰∫Ü„Åó„Åü„Çâ„ÄÅ [`~Trainer.push_to_hub`] „É°„ÇΩ„ÉÉ„Éâ„Çí‰ΩøÁî®„Åó„Å¶„É¢„Éá„É´„Çí„Éè„Éñ„Å´ÂÖ±Êúâ„Åó„ÄÅü§ó „Éè„Éñ„ÅßÊúÄÁµÇ„É¢„Éá„É´„ÇíÂÖ±Êúâ„Åó„Åæ„Åô„ÄÇ\n@@ -376,7 +376,7 @@ GPU (Âà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà)„ÄÇ„Åì„Çå„ÅØ [`Trainer`] „ÅåËá™ÂãïÁöÑ„Å´Âá¶ÁêÜ„Åô„Çã\n „É¢„Éá„É´„ÅØÁîªÂÉè„Å®„ÉÜ„Ç≠„Çπ„Éà„ÇíÂÖ•Âäõ„Å®„Åó„Å¶Âèó„ÅëÂèñ„Çã„Åü„ÇÅ„ÄÅVQA „Éá„Éº„Çø„Çª„ÉÉ„Éà„ÅÆÊúÄÂàù„ÅÆ‰æã„Å®„Åæ„Å£„Åü„ÅèÂêå„ÅòÁîªÂÉè„Å®Ë≥™Âïè„ÅÆ„Éö„Ç¢„Çí‰ΩøÁî®„Åó„Å¶„Åø„Åæ„Åó„Çá„ÅÜ„ÄÇ\n \n \n-```py \n+```py\n >>> example = dataset[0]\n >>> image = Image.open(example['image_id'])\n >>> question = example['question']\n@@ -386,7 +386,7 @@ GPU (Âà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà)„ÄÇ„Åì„Çå„ÅØ [`Trainer`] „ÅåËá™ÂãïÁöÑ„Å´Âá¶ÁêÜ„Åô„Çã\n \n \n ```py\n->>> prompt = f\"Question: {question} Answer:\" \n+>>> prompt = f\"Question: {question} Answer:\"\n ```\n \n Ê¨°„Å´„ÄÅ„É¢„Éá„É´„ÅÆ„Éó„É≠„Çª„ÉÉ„Çµ„ÅßÁîªÂÉè/„Éó„É≠„É≥„Éó„Éà„ÇíÂâçÂá¶ÁêÜ„Åó„ÄÅÂá¶ÁêÜ„Åï„Çå„ÅüÂÖ•Âäõ„Çí„É¢„Éá„É´„Å´Ê∏°„Åó„ÄÅÂá∫Âäõ„Çí„Éá„Ç≥„Éº„Éâ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ\n@@ -397,7 +397,7 @@ GPU (Âà©Áî®ÂèØËÉΩ„Å™Â†¥Âêà)„ÄÇ„Åì„Çå„ÅØ [`Trainer`] „ÅåËá™ÂãïÁöÑ„Å´Âá¶ÁêÜ„Åô„Çã\n >>> generated_ids = model.generate(**inputs, max_new_tokens=10)\n >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n >>> print(generated_text)\n-\"He is looking at the crowd\" \n+\"He is looking at the crowd\"\n ```\n \n „ÅîË¶ß„ÅÆ„Å®„Åä„Çä„ÄÅ„É¢„Éá„É´„ÅØÁæ§Ë°Ü„Å®È°î„ÅÆÂêë„Åç (‰∏ã„ÇíÂêë„ÅÑ„Å¶„ÅÑ„Çã) „ÇíË™çË≠ò„Åó„Åæ„Åó„Åü„Åå„ÄÅË¶ãÈÄÉ„Åó„Å¶„ÅÑ„Çã„Çà„ÅÜ„Åß„Åô„ÄÇ"
        },
        {
            "sha": "c0982db5e093b55b750b655ea6e694c0f62826a8",
            "filename": "docs/source/ko/hpo_train.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fhpo_train.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâ Î∞±ÏóîÎìúÎ°ú ÏÇ¨Ïö©ÌïòÍ∏∞ Ï†ÑÏóê ÏïÑÎûòÏùò Î™ÖÎ†πÏñ¥Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÎùºÏù¥Î∏åÎü¨Î¶¨Îì§ÏùÑ ÏÑ§ÏπòÌïòÏÑ∏Ïöî.\n ```bash\n-pip install optuna/sigopt/wandb/ray[tune] \n+pip install optuna/sigopt/wandb/ray[tune]\n ```\n \n ## ÏòàÏ†úÏóêÏÑú ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞ ÌÉêÏÉâÏùÑ ÌôúÏÑ±ÌôîÌïòÎäî Î∞©Î≤ï [[how-to-enable-hyperparameter-search-in-example]]\n@@ -100,7 +100,7 @@ wandbÏùò Í≤ΩÏö∞, Ìï¥Îãπ [object_parameter](https://docs.wandb.ai/guides/sweeps/c\n ...     train_dataset=small_train_dataset,\n ...     eval_dataset=small_eval_dataset,\n ...     compute_metrics=compute_metrics,\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     model_init=model_init,\n ...     data_collator=data_collator,\n ... )"
        },
        {
            "sha": "06f44e6fd2970c294deb68d4bd5fe2a0ad054fd5",
            "filename": "docs/source/ko/quicktour.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquicktour.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -486,7 +486,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ...     args=training_args,\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )  # doctest: +SKIP\n ```\n@@ -554,4 +554,4 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n \n ## Îã§Ïùå Îã®Í≥ÑÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî? [[whats-next]]\n \n-ü§ó Transformers ÎëòÎü¨Î≥¥Í∏∞Î•º Î™®Îëê ÏùΩÏúºÏÖ®Îã§Î©¥, Í∞ÄÏù¥ÎìúÎ•º ÏÇ¥Ìé¥Î≥¥Í≥† Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Í≤ÉÏùÑ ÏàòÌñâÌïòÎäî Î∞©Î≤ïÏùÑ ÏïåÏïÑÎ≥¥ÏÑ∏Ïöî. Ïù¥Î•ºÌÖåÎ©¥ Ïª§Ïä§ÌÖÄ Î™®Îç∏ Íµ¨Ï∂ïÌïòÎäî Î∞©Î≤ï, Í≥ºÏóÖÏóê ÏïåÎßûÍ≤å Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏Ï°∞Ï†ïÌïòÎäî Î∞©Î≤ï, Ïä§ÌÅ¨Î¶ΩÌä∏Î°ú Î™®Îç∏ ÌõàÎ†®ÌïòÎäî Î∞©Î≤ï Îì±Ïù¥ ÏûàÏäµÎãàÎã§. ü§ó Transformers ÌïµÏã¨ Í∞úÎÖêÏóê ÎåÄÌï¥ Îçî ÏïåÏïÑÎ≥¥Î†§Î©¥ Ïª§Ìîº Ìïú Ïûî Îì§Í≥† Í∞úÎÖê Í∞ÄÏù¥ÎìúÎ•º ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî!\n\\ No newline at end of file\n+ü§ó Transformers ÎëòÎü¨Î≥¥Í∏∞Î•º Î™®Îëê ÏùΩÏúºÏÖ®Îã§Î©¥, Í∞ÄÏù¥ÎìúÎ•º ÏÇ¥Ìé¥Î≥¥Í≥† Îçî Íµ¨Ï≤¥Ï†ÅÏù∏ Í≤ÉÏùÑ ÏàòÌñâÌïòÎäî Î∞©Î≤ïÏùÑ ÏïåÏïÑÎ≥¥ÏÑ∏Ïöî. Ïù¥Î•ºÌÖåÎ©¥ Ïª§Ïä§ÌÖÄ Î™®Îç∏ Íµ¨Ï∂ïÌïòÎäî Î∞©Î≤ï, Í≥ºÏóÖÏóê ÏïåÎßûÍ≤å Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏Ï°∞Ï†ïÌïòÎäî Î∞©Î≤ï, Ïä§ÌÅ¨Î¶ΩÌä∏Î°ú Î™®Îç∏ ÌõàÎ†®ÌïòÎäî Î∞©Î≤ï Îì±Ïù¥ ÏûàÏäµÎãàÎã§. ü§ó Transformers ÌïµÏã¨ Í∞úÎÖêÏóê ÎåÄÌï¥ Îçî ÏïåÏïÑÎ≥¥Î†§Î©¥ Ïª§Ìîº Ìïú Ïûî Îì§Í≥† Í∞úÎÖê Í∞ÄÏù¥ÎìúÎ•º ÏÇ¥Ìé¥Î≥¥ÏÑ∏Ïöî!"
        },
        {
            "sha": "d1e4a5e1d919831fc18ebbc36b82a2fb8269dcbc",
            "filename": "docs/source/ko/tasks/asr.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fasr.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -20,7 +20,7 @@ rendered properly in your Markdown viewer.\n \n <Youtube id=\"TksaY_FDgnk\"/>\n \n-ÏûêÎèô ÏùåÏÑ± Ïù∏Ïãù(Automatic Speech Recognition, ASR)ÏùÄ ÏùåÏÑ± Ïã†Ìò∏Î•º ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ ÏùåÏÑ± ÏûÖÎ†• ÏãúÌÄÄÏä§Î•º ÌÖçÏä§Ìä∏ Ï∂úÎ†•Ïóê Îß§ÌïëÌï©ÎãàÎã§. \n+ÏûêÎèô ÏùåÏÑ± Ïù∏Ïãù(Automatic Speech Recognition, ASR)ÏùÄ ÏùåÏÑ± Ïã†Ìò∏Î•º ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌïòÏó¨ ÏùåÏÑ± ÏûÖÎ†• ÏãúÌÄÄÏä§Î•º ÌÖçÏä§Ìä∏ Ï∂úÎ†•Ïóê Îß§ÌïëÌï©ÎãàÎã§.\n SiriÏôÄ AlexaÏôÄ Í∞ôÏùÄ Í∞ÄÏÉÅ Ïñ¥ÏãúÏä§ÌÑ¥Ìä∏Îäî ASR Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ ÏùºÏÉÅÏ†ÅÏúºÎ°ú ÏÇ¨Ïö©ÏûêÎ•º ÎèïÍ≥† ÏûàÏúºÎ©∞, ÌöåÏùò Ï§ë ÎùºÏù¥Î∏å Ï∫°ÏÖò Î∞è Î©îÎ™® ÏûëÏÑ±Í≥º Í∞ôÏùÄ Ïú†Ïö©Ìïú ÏÇ¨Ïö©Ïûê ÏπúÌôîÏ†Å ÏùëÏö© ÌîÑÎ°úÍ∑∏Îû®ÎèÑ ÎßéÏù¥ ÏûàÏäµÎãàÎã§.\n \n Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑú ÏÜåÍ∞úÌï† ÎÇ¥Ïö©ÏùÄ ÏïÑÎûòÏôÄ Í∞ôÏäµÎãàÎã§:\n@@ -50,7 +50,7 @@ Hugging Face Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÎ©¥ Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Ïª§ÎÆ§ÎãàÌã∞Ïóê\n \n ## MInDS-14 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ Í∞ÄÏ†∏Ïò§Í∏∞[[load-minds-14-dataset]]\n \n-Î®ºÏ†Ä, ü§ó Datasets ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÏùºÎ∂ÄÎ∂ÑÏùÑ Í∞ÄÏ†∏Ïò§ÏÑ∏Ïöî. \n+Î®ºÏ†Ä, ü§ó Datasets ÎùºÏù¥Î∏åÎü¨Î¶¨ÏóêÏÑú [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÏùºÎ∂ÄÎ∂ÑÏùÑ Í∞ÄÏ†∏Ïò§ÏÑ∏Ïöî.\n Ïù¥Î†áÍ≤å ÌïòÎ©¥ Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê ÎåÄÌïú ÌõàÎ†®Ïóê ÏãúÍ∞ÑÏùÑ Îì§Ïù¥Í∏∞ Ï†ÑÏóê Î™®Îì† Í≤ÉÏù¥ ÏûëÎèôÌïòÎäîÏßÄ Ïã§ÌóòÌïòÍ≥† Í≤ÄÏ¶ùÌï† Ïàò ÏûàÏäµÎãàÎã§.\n \n ```py\n@@ -198,7 +198,7 @@ MInDS-14 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÏÉòÌîåÎßÅ Î†àÏù¥Ìä∏Îäî 8000kHzÏù¥ÎØÄÎ°ú([Îç∞Ïù¥ÌÑ∞\n \n ## ÌèâÍ∞ÄÌïòÍ∏∞[[evaluate]]\n \n-ÌõàÎ†® Ï§ëÏóê ÌèâÍ∞Ä ÏßÄÌëúÎ•º Ìè¨Ìï®ÌïòÎ©¥ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏäµÎãàÎã§. ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©ÌïòÎ©¥ ÌèâÍ∞Ä Î∞©Î≤ïÏùÑ Îπ†Î•¥Í≤å Î∂àÎü¨Ïò¨ Ïàò ÏûàÏäµÎãàÎã§. \n+ÌõàÎ†® Ï§ëÏóê ÌèâÍ∞Ä ÏßÄÌëúÎ•º Ìè¨Ìï®ÌïòÎ©¥ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏäµÎãàÎã§. ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©ÌïòÎ©¥ ÌèâÍ∞Ä Î∞©Î≤ïÏùÑ Îπ†Î•¥Í≤å Î∂àÎü¨Ïò¨ Ïàò ÏûàÏäµÎãàÎã§.\n Ïù¥ ÏûëÏóÖÏóêÏÑúÎäî [Îã®Ïñ¥ Ïò§Î•òÏú®(Word Error Rate, WER)](https://huggingface.co/spaces/evaluate-metric/wer) ÌèâÍ∞Ä ÏßÄÌëúÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§.\n (ÌèâÍ∞Ä ÏßÄÌëúÎ•º Î∂àÎü¨Ïò§Í≥† Í≥ÑÏÇ∞ÌïòÎäî Î∞©Î≤ïÏùÄ ü§ó Evaluate [ÎëòÎü¨Î≥¥Í∏∞](https://huggingface.co/docs/evaluate/a_quick_tour)Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî):\n \n@@ -285,7 +285,7 @@ MInDS-14 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÏÉòÌîåÎßÅ Î†àÏù¥Ìä∏Îäî 8000kHzÏù¥ÎØÄÎ°ú([Îç∞Ïù¥ÌÑ∞\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=processor.feature_extractor,\n+...     processing_class=processor.feature_extractor,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )\n@@ -372,4 +372,4 @@ MInDS-14 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÏÉòÌîåÎßÅ Î†àÏù¥Ìä∏Îäî 8000kHzÏù¥ÎØÄÎ°ú([Îç∞Ïù¥ÌÑ∞\n ['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']\n ```\n </pt>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "936b4eb1989827eb2ce9bad854d9bc368e593a8e",
            "filename": "docs/source/ko/tasks/audio_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Faudio_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Faudio_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Faudio_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -235,7 +235,7 @@ MinDS-14 Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÏÉòÌîåÎßÅ ÏÜçÎèÑÎäî 8000khzÏù¥ÎØÄÎ°ú(Ïù¥ Ï†ïÎ≥¥Îäî\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=feature_extractor,\n+...     processing_class=feature_extractor,\n ...     compute_metrics=compute_metrics,\n ... )\n \n@@ -321,4 +321,4 @@ For a more in-depth example of how to finetune a model for audio classification,\n 'cash_deposit'\n ```\n </pt>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "6c2d04f4ee859836401cf1e5d9c37c12c77f5827",
            "filename": "docs/source/ko/tasks/document_question_answering.md",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fdocument_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fdocument_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fdocument_question_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -18,8 +18,8 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Î¨∏ÏÑú ÏãúÍ∞ÅÏ†Å ÏßàÏùò ÏùëÎãµ(Document Visual Question Answering)Ïù¥ÎùºÍ≥†ÎèÑ ÌïòÎäî \n-Î¨∏ÏÑú ÏßàÏùò ÏùëÎãµ(Document Question Answering)ÏùÄ Î¨∏ÏÑú Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÏùÑ Ï£ºÎäî ÌÉúÏä§ÌÅ¨ÏûÖÎãàÎã§. \n+Î¨∏ÏÑú ÏãúÍ∞ÅÏ†Å ÏßàÏùò ÏùëÎãµ(Document Visual Question Answering)Ïù¥ÎùºÍ≥†ÎèÑ ÌïòÎäî\n+Î¨∏ÏÑú ÏßàÏùò ÏùëÎãµ(Document Question Answering)ÏùÄ Î¨∏ÏÑú Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌïú ÏßàÎ¨∏Ïóê ÎãµÎ≥ÄÏùÑ Ï£ºÎäî ÌÉúÏä§ÌÅ¨ÏûÖÎãàÎã§.\n Ïù¥ ÌÉúÏä§ÌÅ¨Î•º ÏßÄÏõêÌïòÎäî Î™®Îç∏Ïùò ÏûÖÎ†•ÏùÄ ÏùºÎ∞òÏ†ÅÏúºÎ°ú Ïù¥ÎØ∏ÏßÄÏôÄ ÏßàÎ¨∏Ïùò Ï°∞Ìï©Ïù¥Í≥†, Ï∂úÎ†•ÏùÄ ÏûêÏó∞Ïñ¥Î°ú Îêú ÎãµÎ≥ÄÏûÖÎãàÎã§. Ïù¥Îü¨Ìïú Î™®Îç∏ÏùÄ ÌÖçÏä§Ìä∏, Îã®Ïñ¥Ïùò ÏúÑÏπò(Î∞îÏö¥Îî© Î∞ïÏä§), Ïù¥ÎØ∏ÏßÄ Îì± Îã§ÏñëÌïú Î™®Îã¨Î¶¨Ìã∞Î•º ÌôúÏö©Ìï©ÎãàÎã§.\n \n Ïù¥ Í∞ÄÏù¥ÎìúÎäî Îã§Ïùå ÎÇ¥Ïö©ÏùÑ ÏÑ§Î™ÖÌï©ÎãàÎã§:\n@@ -72,7 +72,7 @@ pip install -q pytesseract\n \n ## Îç∞Ïù¥ÌÑ∞ Î∂àÎü¨Ïò§Í∏∞ [[load-the-data]]\n \n-Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑúÎäî ü§ó HubÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÎäî Ï†ÑÏ≤òÎ¶¨Îêú DocVQAÏùò ÏûëÏùÄ ÏÉòÌîåÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§. \n+Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑúÎäî ü§ó HubÏóêÏÑú Ï∞æÏùÑ Ïàò ÏûàÎäî Ï†ÑÏ≤òÎ¶¨Îêú DocVQAÏùò ÏûëÏùÄ ÏÉòÌîåÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.\n DocVQAÏùò Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÍ≥† Ïã∂Îã§Î©¥, [DocVQA homepage](https://rrc.cvc.uab.es/?ch=17)Ïóê Í∞ÄÏûÖ ÌõÑ Îã§Ïö¥Î°úÎìú Ìï† Ïàò ÏûàÏäµÎãàÎã§. Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î•º Îã§Ïö¥Î°úÎìú ÌñàÎã§Î©¥, Ïù¥ Í∞ÄÏù¥ÎìúÎ•º Í≥ÑÏÜç ÏßÑÌñâÌïòÍ∏∞ ÏúÑÌï¥ [ü§ó datasetÏóê ÌååÏùºÏùÑ Í∞ÄÏ†∏Ïò§Îäî Î∞©Î≤ï](https://huggingface.co/docs/datasets/loading#local-and-remote-files)ÏùÑ ÌôïÏù∏ÌïòÏÑ∏Ïöî.\n \n ```py\n@@ -124,9 +124,9 @@ DatasetDict({\n >>> updated_dataset = updated_dataset.filter(lambda x: len(x[\"words\"]) + len(x[\"question\"].split()) < 512)\n ```\n \n-Ïù¥ ÏãúÏ†êÏóêÏÑú Ïù¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò OCR ÌäπÏÑ±ÎèÑ Ï†úÍ±∞Ìï¥ Î≥¥Í≤†ÏäµÎãàÎã§. OCR ÌäπÏÑ±ÏùÄ Îã§Î•∏ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞ ÏúÑÌïú Í≤ÉÏúºÎ°ú, Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Î™®Îç∏Ïùò ÏûÖÎ†• ÏöîÍµ¨ ÏÇ¨Ìï≠Í≥º ÏùºÏπòÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥ ÌäπÏÑ±ÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî ÏùºÎ∂Ä Ï≤òÎ¶¨Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§. \n+Ïù¥ ÏãúÏ†êÏóêÏÑú Ïù¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò OCR ÌäπÏÑ±ÎèÑ Ï†úÍ±∞Ìï¥ Î≥¥Í≤†ÏäµÎãàÎã§. OCR ÌäπÏÑ±ÏùÄ Îã§Î•∏ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ∏∞ ÏúÑÌïú Í≤ÉÏúºÎ°ú, Ïù¥ Í∞ÄÏù¥ÎìúÏóêÏÑú ÏÇ¨Ïö©ÌïòÎäî Î™®Îç∏Ïùò ÏûÖÎ†• ÏöîÍµ¨ ÏÇ¨Ìï≠Í≥º ÏùºÏπòÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê Ïù¥ ÌäπÏÑ±ÏùÑ ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ÏÑúÎäî ÏùºÎ∂Ä Ï≤òÎ¶¨Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\n ÎåÄÏã†, ÏõêÎ≥∏ Îç∞Ïù¥ÌÑ∞Ïóê [`LayoutLMv2Processor`]Î•º ÏÇ¨Ïö©ÌïòÏó¨ OCR Î∞è ÌÜ†ÌÅ∞ÌôîÎ•º Î™®Îëê ÏàòÌñâÌï† Ïàò ÏûàÏäµÎãàÎã§.\n-Ïù¥Î†áÍ≤å ÌïòÎ©¥ Î™®Îç∏Ïù¥ ÏöîÍµ¨ÌïòÎäî ÏûÖÎ†•ÏùÑ ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§. \n+Ïù¥Î†áÍ≤å ÌïòÎ©¥ Î™®Îç∏Ïù¥ ÏöîÍµ¨ÌïòÎäî ÏûÖÎ†•ÏùÑ ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.\n Ïù¥ÎØ∏ÏßÄÎ•º ÏàòÎèôÏúºÎ°ú Ï≤òÎ¶¨ÌïòÎ†§Î©¥, [`LayoutLMv2` model documentation](../model_doc/layoutlmv2)ÏóêÏÑú Î™®Îç∏Ïù¥ ÏöîÍµ¨ÌïòÎäî ÏûÖÎ†• Ìè¨Îß∑ÏùÑ ÌôïÏù∏Ìï¥Î≥¥ÏÑ∏Ïöî.\n \n ```py\n@@ -186,7 +186,7 @@ DatasetDict({\n ### ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ [[preprocessing-text-data]]\n \n Ïù¥ÎØ∏ÏßÄÏóê OCRÏùÑ Ï†ÅÏö©ÌñàÏúºÎ©¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÌÖçÏä§Ìä∏ Î∂ÄÎ∂ÑÏùÑ Î™®Îç∏Ïóê ÎßûÍ≤å Ïù∏ÏΩîÎî©Ìï¥Ïïº Ìï©ÎãàÎã§.\n-Ïù¥ Ïù∏ÏΩîÎî©ÏóêÎäî Ïù¥Ï†Ñ Îã®Í≥ÑÏóêÏÑú Í∞ÄÏ†∏Ïò® Îã®Ïñ¥ÏôÄ Î∞ïÏä§Î•º ÌÜ†ÌÅ∞ ÏàòÏ§ÄÏùò `input_ids`, `attention_mask`, `token_type_ids` Î∞è `bbox`Î°ú Î≥ÄÌôòÌïòÎäî ÏûëÏóÖÏù¥ Ìè¨Ìï®Îê©ÎãàÎã§. \n+Ïù¥ Ïù∏ÏΩîÎî©ÏóêÎäî Ïù¥Ï†Ñ Îã®Í≥ÑÏóêÏÑú Í∞ÄÏ†∏Ïò® Îã®Ïñ¥ÏôÄ Î∞ïÏä§Î•º ÌÜ†ÌÅ∞ ÏàòÏ§ÄÏùò `input_ids`, `attention_mask`, `token_type_ids` Î∞è `bbox`Î°ú Î≥ÄÌôòÌïòÎäî ÏûëÏóÖÏù¥ Ìè¨Ìï®Îê©ÎãàÎã§.\n ÌÖçÏä§Ìä∏Î•º Ï†ÑÏ≤òÎ¶¨ÌïòÎ†§Î©¥ ÌîÑÎ°úÏÑ∏ÏÑúÏùò `tokenizer`Í∞Ä ÌïÑÏöîÌï©ÎãàÎã§.\n \n ```py\n@@ -197,8 +197,8 @@ DatasetDict({\n \n Î†àÏù¥Î∏î Ï∂îÍ∞ÄÎ•º ÏúÑÌï¥ÏÑú, Î®ºÏ†Ä Îçî ÌÅ∞ Î¶¨Ïä§Ìä∏(Îã®Ïñ¥ Î¶¨Ïä§Ìä∏)ÏóêÏÑú ÌïòÏúÑ Î¶¨Ïä§Ìä∏(Îã®Ïñ¥Î°ú Î∂ÑÌï†Îêú ÎãµÎ≥Ä)ÏùÑ Ï∞æÏùÑ Ïàò ÏûàÎäî Ìó¨Ìçº Ìï®ÏàòÎ•º Ï†ïÏùòÌï©ÎãàÎã§.\n \n-Ïù¥ Ìï®ÏàòÎäî `words_list`ÏôÄ `answer_list`, Ïù¥Î†áÍ≤å Îëê Î¶¨Ïä§Ìä∏Î•º ÏûÖÎ†•ÏúºÎ°ú Î∞õÏäµÎãàÎã§. \n-Í∑∏Îü∞ Îã§Ïùå `words_list`Î•º Î∞òÎ≥µÌïòÏó¨ `words_list`Ïùò ÌòÑÏû¨ Îã®Ïñ¥(words_list[i])Í∞Ä `answer_list`Ïùò Ï≤´ Î≤àÏß∏ Îã®Ïñ¥(answer_list[0])ÏôÄ Í∞ôÏùÄÏßÄ, \n+Ïù¥ Ìï®ÏàòÎäî `words_list`ÏôÄ `answer_list`, Ïù¥Î†áÍ≤å Îëê Î¶¨Ïä§Ìä∏Î•º ÏûÖÎ†•ÏúºÎ°ú Î∞õÏäµÎãàÎã§.\n+Í∑∏Îü∞ Îã§Ïùå `words_list`Î•º Î∞òÎ≥µÌïòÏó¨ `words_list`Ïùò ÌòÑÏû¨ Îã®Ïñ¥(words_list[i])Í∞Ä `answer_list`Ïùò Ï≤´ Î≤àÏß∏ Îã®Ïñ¥(answer_list[0])ÏôÄ Í∞ôÏùÄÏßÄ,\n ÌòÑÏû¨ Îã®Ïñ¥ÏóêÏÑú ÏãúÏûëÌï¥ `answer_list`ÏôÄ Í∞ôÏùÄ Í∏∏Ïù¥ÎßåÌÅºÏùò `words_list`Ïùò ÌïòÏúÑ Î¶¨Ïä§Ìä∏Í∞Ä `answer_list`ÏôÄ ÏùºÏπòÌïòÎäîÏßÄ ÌôïÏù∏Ìï©ÎãàÎã§.\n Ïù¥ Ï°∞Í±¥Ïù¥ Ï∞∏Ïù¥ÎùºÎ©¥ ÏùºÏπòÌïòÎäî Ìï≠Î™©ÏùÑ Î∞úÍ≤¨ÌñàÏùåÏùÑ ÏùòÎØ∏ÌïòÎ©∞, Ìï®ÏàòÎäî ÏùºÏπò Ìï≠Î™©, ÏãúÏûë Ïù∏Îç±Ïä§(idx) Î∞è Ï¢ÖÎ£å Ïù∏Îç±Ïä§(idx + len(answer_list) - 1)Î•º Í∏∞Î°ùÌï©ÎãàÎã§. ÏùºÏπòÌïòÎäî Ìï≠Î™©Ïù¥ Îëê Í∞ú Ïù¥ÏÉÅ Î∞úÍ≤¨ÎêòÎ©¥ Ìï®ÏàòÎäî Ï≤´ Î≤àÏß∏ Ìï≠Î™©Îßå Î∞òÌôòÌï©ÎãàÎã§. ÏùºÏπòÌïòÎäî Ìï≠Î™©Ïù¥ ÏóÜÎã§Î©¥ Ìï®ÏàòÎäî (`None`, 0, 0)ÏùÑ Î∞òÌôòÌï©ÎãàÎã§.\n \n@@ -349,7 +349,7 @@ end_index 18\n \n ## ÌõàÎ†® [[train]]\n \n-Ï∂ïÌïòÌï©ÎãàÎã§! Ïù¥ Í∞ÄÏù¥ÎìúÏùò Í∞ÄÏû• Ïñ¥Î†§Ïö¥ Î∂ÄÎ∂ÑÏùÑ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌñàÏúºÎãà Ïù¥Ï†ú ÎÇòÎßåÏùò Î™®Îç∏ÏùÑ ÌõàÎ†®Ìï† Ï§ÄÎπÑÍ∞Ä ÎêòÏóàÏäµÎãàÎã§. \n+Ï∂ïÌïòÌï©ÎãàÎã§! Ïù¥ Í∞ÄÏù¥ÎìúÏùò Í∞ÄÏû• Ïñ¥Î†§Ïö¥ Î∂ÄÎ∂ÑÏùÑ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Ï≤òÎ¶¨ÌñàÏúºÎãà Ïù¥Ï†ú ÎÇòÎßåÏùò Î™®Îç∏ÏùÑ ÌõàÎ†®Ìï† Ï§ÄÎπÑÍ∞Ä ÎêòÏóàÏäµÎãàÎã§.\n ÌõàÎ†®ÏùÄ Îã§ÏùåÍ≥º Í∞ôÏùÄ Îã®Í≥ÑÎ°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÏäµÎãàÎã§:\n * Ï†ÑÏ≤òÎ¶¨ÏóêÏÑúÏùò ÎèôÏùºÌïú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º ÏÇ¨Ïö©ÌïòÍ∏∞ ÏúÑÌï¥ [`AutoModelForDocumentQuestionAnswering`]ÏúºÎ°ú Î™®Îç∏ÏùÑ Í∞ÄÏ†∏ÏòµÎãàÎã§.\n * [`TrainingArguments`]Î°ú ÌõàÎ†® ÌïòÏù¥ÌçºÌååÎùºÎØ∏ÌÑ∞Î•º Ï†ïÌï©ÎãàÎã§.\n@@ -406,7 +406,7 @@ end_index 18\n ...     data_collator=data_collator,\n ...     train_dataset=encoded_train_dataset,\n ...     eval_dataset=encoded_test_dataset,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n \n >>> trainer.train()\n@@ -421,7 +421,7 @@ end_index 18\n \n ## Ï∂îÎ°† [[inference]]\n \n-Ïù¥Ï†ú LayoutLMv2 Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ≥† ü§ó HubÏóê ÏóÖÎ°úÎìúÌñàÏúºÎãà Ï∂îÎ°†ÏóêÎèÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. \n+Ïù¥Ï†ú LayoutLMv2 Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÍ≥† ü§ó HubÏóê ÏóÖÎ°úÎìúÌñàÏúºÎãà Ï∂îÎ°†ÏóêÎèÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n Ï∂îÎ°†ÏùÑ ÏúÑÌï¥ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú Î™®Îç∏ÏùÑ ÏÇ¨Ïö©Ìï¥ Î≥¥Îäî Í∞ÄÏû• Í∞ÑÎã®Ìïú Î∞©Î≤ïÏùÄ [`Pipeline`]ÏùÑ ÏÇ¨Ïö©ÌïòÎäî Í≤É ÏûÖÎãàÎã§.\n \n ÏòàÎ•º Îì§Ïñ¥ Î≥¥Í≤†ÏäµÎãàÎã§:\n@@ -473,4 +473,4 @@ end_index 18\n \n >>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])\n 'lee a. waller'\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "4955bd6cdf8108624fb706a0d5c0605814858eb5",
            "filename": "docs/source/ko/tasks/image_classification.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -157,7 +157,7 @@ Hugging Face Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÏó¨ Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Ïª§ÎÆ§ÎãàÌã∞Ïóê\n \n Í≥ºÏ†ÅÌï©ÏùÑ Î∞©ÏßÄÌïòÍ≥† Î™®Îç∏ÏùÑ Î≥¥Îã§ Í≤¨Í≥†ÌïòÍ≤å ÎßåÎì§Í∏∞ ÏúÑÌï¥ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÌõàÎ†® Î∂ÄÎ∂ÑÏóê Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ïÏùÑ Ï∂îÍ∞ÄÌï©ÎãàÎã§.\n Ïó¨Í∏∞ÏÑú Keras Ï†ÑÏ≤òÎ¶¨ Î†àÏù¥Ïñ¥Î°ú ÌõàÎ†® Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú Î≥ÄÌôò(Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Ìè¨Ìï®)Í≥º\n-Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú Î≥ÄÌôò(Ï§ëÏïô ÌÅ¨Î°úÌïë, ÌÅ¨Í∏∞ Ï°∞Ï†ï, Ï†ïÍ∑úÌôîÎßå)ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§. \n+Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Ïóê ÎåÄÌïú Î≥ÄÌôò(Ï§ëÏïô ÌÅ¨Î°úÌïë, ÌÅ¨Í∏∞ Ï°∞Ï†ï, Ï†ïÍ∑úÌôîÎßå)ÏùÑ Ï†ïÏùòÌï©ÎãàÎã§.\n `tf.image` ÎòêÎäî Îã§Î•∏ ÏõêÌïòÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨Î•º ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n \n ```py\n@@ -241,7 +241,7 @@ food[\"test\"].set_transform(preprocess_val)\n ## ÌèâÍ∞Ä[[evaluate]]\n \n ÌõàÎ†® Ï§ëÏóê ÌèâÍ∞Ä ÏßÄÌëúÎ•º Ìè¨Ìï®ÌïòÎ©¥ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÌèâÍ∞ÄÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏäµÎãàÎã§.\n-ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) ÎùºÏù¥Î∏åÎü¨Î¶¨Î°ú ÌèâÍ∞Ä Î∞©Î≤ïÏùÑ Îπ†Î•¥Í≤å Í∞ÄÏ†∏Ïò¨ Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÏûëÏóÖÏóêÏÑúÎäî \n+ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) ÎùºÏù¥Î∏åÎü¨Î¶¨Î°ú ÌèâÍ∞Ä Î∞©Î≤ïÏùÑ Îπ†Î•¥Í≤å Í∞ÄÏ†∏Ïò¨ Ïàò ÏûàÏäµÎãàÎã§. Ïù¥ ÏûëÏóÖÏóêÏÑúÎäî\n [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) ÌèâÍ∞Ä ÏßÄÌëúÎ•º Í∞ÄÏ†∏ÏòµÎãàÎã§. (ü§ó Evaluate [Îπ†Î•∏ ÎëòÎü¨Î≥¥Í∏∞](https://huggingface.co/docs/evaluate/a_quick_tour)Î•º Ï∞∏Ï°∞ÌïòÏó¨ ÌèâÍ∞Ä ÏßÄÌëúÎ•º Í∞ÄÏ†∏Ïò§Í≥† Í≥ÑÏÇ∞ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥ÏÑ∏Ïöî):\n \n ```py\n@@ -317,7 +317,7 @@ food[\"test\"].set_transform(preprocess_val)\n ...     data_collator=data_collator,\n ...     train_dataset=food[\"train\"],\n ...     eval_dataset=food[\"test\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     compute_metrics=compute_metrics,\n ... )\n \n@@ -404,7 +404,7 @@ TensorFlowÏóêÏÑú Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÎ†§Î©¥ Îã§Ïùå Îã®Í≥ÑÎ•º Îî∞Î•¥ÏÑ∏Ïöî:\n ```\n \n ÏòàÏ∏°ÏóêÏÑú Ï†ïÌôïÎèÑÎ•º Í≥ÑÏÇ∞ÌïòÍ≥† Î™®Îç∏ÏùÑ ü§ó HubÎ°ú Ìë∏ÏãúÌïòÎ†§Î©¥ [Keras callbacks](../main_classes/keras_callbacks)Î•º ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.\n-`compute_metrics` Ìï®ÏàòÎ•º [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)Ïóê Ï†ÑÎã¨ÌïòÍ≥†, \n+`compute_metrics` Ìï®ÏàòÎ•º [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)Ïóê Ï†ÑÎã¨ÌïòÍ≥†,\n [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback)ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌï©ÎãàÎã§:\n \n ```py"
        },
        {
            "sha": "f2755da4a8bf37924bb9e633dd14af4cd0c4b919",
            "filename": "docs/source/ko/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -270,7 +270,7 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n ...     args=training_args,\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "e027ad65a9ada8c07b0af7403cf62d1e31f37099",
            "filename": "docs/source/ko/tasks/object_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fobject_detection.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -361,7 +361,7 @@ DatasetDict({\n ...     args=training_args,\n ...     data_collator=collate_fn,\n ...     train_dataset=cppe5[\"train\"],\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ... )\n \n >>> trainer.train()"
        },
        {
            "sha": "8309dd7d753244f355747d0bff29e564df2cde33",
            "filename": "docs/source/ko/tasks/question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fquestion_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -223,7 +223,7 @@ pip install transformers datasets evaluate\n ...     args=training_args,\n ...     train_dataset=tokenized_squad[\"train\"],\n ...     eval_dataset=tokenized_squad[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n "
        },
        {
            "sha": "11dae1a965a4f2634cf304bb8c805715e2791f9b",
            "filename": "docs/source/ko/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fsequence_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -190,7 +190,7 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n ...     args=training_args,\n ...     train_dataset=tokenized_imdb[\"train\"],\n ...     eval_dataset=tokenized_imdb[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "a2b2b1fbc95498baaeb02b00b39894d60e801370",
            "filename": "docs/source/ko/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fsummarization.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -223,7 +223,7 @@ Hugging Face Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÎ©¥ Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Ïª§ÎÆ§ÎãàÌã∞Ïóê\n ...     args=training_args,\n ...     train_dataset=tokenized_billsum[\"train\"],\n ...     eval_dataset=tokenized_billsum[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "a65503092cee1decc8e2983ba4df1bf5e1db6921",
            "filename": "docs/source/ko/tasks/token_classification.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Ftoken_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -107,7 +107,7 @@ Hugging Face Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÏó¨ Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Ïª§ÎÆ§ÎãàÌã∞Ïóê\n >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n ```\n \n-ÏúÑÏùò ÏòàÏ†ú `tokens` ÌïÑÎìúÎ•º Î≥¥Î©¥ ÏûÖÎ†•Ïù¥ Ïù¥ÎØ∏ ÌÜ†ÌÅ∞ÌôîÎêú Í≤ÉÏ≤òÎüº Î≥¥ÏûÖÎãàÎã§. Í∑∏Îü¨ÎÇò Ïã§Ï†úÎ°ú ÏûÖÎ†•ÏùÄ ÏïÑÏßÅ ÌÜ†ÌÅ∞ÌôîÎêòÏßÄ ÏïäÏïòÏúºÎØÄÎ°ú Îã®Ïñ¥Î•º ÌïòÏúÑ Îã®Ïñ¥Î°ú ÌÜ†ÌÅ∞ÌôîÌïòÍ∏∞ ÏúÑÌï¥ `is_split_into_words=True`Î•º ÏÑ§Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§. ÏòàÏ†úÎ°ú ÌôïÏù∏Ìï©ÎãàÎã§: \n+ÏúÑÏùò ÏòàÏ†ú `tokens` ÌïÑÎìúÎ•º Î≥¥Î©¥ ÏûÖÎ†•Ïù¥ Ïù¥ÎØ∏ ÌÜ†ÌÅ∞ÌôîÎêú Í≤ÉÏ≤òÎüº Î≥¥ÏûÖÎãàÎã§. Í∑∏Îü¨ÎÇò Ïã§Ï†úÎ°ú ÏûÖÎ†•ÏùÄ ÏïÑÏßÅ ÌÜ†ÌÅ∞ÌôîÎêòÏßÄ ÏïäÏïòÏúºÎØÄÎ°ú Îã®Ïñ¥Î•º ÌïòÏúÑ Îã®Ïñ¥Î°ú ÌÜ†ÌÅ∞ÌôîÌïòÍ∏∞ ÏúÑÌï¥ `is_split_into_words=True`Î•º ÏÑ§Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§. ÏòàÏ†úÎ°ú ÌôïÏù∏Ìï©ÎãàÎã§:\n \n ```py\n >>> example = wnut[\"train\"][0]\n@@ -294,7 +294,7 @@ Hugging Face Í≥ÑÏ†ïÏóê Î°úÍ∑∏Ïù∏ÌïòÏó¨ Î™®Îç∏ÏùÑ ÏóÖÎ°úÎìúÌïòÍ≥† Ïª§ÎÆ§ÎãàÌã∞Ïóê\n ...     args=training_args,\n ...     train_dataset=tokenized_wnut[\"train\"],\n ...     eval_dataset=tokenized_wnut[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )\n@@ -405,8 +405,8 @@ TensorFlowÏóêÏÑú Î™®Îç∏ÏùÑ ÌååÏù∏ ÌäúÎãùÌïòÎ†§Î©¥, Î®ºÏ†Ä ÏòµÌã∞ÎßàÏù¥Ï†Ä Ìï®Ïàò\n \n <Tip>\n \n-ÌÜ†ÌÅ∞ Î∂ÑÎ•òÎ•º ÏúÑÌïú Î™®Îç∏ÏùÑ ÌååÏù∏ ÌäúÎãùÌïòÎäî ÏûêÏÑ∏Ìïú ÏòàÏ†úÎäî Îã§Ïùå \n-[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb) \n+ÌÜ†ÌÅ∞ Î∂ÑÎ•òÎ•º ÏúÑÌïú Î™®Îç∏ÏùÑ ÌååÏù∏ ÌäúÎãùÌïòÎäî ÏûêÏÑ∏Ìïú ÏòàÏ†úÎäî Îã§Ïùå\n+[PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)\n ÎòêÎäî [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)Î•º Ï∞∏Ï°∞ÌïòÏÑ∏Ïöî.\n \n </Tip>"
        },
        {
            "sha": "5b4eaaa6125a11b0167be3c45d8cc1d021846ec0",
            "filename": "docs/source/ko/tasks/translation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Ftranslation.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -221,7 +221,7 @@ pip install transformers datasets evaluate sacrebleu\n ...     args=training_args,\n ...     train_dataset=tokenized_books[\"train\"],\n ...     eval_dataset=tokenized_books[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )"
        },
        {
            "sha": "10569083c09f85f6db4e81f88bf78d73742a4702",
            "filename": "docs/source/ko/tasks/video_classification.md",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fvideo_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -61,7 +61,7 @@ pip install -q pytorchvideo transformers evaluate\n ```\n \n Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò ÌïòÏúÑ ÏßëÌï©Ïù¥ Îã§Ïö¥Î°úÎìú ÎêòÎ©¥, ÏïïÏ∂ïÎêú ÌååÏùºÏùò ÏïïÏ∂ïÏùÑ Ìï¥Ï†úÌï¥Ïïº Ìï©ÎãàÎã§:\n-```py \n+```py\n >>> import tarfile\n \n >>> with tarfile.open(file_path) as t:\n@@ -124,9 +124,9 @@ UCF101_subset/\n Í∑∏ Îã§ÏùåÏúºÎ°ú, Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê Ï°¥Ïû¨ÌïòÎäî ÎùºÎ≤®ÏùÑ Ï∂îÏ∂úÌï©ÎãàÎã§. ÎòêÌïú, Î™®Îç∏ÏùÑ Ï¥àÍ∏∞ÌôîÌï† Îïå ÎèÑÏõÄÏù¥ Îê† ÎîïÏÖîÎÑàÎ¶¨(dictionary data type)Î•º ÏÉùÏÑ±Ìï©ÎãàÎã§.\n \n * `label2id`: ÌÅ¥ÎûòÏä§ Ïù¥Î¶ÑÏùÑ Ï†ïÏàòÏóê Îß§ÌïëÌï©ÎãàÎã§.\n-* `id2label`: Ï†ïÏàòÎ•º ÌÅ¥ÎûòÏä§ Ïù¥Î¶ÑÏóê Îß§ÌïëÌï©ÎãàÎã§. \n+* `id2label`: Ï†ïÏàòÎ•º ÌÅ¥ÎûòÏä§ Ïù¥Î¶ÑÏóê Îß§ÌïëÌï©ÎãàÎã§.\n \n-```py \n+```py\n >>> class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n >>> label2id = {label: i for i, label in enumerate(class_labels)}\n >>> id2label = {i: label for label, i in label2id.items()}\n@@ -142,7 +142,7 @@ UCF101_subset/\n \n ÏÇ¨Ï†Ñ ÌõàÎ†®Îêú Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏ÏôÄ Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Ïóê Ïó∞Í¥ÄÎêú Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ÏòÅÏÉÅ Î∂ÑÎ•ò Î™®Îç∏ÏùÑ Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌï©ÎãàÎã§. Î™®Îç∏Ïùò Ïù∏ÏΩîÎçîÏóêÎäî ÎØ∏Î¶¨ ÌïôÏäµÎêú Îß§Í∞úÎ≥ÄÏàòÍ∞Ä Ï†úÍ≥µÎêòÎ©∞, Î∂ÑÎ•ò Ìó§Îìú(Îç∞Ïù¥ÌÑ∞Î•º Î∂ÑÎ•òÌïòÎäî ÎßàÏßÄÎßâ Î†àÏù¥Ïñ¥)Îäî Î¨¥ÏûëÏúÑÎ°ú Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§. Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïùò Ï†ÑÏ≤òÎ¶¨ ÌååÏù¥ÌîÑÎùºÏù∏ÏùÑ ÏûëÏÑ±Ìï† ÎïåÎäî Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÍ∞Ä Ïú†Ïö©Ìï©ÎãàÎã§.\n \n-```py \n+```py\n >>> from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n \n >>> model_ckpt = \"MCG-NJU/videomae-base\"\n@@ -174,7 +174,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n ÏòÅÏÉÅ Ï†ÑÏ≤òÎ¶¨Î•º ÏúÑÌï¥ [PyTorchVideo ÎùºÏù¥Î∏åÎü¨Î¶¨](https://pytorchvideo.org/)Î•º ÌôúÏö©Ìï† Í≤ÉÏûÖÎãàÎã§. ÌïÑÏöîÌïú Ï¢ÖÏÜçÏÑ±ÏùÑ Í∞ÄÏ†∏Ïò§Îäî Í≤ÉÏúºÎ°ú ÏãúÏûëÌïòÏÑ∏Ïöî.\n \n-```py \n+```py\n >>> import pytorchvideo.data\n \n >>> from pytorchvideo.transforms import (\n@@ -223,7 +223,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n Ïù¥Ï†ú Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Ïóê ÌäπÌôîÎêú Ï†ÑÏ≤òÎ¶¨(transform)Í≥º Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ ÏûêÏ≤¥Î•º Ï†ïÏùòÌï©ÎãàÎã§. Î®ºÏ†Ä ÌõàÎ†® Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏Î°ú ÏãúÏûëÌï©ÎãàÎã§:\n \n-```py \n+```py\n >>> train_transform = Compose(\n ...     [\n ...         ApplyTransformToKey(\n@@ -252,7 +252,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n Í∞ôÏùÄ Î∞©ÏãùÏùò ÏûëÏóÖ ÌùêÎ¶ÑÏùÑ Í≤ÄÏ¶ùÍ≥º ÌèâÍ∞Ä ÏÑ∏Ìä∏ÏóêÎèÑ Ï†ÅÏö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n \n-```py \n+```py\n >>> val_transform = Compose(\n ...     [\n ...         ApplyTransformToKey(\n@@ -296,7 +296,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n ## Îçî ÎÇòÏùÄ ÎîîÎ≤ÑÍπÖÏùÑ ÏúÑÌï¥ Ï†ÑÏ≤òÎ¶¨ ÏòÅÏÉÅ ÏãúÍ∞ÅÌôîÌïòÍ∏∞[[visualize-the-preprocessed-video-for-better-debugging]]\n \n-```py \n+```py\n >>> import imageio\n >>> import numpy as np\n >>> from IPython.display import Image\n@@ -309,7 +309,7 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n \n >>> def create_gif(video_tensor, filename=\"sample.gif\"):\n ...     \"\"\"Prepares a GIF from a video tensor.\n-...     \n+...\n ...     The video tensor is expected to have the following shape:\n ...     (num_frames, num_channels, height, width).\n ...     \"\"\"\n@@ -336,13 +336,13 @@ You should probably TRAIN this model on a down-stream task to be able to use it\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif\" alt=\"Person playing basketball\"/>\n </div>\n \n-## Î™®Îç∏ ÌõàÎ†®ÌïòÍ∏∞[[train-the-model]] \n+## Î™®Îç∏ ÌõàÎ†®ÌïòÍ∏∞[[train-the-model]]\n \n ü§ó TransformersÏùò [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer)Î•º ÏÇ¨Ïö©ÌïòÏó¨ Î™®Îç∏ÏùÑ ÌõàÎ†®ÏãúÏºúÎ≥¥ÏÑ∏Ïöî. `Trainer`Î•º Ïù∏Ïä§ÌÑ¥Ïä§ÌôîÌïòÎ†§Î©¥ ÌõàÎ†® ÏÑ§Ï†ïÍ≥º ÌèâÍ∞Ä ÏßÄÌëúÎ•º Ï†ïÏùòÌï¥Ïïº Ìï©ÎãàÎã§.  Í∞ÄÏû• Ï§ëÏöîÌïú Í≤ÉÏùÄ [`TrainingArguments`](https://huggingface.co/transformers/main_classes/trainer.html#transformers.TrainingArguments)ÏûÖÎãàÎã§. Ïù¥ ÌÅ¥ÎûòÏä§Îäî ÌõàÎ†®ÏùÑ Íµ¨ÏÑ±ÌïòÎäî Î™®Îì† ÏÜçÏÑ±ÏùÑ Ìè¨Ìï®ÌïòÎ©∞, ÌõàÎ†® Ï§ë Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Î•º Ï†ÄÏû•Ìï† Ï∂úÎ†• Ìè¥Îçî Ïù¥Î¶ÑÏùÑ ÌïÑÏöîÎ°ú Ìï©ÎãàÎã§. ÎòêÌïú ü§ó HubÏùò Î™®Îç∏ Ï†ÄÏû•ÏÜåÏùò Î™®Îì† Ï†ïÎ≥¥Î•º ÎèôÍ∏∞ÌôîÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ Îê©ÎãàÎã§.\n \n ÎåÄÎ∂ÄÎ∂ÑÏùò ÌõàÎ†® Ïù∏ÏàòÎäî Îî∞Î°ú ÏÑ§Î™ÖÌï† ÌïÑÏöîÎäî ÏóÜÏäµÎãàÎã§. ÌïòÏßÄÎßå Ïó¨Í∏∞ÏóêÏÑú Ï§ëÏöîÌïú Ïù∏ÏàòÎäî `remove_unused_columns=False` ÏûÖÎãàÎã§. Ïù¥ Ïù∏ÏûêÎäî Î™®Îç∏Ïùò Ìò∏Ï∂ú Ìï®ÏàòÏóêÏÑú ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî Î™®Îì† ÏÜçÏÑ± Ïó¥(columns)ÏùÑ ÏÇ≠Ï†úÌï©ÎãàÎã§. Í∏∞Î≥∏Í∞íÏùÄ ÏùºÎ∞òÏ†ÅÏúºÎ°ú TrueÏûÖÎãàÎã§. Ïù¥Îäî ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî Í∏∞Îä• Ïó¥ÏùÑ ÏÇ≠Ï†úÌïòÎäî Í≤ÉÏù¥ Ïù¥ÏÉÅÏ†ÅÏù¥Î©∞, ÏûÖÎ†•ÏùÑ Î™®Îç∏Ïùò Ìò∏Ï∂ú Ìï®ÏàòÎ°ú ÌíÄÍ∏∞(unpack)Í∞Ä Ïâ¨ÏõåÏßÄÍ∏∞ ÎïåÎ¨∏ÏûÖÎãàÎã§. ÌïòÏßÄÎßå Ïù¥ Í≤ΩÏö∞ÏóêÎäî `pixel_values`(Î™®Îç∏Ïùò ÏûÖÎ†•ÏúºÎ°ú ÌïÑÏàòÏ†ÅÏù∏ ÌÇ§)Î•º ÏÉùÏÑ±ÌïòÍ∏∞ ÏúÑÌï¥ ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî Í∏∞Îä•('video'Í∞Ä ÌäπÌûà Í∑∏Î†áÏäµÎãàÎã§)Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§. Îî∞ÎùºÏÑú remove_unused_columnsÏùÑ FalseÎ°ú ÏÑ§Ï†ïÌï¥Ïïº Ìï©ÎãàÎã§.\n \n-```py \n+```py\n >>> from transformers import TrainingArguments, Trainer\n \n >>> model_name = model_ckpt.split(\"/\")[-1]\n@@ -387,7 +387,7 @@ def compute_metrics(eval_pred):\n \n ÎòêÌïú, ÏòàÏ†úÎ•º Î¨∂Ïñ¥ÏÑú Î∞∞ÏπòÎ•º ÌòïÏÑ±ÌïòÎäî `collate_fn`ÏùÑ Ï†ïÏùòÌï¥ÏïºÌï©ÎãàÎã§. Í∞Å Î∞∞ÏπòÎäî `pixel_values`ÏôÄ `labels`ÎùºÎäî 2Í∞úÏùò ÌÇ§Î°ú Íµ¨ÏÑ±Îê©ÎãàÎã§.\n \n-```py \n+```py\n >>> def collate_fn(examples):\n ...     # permute to (num_frames, num_channels, height, width)\n ...     pixel_values = torch.stack(\n@@ -399,13 +399,13 @@ def compute_metrics(eval_pred):\n \n Í∑∏Îü∞ Îã§Ïùå Ïù¥ Î™®Îì† Í≤ÉÏùÑ Îç∞Ïù¥ÌÑ∞ ÏÑ∏Ìä∏ÏôÄ Ìï®Íªò `Trainer`Ïóê Ï†ÑÎã¨ÌïòÍ∏∞Îßå ÌïòÎ©¥ Îê©ÎãàÎã§:\n \n-```py \n+```py\n >>> trainer = Trainer(\n ...     model,\n ...     args,\n ...     train_dataset=train_dataset,\n ...     eval_dataset=val_dataset,\n-...     tokenizer=image_processor,\n+...     processing_class=image_processor,\n ...     compute_metrics=compute_metrics,\n ...     data_collator=collate_fn,\n ... )\n@@ -415,7 +415,7 @@ def compute_metrics(eval_pred):\n \n `train` Î©îÏÜåÎìúÎ•º Ìò∏Ï∂úÌïòÏó¨ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÏÑ∏Ïöî:\n \n-```py \n+```py\n >>> train_results = trainer.train()\n ```\n \n@@ -429,7 +429,7 @@ def compute_metrics(eval_pred):\n Ï¢ãÏäµÎãàÎã§. Ïù¥Ï†ú ÎØ∏ÏÑ∏ Ï°∞Ï†ïÎêú Î™®Îç∏ÏùÑ Ï∂îÎ°†ÌïòÎäî Îç∞ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n \n Ï∂îÎ°†Ïóê ÏÇ¨Ïö©Ìï† ÏòÅÏÉÅÏùÑ Î∂àÎü¨Ïò§ÏÑ∏Ïöî:\n-```py \n+```py\n >>> sample_test_video = next(iter(test_dataset))\n ```\n \n@@ -485,7 +485,7 @@ def compute_metrics(eval_pred):\n \n `logits`ÏùÑ ÎîîÏΩîÎî©ÌïòÎ©¥, Ïö∞Î¶¨Îäî Îã§Ïùå Í≤∞Í≥ºÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§:\n \n-```py \n+```py\n >>> predicted_class_idx = logits.argmax(-1).item()\n >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n # Predicted class: BasketballDunk"
        },
        {
            "sha": "9bc87c071e62b124ed6c941d83242c845fb9aeab",
            "filename": "docs/source/ko/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fko%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fvisual_question_answering.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -35,7 +35,7 @@ VQAÏùò Ï£ºÏöî ÏÇ¨Ïö© ÏÇ¨Î°ÄÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§:\n ## ViLT ÎØ∏ÏÑ∏ Ï°∞Ï†ï [[finetuning-vilt]]\n \n ViLTÎäî Vision Transformer (ViT) ÎÇ¥Ïóê ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî©ÏùÑ Ìè¨Ìï®ÌïòÏó¨ ÎπÑÏ†Ñ/ÏûêÏó∞Ïñ¥ ÏÇ¨Ï†ÑÌõàÎ†®(VLP; Vision-and-Language Pretraining)ÏùÑ ÏúÑÌïú Í∏∞Î≥∏ ÎîîÏûêÏù∏ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.\n-ViLT Î™®Îç∏ÏùÄ ÎπÑÏ†Ñ Ìä∏ÎûúÏä§Ìè¨Î®∏(ViT)Ïóê ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî©ÏùÑ ÎÑ£Ïñ¥ ÎπÑÏ†Ñ/Ïñ∏Ïñ¥ ÏÇ¨Ï†ÑÌõàÎ†®(VLP; Vision-and-Language Pre-training)ÏùÑ ÏúÑÌïú Í∏∞Î≥∏Ï†ÅÏù∏ ÎîîÏûêÏù∏ÏùÑ Í∞ñÏ∑ÑÏäµÎãàÎã§. Ïù¥ Î™®Îç∏ÏùÄ Ïó¨Îü¨ Îã§Ïö¥Ïä§Ìä∏Î¶º ÏûëÏóÖÏóê ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. VQA ÌÉúÏä§ÌÅ¨ÏóêÏÑúÎäî (`[CLS]` ÌÜ†ÌÅ∞Ïùò ÏµúÏ¢Ö ÏùÄÎãâ ÏÉÅÌÉú ÏúÑÏóê ÏÑ†Ìòï Î†àÏù¥Ïñ¥Ïù∏) Î∂ÑÎ•ò Ìó§ÎçîÍ∞Ä ÏûàÏúºÎ©∞ Î¨¥ÏûëÏúÑÎ°ú Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§. \n+ViLT Î™®Îç∏ÏùÄ ÎπÑÏ†Ñ Ìä∏ÎûúÏä§Ìè¨Î®∏(ViT)Ïóê ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî©ÏùÑ ÎÑ£Ïñ¥ ÎπÑÏ†Ñ/Ïñ∏Ïñ¥ ÏÇ¨Ï†ÑÌõàÎ†®(VLP; Vision-and-Language Pre-training)ÏùÑ ÏúÑÌïú Í∏∞Î≥∏Ï†ÅÏù∏ ÎîîÏûêÏù∏ÏùÑ Í∞ñÏ∑ÑÏäµÎãàÎã§. Ïù¥ Î™®Îç∏ÏùÄ Ïó¨Îü¨ Îã§Ïö¥Ïä§Ìä∏Î¶º ÏûëÏóÖÏóê ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏäµÎãàÎã§. VQA ÌÉúÏä§ÌÅ¨ÏóêÏÑúÎäî (`[CLS]` ÌÜ†ÌÅ∞Ïùò ÏµúÏ¢Ö ÏùÄÎãâ ÏÉÅÌÉú ÏúÑÏóê ÏÑ†Ìòï Î†àÏù¥Ïñ¥Ïù∏) Î∂ÑÎ•ò Ìó§ÎçîÍ∞Ä ÏûàÏúºÎ©∞ Î¨¥ÏûëÏúÑÎ°ú Ï¥àÍ∏∞ÌôîÎê©ÎãàÎã§.\n Îî∞ÎùºÏÑú Ïó¨Í∏∞ÏóêÏÑú ÏãúÍ∞ÅÏ†Å ÏßàÏùòÏùëÎãµÏùÄ **Î∂ÑÎ•ò Î¨∏Ï†ú**Î°ú Ï∑®Í∏âÎê©ÎãàÎã§.\n \n ÏµúÍ∑ºÏùò BLIP, BLIP-2, InstructBLIPÏôÄ Í∞ôÏùÄ Î™®Îç∏Îì§ÏùÄ VQAÎ•º ÏÉùÏÑ±Ìòï ÏûëÏóÖÏúºÎ°ú Í∞ÑÏ£ºÌï©ÎãàÎã§. Í∞ÄÏù¥ÎìúÏùò ÌõÑÎ∞òÎ∂ÄÏóêÏÑúÎäî Ïù¥Îü∞ Î™®Îç∏Îì§ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ï†úÎ°úÏÉ∑ VQA Ï∂îÎ°†ÏùÑ ÌïòÎäî Î∞©Î≤ïÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌïòÍ≤†ÏäµÎãàÎã§.\n@@ -104,7 +104,7 @@ Dataset({\n \n ÎÇòÎ®∏ÏßÄ ÌäπÏÑ±Îì§ÏùÄ ÌïÑÏöîÌïòÏßÄ ÏïäÍ∏∞ ÎïåÎ¨∏Ïóê ÏÇ≠Ï†úÌï¥ÎèÑ Îê©ÎãàÎã§:\n \n-```py \n+```py\n >>> dataset = dataset.remove_columns(['question_type', 'question_id', 'answer_type'])\n ```\n \n@@ -137,7 +137,7 @@ Dataset({\n >>> unique_labels = list(set(flattened_labels))\n \n >>> label2id = {label: idx for idx, label in enumerate(unique_labels)}\n->>> id2label = {idx: label for label, idx in label2id.items()} \n+>>> id2label = {idx: label for label, idx in label2id.items()}\n ```\n \n Ïù¥Ï†ú Îß§ÌïëÏù¥ ÏôÑÎ£åÎêòÏóàÏúºÎØÄÎ°ú Î¨∏ÏûêÏó¥ ÎãµÎ≥ÄÏùÑ Ìï¥Îãπ idÎ°ú ÍµêÏ≤¥ÌïòÍ≥†, Îç∞Ïù¥ÌÑ∞ÏÑ∏Ìä∏Ïùò Îçî Ìé∏Î¶¨Ìïú ÌõÑÏ≤òÎ¶¨Î•º ÏúÑÌï¥ Ìé∏ÌèâÌôî Ìï† Ïàò ÏûàÏäµÎãàÎã§.\n@@ -159,10 +159,10 @@ Dataset({\n \n ## Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ [[preprocessing-data]]\n \n-Îã§Ïùå Îã®Í≥ÑÎäî Î™®Îç∏ÏùÑ ÏúÑÌï¥ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î•º Ï§ÄÎπÑÌïòÍ∏∞ ÏúÑÌï¥ ViLT ÌîÑÎ°úÏÑ∏ÏÑúÎ•º Í∞ÄÏ†∏Ïò§Îäî Í≤ÉÏûÖÎãàÎã§. \n+Îã§Ïùå Îã®Í≥ÑÎäî Î™®Îç∏ÏùÑ ÏúÑÌï¥ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞Î•º Ï§ÄÎπÑÌïòÍ∏∞ ÏúÑÌï¥ ViLT ÌîÑÎ°úÏÑ∏ÏÑúÎ•º Í∞ÄÏ†∏Ïò§Îäî Í≤ÉÏûÖÎãàÎã§.\n [`ViltProcessor`]Îäî BERT ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏôÄ ViLT Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°úÏÑ∏ÏÑúÎ•º Ìé∏Î¶¨ÌïòÍ≤å ÌïòÎÇòÏùò ÌîÑÎ°úÏÑ∏ÏÑúÎ°ú Î¨∂ÏäµÎãàÎã§:\n \n-```py \n+```py\n >>> from transformers import ViltProcessor\n \n >>> processor = ViltProcessor.from_pretrained(model_checkpoint)\n@@ -181,25 +181,25 @@ Dataset({\n >>> def preprocess_data(examples):\n ...     image_paths = examples['image_id']\n ...     images = [Image.open(image_path) for image_path in image_paths]\n-...     texts = examples['question']    \n+...     texts = examples['question']\n \n ...     encoding = processor(images, texts, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n \n ...     for k, v in encoding.items():\n ...           encoding[k] = v.squeeze()\n-    \n+\n ...     targets = []\n \n ...     for labels, scores in zip(examples['label.ids'], examples['label.weights']):\n ...         target = torch.zeros(len(id2label))\n \n ...         for label, score in zip(labels, scores):\n ...             target[label] = score\n-      \n+\n ...         targets.append(target)\n \n ...     encoding[\"labels\"] = targets\n-    \n+\n ...     return encoding\n ```\n \n@@ -264,14 +264,14 @@ Dataset({\n ...     args=training_args,\n ...     data_collator=data_collator,\n ...     train_dataset=processed_dataset,\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ... )\n ```\n \n 3. [`~Trainer.train`]ÏùÑ Ìò∏Ï∂úÌïòÏó¨ Î™®Îç∏ÏùÑ ÎØ∏ÏÑ∏ Ï°∞Ï†ïÌïòÏÑ∏Ïöî:\n \n ```py\n->>> trainer.train() \n+>>> trainer.train()\n ```\n \n ÌõàÎ†®Ïù¥ ÏôÑÎ£åÎêòÎ©¥, [`~Trainer.push_to_hub`] Î©îÏÜåÎìúÎ•º ÏÇ¨Ïö©ÌïòÏó¨ ü§ó HubÏóê Î™®Îç∏ÏùÑ Í≥µÏú†ÌïòÏÑ∏Ïöî:\n@@ -349,7 +349,7 @@ Predicted answer: down\n \n Î™®Îç∏ÏùÄ Ïù¥ÎØ∏ÏßÄÏôÄ ÌÖçÏä§Ìä∏Î•º ÏûÖÎ†•ÏúºÎ°ú Î∞õÏúºÎØÄÎ°ú, VQA Îç∞Ïù¥ÌÑ∞ÏÑ∏Ìä∏Ïùò Ï≤´ Î≤àÏß∏ ÏòàÏ†úÏóêÏÑúÏôÄ ÎèôÏùºÌïú Ïù¥ÎØ∏ÏßÄ/ÏßàÎ¨∏ ÏåçÏùÑ ÏÇ¨Ïö©Ìï¥ Î≥¥Í≤†ÏäµÎãàÎã§:\n \n-```py \n+```py\n >>> example = dataset[0]\n >>> image = Image.open(example['image_id'])\n >>> question = example['question']\n@@ -358,7 +358,7 @@ Predicted answer: down\n BLIP-2Î•º ÏãúÍ∞ÅÏ†Å ÏßàÏùòÏùëÎãµ ÏûëÏóÖÏóê ÏÇ¨Ïö©ÌïòÎ†§Î©¥ ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑÌä∏Í∞Ä `Question: {} Answer:` ÌòïÏãùÏùÑ Îî∞ÎùºÏïº Ìï©ÎãàÎã§.\n \n ```py\n->>> prompt = f\"Question: {question} Answer:\" \n+>>> prompt = f\"Question: {question} Answer:\"\n ```\n \n Ïù¥Ï†ú Î™®Îç∏Ïùò ÌîÑÎ°úÏÑ∏ÏÑúÎ°ú Ïù¥ÎØ∏ÏßÄ/ÌîÑÎ°¨ÌîÑÌä∏Î•º Ï†ÑÏ≤òÎ¶¨ÌïòÍ≥†, Ï≤òÎ¶¨Îêú ÏûÖÎ†•ÏùÑ Î™®Îç∏ÏùÑ ÌÜµÌï¥ Ï†ÑÎã¨ÌïòÍ≥†, Ï∂úÎ†•ÏùÑ ÎîîÏΩîÎìúÌï¥Ïïº Ìï©ÎãàÎã§:\n@@ -369,7 +369,7 @@ BLIP-2Î•º ÏãúÍ∞ÅÏ†Å ÏßàÏùòÏùëÎãµ ÏûëÏóÖÏóê ÏÇ¨Ïö©ÌïòÎ†§Î©¥ ÌÖçÏä§Ìä∏ ÌîÑÎ°¨ÌîÑ\n >>> generated_ids = model.generate(**inputs, max_new_tokens=10)\n >>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n >>> print(generated_text)\n-\"He is looking at the crowd\" \n+\"He is looking at the crowd\"\n ```\n \n Î≥¥ÏãúÎã§ÏãúÌîº Î™®Îç∏ÏùÄ Íµ∞Ï§ëÏùÑ Ïù∏ÏãùÌïòÍ≥†, ÏñºÍµ¥Ïùò Î∞©Ìñ•(ÏïÑÎûòÏ™ΩÏùÑ Î≥¥Í≥† ÏûàÏùå)ÏùÑ Ïù∏ÏãùÌñàÏßÄÎßå, Íµ∞Ï§ëÏù¥ Ïä§ÏºÄÏù¥ÌÑ∞ Îí§Ïóê ÏûàÎã§Îäî ÏÇ¨Ïã§ÏùÑ ÎÜìÏ≥§ÏäµÎãàÎã§. Í∑∏Îü¨ÎÇò ÏÇ¨ÎûåÏù¥ ÏßÅÏ†ë ÎùºÎ≤®ÎßÅÌïú Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ ÏñªÏùÑ Ïàò ÏóÜÎäî Í≤ΩÏö∞Ïóê, Ïù¥ Ï†ëÍ∑ºÎ≤ïÏùÄ Îπ†Î•¥Í≤å Ïú†Ïö©Ìïú Í≤∞Í≥ºÎ•º ÏÉùÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§."
        },
        {
            "sha": "a2e6865c92e5b619d2427c972bf54122dfcc9096",
            "filename": "docs/source/pt/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fpt%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fpt%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Ftasks%2Fsequence_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -134,7 +134,7 @@ Nesse ponto, restam apenas tr√™s passos:\n ...     args=training_args,\n ...     train_dataset=tokenized_imdb[\"train\"],\n ...     eval_dataset=tokenized_imdb[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n \n@@ -213,4 +213,4 @@ Chame o m√©todo [`fit`](https://keras.io/api/models/model_training_apis/#fit-met\n \n Para obter um exemplo mais aprofundado de como executar o fine-tuning de um modelo para classifica√ß√£o de texto, d√™ uma olhada nesse [notebook utilizando PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb) ou nesse [notebook utilizando TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n \n-</Tip>\n\\ No newline at end of file\n+</Tip>"
        },
        {
            "sha": "45ce0d87429c58cf2f27b7b50666cb42df100ec4",
            "filename": "docs/source/pt/tasks/token_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fpt%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fpt%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Ftasks%2Ftoken_classification.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -193,7 +193,7 @@ Nesse ponto, restam apenas tr√™s passos:\n ...     args=training_args,\n ...     train_dataset=tokenized_wnut[\"train\"],\n ...     eval_dataset=tokenized_wnut[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )\n \n@@ -269,4 +269,4 @@ Chame o m√©todo [`fit`](https://keras.io/api/models/model_training_apis/#fit-met\n \n Para obter um exemplo mais aprofundado de como executar o fine-tuning de um modelo para classifica√ß√£o de tokens, d√™ uma olhada nesse [notebook utilizando PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb) ou nesse [notebook utilizando TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n \n-</Tip>\n\\ No newline at end of file\n+</Tip>"
        },
        {
            "sha": "67e530f35f3294f366de0cbc1f38968afc20cf7d",
            "filename": "docs/source/te/quicktour.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fte%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fte%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fte%2Fquicktour.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -142,7 +142,7 @@ label: NEGATIVE, with score: 0.5309\n ```\n \n <frameworkcontent>\n-<pt> \n+<pt>\n ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞ó‡∞æ ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞™‡±ä‡∞Ç‡∞¶‡∞ø‡∞® ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø [`AutoModelForSequenceClassification`] ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å [`AutoTokenizer`]‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¶‡∞æ‡∞®‡∞ø ‡∞Ö‡∞®‡±Å‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç (‡∞§‡∞¶‡±Å‡∞™‡∞∞‡∞ø ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞Ç‡∞≤‡±ã `AutoClass`‡∞™‡±à ‡∞Æ‡∞∞‡∞ø‡∞®‡±ç‡∞®‡∞ø):\n \n ```py\n@@ -154,7 +154,7 @@ label: NEGATIVE, with score: 0.5309\n </pt>\n <tf>\n ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å‡∞ó‡∞æ ‡∞∂‡∞ø‡∞ï‡±ç‡∞∑‡∞£ ‡∞™‡±ä‡∞Ç‡∞¶‡∞ø‡∞® ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞≤‡±ã‡∞°‡±ç ‡∞ö‡±á‡∞Ø‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø [`TFAutoModelForSequenceClassification`] ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å [`AutoTokenizer`]‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞¶‡∞æ‡∞®‡∞ø ‡∞Ö‡∞®‡±Å‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç (‡∞§‡∞¶‡±Å‡∞™‡∞∞‡∞ø ‡∞µ‡∞ø‡∞≠‡∞æ‡∞ó‡∞Ç‡∞≤‡±ã `TFAutoClass`‡∞™‡±à ‡∞Æ‡∞∞‡∞ø‡∞®‡±ç‡∞®‡∞ø):\n-  \n+\n ```py\n >>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n \n@@ -329,7 +329,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n <frameworkcontent>\n <pt>\n ‡∞Æ‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ö‡∞ï‡±ç‡∞ï‡∞ó‡∞æ ‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞¶‡∞æ‡∞®‡∞ø‡∞®‡∞ø [`PreTrainedModel.save_pretrained`]‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞¶‡∞æ‡∞®‡∞ø ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç‚Äå‡∞§‡±ã ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å:\n-  \n+\n ```py\n >>> pt_save_directory = \"./pt_save_pretrained\"\n >>> tokenizer.save_pretrained(pt_save_directory)  # doctest: +IGNORE_RESULT\n@@ -344,7 +344,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n </pt>\n <tf>\n ‡∞Æ‡±Ä ‡∞Æ‡±ã‡∞°‡∞≤‡±ç ‡∞ö‡∞ï‡±ç‡∞ï‡∞ó‡∞æ ‡∞ü‡±ç‡∞Ø‡±Ç‡∞®‡±ç ‡∞ö‡±á‡∞Ø‡∞¨‡∞°‡∞ø‡∞® ‡∞§‡∞∞‡±ç‡∞µ‡∞æ‡∞§, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞¶‡∞æ‡∞®‡∞ø‡∞®‡∞ø [`TFPreTrainedModel.save_pretrained`]‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞¶‡∞æ‡∞®‡∞ø ‡∞ü‡±ã‡∞ï‡±Ü‡∞®‡±à‡∞ú‡∞∞‡±ç‚Äå‡∞§‡±ã ‡∞∏‡±á‡∞µ‡±ç ‡∞ö‡±á‡∞Ø‡∞µ‡∞ö‡±ç‡∞ö‡±Å:\n-  \n+\n ```py\n >>> tf_save_directory = \"./tf_save_pretrained\"\n >>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n@@ -395,7 +395,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n <frameworkcontent>\n <pt>\n [`AutoModel.from_config`]‡∞§‡±ã ‡∞Æ‡±Ä ‡∞Ö‡∞®‡±Å‡∞ï‡±Ç‡∞≤ ‡∞ï‡∞æ‡∞®‡±ç‡∞´‡∞ø‡∞ó‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø:\n-  \n+\n ```py\n >>> from transformers import AutoModel\n \n@@ -404,7 +404,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n </pt>\n <tf>\n [`TFAutoModel.from_config`]‡∞§‡±ã ‡∞Æ‡±Ä ‡∞Ö‡∞®‡±Å‡∞ï‡±Ç‡∞≤ ‡∞ï‡∞æ‡∞®‡±ç‡∞´‡∞ø‡∞ó‡∞∞‡±á‡∞∑‡∞®‡±ç ‡∞®‡±Å‡∞Ç‡∞°‡∞ø ‡∞Æ‡±ã‡∞°‡∞≤‡±ç‚Äå‡∞®‡±Å ‡∞∏‡±É‡∞∑‡±ç‡∞ü‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø:\n-  \n+\n ```py\n >>> from transformers import TFAutoModel\n \n@@ -465,7 +465,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n    ```\n \n    ‡∞Ü‡∞™‡±à ‡∞¶‡∞æ‡∞®‡∞ø‡∞®‡∞ø [`~datasets.Dataset.map`]‡∞§‡±ã ‡∞Æ‡±ä‡∞§‡±ç‡∞§‡∞Ç ‡∞°‡±á‡∞ü‡∞æ‡∞∏‡±Ü‡∞ü‡±ç‚Äå‡∞≤‡±ã ‡∞µ‡∞∞‡±ç‡∞§‡∞ø‡∞Ç‡∞™‡∞ú‡±á‡∞Ø‡∞Ç‡∞°‡∞ø:\n-   \n+\n    ```py\n    >>> dataset = dataset.map(tokenize_dataset, batched=True)\n    ```\n@@ -488,7 +488,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ...     args=training_args,\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )  # doctest: +SKIP\n ```"
        },
        {
            "sha": "907be0a21fa8ae2899e1e6b0bfe32b0aa8747135",
            "filename": "docs/source/zh/hpo_train.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fzh%2Fhpo_train.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fhpo_train.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -15,7 +15,7 @@ rendered properly in your Markdown viewer.\n \n # ‰ΩøÁî®Trainer APIËøõË°åË∂ÖÂèÇÊï∞ÊêúÁ¥¢\n \n-ü§ó TransformersÂ∫ìÊèê‰æõ‰∫Ü‰∏Ä‰∏™‰ºòÂåñËøáÁöÑ[`Trainer`]Á±ªÔºåÁî®‰∫éËÆ≠ÁªÉü§ó TransformersÊ®°ÂûãÔºåÁõ∏ÊØî‰∫éÊâãÂä®ÁºñÂÜôËá™Â∑±ÁöÑËÆ≠ÁªÉÂæ™ÁéØÔºåËøôÊõ¥ÂÆπÊòìÂºÄÂßãËÆ≠ÁªÉ„ÄÇ[`Trainer`]Êèê‰æõ‰∫ÜË∂ÖÂèÇÊï∞ÊêúÁ¥¢ÁöÑAPI„ÄÇÊú¨ÊñáÊ°£Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂú®Á§∫‰æã‰∏≠ÂêØÁî®ÂÆÉ„ÄÇ \n+ü§ó TransformersÂ∫ìÊèê‰æõ‰∫Ü‰∏Ä‰∏™‰ºòÂåñËøáÁöÑ[`Trainer`]Á±ªÔºåÁî®‰∫éËÆ≠ÁªÉü§ó TransformersÊ®°ÂûãÔºåÁõ∏ÊØî‰∫éÊâãÂä®ÁºñÂÜôËá™Â∑±ÁöÑËÆ≠ÁªÉÂæ™ÁéØÔºåËøôÊõ¥ÂÆπÊòìÂºÄÂßãËÆ≠ÁªÉ„ÄÇ[`Trainer`]Êèê‰æõ‰∫ÜË∂ÖÂèÇÊï∞ÊêúÁ¥¢ÁöÑAPI„ÄÇÊú¨ÊñáÊ°£Â±ïÁ§∫‰∫ÜÂ¶Ç‰ΩïÂú®Á§∫‰æã‰∏≠ÂêØÁî®ÂÆÉ„ÄÇ\n \n \n ## Ë∂ÖÂèÇÊï∞ÊêúÁ¥¢ÂêéÁ´Ø\n@@ -25,7 +25,7 @@ rendered properly in your Markdown viewer.\n Âú®‰ΩøÁî®ÂÆÉ‰ª¨‰πãÂâçÔºåÊÇ®Â∫îËØ•ÂÖàÂÆâË£ÖÂÆÉ‰ª¨‰Ωú‰∏∫Ë∂ÖÂèÇÊï∞ÊêúÁ¥¢ÂêéÁ´Ø„ÄÇ\n \n ```bash\n-pip install optuna/sigopt/wandb/ray[tune] \n+pip install optuna/sigopt/wandb/ray[tune]\n ```\n \n ## Â¶Ç‰ΩïÂú®Á§∫‰æã‰∏≠ÂêØÁî®Ë∂ÖÂèÇÊï∞ÊêúÁ¥¢\n@@ -115,7 +115,7 @@ OptunaÊèê‰æõ‰∫ÜÂ§öÁõÆÊ†áHPO„ÄÇÊÇ®ÂèØ‰ª•Âú®`hyperparameter_search`‰∏≠‰º†ÈÄí`direc\n ...     train_dataset=small_train_dataset,\n ...     eval_dataset=small_eval_dataset,\n ...     compute_metrics=compute_metrics,\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     model_init=model_init,\n ...     data_collator=data_collator,\n ... )\n@@ -136,4 +136,4 @@ OptunaÊèê‰æõ‰∫ÜÂ§öÁõÆÊ†áHPO„ÄÇÊÇ®ÂèØ‰ª•Âú®`hyperparameter_search`‰∏≠‰º†ÈÄí`direc\n ```\n \n ## ÈíàÂØπDDPÂæÆË∞ÉÁöÑË∂ÖÂèÇÊï∞ÊêúÁ¥¢\n-ÁõÆÂâçÔºåOptunaÂíåSigoptÂ∑≤ÂêØÁî®ÈíàÂØπDDPÁöÑË∂ÖÂèÇÊï∞ÊêúÁ¥¢„ÄÇÂè™Êúârank-zeroËøõÁ®ã‰ºöËøõË°åË∂ÖÂèÇÊï∞ÊêúÁ¥¢Âπ∂Â∞ÜÂèÇÊï∞‰º†ÈÄíÁªôÂÖ∂‰ªñËøõÁ®ã„ÄÇ\n\\ No newline at end of file\n+ÁõÆÂâçÔºåOptunaÂíåSigoptÂ∑≤ÂêØÁî®ÈíàÂØπDDPÁöÑË∂ÖÂèÇÊï∞ÊêúÁ¥¢„ÄÇÂè™Êúârank-zeroËøõÁ®ã‰ºöËøõË°åË∂ÖÂèÇÊï∞ÊêúÁ¥¢Âπ∂Â∞ÜÂèÇÊï∞‰º†ÈÄíÁªôÂÖ∂‰ªñËøõÁ®ã„ÄÇ"
        },
        {
            "sha": "acc59539712820dbd3ae7ccb2e603962cec02818",
            "filename": "docs/source/zh/quicktour.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fzh%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fzh%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fquicktour.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -476,7 +476,7 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ...     args=training_args,\n ...     train_dataset=dataset[\"train\"],\n ...     eval_dataset=dataset[\"test\"],\n-...     tokenizer=tokenizer,\n+...     processing_class=tokenizer,\n ...     data_collator=data_collator,\n ... )  # doctest: +SKIP\n ```"
        },
        {
            "sha": "a4fdbd308e4badc4d5a7ae13265a7c2a3784bc65",
            "filename": "docs/source/zh/tasks/asr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -298,7 +298,7 @@ Wav2Vec2 ÂàÜËØçÂô®‰ªÖËÆ≠ÁªÉ‰∫ÜÂ§ßÂÜôÂ≠óÁ¨¶ÔºåÂõ†Ê≠§ÊÇ®ÈúÄË¶ÅÁ°Æ‰øùÊñáÊú¨‰∏éÂàÜ\n ...     args=training_args,\n ...     train_dataset=encoded_minds[\"train\"],\n ...     eval_dataset=encoded_minds[\"test\"],\n-...     tokenizer=processor,\n+...     processing_class=processor,\n ...     data_collator=data_collator,\n ...     compute_metrics=compute_metrics,\n ... )\n@@ -389,4 +389,4 @@ Wav2Vec2 ÂàÜËØçÂô®‰ªÖËÆ≠ÁªÉ‰∫ÜÂ§ßÂÜôÂ≠óÁ¨¶ÔºåÂõ†Ê≠§ÊÇ®ÈúÄË¶ÅÁ°Æ‰øùÊñáÊú¨‰∏éÂàÜ\n ['I WOUL LIKE O SET UP JOINT ACOUNT WTH Y PARTNER']\n ```\n </pt>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "5ede86ee082265220a1a5453919467a9d3cba815",
            "filename": "examples/legacy/seq2seq/finetune_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -302,7 +302,7 @@ def main():\n             tokenizer, data_args, model.config.decoder_start_token_id, training_args.tpu_num_cores\n         ),\n         compute_metrics=compute_metrics_fn,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n     )\n \n     all_metrics = {}"
        },
        {
            "sha": "009a1f6372433a1c6d5c424f500147d2b81a4f30",
            "filename": "examples/pytorch/audio-classification/run_audio_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Faudio-classification%2Frun_audio_classification.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -394,7 +394,7 @@ def compute_metrics(eval_pred):\n         train_dataset=raw_datasets[\"train\"] if training_args.do_train else None,\n         eval_dataset=raw_datasets[\"eval\"] if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=feature_extractor,\n+        processing_class=feature_extractor,\n     )\n \n     # Training"
        },
        {
            "sha": "0a9789426c2c461329bf830a32df826f41c3f1d6",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -396,7 +396,7 @@ def val_transforms(example_batch):\n         train_dataset=dataset[\"train\"] if training_args.do_train else None,\n         eval_dataset=dataset[\"validation\"] if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=image_processor,\n+        processing_class=image_processor,\n         data_collator=collate_fn,\n     )\n "
        },
        {
            "sha": "46863cbbf1ce3eb197fb73e4e1d76d4402d24d8b",
            "filename": "examples/pytorch/image-pretraining/run_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -364,7 +364,7 @@ def preprocess_images(examples):\n         args=training_args,\n         train_dataset=ds[\"train\"] if training_args.do_train else None,\n         eval_dataset=ds[\"validation\"] if training_args.do_eval else None,\n-        tokenizer=image_processor,\n+        processing_class=image_processor,\n         data_collator=collate_fn,\n     )\n "
        },
        {
            "sha": "3912c693440192c9dc7f61974de0607e7a2e86fa",
            "filename": "examples/pytorch/image-pretraining/run_mim.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -443,7 +443,7 @@ def preprocess_images(examples):\n         args=training_args,\n         train_dataset=ds[\"train\"] if training_args.do_train else None,\n         eval_dataset=ds[\"validation\"] if training_args.do_eval else None,\n-        tokenizer=image_processor,\n+        processing_class=image_processor,\n         data_collator=collate_fn,\n     )\n "
        },
        {
            "sha": "aeb78f95d28878ebdd6d15151e6fcb490fc54c13",
            "filename": "examples/pytorch/instance-segmentation/run_instance_segmentation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Finstance-segmentation%2Frun_instance_segmentation.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -445,7 +445,7 @@ def main():\n         args=training_args,\n         train_dataset=dataset[\"train\"] if training_args.do_train else None,\n         eval_dataset=dataset[\"validation\"] if training_args.do_eval else None,\n-        tokenizer=image_processor,\n+        processing_class=image_processor,\n         data_collator=collate_fn,\n         compute_metrics=compute_metrics,\n     )"
        },
        {
            "sha": "656571eb37e40e4f5b562f88c55f0f090a10c7cf",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -586,7 +586,7 @@ def compute_metrics(eval_preds):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         # Data collator will default to DataCollatorWithPadding, so we change it.\n         data_collator=default_data_collator,\n         compute_metrics=compute_metrics if training_args.do_eval and not is_torch_xla_available() else None,"
        },
        {
            "sha": "154fc1518384e4205aae0e87c72b84835294b417",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -793,7 +793,7 @@ def compute_metrics(eval_preds):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         # Data collator will default to DataCollatorWithPadding, so we change it.\n         data_collator=default_data_collator,\n         compute_metrics=compute_metrics if training_args.do_eval and not is_torch_tpu_available() else None,"
        },
        {
            "sha": "d021318ae065d961710294c1fad374afb5896095",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -622,7 +622,7 @@ def compute_metrics(eval_preds):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics if training_args.do_eval and not is_torch_xla_available() else None,\n         preprocess_logits_for_metrics=preprocess_logits_for_metrics"
        },
        {
            "sha": "0a207b80479ce23d129d373cea35d5720b118b15",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -519,7 +519,7 @@ def group_texts(examples):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_classtokenizer=tokenizer,\n         data_collator=data_collator,\n     )\n "
        },
        {
            "sha": "ac5db5f6b02727a0c29f3e09f48666a0fc4701c4",
            "filename": "examples/pytorch/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -440,7 +440,7 @@ def compute_metrics(eval_predictions):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics,\n     )"
        },
        {
            "sha": "0aea1a11c14ca97d27dadd291bbe46a3692a341e",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -488,7 +488,7 @@ def main():\n         args=training_args,\n         train_dataset=dataset[\"train\"] if training_args.do_train else None,\n         eval_dataset=dataset[\"validation\"] if training_args.do_eval else None,\n-        tokenizer=image_processor,\n+        processing_class=image_processor,\n         data_collator=collate_fn,\n         compute_metrics=eval_compute_metrics_fn,\n     )"
        },
        {
            "sha": "bb0a64559261976bf63cbc09254d58bc67ed217f",
            "filename": "examples/pytorch/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -640,7 +640,7 @@ def compute_metrics(p: EvalPrediction):\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         eval_examples=eval_examples if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         post_process_function=post_processing_function,\n         compute_metrics=compute_metrics,"
        },
        {
            "sha": "b3d9ee1e9c793457bf0418912db6554936ed55ef",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -666,7 +666,7 @@ def compute_metrics(p: EvalPrediction):\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         eval_examples=eval_examples if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         post_process_function=post_processing_function,\n         compute_metrics=compute_metrics,"
        },
        {
            "sha": "7cf50cf94a03b05a9623a5bb5a4ed5405326ae65",
            "filename": "examples/pytorch/question-answering/run_seq2seq_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_seq2seq_qa.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -663,7 +663,7 @@ def post_processing_function(\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         eval_examples=eval_examples if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n         post_process_function=post_processing_function,"
        },
        {
            "sha": "4c119dcbb4a450681e9c63a203d63bccfe52f886",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -403,7 +403,7 @@ def preprocess_batch(example_batch, transforms: A.Compose):\n         train_dataset=dataset[\"train\"] if training_args.do_train else None,\n         eval_dataset=dataset[\"validation\"] if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=image_processor,\n+        processing_class=image_processor,\n         data_collator=default_data_collator,\n     )\n "
        },
        {
            "sha": "ff5da5ed49ad686e71efdf1237abee2770c9fab5",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -751,7 +751,7 @@ def compute_metrics(pred):\n         compute_metrics=compute_metrics,\n         train_dataset=vectorized_datasets[\"train\"] if training_args.do_train else None,\n         eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n-        tokenizer=processor,\n+        processing_class=processor,\n         preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n     )\n "
        },
        {
            "sha": "66a75ca5d09269f04e9ef2cdaa25706e41cbc600",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -747,7 +747,7 @@ def compute_metrics(pred):\n         compute_metrics=compute_metrics,\n         train_dataset=vectorized_datasets[\"train\"] if training_args.do_train else None,\n         eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n-        tokenizer=processor,\n+        processing_class=processor,\n     )\n \n     # 8. Finally, we can start training"
        },
        {
            "sha": "8740ec5f88fa65c2a0d247de9a7693b348283c5a",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_seq2seq.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -569,7 +569,7 @@ def compute_metrics(pred):\n         args=training_args,\n         train_dataset=vectorized_datasets[\"train\"] if training_args.do_train else None,\n         eval_dataset=vectorized_datasets[\"eval\"] if training_args.do_eval else None,\n-        tokenizer=feature_extractor,\n+        processing_class=feature_extractor,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n     )"
        },
        {
            "sha": "9a25d944053ee23b5d36a3cd0d5d9c804da804d3",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -677,7 +677,7 @@ def compute_metrics(eval_preds):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n     )"
        },
        {
            "sha": "a440a48110aa414f66c3586c8ea81262759a5630",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -674,7 +674,7 @@ def compute_metrics(p: EvalPrediction):\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n     )\n "
        },
        {
            "sha": "4284fdf12f80a2989437b8aff78e470e63e54851",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -531,7 +531,7 @@ def compute_metrics(p: EvalPrediction):\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n     )\n "
        },
        {
            "sha": "6578e96dc9c585f035bb743332ecaf5b697a5f04",
            "filename": "examples/pytorch/text-classification/run_xnli.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_xnli.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -393,7 +393,7 @@ def compute_metrics(p: EvalPrediction):\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n     )\n "
        },
        {
            "sha": "ef1c0ac917b76702838ff6a842b6e8e33f07d1b1",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -567,7 +567,7 @@ def compute_metrics(p):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics,\n     )"
        },
        {
            "sha": "4e164010185ea5ce1f9a14f584b6d3bb9aa1b9a4",
            "filename": "examples/pytorch/translation/run_translation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/examples%2Fpytorch%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -597,7 +597,7 @@ def compute_metrics(eval_preds):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n         compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n     )"
        },
        {
            "sha": "5f0ac55d0eb5fd7ac2d6800c904610e4d4b74a8c",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -915,7 +915,7 @@ def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwarg\n         if self._log_model.is_enabled and self._initialized and state.is_world_process_zero:\n             from ..trainer import Trainer\n \n-            fake_trainer = Trainer(args=args, model=model, tokenizer=tokenizer)\n+            fake_trainer = Trainer(args=args, model=model, processing_class=tokenizer)\n             with tempfile.TemporaryDirectory() as temp_dir:\n                 fake_trainer.save_model(temp_dir)\n                 metadata = (\n@@ -2112,7 +2112,7 @@ def on_train_end(self, args, state, control, **kwargs):\n             from transformers.trainer import Trainer\n \n             if self._log_model is True:\n-                fake_trainer = Trainer(args=args, model=kwargs.get(\"model\"), tokenizer=kwargs.get(\"tokenizer\"))\n+                fake_trainer = Trainer(args=args, model=kwargs.get(\"model\"), processing_class=kwargs.get(\"tokenizer\"))\n                 name = \"best\" if args.load_best_model_at_end else \"last\"\n                 output_dir = os.path.join(args.output_dir, name)\n                 fake_trainer.save_model(output_dir)"
        },
        {
            "sha": "6ca4520fca582f70b26fd0a49cb087f5b80d7e7d",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 46,
            "deletions": 15,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -60,7 +60,9 @@\n from .data.data_collator import DataCollator, DataCollatorWithPadding, default_data_collator\n from .debug_utils import DebugOption, DebugUnderflowOverflow\n from .feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from .feature_extraction_utils import FeatureExtractionMixin\n from .hyperparameter_search import ALL_HYPERPARAMETER_SEARCH_BACKENDS, default_hp_search_backend\n+from .image_processing_utils import BaseImageProcessor\n from .integrations.deepspeed import deepspeed_init, deepspeed_load_checkpoint, is_deepspeed_available\n from .integrations.tpu import tpu_spmd_dataloader\n from .modelcard import TrainingSummary\n@@ -70,6 +72,7 @@\n     MODEL_MAPPING_NAMES,\n )\n from .optimization import Adafactor, get_scheduler\n+from .processing_utils import ProcessorMixin\n from .pytorch_utils import (\n     ALL_LAYERNORM_LAYERS,\n     is_torch_greater_or_equal_than_1_13,\n@@ -308,8 +311,8 @@ class Trainer:\n             `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n         data_collator (`DataCollator`, *optional*):\n             The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n-            default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n-            [`DataCollatorWithPadding`] otherwise.\n+            default to [`default_data_collator`] if no `processing_class` is provided, an instance of\n+            [`DataCollatorWithPadding`] otherwise if the processing_class is a feature extractor or tokenizer.\n         train_dataset (Union[`torch.utils.data.Dataset`, `torch.utils.data.IterableDataset`, `datasets.Dataset`], *optional*):\n             The dataset to use for training. If it is a [`~datasets.Dataset`], columns not accepted by the\n             `model.forward()` method are automatically removed.\n@@ -327,6 +330,12 @@ class Trainer:\n             The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs to the\n             maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n             interrupted training or reuse the fine-tuned model.\n+            This is now deprecated.\n+        processing_class (`PreTrainedTokenizerBase` or `BaseImageProcessor` or `FeatureExtractionMixin` or `ProcessorMixin`, *optional*):\n+            Processing class used to process the data. If provided, will be used to automatically process the inputs\n+            for the model, and it will be saved along the model to make it easier to rerun an interrupted training or\n+            reuse the fine-tuned model.\n+            This supercedes the `tokenizer` argument, which is now deprecated.\n         model_init (`Callable[[], PreTrainedModel]`, *optional*):\n             A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n             from a new instance of the model as given by this function.\n@@ -384,6 +393,9 @@ def __init__(\n         train_dataset: Optional[Union[Dataset, IterableDataset, \"datasets.Dataset\"]] = None,\n         eval_dataset: Optional[Union[Dataset, Dict[str, Dataset], \"datasets.Dataset\"]] = None,\n         tokenizer: Optional[PreTrainedTokenizerBase] = None,\n+        processing_class: Optional[\n+            Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]\n+        ] = None,\n         model_init: Optional[Callable[[], PreTrainedModel]] = None,\n         compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n         callbacks: Optional[List[TrainerCallback]] = None,\n@@ -425,6 +437,17 @@ def __init__(\n         # force device and distributed setup init explicitly\n         args._setup_devices\n \n+        if tokenizer is not None:\n+            if processing_class is not None:\n+                raise ValueError(\n+                    \"You cannot specify both `tokenizer` and `processing_class` at the same time. Please use `processing_class`.\"\n+                )\n+            warnings.warn(\n+                \"`tokenizer` is now deprecated and will be removed in v5, please use `processing_class` instead.\",\n+                FutureWarning,\n+            )\n+            processing_class = tokenizer\n+\n         if model is None:\n             if model_init is not None:\n                 self.model_init = model_init\n@@ -541,14 +564,15 @@ def __init__(\n             self.place_model_on_device = False\n \n         default_collator = (\n-            DataCollatorWithPadding(tokenizer)\n-            if tokenizer is not None and isinstance(tokenizer, (PreTrainedTokenizerBase, SequenceFeatureExtractor))\n+            DataCollatorWithPadding(processing_class)\n+            if processing_class is not None\n+            and isinstance(processing_class, (PreTrainedTokenizerBase, SequenceFeatureExtractor))\n             else default_data_collator\n         )\n         self.data_collator = data_collator if data_collator is not None else default_collator\n         self.train_dataset = train_dataset\n         self.eval_dataset = eval_dataset\n-        self.tokenizer = tokenizer\n+        self.processing_class = processing_class\n \n         # Bnb Quantized models doesn't support `.to` operation.\n         if (\n@@ -600,7 +624,7 @@ def __init__(\n         default_callbacks = DEFAULT_CALLBACKS + get_reporting_integration_callbacks(self.args.report_to)\n         callbacks = default_callbacks if callbacks is None else default_callbacks + callbacks\n         self.callback_handler = CallbackHandler(\n-            callbacks, self.model, self.tokenizer, self.optimizer, self.lr_scheduler\n+            callbacks, self.model, self.processing_class, self.optimizer, self.lr_scheduler\n         )\n         self.add_callback(PrinterCallback if self.args.disable_tqdm else DEFAULT_PROGRESS_CALLBACK)\n \n@@ -728,6 +752,11 @@ def __init__(\n             xs.set_global_mesh(xs.Mesh(np.array(range(num_devices)), (num_devices, 1), axis_names=(\"fsdp\", \"tensor\")))\n         self.is_fsdp_xla_v1_enabled = self.is_fsdp_xla_enabled and not self.is_fsdp_xla_v2_enabled\n \n+    @property\n+    def tokenizer(self) -> Optional[PreTrainedTokenizerBase]:\n+        logger.warning(\"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\")\n+        return self.processing_class\n+\n     def _activate_neftune(self, model):\n         r\"\"\"\n         Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:\n@@ -887,7 +916,9 @@ def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n                 )\n             else:\n                 lengths = None\n-            model_input_name = self.tokenizer.model_input_names[0] if self.tokenizer is not None else None\n+            model_input_name = (\n+                self.processing_class.model_input_names[0] if self.processing_class is not None else None\n+            )\n             return LengthGroupedSampler(\n                 self.args.train_batch_size * self.args.gradient_accumulation_steps,\n                 dataset=self.train_dataset,\n@@ -3699,8 +3730,8 @@ def _save_tpu(self, output_dir: Optional[str] = None):\n                 safe_serialization=self.args.save_safetensors,\n                 state_dict=xm._maybe_convert_to_cpu(model.state_dict()),\n             )\n-        if self.tokenizer is not None and self.args.should_save:\n-            self.tokenizer.save_pretrained(output_dir)\n+        if self.processing_class is not None and self.args.should_save:\n+            self.processing_class.save_pretrained(output_dir)\n \n     def _save(self, output_dir: Optional[str] = None, state_dict=None):\n         # If we are executing this function, we are the process zero, so we don't check for that.\n@@ -3732,8 +3763,8 @@ def _save(self, output_dir: Optional[str] = None, state_dict=None):\n                 output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n             )\n \n-        if self.tokenizer is not None:\n-            self.tokenizer.save_pretrained(output_dir)\n+        if self.processing_class is not None:\n+            self.processing_class.save_pretrained(output_dir)\n \n         # Good practice: save your training arguments together with the trained model\n         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n@@ -4442,9 +4473,9 @@ def _push_from_checkpoint(self, checkpoint_folder):\n         for modeling_file in modeling_files:\n             if os.path.isfile(os.path.join(checkpoint_folder, modeling_file)):\n                 shutil.copy(os.path.join(checkpoint_folder, modeling_file), os.path.join(output_dir, modeling_file))\n-        # Saving the tokenizer is fast and we don't know how many files it may have spawned, so we resave it to be sure.\n-        if self.tokenizer is not None:\n-            self.tokenizer.save_pretrained(output_dir)\n+        # Saving the processing class is fast and we don't know how many files it may have spawned, so we resave it to be sure.\n+        if self.processing_class is not None:\n+            self.processing_class.save_pretrained(output_dir)\n         # Same for the training arguments\n         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n \n@@ -4499,7 +4530,7 @@ def push_to_hub(\n         **kwargs,\n     ) -> str:\n         \"\"\"\n-        Upload `self.model` and `self.tokenizer` to the ü§ó model hub on the repo `self.args.hub_model_id`.\n+        Upload `self.model` and `self.processing_class` to the ü§ó model hub on the repo `self.args.hub_model_id`.\n \n         Parameters:\n             commit_message (`str`, *optional*, defaults to `\"End of training\"`):"
        },
        {
            "sha": "405874acf8f4c4f6640db7244938774130cadef6",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -272,7 +272,9 @@ class TrainerCallback:\n         model ([`PreTrainedModel`] or `torch.nn.Module`):\n             The model being trained.\n         tokenizer ([`PreTrainedTokenizer`]):\n-            The tokenizer used for encoding the data.\n+            The tokenizer used for encoding the data. This is deprecated in favour of `processing_class`.\n+        processing_class ([`PreTrainedTokenizer` or `BaseImageProcessor` or `ProcessorMixin` or `FeatureExtractionMixin`]):\n+            The processing class used for encoding the data. Can be a tokenizer, a processor, an image processor or a feature extractor.\n         optimizer (`torch.optim.Optimizer`):\n             The optimizer used for the training steps.\n         lr_scheduler (`torch.optim.lr_scheduler.LambdaLR`):\n@@ -403,12 +405,12 @@ def on_prediction_step(self, args: TrainingArguments, state: TrainerState, contr\n class CallbackHandler(TrainerCallback):\n     \"\"\"Internal class that just calls the list of callbacks in order.\"\"\"\n \n-    def __init__(self, callbacks, model, tokenizer, optimizer, lr_scheduler):\n+    def __init__(self, callbacks, model, processing_class, optimizer, lr_scheduler):\n         self.callbacks = []\n         for cb in callbacks:\n             self.add_callback(cb)\n         self.model = model\n-        self.tokenizer = tokenizer\n+        self.processing_class = processing_class\n         self.optimizer = optimizer\n         self.lr_scheduler = lr_scheduler\n         self.train_dataloader = None\n@@ -518,7 +520,7 @@ def call_event(self, event, args, state, control, **kwargs):\n                 state,\n                 control,\n                 model=self.model,\n-                tokenizer=self.tokenizer,\n+                processing_class=self.processing_class,\n                 optimizer=self.optimizer,\n                 lr_scheduler=self.lr_scheduler,\n                 train_dataloader=self.train_dataloader,"
        },
        {
            "sha": "241bea786044bf50bd4cfccedbf77132b4273768",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -29,7 +29,10 @@\n \n if TYPE_CHECKING:\n     from .data.data_collator import DataCollator\n+    from .feature_extraction_utils import FeatureExtractionMixin\n+    from .image_processing_utils import BaseImageProcessor\n     from .modeling_utils import PreTrainedModel\n+    from .processing_utils import ProcessorMixin\n     from .tokenization_utils_base import PreTrainedTokenizerBase\n     from .trainer_callback import TrainerCallback\n     from .trainer_utils import EvalPrediction, PredictionOutput\n@@ -48,6 +51,9 @@ def __init__(\n         train_dataset: Optional[Dataset] = None,\n         eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,\n         tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n+        processing_class: Optional[\n+            Union[\"PreTrainedTokenizerBase\", \"BaseImageProcessor\", \"FeatureExtractionMixin\", \"ProcessorMixin\"]\n+        ] = None,\n         model_init: Optional[Callable[[], \"PreTrainedModel\"]] = None,\n         compute_metrics: Optional[Callable[[\"EvalPrediction\"], Dict]] = None,\n         callbacks: Optional[List[\"TrainerCallback\"]] = None,\n@@ -61,6 +67,7 @@ def __init__(\n             train_dataset=train_dataset,\n             eval_dataset=eval_dataset,\n             tokenizer=tokenizer,\n+            processing_class=processing_class,\n             model_init=model_init,\n             compute_metrics=compute_metrics,\n             callbacks=callbacks,"
        },
        {
            "sha": "26da84dfe2391956d3d1ca91ff3a2f60df03d080",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -683,9 +683,9 @@ class TrainingArguments:\n         hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n             Defines the scope of what is pushed to the Hub and when. Possible values are:\n \n-            - `\"end\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and a\n+            - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n               draft of a model card when the [`~Trainer.save_model`] method is called.\n-            - `\"every_save\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and\n+            - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n               a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n               training, and in case the save are very frequent, a new push is only attempted if the previous one is\n               finished. A last push is made with the final model at the end of training.\n@@ -2854,9 +2854,9 @@ def set_push_to_hub(\n             strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n                 Defines the scope of what is pushed to the Hub and when. Possible values are:\n \n-                - `\"end\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`]) and a\n+                - `\"end\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n                 draft of a model card when the [`~Trainer.save_model`] method is called.\n-                - `\"every_save\"`: push the model, its configuration, the tokenizer (if passed along to the [`Trainer`])\n+                - `\"every_save\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`])\n                   and\n                 a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n                 training, and in case the save are very frequent, a new push is only attempted if the previous one is"
        },
        {
            "sha": "c2b753c103e18478f3dfb0273c7b1e6341468498",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -450,7 +450,7 @@ def tokenize_function(examples):\n         args=training_args,\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n     )\n "
        },
        {
            "sha": "8eaa00bc76842820ca90afadcb8debc0dae23b6e",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -1036,7 +1036,7 @@ def get_dataset():\n \n             trainer = Trainer(\n                 model=model,\n-                tokenizer=tokenizer,\n+                processing_class=tokenizer,\n                 args=training_args,\n                 train_dataset=train_dataset,\n                 eval_dataset=eval_dataset,"
        },
        {
            "sha": "2e3d9fdc7f2f33467e8b9c1258518b2a42d0f814",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -455,7 +455,7 @@ def compute_metrics(p: EvalPrediction):\n         train_dataset=train_dataset if training_args.do_train else None,\n         eval_dataset=eval_dataset if training_args.do_eval else None,\n         compute_metrics=compute_metrics,\n-        tokenizer=tokenizer,\n+        processing_class=tokenizer,\n         data_collator=data_collator,\n     )\n "
        },
        {
            "sha": "8020b0e711cf1f051674443c8643b62eb69ab3f9",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -4293,7 +4293,7 @@ def test_saving_tokenizer_trainer(self):\n                     # Load tokenizer from a folder without legacy files\n                     tokenizer = self.rust_tokenizer_class.from_pretrained(tmp_dir)\n                     training_args = TrainingArguments(output_dir=tmp_dir, do_train=True, no_cuda=True)\n-                    trainer = Trainer(model=model, args=training_args, tokenizer=tokenizer)\n+                    trainer = Trainer(model=model, args=training_args, processing_class=tokenizer)\n \n                     # Should not raise an error\n                     trainer.save_model(os.path.join(tmp_dir, \"checkpoint\"))"
        },
        {
            "sha": "6ce961257de8a22629980241359bed2ac1229531",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 97,
            "deletions": 2,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -38,6 +38,9 @@\n from requests.exceptions import HTTPError\n \n from transformers import (\n+    AutoFeatureExtractor,\n+    AutoImageProcessor,\n+    AutoProcessor,\n     AutoTokenizer,\n     IntervalStrategy,\n     PretrainedConfig,\n@@ -1059,14 +1062,14 @@ def test_multiple_peft_adapters(self):\n                 max_steps=10,\n                 use_cpu=True,\n             )\n-            trainer = Trainer(tiny_model, args, tokenizer=tokenizer, train_dataset=train_dataset)\n+            trainer = Trainer(tiny_model, args, processing_class=tokenizer, train_dataset=train_dataset)\n \n             trainer.train()\n             parameters = dict(tiny_model.named_parameters())\n             state = dataclasses.asdict(trainer.state)\n \n             # Reinitialize trainer\n-            trainer = Trainer(tiny_model, args, tokenizer=tokenizer, train_dataset=train_dataset)\n+            trainer = Trainer(tiny_model, args, processing_class=tokenizer, train_dataset=train_dataset)\n \n             checkpoint = os.path.join(tmpdir, \"checkpoint-5\")\n \n@@ -3786,6 +3789,98 @@ def test_eval_use_gather_object(self):\n         _ = trainer.evaluate()\n         _ = trainer.predict(eval_dataset)\n \n+    def test_trainer_saves_tokenizer(self):\n+        MODEL_ID = \"google-bert/bert-base-uncased\"\n+        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = RegressionModelConfig(a=1.5, b=2.5)\n+            trainer = Trainer(\n+                model=RegressionPreTrainedModel(config),\n+                args=TrainingArguments(output_dir=tmp_dir),\n+                processing_class=tokenizer,\n+            )\n+            trainer.save_model()\n+\n+            reloaded_tokenizer = AutoTokenizer.from_pretrained(tmp_dir)\n+\n+        # For tokenizers, there isn't a direct to_dict method and the properties stored in the configs e.g.\n+        # saved tokens change overtime, so we check that two tokenizers are equal by comparing their encoded outputs\n+        test_sentence = \"This is a test sentence\"\n+        self.assertListEqual(\n+            tokenizer(test_sentence, padding=\"max_length\").input_ids,\n+            reloaded_tokenizer(test_sentence, padding=\"max_length\").input_ids,\n+        )\n+\n+    def test_trainer_saves_image_processor(self):\n+        MODEL_ID = \"openai/clip-vit-base-patch32\"\n+        image_processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = RegressionModelConfig(a=1.5, b=2.5)\n+            trainer = Trainer(\n+                model=RegressionPreTrainedModel(config),\n+                args=TrainingArguments(output_dir=tmp_dir),\n+                processing_class=image_processor,\n+            )\n+            trainer.save_model()\n+            reloaded_image_processor = AutoImageProcessor.from_pretrained(tmp_dir)\n+\n+        self.assertDictEqual(image_processor.to_dict(), reloaded_image_processor.to_dict())\n+\n+    def test_trainer_saves_feature_extractor(self):\n+        MODEL_ID = \"facebook/wav2vec2-base-960h\"\n+        feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID)\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = RegressionModelConfig(a=1.5, b=2.5)\n+            trainer = Trainer(\n+                model=RegressionPreTrainedModel(config),\n+                args=TrainingArguments(output_dir=tmp_dir),\n+                processing_class=feature_extractor,\n+            )\n+            trainer.save_model()\n+\n+            reloaded_feature_extractor = AutoFeatureExtractor.from_pretrained(tmp_dir)\n+\n+        self.assertDictEqual(feature_extractor.to_dict(), reloaded_feature_extractor.to_dict())\n+\n+    def test_trainer_saves_processor(self):\n+        MODEL_ID = \"openai/clip-vit-base-patch32\"\n+        image_processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n+        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)\n+        processor = AutoProcessor.from_pretrained(MODEL_ID)\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = RegressionModelConfig(a=1.5, b=2.5)\n+            trainer = Trainer(\n+                model=RegressionPreTrainedModel(config),\n+                args=TrainingArguments(output_dir=tmp_dir),\n+                processing_class=processor,\n+            )\n+            trainer.save_model()\n+\n+            reloaded_processor = AutoProcessor.from_pretrained(tmp_dir)\n+            reloaded_image_processor = AutoImageProcessor.from_pretrained(tmp_dir)\n+            reloaded_tokenizer = AutoTokenizer.from_pretrained(tmp_dir)\n+\n+        self.assertDictEqual(reloaded_processor.to_dict(), processor.to_dict())\n+\n+        image_processor_dict = image_processor.to_dict()\n+        reloaded_image_processor_dict = reloaded_image_processor.to_dict()\n+        # When the processor is saved in the trainer, the _processor_class gets set in the reload_image_processor dict\n+        image_processor_dict.pop(\"_processor_class\")\n+        reloaded_image_processor_dict.pop(\"_processor_class\")\n+        self.assertDictEqual(image_processor_dict, reloaded_image_processor_dict)\n+\n+        # For tokenizers, there isn't a direct to_dict method and the properties stored in the configs e.g.\n+        # saved tokens change overtime, so we check that two tokenizers are equal by comparing their encoded outputs\n+        test_sentence = \"This is a test sentence\"\n+        self.assertListEqual(\n+            tokenizer(test_sentence, padding=\"max_length\").input_ids,\n+            reloaded_tokenizer(test_sentence, padding=\"max_length\").input_ids,\n+        )\n+\n \n @require_torch\n @is_staging_test"
        },
        {
            "sha": "30dd2ed460c99d66a572f2695204efe2d3c02d43",
            "filename": "tests/trainer/test_trainer_seq2seq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_seq2seq.py?ref=b7474f211d4d1b6f83ba3b6cb9f942a18d3b02a1",
            "patch": "@@ -129,7 +129,7 @@ def _compute_metrics(pred):\n             compute_metrics=_compute_metrics,\n             train_dataset=train_dataset,\n             eval_dataset=val_dataset,\n-            tokenizer=tokenizer,\n+            processing_class=tokenizer,\n         )\n \n         # start training\n@@ -158,7 +158,7 @@ def test_return_sequences(self):\n         trainer = Seq2SeqTrainer(\n             model=model,\n             args=training_args,\n-            tokenizer=tokenizer,\n+            processing_class=tokenizer,\n             data_collator=data_collator,\n             compute_metrics=lambda x: {\"samples\": x[0].shape[0]},\n         )\n@@ -199,7 +199,7 @@ def test_bad_generation_config_fail_early(self):\n             _ = Seq2SeqTrainer(\n                 model=model,\n                 args=training_args,\n-                tokenizer=tokenizer,\n+                processing_class=tokenizer,\n                 data_collator=data_collator,\n                 compute_metrics=lambda x: {\"samples\": x[0].shape[0]},\n             )"
        }
    ],
    "stats": {
        "total": 1011,
        "additions": 569,
        "deletions": 442
    }
}