{
    "author": "burtenshaw",
    "message": "[MODEL] Nanochat implementation (#41634)\n\n* first draft on modelling\n\n* add modelling to auto\n\n* add to init\n\n* [WIP] add scripts for conversion\n\n* hack at attention and rotary using logit comparison unitl it works\n\n* update conversion script\n\n* fix test\n\n* tody up decoding inputs\n\n* fix all naming nanogpt >> nanochat\n\n* add return as tensors\n\n* use batch encoding\n\n* include tokenizer conversion in the conversion script\n\n* add repo revision to vocab pull\n\n* move attention into func and add kward to all signatures (for vllm)\n\n* rename everything NanoGPT >> NanoChat\n\n* improve docstrings in modelling code\n\n* run ruff\n\n* delete working test file\n\n* remove custom tokenizer\n\n* protect imports with lazy\n\n* removed unused variables\n\n* format config\n\n* sort auto model configs\n\n* make sure nanochat config is in all (fixes vllm)\n\n* integrate tests\n\n* remove custom tokenizer from autos\n\n* formate tests\n\n* add documentation for nanochat\n\n* update toc with nanochat\n\n* lose some attention notes\n\n* remove excess configuration\n\n* move init\n\n* move script into modeling dir\n\n* switch to modular and inheret llama\n\n* add causal flag\n\n* remove excess  and just set params in model config\n\n* liscence\n\n* update model flags\n\n* re use init weights\n\n* lose functionals from the MLP\n\n* use rmsnorm module instead of functional\n\n* inherit rms from NanoChatRMSNorm\n\n* run modular convert and update modeling\n\n* update docs\n\n* remove excess changes in auto\n\n* Update src/transformers/models/nanochat/configuration_nanochat.py\n\nCo-authored-by: Joshua Lochner <admin@xenova.com>\n\n* add a basic tp/pp plan\n\n* use cleanup util for tests\n\n* update conversion script\n\n* add docstring to nanochat config\n\n* update auto config with NanoChatForCausalLM\n\n* remove forward snippet\n\n* fix auto modeling\n\n* delete excess modelling file\n\n* delete excess modelling file and regenerate modeling\n\n* revert changes and use less llama modules\n\n* use llama rope not custom\n\n* cut down on notes and docstrings\n\n* revert again to custom rope but this time using Qwen3 attention\n\n* tidy notes in attention module\n\n* update cache kwargs with sin and cos\n\n* inheret from CLIPMLP and overwrite projections\n\n* inheret llama decode layer\n\n* re use more from LlamaPreTrainedModel\n\n* inherit LlamaModel and copy forward except for initial_norm\n\n* inheret more from LlamaForCausalLM\n\n* more modular friendly\n\n* adjust to changes in main\n\n* last nits\n\n* apparently we do this now lol\n\n* mustve been a blank smthn smthn\n\n* update conversion script to use original config\n\n* fix attetion_bias everywhere\n\n* resolve todos in config\n\n* todos: layer names and repo id in modular\n\n* last todos in config and modelling\n\n* usng karapthy repo\n\n* inherit from gemma instead\n\n* fix vaultgemma\n\n* ci common\n\n* do rope config after super init\n\n* update logit values in tests\n\n* update logit tests one last time\n\n* update and format conversion script\n\n* update all logits again\n\n* update test tollerence\n\n* fix tests\n\n* auto tokenizer\n\n* fix test, to be updated upstream on the hub\n\n* fix\n\n* update conversion script to deal with new d34 model\n\n* fix eval in docs\n\n* Apply suggestions from code review\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\n\n* no separate initial_norm and do not iter over module\n\n* update _tp_plan\n\n* update modeling with changes\n\n* style\n\n* fix tp_plan\n\n* oupppss this one almost slipped through nice I checked\n\n---------\n\nCo-authored-by: Joshua Lochner <admin@xenova.com>\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
    "files": [
        {
            "sha": "f625d0f24a18b4d7daa997d970d1727ff00176cf",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -620,6 +620,8 @@\n         title: MVP\n       - local: model_doc/myt5\n         title: myt5\n+      - local: model_doc/nanochat\n+        title: NanoChat\n       - local: model_doc/nemotron\n         title: Nemotron\n       - local: model_doc/nllb"
        },
        {
            "sha": "a951af07c3ca0f17fff7ebabbf707714147770d8",
            "filename": "docs/source/en/model_doc/nanochat.md",
            "status": "added",
            "additions": 118,
            "deletions": 0,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/docs%2Fsource%2Fen%2Fmodel_doc%2Fnanochat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/docs%2Fsource%2Fen%2Fmodel_doc%2Fnanochat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnanochat.md?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,118 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# NanoChat\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+[NanoChat](https://huggingface.co/karpathy/nanochat-d32) is a compact decoder-only transformer model designed for educational purposes and efficient training. The model features several fundamental architectural innovations which are common in modern transformer models. Therefore, it is a good model to use as a starting point to understand the principles of modern transformer models. NanoChat is a variant of the [Llama](https://huggingface.co/docs/transformers/en/model_doc/llama) architecture, with simplified attention mechanism and normalization layers. \n+\n+The architecture is based on [nanochat](https://github.com/karpathy/nanochat) by [Andrej Karpathy](https://huggingface.co/karpathy), adapted for the Hugging Face Transformers library by [Ben Burtenshaw](https://huggingface.co/burtenshaw).\n+\n+> [!TIP]\n+> This model was contributed by the Hugging Face team.\n+\n+The example below demonstrates how to use NanoChat for text generation with chat templates.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+chatbot = pipeline(\n+    task=\"text-generation\",\n+    model=\"karpathy/nanochat-d32\",\n+    dtype=torch.bfloat16,\n+    device=0\n+)\n+\n+conversation = [\n+    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+]\n+\n+outputs = chatbot(conversation, max_new_tokens=64)\n+print(outputs[0][\"generated_text\"][-1][\"content\"])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_id = \"karpathy/nanochat-d32\"\n+device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_id,\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+)\n+\n+conversation = [\n+    {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+]\n+\n+inputs = tokenizer.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(device)\n+\n+with torch.no_grad():\n+    outputs = model.generate(\n+        **inputs,\n+        max_new_tokens=64,\n+    )\n+\n+# Decode only the generated tokens (excluding the input prompt)\n+generated_tokens = outputs[0, inputs[\"input_ids\"].shape[1]:]\n+print(tokenizer.decode(generated_tokens, skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e '{\"role\": \"user\", \"content\": \"What is the capital of France?\"}' | transformers run --task text-generation --model karpathy/nanochat-d32 --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## NanoChatConfig\n+\n+[[autodoc]] NanoChatConfig\n+\n+## NanoChatModel\n+\n+[[autodoc]] NanoChatModel\n+    - forward\n+\n+## NanoChatForCausalLM\n+\n+[[autodoc]] NanoChatForCausalLM\n+    - forward"
        },
        {
            "sha": "f0d404718e7c5f8c3ad0bb431e749dfcf0387ef8",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -244,6 +244,7 @@\n     from .musicgen_melody import *\n     from .mvp import *\n     from .myt5 import *\n+    from .nanochat import *\n     from .nemotron import *\n     from .nllb import *\n     from .nllb_moe import *"
        },
        {
            "sha": "86f0670fea588f1037652443fdfac9d2dc7e0152",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -279,6 +279,7 @@\n         (\"musicgen\", \"MusicgenConfig\"),\n         (\"musicgen_melody\", \"MusicgenMelodyConfig\"),\n         (\"mvp\", \"MvpConfig\"),\n+        (\"nanochat\", \"NanoChatConfig\"),\n         (\"nemotron\", \"NemotronConfig\"),\n         (\"nllb-moe\", \"NllbMoeConfig\"),\n         (\"nougat\", \"VisionEncoderDecoderConfig\"),\n@@ -724,6 +725,7 @@\n         (\"musicgen_melody\", \"MusicGen Melody\"),\n         (\"mvp\", \"MVP\"),\n         (\"myt5\", \"myt5\"),\n+        (\"nanochat\", \"NanoChat\"),\n         (\"nemotron\", \"Nemotron\"),\n         (\"nllb\", \"NLLB\"),\n         (\"nllb-moe\", \"NLLB-MOE\"),"
        },
        {
            "sha": "90e7982ab23071d83d55faede336886519108e80",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -279,6 +279,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"musicgen\", \"MusicgenModel\"),\n         (\"musicgen_melody\", \"MusicgenMelodyModel\"),\n         (\"mvp\", \"MvpModel\"),\n+        (\"nanochat\", \"NanoChatModel\"),\n         (\"nemotron\", \"NemotronModel\"),\n         (\"nllb-moe\", \"NllbMoeModel\"),\n         (\"nystromformer\", \"NystromformerModel\"),\n@@ -490,6 +491,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mpt\", \"MptForCausalLM\"),\n         (\"mra\", \"MraForMaskedLM\"),\n         (\"mvp\", \"MvpForConditionalGeneration\"),\n+        (\"nanochat\", \"NanoChatForCausalLM\"),\n         (\"nllb-moe\", \"NllbMoeForConditionalGeneration\"),\n         (\"openai-gpt\", \"OpenAIGPTLMHeadModel\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n@@ -702,6 +704,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"musicgen\", \"MusicgenForCausalLM\"),\n         (\"musicgen_melody\", \"MusicgenMelodyForCausalLM\"),\n         (\"mvp\", \"MvpForCausalLM\"),\n+        (\"nanochat\", \"NanoChatForCausalLM\"),\n         (\"nemotron\", \"NemotronForCausalLM\"),\n         (\"olmo\", \"OlmoForCausalLM\"),\n         (\"olmo2\", \"Olmo2ForCausalLM\"),"
        },
        {
            "sha": "2511bb8f24e05e48cfb9bf2b715a33c28abe7e27",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -496,6 +496,7 @@\n         (\"musicgen_melody\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"mvp\", (\"MvpTokenizer\", \"MvpTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"myt5\", (\"MyT5Tokenizer\", None)),\n+        (\"nanochat\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"nemotron\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"nllb\","
        },
        {
            "sha": "8b63204f223769a27436e2861e0269ff0ea31e8c",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 61,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -40,14 +40,11 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_gemma2 import Gemma2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class Gemma2RMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -326,8 +323,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n@@ -336,14 +331,12 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -356,12 +349,7 @@ def forward(\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -418,30 +406,16 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -480,41 +454,22 @@ def forward(\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -544,11 +499,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n         Example:\n@@ -567,11 +520,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -580,8 +528,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "fd08997fece96fd83f3f8f591d8592c2e59ae2ba",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 57,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -381,8 +381,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n@@ -391,14 +389,12 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -411,12 +407,7 @@ def forward(\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class Gemma2PreTrainedModel(GemmaPreTrainedModel):\n@@ -439,30 +430,16 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -501,41 +478,22 @@ def forward(\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -554,11 +512,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n         Example:\n@@ -577,11 +533,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -590,8 +541,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "323e87db2ce53e69787a4711e1e944e1b9c7bd46",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -653,11 +653,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n         Example:\n@@ -676,11 +674,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -689,8 +682,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "8520394c682bf22ca3245ed73e56b4e47ae2806c",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -1545,8 +1545,6 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n@@ -1555,14 +1553,12 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -1575,12 +1571,7 @@ def forward(\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -1961,11 +1952,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n         Example:\n@@ -1984,11 +1973,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -1997,8 +1981,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "6610a71fa701d0f8854c96ed5b63ef389666cc90",
            "filename": "src/transformers/models/nanochat/__init__.py",
            "status": "added",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2F__init__.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,14 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_nanochat import *\n+    from .modeling_nanochat import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "998b08b31959c389280c847ecf79c02a893a6b93",
            "filename": "src/transformers/models/nanochat/configuration_nanochat.py",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,164 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import RopeParameters, rope_config_validation, standardize_rope_params\n+\n+\n+class NanoChatConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`NanoChatModel`]. It is used to instantiate a\n+    NanoChat model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the [karpathy/nanochat-d32](https://huggingface.co/karpathy/nanochat-d32).\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 50304):\n+            Vocabulary size of the NanoChat model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`NanoChatModel`].\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 8192):\n+            Dimension of the MLP representations. If `None`, it will be computed based on the model architecture.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 6):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"relu2\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionaty should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        final_logit_softcapping (`float`, *optional*, defaults to 15.0):\n+            scaling factor when applying tanh softcapping on the logits.\n+        attention_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, and value projection layers during self-attention.\n+        bos_token_id (`int`, *optional*, defaults to 0):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 1):\n+            End of stream token id.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            Padding token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+\n+    ```python\n+    >>> from transformers import NanoChatModel, NanoChatConfig\n+\n+    >>> # Initializing a NanoChat style configuration\n+    >>> configuration = NanoChatConfig()\n+\n+    >>> # Initializing a model from the NanoChat style configuration\n+    >>> model = NanoChatModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"nanochat\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",\n+        \"layers.*.mlp.fc1\": \"colwise\",\n+        \"layers.*.mlp.fc2\": \"rowwise\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size: int = 50304,\n+        hidden_size: int = 768,\n+        intermediate_size: int | None = 8192,\n+        num_hidden_layers: int = 12,\n+        num_attention_heads: int = 6,\n+        num_key_value_heads: int | None = None,\n+        max_position_embeddings: int = 2048,\n+        hidden_act: str = \"relu2\",\n+        attention_dropout: float = 0.0,\n+        rms_norm_eps: float = 1e-6,\n+        initializer_range: float = 0.02,\n+        rope_parameters: RopeParameters | dict | None = None,\n+        use_cache: bool = True,\n+        final_logit_softcapping: float | None = 15.0,\n+        attention_bias: bool = False,\n+        bos_token_id: int = 0,\n+        eos_token_id: int = 1,\n+        pad_token_id: int = 1,\n+        tie_word_embeddings: bool = False,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_act = hidden_act\n+        self.attention_dropout = attention_dropout\n+        self.rms_norm_eps = rms_norm_eps\n+        self.initializer_range = initializer_range\n+        self.use_cache = use_cache\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attention_bias = attention_bias\n+\n+        super().__init__(\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            pad_token_id=pad_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+        # Validate the correctness of rotary position embeddings parameters\n+        # Must be done after super().__init__() to avoid being overridden by kwargs\n+        self.rope_parameters = rope_parameters\n+        rope_theta = kwargs.get(\"rope_theta\", 10000.0)\n+        standardize_rope_params(self, rope_theta=rope_theta)\n+        rope_config_validation(self)\n+\n+\n+__all__ = [\"NanoChatConfig\"]"
        },
        {
            "sha": "e705bd3fa1102935709cf79579c3f56e5f05528e",
            "filename": "src/transformers/models/nanochat/convert_nanochat_checkpoints.py",
            "status": "added",
            "additions": 314,
            "deletions": 0,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconvert_nanochat_checkpoints.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconvert_nanochat_checkpoints.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconvert_nanochat_checkpoints.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,314 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import json\n+import os\n+from pathlib import Path\n+\n+import torch\n+\n+from transformers import AutoTokenizer, NanoChatConfig, NanoChatForCausalLM\n+\n+\n+def infer_kv_heads(config: NanoChatConfig, state_dict: dict[str, torch.Tensor]) -> int:\n+    key_weight = state_dict.get(\"transformer.h.0.attn.c_k.weight\")\n+    if key_weight is None:\n+        return config.num_key_value_heads\n+    rows = key_weight.shape[0]\n+    head_dim = config.hidden_size // config.num_attention_heads\n+    if rows % head_dim != 0:\n+        return config.num_key_value_heads\n+    inferred = rows // head_dim\n+    print(f\"Inferred {inferred} key_value heads from checkpoint\")\n+    return max(inferred, 1)\n+\n+\n+def convert_layer(old_prefix: str, new_prefix: str) -> dict[str, str]:\n+    return {\n+        f\"{old_prefix}.attn.c_q.weight\": f\"{new_prefix}.self_attn.q_proj.weight\",\n+        f\"{old_prefix}.attn.c_k.weight\": f\"{new_prefix}.self_attn.k_proj.weight\",\n+        f\"{old_prefix}.attn.c_v.weight\": f\"{new_prefix}.self_attn.v_proj.weight\",\n+        f\"{old_prefix}.attn.c_proj.weight\": f\"{new_prefix}.self_attn.o_proj.weight\",\n+        f\"{old_prefix}.mlp.c_fc.weight\": f\"{new_prefix}.mlp.fc1.weight\",\n+        f\"{old_prefix}.mlp.c_proj.weight\": f\"{new_prefix}.mlp.fc2.weight\",\n+    }\n+\n+\n+def load_config_from_checkpoint(input_path: Path) -> NanoChatConfig:\n+    \"\"\"Load config from either meta_*.json or config.json in the checkpoint directory.\"\"\"\n+    # Try to find meta_*.json first\n+    meta_files = list(input_path.glob(\"meta_*.json\"))\n+\n+    if meta_files:\n+        meta_file = meta_files[0]\n+        print(f\"Loading config from {meta_file.name}\")\n+        with open(meta_file, \"r\") as f:\n+            meta_config = json.load(f)\n+\n+        # Extract model config from meta file\n+        if \"model_config\" in meta_config:\n+            model_config = meta_config[\"model_config\"]\n+        else:\n+            model_config = meta_config\n+\n+        # Map to NanoChat config parameters\n+        config_kwargs = {\n+            \"vocab_size\": model_config.get(\"vocab_size\", 50304),\n+            \"hidden_size\": model_config.get(\"n_embd\", 768),\n+            \"num_hidden_layers\": model_config.get(\"n_layer\", 12),\n+            \"num_attention_heads\": model_config.get(\"n_head\", 6),\n+            \"num_key_value_heads\": model_config.get(\"n_kv_head\"),\n+            \"max_position_embeddings\": model_config.get(\"sequence_len\", 2048),\n+            \"intermediate_size\": model_config.get(\"intermediate_size\", model_config.get(\"n_embd\", 768) * 4),\n+        }\n+\n+        # Try to load existing config.json for additional parameters\n+        config_file = input_path / \"config.json\"\n+        if config_file.exists():\n+            print(\"Loading additional config from config.json\")\n+            with open(config_file, \"r\") as f:\n+                extra_config = json.load(f)\n+\n+            # Add additional parameters from config.json\n+            for key in [\n+                \"hidden_act\",\n+                \"attention_dropout\",\n+                \"rms_norm_eps\",\n+                \"initializer_range\",\n+                \"logits_soft_cap\",\n+                \"attention_bias\",\n+                \"intermediate_size\",\n+                \"bos_token_id\",\n+                \"eos_token_id\",\n+                \"pad_token_id\",\n+            ]:\n+                if key in extra_config:\n+                    config_kwargs[key] = extra_config[key]\n+                # Handle legacy qkv_bias -> attention_bias conversion\n+                elif key == \"attention_bias\" and \"qkv_bias\" in extra_config:\n+                    config_kwargs[key] = extra_config[\"qkv_bias\"]\n+\n+            # Handle rope_theta as a direct kwarg for the rope_parameters processing\n+            if \"rope_theta\" in extra_config:\n+                config_kwargs[\"rope_theta\"] = extra_config[\"rope_theta\"]\n+\n+            # Handle rope_parameters or rope_scaling if present\n+            if \"rope_parameters\" in extra_config:\n+                config_kwargs[\"rope_parameters\"] = extra_config[\"rope_parameters\"]\n+            elif \"rope_scaling\" in extra_config and extra_config[\"rope_scaling\"] is not None:\n+                config_kwargs[\"rope_parameters\"] = extra_config[\"rope_scaling\"]\n+\n+        config = NanoChatConfig(**config_kwargs)\n+    else:\n+        # Fallback to loading from config.json if it exists\n+        config_file = input_path / \"config.json\"\n+        if config_file.exists():\n+            print(\"Loading config from config.json\")\n+            config = NanoChatConfig.from_pretrained(input_path)\n+            # Handle legacy qkv_bias -> attention_bias conversion\n+            if hasattr(config, \"qkv_bias\") and not hasattr(config, \"attention_bias\"):\n+                config.attention_bias = config.qkv_bias\n+        else:\n+            raise ValueError(f\"No config file found in {input_path}. Expected meta_*.json or config.json\")\n+\n+    return config\n+\n+\n+def write_model(input_dir, output_dir, safe_serialization=True):\n+    \"\"\"Convert NanoChat model from original checkpoint format to HuggingFace format.\"\"\"\n+    print(\"Converting the model.\")\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    input_path = Path(input_dir)\n+\n+    # Load config\n+    config = load_config_from_checkpoint(input_path)\n+    print(f\"Loaded config hidden_size={config.hidden_size} num_layers={config.num_hidden_layers}\")\n+\n+    # Load checkpoint - try model_*.pt first, then pytorch_model.bin\n+    checkpoint_files = list(input_path.glob(\"model_*.pt\"))\n+    if checkpoint_files:\n+        checkpoint_path = checkpoint_files[0]\n+    else:\n+        checkpoint_path = input_path / \"pytorch_model.bin\"\n+\n+    print(f\"Fetching all parameters from the checkpoint at {checkpoint_path}...\")\n+    old_state = torch.load(checkpoint_path, map_location=\"cpu\")\n+\n+    # Original nanochat weights are in bfloat16\n+    for key in old_state:\n+        if old_state[key].dtype == torch.float32:\n+            old_state[key] = old_state[key].to(torch.bfloat16)\n+\n+    # Infer key-value heads from checkpoint\n+    inferred_kv = infer_kv_heads(config, old_state)\n+    config.num_key_value_heads = inferred_kv\n+    if config.num_attention_heads % config.num_key_value_heads != 0:\n+        print(f\"Adjusting num_attention_heads from {config.num_attention_heads} to {config.num_key_value_heads}\")\n+        config.num_attention_heads = config.num_key_value_heads\n+\n+    print(\"Converting model...\")\n+    state_dict = {}\n+    rename_map = {}\n+\n+    def assign(\n+        old_key: str,\n+        new_key: str,\n+        old_state: dict[str, torch.Tensor],\n+        state_dict: dict[str, torch.Tensor],\n+        rename_map: dict[str, str],\n+    ) -> None:\n+        tensor = old_state.get(old_key)\n+        if tensor is None:\n+            return\n+        state_dict[new_key] = tensor.clone()\n+        rename_map[old_key] = new_key\n+\n+    # Convert embeddings and head\n+    assign(\"transformer.wte.weight\", \"model.embed_tokens.weight\", old_state, state_dict, rename_map)\n+    assign(\"lm_head.weight\", \"lm_head.weight\", old_state, state_dict, rename_map)\n+\n+    # Convert layers\n+    for layer_idx in range(config.num_hidden_layers):\n+        old_prefix = f\"transformer.h.{layer_idx}\"\n+        new_prefix = f\"model.layers.{layer_idx}\"\n+        mapping = convert_layer(old_prefix, new_prefix)\n+        for old_key, new_key in mapping.items():\n+            assign(old_key, new_key, old_state, state_dict, rename_map)\n+\n+    missing = [key for key in old_state.keys() if key not in rename_map]\n+    if missing:\n+        print(f\"Skipped {len(missing)} legacy entries that have no equivalent in the shared implementation\")\n+\n+    del old_state\n+    gc.collect()\n+\n+    # Update config\n+    config.torch_dtype = torch.bfloat16\n+    config.tie_word_embeddings = False\n+\n+    # Load the checkpoint into the model\n+    print(\"Loading the checkpoint in a NanoChat model.\")\n+    with torch.device(\"meta\"):\n+        model = NanoChatForCausalLM(config)\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+\n+    if hasattr(model.config, \"_name_or_path\"):\n+        del model.config._name_or_path\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    NanoChatForCausalLM.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+\n+\n+def write_tokenizer(input_dir, output_dir):\n+    \"\"\"Convert and save the tokenizer.\"\"\"\n+    input_path = Path(input_dir)\n+\n+    # Convert the pickle tokenizer to HF format\n+    tokenizer_pkl = input_path / \"tokenizer.pkl\"\n+    if tokenizer_pkl.exists():\n+        try:\n+            import pickle\n+\n+            from transformers.integrations.tiktoken import convert_tiktoken_to_fast\n+\n+            with open(tokenizer_pkl, \"rb\") as f:\n+                tok_pkl = pickle.load(f)\n+            convert_tiktoken_to_fast(tok_pkl, output_dir)\n+            print(\"Converted tokenizer.pkl to HuggingFace format\")\n+        except Exception as e:\n+            print(f\"Warning: Failed to convert tokenizer.pkl: {e}\")\n+            # Fallback: copy tokenizer files if they exist\n+            for filename in (\"tokenizer.json\", \"tokenizer_config.json\"):\n+                src = input_path / filename\n+                if src.exists():\n+                    (Path(output_dir) / filename).write_bytes(src.read_bytes())\n+    else:\n+        # No pickle tokenizer, copy JSON files\n+        for filename in (\"tokenizer.json\", \"tokenizer_config.json\", \"special_tokens_map.json\"):\n+            src = input_path / filename\n+            if src.exists():\n+                (Path(output_dir) / filename).write_bytes(src.read_bytes())\n+\n+    print(\"Tokenizer saved successfully.\")\n+\n+\n+def run_test(output_dir: str, prompt: str, max_new_tokens: int = 64) -> None:\n+    \"\"\"Run a quick generation test to verify the converted model works correctly.\"\"\"\n+    print(f\"Running quick generation test with prompt: {prompt}\")\n+    tokenizer = AutoTokenizer.from_pretrained(output_dir)\n+    model = NanoChatForCausalLM.from_pretrained(output_dir, torch_dtype=torch.bfloat16)\n+    model.eval()\n+    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+    model = model.to(device)\n+    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n+    with torch.no_grad():\n+        output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n+    generated = tokenizer.decode(output[0, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+    print(f\"Generated text: {generated}\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert NanoChat checkpoints to HuggingFace format\")\n+    parser.add_argument(\n+        \"--input_dir\",\n+        type=str,\n+        required=True,\n+        help=\"Path to the original checkpoint directory\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        type=str,\n+        required=True,\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\",\n+        action=\"store_true\",\n+        default=True,\n+        help=\"Whether or not to save using `safetensors`.\",\n+    )\n+    parser.add_argument(\n+        \"--test_prompt\",\n+        type=str,\n+        default=None,\n+        help=\"Optional prompt for a quick generation test\",\n+    )\n+    args = parser.parse_args()\n+\n+    write_model(\n+        args.input_dir,\n+        args.output_dir,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+    write_tokenizer(args.input_dir, args.output_dir)\n+\n+    if args.test_prompt:\n+        run_test(args.output_dir, args.test_prompt)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "0488405d12e950de7305a1d3e850837b7e4c2eb2",
            "filename": "src/transformers/models/nanochat/modeling_nanochat.py",
            "status": "added",
            "additions": 531,
            "deletions": 0,
            "changes": 531,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodeling_nanochat.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,531 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/nanochat/modular_nanochat.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_nanochat.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ... import initialization as init\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from .configuration_nanochat import NanoChatConfig\n+\n+\n+class NanoChatRMSNorm(torch.nn.Module):\n+    def __init__(self, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    def forward(self, x):\n+        return self._norm(x.float()).type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"eps={self.eps}\"\n+\n+\n+class NanoChatRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: NanoChatConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[NanoChatConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input with flipped signs for NanoChat.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((x2, -x1), dim=-1)\n+\n+\n+class NanoChatAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: NanoChatConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+        self.q_norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+        self.k_norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        # RoPE -> Norm (instead of usual Norm -> RoPE)\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class NanoChatMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class NanoChatDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: NanoChatConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = NanoChatAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = NanoChatMLP(config)\n+\n+        self.input_layernorm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class NanoChatPreTrainedModel(PreTrainedModel):\n+    config: NanoChatConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"NanoChatDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": NanoChatDecoderLayer,\n+        \"attentions\": NanoChatAttention,\n+    }\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        super()._init_weights(module)\n+        if isinstance(module, NanoChatAttention):\n+            init.normal_(\n+                module.o_proj.weight,\n+                mean=0.0,\n+                std=self.config.initializer_range / math.sqrt(2 * self.config.num_hidden_layers),\n+            )\n+\n+\n+@auto_docstring\n+class NanoChatModel(NanoChatPreTrainedModel):\n+    def __init__(self, config: NanoChatConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [NanoChatDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+        self.norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+        self.rotary_emb = NanoChatRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs()\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n+\n+        hidden_states = self.norm(hidden_states)  # Additional norm before the layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class NanoChatForCausalLM(NanoChatPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = NanoChatModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"karpathy/nanochat-d32\")\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"karpathy/nanochat-d32\")\n+\n+        >>> conversation = [\n+                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+            ]\n+\n+        >>> inputs = tokenizer.apply_chat_template(\n+                conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+            ).to(device)\n+\n+        >>> with torch.no_grad():\n+        >>>     outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n+\n+        >>> generated_tokens = outputs[0, inputs[\"input_ids\"].shape[1] :]\n+        >>> output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        ```\"\"\"\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+        if self.config.final_logit_softcapping is not None:\n+            logits = logits / self.config.final_logit_softcapping\n+            logits = torch.tanh(logits)\n+            logits = logits * self.config.final_logit_softcapping\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"NanoChatPreTrainedModel\", \"NanoChatModel\", \"NanoChatForCausalLM\"]"
        },
        {
            "sha": "e93c38e7d034973fbfde68399ad495ddf4423e51",
            "filename": "src/transformers/models/nanochat/modular_nanochat.py",
            "status": "added",
            "additions": 247,
            "deletions": 0,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodular_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodular_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fmodular_nanochat.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,247 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from collections.abc import Callable\n+from typing import Optional\n+\n+import torch\n+import torch.nn as nn\n+\n+from ... import initialization as init\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ..clip.modeling_clip import CLIPMLP\n+from ..gemma2.modeling_gemma2 import Gemma2ForCausalLM\n+from ..llama.modeling_llama import (\n+    LlamaDecoderLayer,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from ..llama4.modeling_llama4 import Llama4TextL2Norm\n+from ..qwen3.modeling_qwen3 import Qwen3Attention\n+from .configuration_nanochat import NanoChatConfig\n+\n+\n+class NanoChatRMSNorm(Llama4TextL2Norm):\n+    pass\n+\n+\n+class NanoChatRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input with flipped signs for NanoChat.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((x2, -x1), dim=-1)\n+\n+\n+class NanoChatAttention(Qwen3Attention):\n+    def __init__(self, config: NanoChatConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        del self.sliding_window\n+        del self.layer_type\n+\n+        self.q_norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+        self.k_norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        # RoPE -> Norm (instead of usual Norm -> RoPE)\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class NanoChatMLP(CLIPMLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n+\n+\n+class NanoChatDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: NanoChatConfig, layer_idx: int):\n+        super().__init__()\n+\n+        self.input_layernorm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+\n+\n+@auto_docstring\n+class NanoChatPreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module: nn.Module) -> None:\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, NanoChatAttention):\n+            init.normal_(\n+                module.o_proj.weight,\n+                mean=0.0,\n+                std=self.config.initializer_range / math.sqrt(2 * self.config.num_hidden_layers),\n+            )\n+\n+\n+@auto_docstring\n+class NanoChatModel(LlamaModel):\n+    def __init__(self, config: NanoChatConfig):\n+        super().__init__(config)\n+\n+        self.norm = NanoChatRMSNorm(eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids=position_ids)\n+\n+        hidden_states = self.norm(hidden_states)  # Additional norm before the layers\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class NanoChatForCausalLM(Gemma2ForCausalLM):\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+\n+    def forward(self, **super_kwargs) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"karpathy/nanochat-d32\")\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"karpathy/nanochat-d32\")\n+\n+        >>> conversation = [\n+                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+            ]\n+\n+        >>> inputs = tokenizer.apply_chat_template(\n+                conversation, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+            ).to(device)\n+\n+        >>> with torch.no_grad():\n+        >>>     outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n+\n+        >>> generated_tokens = outputs[0, inputs[\"input_ids\"].shape[1] :]\n+        >>> output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        ```\"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"NanoChatPreTrainedModel\",\n+    \"NanoChatModel\",\n+    \"NanoChatForCausalLM\",\n+]"
        },
        {
            "sha": "c0e3a69403d0d750fc1be0ba9647c1ace89bc0c6",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 7,
            "deletions": 60,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -36,14 +36,11 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_vaultgemma import VaultGemmaConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class VaultGemmaRMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -255,22 +252,18 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -281,11 +274,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class VaultGemmaRotaryEmbedding(nn.Module):\n@@ -407,30 +396,16 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None and not self.training:\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n         if cache_position is None:\n@@ -469,41 +444,22 @@ def forward(\n         normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)\n         hidden_states = hidden_states * normalizer\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n         )\n \n \n@@ -533,11 +489,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n         Example:\n@@ -556,11 +510,6 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is your favorite condiment?\"\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -569,8 +518,6 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "e386ebb6b9d453071d5a3e61eb79077f69fe9d34",
            "filename": "src/transformers/models/vaultgemma/modular_vaultgemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodular_vaultgemma.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -191,22 +191,18 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -217,11 +213,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class VaultGemmaForCausalLM(Gemma2ForCausalLM):"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/nanochat/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/tests%2Fmodels%2Fnanochat%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/tests%2Fmodels%2Fnanochat%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnanochat%2F__init__.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf"
        },
        {
            "sha": "334f25e8a2226e637c16ce88b5f0d59512ec7727",
            "filename": "tests/models/nanochat/test_modeling_nanochat.py",
            "status": "added",
            "additions": 233,
            "deletions": 0,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/tests%2Fmodels%2Fnanochat%2Ftest_modeling_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c8d72bde8fe7c362aa3294f4244a8dd67af0abf/tests%2Fmodels%2Fnanochat%2Ftest_modeling_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnanochat%2Ftest_modeling_nanochat.py?ref=7c8d72bde8fe7c362aa3294f4244a8dd67af0abf",
            "patch": "@@ -0,0 +1,233 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch NanoChat model.\"\"\"\n+\n+import unittest\n+\n+from transformers import AutoTokenizer, NanoChatConfig, is_torch_available\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        NanoChatForCausalLM,\n+        NanoChatModel,\n+    )\n+\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class NanoChatModelTester(CausalLMModelTester):\n+    config_class = NanoChatConfig\n+    if is_torch_available():\n+        base_model_class = NanoChatModel\n+        causal_lm_class = NanoChatForCausalLM\n+\n+\n+@require_torch\n+class NanoChatModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = NanoChatModelTester\n+\n+\n+@require_torch\n+class NanoChatIntegrationTest(unittest.TestCase):\n+    \"\"\"Integration tests for NanoChat models using real checkpoints.\"\"\"\n+\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_d20_logits(self):\n+        \"\"\"Test that d20 model logits are computed correctly.\"\"\"\n+        model_id = \"nanochat-students/nanochat-d20\"\n+        model = NanoChatForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+        # Simple test input - \"Hello world\"\n+        test_text = \"Hello world\"\n+        input_ids = tokenizer.encode(test_text, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.no_grad():\n+            outputs = model(input_ids)\n+            logits = outputs.logits.float().cpu()\n+\n+        # Basic shape checks\n+        self.assertEqual(logits.shape[0], 1)  # batch size\n+        self.assertEqual(logits.shape[1], input_ids.shape[1])  # sequence length\n+        self.assertEqual(logits.shape[2], model.config.vocab_size)  # vocab size 65536\n+\n+        # Check logits are not NaN or Inf\n+        self.assertFalse(torch.isnan(logits).any())\n+        self.assertFalse(torch.isinf(logits).any())\n+\n+        # Check expected mean logits (with tolerance for numerical variation)\n+        EXPECTED_MEAN = torch.tensor([[-6.6607, -7.8095]])\n+\n+        # Check first 10 logits at position [0,0,:10]\n+        EXPECTED_SLICE = torch.tensor(\n+            [-12.8750, -13.0625, -13.1875, -13.1875, -13.1875, -13.1875, -13.1875, -13.1875, -12.6250, -4.4062]\n+        )\n+\n+        torch.testing.assert_close(logits.mean(-1), EXPECTED_MEAN, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(logits[0, 0, :10], EXPECTED_SLICE, rtol=1e-3, atol=1e-3)\n+\n+    @slow\n+    def test_model_d20_generation(self):\n+        \"\"\"Test that d20 model generates text correctly.\"\"\"\n+        model_id = \"nanochat-students/nanochat-d20\"\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        model = NanoChatForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+\n+        # Test generation with chat template\n+        conversation = [\n+            [\n+                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+            ],\n+            [\n+                {\"role\": \"user\", \"content\": \"Tell me something.\"},\n+            ],\n+        ]\n+\n+        inputs = tokenizer.apply_chat_template(\n+            conversation,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            padding=True,\n+            tokenizer_kwargs={\"padding_side\": \"left\"},\n+            return_tensors=\"pt\",\n+        ).to(model.device)\n+\n+        # Generate with greedy decoding for reproducibility\n+        with torch.no_grad():\n+            generated_ids = model.generate(\n+                **inputs,\n+                max_new_tokens=32,\n+                do_sample=False,\n+            )\n+\n+        # Decode only the generated tokens\n+        generated_text = [\n+            tokenizer.decode(generated_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True),\n+            tokenizer.decode(generated_ids[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True),\n+        ]\n+\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"The capital of France is Paris.\",\n+            \"I'm ready to help. What's the first thing you'd like to know or discuss?\",\n+        ]\n+\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION[0], generated_text[0])\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION[1], generated_text[1])\n+\n+    @slow\n+    def test_model_d32_logits(self):\n+        \"\"\"Test that d32 model logits are computed correctly.\"\"\"\n+        model_id = \"karpathy/nanochat-d32\"\n+        revision = \"refs/pr/1\"  # TODO: update when merged to hub\n+        model = NanoChatForCausalLM.from_pretrained(\n+            model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, revision=revision\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n+\n+        # Simple test input - \"Hello world\"\n+        test_text = \"Hello world\"\n+        input_ids = tokenizer.encode(test_text, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.no_grad():\n+            outputs = model(input_ids)\n+            logits = outputs.logits.float().cpu()\n+\n+        # Basic shape checks\n+        self.assertEqual(logits.shape[0], 1)  # batch size\n+        self.assertEqual(logits.shape[1], input_ids.shape[1])  # sequence length\n+        self.assertEqual(logits.shape[2], model.config.vocab_size)  # vocab size 65536\n+\n+        # Check logits are not NaN or Inf\n+        self.assertFalse(torch.isnan(logits).any())\n+        self.assertFalse(torch.isinf(logits).any())\n+\n+        # Check expected mean logits (with tolerance for numerical variation)\n+        EXPECTED_MEAN = torch.tensor([[-5.5791, -8.3456]])\n+\n+        # Check first 10 logits at position [0,0,:10]\n+        EXPECTED_SLICE = torch.tensor(\n+            [-12.3125, -13.1250, -12.8125, -13.1250, -13.1250, -13.1250, -13.1250, -13.1250, -11.8125, -1.4688]\n+        )\n+\n+        torch.testing.assert_close(logits.mean(-1), EXPECTED_MEAN, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(logits[0, 0, :10], EXPECTED_SLICE, rtol=1e-3, atol=1e-3)\n+\n+    @slow\n+    def test_model_d32_generation(self):\n+        \"\"\"Test that d32 model generates text correctly.\"\"\"\n+        model_id = \"karpathy/nanochat-d32\"\n+        revision = \"refs/pr/1\"  # TODO: update when merged to hub\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)\n+        model = NanoChatForCausalLM.from_pretrained(\n+            model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, revision=revision\n+        )\n+\n+        # Test generation with chat template\n+        conversation = [\n+            [\n+                {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+            ],\n+            [\n+                {\"role\": \"user\", \"content\": \"Tell me something.\"},\n+            ],\n+        ]\n+\n+        inputs = tokenizer.apply_chat_template(\n+            conversation,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            padding=True,\n+            tokenizer_kwargs={\"padding_side\": \"left\"},\n+            return_tensors=\"pt\",\n+        ).to(model.device)\n+\n+        # Generate with greedy decoding for reproducibility\n+        with torch.no_grad():\n+            generated_ids = model.generate(\n+                **inputs,\n+                max_new_tokens=32,\n+                do_sample=False,\n+            )\n+\n+        # Decode only the generated tokens\n+        generated_text = [\n+            tokenizer.decode(generated_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True),\n+            tokenizer.decode(generated_ids[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True),\n+        ]\n+\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"The capital of France is Paris.\",\n+            \"I'm here to help you explore your creative writing endeavors. What's been on your mind lately? Do you have a story idea you'd like to develop,\",\n+        ]\n+\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION[0], generated_text[0])\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION[1], generated_text[1])"
        }
    ],
    "stats": {
        "total": 1875,
        "additions": 1656,
        "deletions": 219
    }
}