{
    "author": "Cyrilvallez",
    "message": "ðŸ§¹ Remove deprecated RotaryEmbedding parts in the Attention layers (#34858)\n\n* update\r\n\r\n* style\r\n\r\n* fix missing args\r\n\r\n* remove last trace of old rope classes\r\n\r\n* remove deprecated copied from\r\n\r\n* fix copies\r\n\r\n* trigger CIs\r\n\r\n* post rebase clean-up\r\n\r\n* reverse mistral\r\n\r\n* cleanup after dropping commits\r\n\r\n* Add comment",
    "sha": "d363e71d0e32f44d7a5b3571d4921371907bd0ee",
    "files": [
        {
            "sha": "6172c9acfd21140890db26c54ed4d7db7ce365b9",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -228,9 +228,6 @@ def __init__(self, config: DummyConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        self.rotary_emb = DummyRotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -240,7 +237,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -254,16 +251,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -326,7 +314,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -350,16 +338,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -441,7 +420,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -472,16 +451,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -551,7 +521,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "562e7dcab2b9f2b3c788edf3be67b7681b54cffa",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -228,9 +228,6 @@ def __init__(self, config: Multimodal1TextConfig, layer_idx: Optional[int] = Non\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        self.rotary_emb = Multimodal1TextRotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -240,7 +237,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -254,16 +251,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -326,7 +314,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -350,16 +338,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -441,7 +420,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -472,16 +451,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -553,7 +523,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "79e5ab15a5eda65284e7323d9a3540a93804b53c",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 7,
            "deletions": 37,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -228,9 +228,6 @@ def __init__(self, config: SuperConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        self.rotary_emb = SuperRotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -240,7 +237,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -254,16 +251,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -326,7 +314,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -350,16 +338,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -441,7 +420,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -472,16 +451,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -551,7 +521,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "dfac0ef1cc6fc2debd338c70e49788a8f7ed9f97",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -115,8 +115,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Chameleon\n-# TODO(joao): add me back asap :)\n class ChameleonLinearScalingRotaryEmbedding(ChameleonRotaryEmbedding):\n     \"\"\"ChameleonRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n@@ -127,8 +125,6 @@ def forward(self, x, position_ids):\n         return cos, sin\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Chameleon\n-# TODO(joao): add me back asap :)\n class ChameleonDynamicNTKScalingRotaryEmbedding(ChameleonRotaryEmbedding):\n     \"\"\"ChameleonRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n "
        },
        {
            "sha": "b9a235ed500c0c1a925beb875222a086b79c4420",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 8,
            "deletions": 37,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -283,9 +283,6 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        self.rotary_emb = CohereRotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -295,7 +292,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -314,16 +311,7 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -389,7 +377,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -415,16 +403,7 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -502,7 +481,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -518,6 +497,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -536,16 +516,7 @@ def forward(\n         key_states = key_states.transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -615,7 +586,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:"
        },
        {
            "sha": "51d9ff39d48f88c77753af4c582a6f9295ff250a",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 50,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -197,33 +197,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Falcon\n-class FalconLinearScalingRotaryEmbedding(FalconRotaryEmbedding):\n-    \"\"\"FalconRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`FalconLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`FalconRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"linear\"\n-        super().__init__(*args, **kwargs)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Falcon\n-class FalconDynamicNTKScalingRotaryEmbedding(FalconRotaryEmbedding):\n-    \"\"\"FalconRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`FalconDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`FalconRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n-            \"__init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"dynamic\"\n-        super().__init__(*args, **kwargs)\n-\n-\n def build_alibi_tensor(attention_mask: torch.Tensor, num_heads: int, dtype: torch.dtype) -> torch.Tensor:\n     batch_size, seq_length = attention_mask.shape\n     closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))\n@@ -388,7 +361,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n         num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n@@ -402,16 +375,7 @@ def forward(\n         value_layer = value_layer.transpose(1, 2).reshape(batch_size, num_kv_heads, query_length, self.head_dim)\n \n         if alibi is None:\n-            if position_embeddings is None:\n-                logger.warning_once(\n-                    \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                    \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                    \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                    \"removed and `position_embeddings` will be mandatory.\"\n-                )\n-                cos, sin = self.rotary_emb(value_layer, position_ids)\n-            else:\n-                cos, sin = position_embeddings\n+            cos, sin = position_embeddings\n             query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin)\n \n         if layer_past is not None:\n@@ -548,7 +512,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n         num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n@@ -562,16 +526,7 @@ def forward(\n         value_layer = value_layer.transpose(1, 2).reshape(batch_size, num_kv_heads, query_length, self.head_dim)\n \n         if alibi is None:\n-            if position_embeddings is None:\n-                logger.warning_once(\n-                    \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                    \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                    \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                    \"removed and `position_embeddings` will be mandatory.\"\n-                )\n-                cos, sin = self.rotary_emb(value_layer, position_ids)\n-            else:\n-                cos, sin = position_embeddings\n+            cos, sin = position_embeddings\n             query_layer, key_layer = apply_rotary_pos_emb(query_layer, key_layer, cos, sin)\n \n         if layer_past is not None:\n@@ -695,7 +650,7 @@ def forward(\n         use_cache: bool = False,\n         output_attentions: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ):\n         residual = hidden_states"
        },
        {
            "sha": "b4a292d69de929365374b8e2402df733608323e3",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -227,7 +227,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -303,7 +303,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         output_attentions = False\n \n@@ -402,7 +402,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -503,7 +503,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "70ff07ed7f6dcc82fc4fe200c97dc4141feea651",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 40,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -311,7 +311,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         padding_mask: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         bsz, seq_len, _ = hidden_states.shape\n \n@@ -404,7 +404,7 @@ def _attn_projections_and_rope(\n         layer_past: Optional[Tuple[torch.Tensor]] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         # Compute QKV\n         # Attention heads [batch, seq_len, hidden_size]\n@@ -427,16 +427,7 @@ def _attn_projections_and_rope(\n         key_rot = key[..., : self.rotary_ndims]\n         key_pass = key[..., self.rotary_ndims :]\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n         query = torch.cat((query, query_pass), dim=-1)\n         key = torch.cat((key, key_pass), dim=-1)\n@@ -583,33 +574,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->GPTNeoX\n-class GPTNeoXLinearScalingRotaryEmbedding(GPTNeoXRotaryEmbedding):\n-    \"\"\"GPTNeoXRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`GPTNeoXLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`GPTNeoXRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"linear\"\n-        super().__init__(*args, **kwargs)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->GPTNeoX\n-class GPTNeoXDynamicNTKScalingRotaryEmbedding(GPTNeoXRotaryEmbedding):\n-    \"\"\"GPTNeoXRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`GPTNeoXDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`GPTNeoXRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n-            \"__init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"dynamic\"\n-        super().__init__(*args, **kwargs)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -688,7 +652,7 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         attention_layer_outputs = self.attention(\n             self.input_layernorm(hidden_states),"
        },
        {
            "sha": "c9e1b2d721358701eff8b9008a84830063a6463f",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -105,7 +105,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         # Compute QKV\n         # Attention heads [batch, seq_len, hidden_size]\n@@ -128,16 +128,7 @@ def forward(\n         key_rot = key[..., : self.rotary_ndims]\n         key_pass = key[..., self.rotary_ndims :]\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query, key = apply_rotary_pos_emb(query_rot, key_rot, cos, sin)\n         query = torch.cat((query, query_pass), dim=-1)\n         key = torch.cat((key, key_pass), dim=-1)\n@@ -415,7 +406,7 @@ def forward(\n         layer_past: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         residual = hidden_states\n         ln_out = self.input_layernorm(hidden_states)"
        },
        {
            "sha": "8cd24265d9edcffd9b9e8082aabbae68814fc814",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -242,7 +242,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -318,7 +318,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         output_attentions = False\n \n@@ -417,7 +417,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -520,7 +520,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "9f5fdeea07d4b167435858a50258ef445a177579",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -458,7 +458,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -535,7 +535,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         output_attentions = False\n \n@@ -635,7 +635,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -739,7 +739,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         output_router_logits: Optional[bool] = False,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "8e06098b04c63adf901b33f3c73ab70127d7bd19",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 62,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -168,31 +168,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n-    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`LlamaLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`LlamaRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"linear\"\n-        super().__init__(*args, **kwargs)\n-\n-\n-class LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n-    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`LlamaDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`LlamaRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n-            \"__init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"dynamic\"\n-        super().__init__(*args, **kwargs)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -284,9 +259,6 @@ def __init__(self, config: LlamaConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        self.rotary_emb = LlamaRotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -296,7 +268,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -310,16 +282,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -382,7 +345,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n@@ -406,16 +369,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -497,7 +451,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -528,16 +482,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -607,7 +552,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "6ed8178ed9821e5c294d14d933037dccbeb321bb",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -314,10 +314,6 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += cache_position[0]\n-\n         cos, sin = self.rotary_emb(value_states, position_ids)\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n "
        },
        {
            "sha": "d53a80dd8929015aeb2f033703906ab15abbbc27",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -858,7 +858,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:"
        },
        {
            "sha": "78dace1a53ce557115f4e84072803f6fcef9681d",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -536,7 +536,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "8b40c41e34dcd3c130e4fbaccd881627ed0bf3e1",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -105,8 +105,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Olmo\n-# TODO(joao): add me back asap :)\n class OlmoLinearScalingRotaryEmbedding(OlmoRotaryEmbedding):\n     \"\"\"OlmoRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n@@ -117,8 +115,6 @@ def forward(self, x, position_ids):\n         return cos, sin\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Olmo\n-# TODO(joao): add me back asap :)\n class OlmoDynamicNTKScalingRotaryEmbedding(OlmoRotaryEmbedding):\n     \"\"\"OlmoRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n "
        },
        {
            "sha": "6c35587f1f14fca13ff12bb16ae040402970b383",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -87,8 +87,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Olmo2\n-# TODO(joao): add me back asap :)\n class Olmo2LinearScalingRotaryEmbedding(Olmo2RotaryEmbedding):\n     \"\"\"Olmo2RotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n@@ -99,8 +97,6 @@ def forward(self, x, position_ids):\n         return cos, sin\n \n \n-# copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Olmo2\n-# TODO(joao): add me back asap :)\n class Olmo2DynamicNTKScalingRotaryEmbedding(Olmo2RotaryEmbedding):\n     \"\"\"Olmo2RotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n "
        },
        {
            "sha": "884ee4d86aafcc95b5624d55cb9ea22d553d0864",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 39,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -143,33 +143,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Persimmon\n-class PersimmonLinearScalingRotaryEmbedding(PersimmonRotaryEmbedding):\n-    \"\"\"PersimmonRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`PersimmonLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`PersimmonRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"linear\"\n-        super().__init__(*args, **kwargs)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Persimmon\n-class PersimmonDynamicNTKScalingRotaryEmbedding(PersimmonRotaryEmbedding):\n-    \"\"\"PersimmonRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`PersimmonDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`PersimmonRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n-            \"__init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"dynamic\"\n-        super().__init__(*args, **kwargs)\n-\n-\n # Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n@@ -286,7 +259,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -305,16 +278,7 @@ def forward(\n         value_states = value_states.transpose(1, 2)\n         key_states = key_states.transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -390,7 +354,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:"
        },
        {
            "sha": "8e60798e857f03de4e97e04ed0c37c667fa886d3",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 61,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -147,33 +147,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->Phi\n-class PhiLinearScalingRotaryEmbedding(PhiRotaryEmbedding):\n-    \"\"\"PhiRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`PhiLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`PhiRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"linear\"\n-        super().__init__(*args, **kwargs)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->Phi\n-class PhiDynamicNTKScalingRotaryEmbedding(PhiRotaryEmbedding):\n-    \"\"\"PhiRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`PhiDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`PhiRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n-            \"__init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"dynamic\"\n-        super().__init__(*args, **kwargs)\n-\n-\n # Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n@@ -294,7 +267,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -310,16 +283,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -406,7 +370,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # PhiFlashAttention2 attention does not support output_attentions\n@@ -430,16 +394,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -542,7 +497,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -559,6 +514,8 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -575,16 +532,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -671,7 +619,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "f4875386253c43dc602967ddecec6d3a51c33fd4",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 34,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -284,7 +284,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -296,16 +296,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -370,7 +361,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -382,16 +373,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -479,7 +461,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -494,6 +476,8 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -506,16 +490,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -590,7 +565,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "a1e36b8ad7bc20093b207445ee908f2f59bc83d1",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 35,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -368,7 +368,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -380,17 +380,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n-\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n \n         if past_key_value is not None:\n@@ -457,7 +447,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -469,16 +459,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -567,7 +548,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -582,6 +563,8 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -594,16 +577,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -742,7 +716,7 @@ def forward(\n         output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "dce0702b08194275ac3a22bc252a6f1d07788a0d",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 35,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -537,7 +537,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -549,16 +549,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n@@ -630,7 +621,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -643,17 +634,7 @@ def forward(\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n         # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n-\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n@@ -742,7 +723,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -758,6 +739,7 @@ def forward(\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -770,16 +752,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_multimodal_rotary_pos_emb(\n             query_states, key_states, cos, sin, self.rope_scaling[\"mrope_section\"]\n         )\n@@ -856,7 +829,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "0ce550697e79ab68868b562f1aa5835445d99ba6",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 61,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -149,33 +149,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaLinearScalingRotaryEmbedding with Llama->StableLm\n-class StableLmLinearScalingRotaryEmbedding(StableLmRotaryEmbedding):\n-    \"\"\"StableLmRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`StableLmLinearScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`StableLmRotaryEmbedding`, which now also does linear scaling (simply pass the model config to __init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"linear\"\n-        super().__init__(*args, **kwargs)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaDynamicNTKScalingRotaryEmbedding with Llama->StableLm\n-class StableLmDynamicNTKScalingRotaryEmbedding(StableLmRotaryEmbedding):\n-    \"\"\"StableLmRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        logger.warning_once(\n-            \"`StableLmDynamicNTKScalingRotaryEmbedding` is deprecated an will be removed in v4.46. Please use \"\n-            \"`StableLmRotaryEmbedding`, which now also does dynamic ntk scaling (simply pass the model config to \"\n-            \"__init__).\"\n-        )\n-        kwargs[\"rope_type\"] = \"dynamic\"\n-        super().__init__(*args, **kwargs)\n-\n-\n # Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n@@ -307,7 +280,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -323,16 +296,7 @@ def forward(\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -403,7 +367,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -418,6 +382,8 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -434,16 +400,7 @@ def forward(\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -533,7 +490,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         # StableLmFlashAttention2 attention does not support output_attentions\n@@ -557,16 +514,7 @@ def forward(\n             query_states = self.q_layernorm(query_states)\n             key_states = self.k_layernorm(key_states)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n \n         # Partial rotary embedding\n         query_rot, query_pass = (\n@@ -650,7 +598,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:"
        },
        {
            "sha": "eb218accdb8c03175758a0306cd1218cda9212bf",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 36,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -251,8 +251,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.use_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=self.use_bias)\n \n-        self.rotary_emb = Starcoder2RotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -262,7 +260,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -274,16 +272,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -346,7 +335,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -358,16 +347,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -446,7 +426,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -461,6 +441,8 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -473,16 +455,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -555,7 +528,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\""
        },
        {
            "sha": "a1cec871baca28c2bd8bff3088369b1e192eafc7",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 35,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -117,8 +117,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.use_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=self.use_bias)\n \n-        self.rotary_emb = Starcoder2RotaryEmbedding(config=self.config)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -128,7 +126,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -140,16 +138,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -212,7 +201,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ):\n         bsz, q_len, _ = hidden_states.size()\n \n@@ -224,16 +213,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n@@ -312,7 +292,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n             # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n@@ -327,6 +307,8 @@ def forward(\n                 past_key_value=past_key_value,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n             )\n \n         bsz, q_len, _ = hidden_states.size()\n@@ -339,16 +321,7 @@ def forward(\n         key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n         value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n-        if position_embeddings is None:\n-            logger.warning_once(\n-                \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n-                \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n-                \"removed and `position_embeddings` will be mandatory.\"\n-            )\n-            cos, sin = self.rotary_emb(value_states, position_ids)\n-        else:\n-            cos, sin = position_embeddings\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:"
        },
        {
            "sha": "129bd346a10d8fb4524bb1c89b7a5b715e6179f6",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -50,8 +50,6 @@\n         FalconModel,\n     )\n     from transformers.models.falcon.modeling_falcon import (\n-        FalconDynamicNTKScalingRotaryEmbedding,\n-        FalconLinearScalingRotaryEmbedding,\n         FalconRotaryEmbedding,\n     )\n \n@@ -484,11 +482,12 @@ def test_model_rope_scaling(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        linear_scaling_rope = FalconLinearScalingRotaryEmbedding(\n+        linear_scaling_rope = FalconRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"linear\",\n         ).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n@@ -502,11 +501,12 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        ntk_scaling_rope = FalconDynamicNTKScalingRotaryEmbedding(\n+        ntk_scaling_rope = FalconRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"dynamic\",\n         ).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)"
        },
        {
            "sha": "ca9fbb225c6d873fa6abd2dd9569c7f943755a0f",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -37,11 +37,7 @@\n         GPTNeoXForTokenClassification,\n         GPTNeoXModel,\n     )\n-    from transformers.models.gpt_neox.modeling_gpt_neox import (\n-        GPTNeoXDynamicNTKScalingRotaryEmbedding,\n-        GPTNeoXLinearScalingRotaryEmbedding,\n-        GPTNeoXRotaryEmbedding,\n-    )\n+    from transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXRotaryEmbedding\n \n \n class GPTNeoXModelTester:\n@@ -400,11 +396,12 @@ def test_model_rope_scaling(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        linear_scaling_rope = GPTNeoXLinearScalingRotaryEmbedding(\n+        linear_scaling_rope = GPTNeoXRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rotary_emb_base,\n             scaling_factor=scaling_factor,\n+            rope_type=\"linear\",\n         ).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n@@ -418,11 +415,12 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        ntk_scaling_rope = GPTNeoXDynamicNTKScalingRotaryEmbedding(\n+        ntk_scaling_rope = GPTNeoXRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rotary_emb_base,\n             scaling_factor=scaling_factor,\n+            rope_type=\"dynamic\",\n         ).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)"
        },
        {
            "sha": "0790de4e133b9705adeacab38b00d83ecaf5fb18",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 38,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -51,7 +51,7 @@\n         LlamaModel,\n         LlamaTokenizer,\n     )\n-    from transformers.models.llama.modeling_llama import LlamaLinearScalingRotaryEmbedding, LlamaRotaryEmbedding\n+    from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n \n \n class LlamaModelTester:\n@@ -489,43 +489,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    def test_rope_class_retrocompatibility(self):\n-        # Delete me when we remove compatibility for the old API :)\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        scaling_factor = 10\n-        short_input_length = 10\n-        long_input_length = int(config.max_position_embeddings * 1.5)\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": 10}\n-\n-        # Inputs\n-        x = torch.randn(1, dtype=torch.float32, device=torch_device)  # used exlusively to get the dtype and the device\n-        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_short = position_ids_short.unsqueeze(0)\n-        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n-        position_ids_long = position_ids_long.unsqueeze(0)\n-\n-        # Old API -- under the hood, \"type\": \"linear\" is set and `LlamaRotaryEmbedding` is called\n-        old_api_rope = LlamaLinearScalingRotaryEmbedding(\n-            config.hidden_size // config.num_attention_heads,\n-            max_position_embeddings=config.max_position_embeddings,\n-            base=config.rope_theta,\n-            scaling_factor=scaling_factor,\n-        ).to(torch_device)\n-        old_cos_short, old_sin_short = old_api_rope(x, position_ids_short)\n-        old_cos_long, old_sin_long = old_api_rope(x, position_ids_long)\n-\n-        # New API\n-        config.rope_scaling = {\"type\": \"linear\", \"factor\": scaling_factor}\n-        new_api_rope = LlamaRotaryEmbedding(config=config).to(torch_device)\n-        new_cos_short, new_sin_short = new_api_rope(x, position_ids_short)\n-        new_cos_long, new_sin_long = new_api_rope(x, position_ids_long)\n-\n-        # The results should match\n-        torch.testing.assert_close(old_cos_short, new_cos_short)\n-        torch.testing.assert_close(old_sin_short, new_sin_short)\n-        torch.testing.assert_close(old_cos_long, new_cos_long)\n-        torch.testing.assert_close(old_sin_long, new_sin_long)\n-\n     def test_model_loading_old_rope_configs(self):\n         def _reinitialize_config(base_config, new_kwargs):\n             # Reinitialize the config with the new kwargs, forcing the config to go through its __init__ validation"
        },
        {
            "sha": "54ee49b65343ee695859a74abe951052432b4c88",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -46,11 +46,7 @@\n         PersimmonForTokenClassification,\n         PersimmonModel,\n     )\n-    from transformers.models.persimmon.modeling_persimmon import (\n-        PersimmonDynamicNTKScalingRotaryEmbedding,\n-        PersimmonLinearScalingRotaryEmbedding,\n-        PersimmonRotaryEmbedding,\n-    )\n+    from transformers.models.persimmon.modeling_persimmon import PersimmonRotaryEmbedding\n \n \n # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester with Llama->Persimmon\n@@ -451,11 +447,12 @@ def test_model_rope_scaling(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        linear_scaling_rope = PersimmonLinearScalingRotaryEmbedding(\n+        linear_scaling_rope = PersimmonRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"linear\",\n         ).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n@@ -469,11 +466,12 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        ntk_scaling_rope = PersimmonDynamicNTKScalingRotaryEmbedding(\n+        ntk_scaling_rope = PersimmonRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"dynamic\",\n         ).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)"
        },
        {
            "sha": "df5278cb34e315342dd86cac320fcab055062ba1",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -42,11 +42,7 @@\n         PhiForTokenClassification,\n         PhiModel,\n     )\n-    from transformers.models.phi.modeling_phi import (\n-        PhiDynamicNTKScalingRotaryEmbedding,\n-        PhiLinearScalingRotaryEmbedding,\n-        PhiRotaryEmbedding,\n-    )\n+    from transformers.models.phi.modeling_phi import PhiRotaryEmbedding\n \n \n class PhiModelTester:\n@@ -430,11 +426,12 @@ def test_model_rope_scaling(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        linear_scaling_rope = PhiLinearScalingRotaryEmbedding(\n+        linear_scaling_rope = PhiRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"linear\",\n         ).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n@@ -448,11 +445,12 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        ntk_scaling_rope = PhiDynamicNTKScalingRotaryEmbedding(\n+        ntk_scaling_rope = PhiRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"dynamic\",\n         ).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)"
        },
        {
            "sha": "bfab01578229ec82f39c65f6532080dbf8e3ad38",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d363e71d0e32f44d7a5b3571d4921371907bd0ee/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=d363e71d0e32f44d7a5b3571d4921371907bd0ee",
            "patch": "@@ -44,11 +44,7 @@\n         StableLmForTokenClassification,\n         StableLmModel,\n     )\n-    from transformers.models.stablelm.modeling_stablelm import (\n-        StableLmDynamicNTKScalingRotaryEmbedding,\n-        StableLmLinearScalingRotaryEmbedding,\n-        StableLmRotaryEmbedding,\n-    )\n+    from transformers.models.stablelm.modeling_stablelm import StableLmRotaryEmbedding\n \n \n # Copied from transformers.tests.models.persimmon.test_modeling_persimmon.PersimmonModelTester with Persimmon -> StableLm\n@@ -436,11 +432,12 @@ def test_model_rope_scaling(self):\n \n         # Sanity check linear RoPE scaling\n         # New position \"x\" should match original position with index \"x/scaling_factor\"\n-        linear_scaling_rope = StableLmLinearScalingRotaryEmbedding(\n+        linear_scaling_rope = StableLmRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"linear\",\n         ).to(torch_device)\n         linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n         linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n@@ -454,11 +451,12 @@ def test_model_rope_scaling(self):\n         # Sanity check Dynamic NTK RoPE scaling\n         # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n         # with scaling_factor (or that `inv_freq` decreases)\n-        ntk_scaling_rope = StableLmDynamicNTKScalingRotaryEmbedding(\n+        ntk_scaling_rope = StableLmRotaryEmbedding(\n             head_dim,\n             max_position_embeddings=config.max_position_embeddings,\n             base=config.rope_theta,\n             scaling_factor=scaling_factor,\n+            rope_type=\"dynamic\",\n         ).to(torch_device)\n         ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n         ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)"
        }
    ],
    "stats": {
        "total": 899,
        "additions": 151,
        "deletions": 748
    }
}