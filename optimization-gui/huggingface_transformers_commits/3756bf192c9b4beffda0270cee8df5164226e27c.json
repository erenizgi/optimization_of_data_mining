{
    "author": "IsaacBreen",
    "message": "Add support for specifying revisions when pushing to Hub via internal Trainer call (#36852)\n\n* Update training_args.py\n\n* Update trainer.py\n\n* fixes\n\n* fix\n\n* remove extraneous comments\n\n* explicit revision arg\n\n* add msg\n\n* fixup\n\n* fix field name\n\n* rename field revision to hub_revision\n\n* restore gradient_checkpointing doc\n\n* fix ws\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "3756bf192c9b4beffda0270cee8df5164226e27c",
    "files": [
        {
            "sha": "a1f7902c91c449625fb53f6aa79e55813cfaf353",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3756bf192c9b4beffda0270cee8df5164226e27c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3756bf192c9b4beffda0270cee8df5164226e27c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=3756bf192c9b4beffda0270cee8df5164226e27c",
            "patch": "@@ -3938,7 +3938,7 @@ def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = Fa\n \n         # Push to the Hub when `save_model` is called by the user.\n         if self.args.push_to_hub and not _internal_call:\n-            self.push_to_hub(commit_message=\"Model save\")\n+            self.push_to_hub(commit_message=\"Model save\", revision=self.args.hub_revision)\n \n     def _save_tpu(self, output_dir: Optional[str] = None):\n         output_dir = output_dir if output_dir is not None else self.args.output_dir\n@@ -4788,6 +4788,7 @@ def _push_from_checkpoint(self, checkpoint_folder):\n             token=self.args.hub_token,\n             run_as_future=True,\n             ignore_patterns=[\"_*\", f\"{PREFIX_CHECKPOINT_DIR}-*\"],\n+            revision=self.args.hub_revision,\n         )\n \n         push_jobs = [model_push_job]\n@@ -4803,6 +4804,7 @@ def _push_from_checkpoint(self, checkpoint_folder):\n                 commit_message=commit_message + \", checkpoint\",\n                 token=self.args.hub_token,\n                 run_as_future=True,\n+                revision=self.args.hub_revision,\n             )\n             push_jobs.append(checkpoint_push)\n \n@@ -4882,8 +4884,12 @@ def push_to_hub(\n \n         self.create_model_card(model_name=model_name, **kwargs)\n \n+        if revision is None:\n+            revision = self.args.hub_revision\n+\n         # Wait for the current upload to be finished.\n         self._finish_current_push()\n+\n         return upload_folder(\n             repo_id=self.hub_model_id,\n             folder_path=self.args.output_dir,"
        },
        {
            "sha": "679975309c42f4cf5e3301f7599c1e01fa832ea4",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3756bf192c9b4beffda0270cee8df5164226e27c/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3756bf192c9b4beffda0270cee8df5164226e27c/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=3756bf192c9b4beffda0270cee8df5164226e27c",
            "patch": "@@ -693,6 +693,8 @@ class TrainingArguments:\n             Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n         hub_always_push (`bool`, *optional*, defaults to `False`):\n             Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n+        hub_revision (`str`, *optional*):\n+            The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n         gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n             If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n         gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n@@ -1361,6 +1363,12 @@ class TrainingArguments:\n         default=False,\n         metadata={\"help\": \"Unless `True`, the Trainer will skip pushes if the previous one wasn't finished yet.\"},\n     )\n+    hub_revision: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\"\n+        },\n+    )\n     gradient_checkpointing: bool = field(\n         default=False,\n         metadata={\n@@ -2861,6 +2869,7 @@ def set_push_to_hub(\n         token: Optional[str] = None,\n         private_repo: Optional[bool] = None,\n         always_push: bool = False,\n+        revision: Optional[str] = None,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n@@ -2904,6 +2913,8 @@ def set_push_to_hub(\n             always_push (`bool`, *optional*, defaults to `False`):\n                 Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not\n                 finished.\n+            revision (`str`, *optional*):\n+                The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n \n         Example:\n \n@@ -2922,6 +2933,7 @@ def set_push_to_hub(\n         self.hub_token = token\n         self.hub_private_repo = private_repo\n         self.hub_always_push = always_push\n+        self.hub_revision = revision\n         return self\n \n     def set_optimizer("
        }
    ],
    "stats": {
        "total": 20,
        "additions": 19,
        "deletions": 1
    }
}