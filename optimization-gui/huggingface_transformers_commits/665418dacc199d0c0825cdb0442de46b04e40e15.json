{
    "author": "learning-chip",
    "message": "Remove device check in HQQ quantizer (#39299)\n\n* Remove device check in HQQ quantizer\r\n\r\nFix https://github.com/huggingface/transformers/issues/38439\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "665418dacc199d0c0825cdb0442de46b04e40e15",
    "files": [
        {
            "sha": "896b125b8c19931cb47dfa79d880599ced4f473c",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/665418dacc199d0c0825cdb0442de46b04e40e15/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/665418dacc199d0c0825cdb0442de46b04e40e15/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=665418dacc199d0c0825cdb0442de46b04e40e15",
            "patch": "@@ -15,7 +15,7 @@\n from typing import TYPE_CHECKING, Any\n \n from ..integrations import prepare_for_hqq_linear\n-from ..utils import is_accelerate_available, is_hqq_available, is_torch_available, is_torch_xpu_available, logging\n+from ..utils import is_accelerate_available, is_hqq_available, is_torch_available, logging\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -71,9 +71,6 @@ def validate_environment(self, *args, **kwargs):\n                 \" sure the weights are in PyTorch format.\"\n             )\n \n-        if not (torch.cuda.is_available() or is_torch_xpu_available()):\n-            raise RuntimeError(\"No GPU or XPU found. A GPU or XPU is needed for quantization.\")\n-\n         if self.torch_dtype is None:\n             if \"torch_dtype\" in kwargs:\n                 self.torch_dtype = kwargs[\"torch_dtype\"]"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 1,
        "deletions": 4
    }
}