{
    "author": "MekkCyber",
    "message": "[core] Fix quark (#42457)\n\n* intial\n\n* update\n\n* add convert\n\n* fix\n\n* style\n\n* rm comment\n\n* explain\n\n* loop\n\n* fix\n\n* fix\n\n* update\n\n* Apply style fixes\n\n* fix\n\n* style\n\n* update\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "bebfab0682839718443c7bf5020567ab3b434057",
    "files": [
        {
            "sha": "db977016dbf1d44bd09bfc36cb4e0b2922ca9b3b",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bebfab0682839718443c7bf5020567ab3b434057/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bebfab0682839718443c7bf5020567ab3b434057/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=bebfab0682839718443c7bf5020567ab3b434057",
            "patch": "@@ -462,9 +462,15 @@ def convert(\n         full_name = layer_name\n         if \".*.\" in layer_name:\n             full_name = layer_name.replace(\".*.\", \".0.\")\n-        prefix, _, suffix = next(full_name.partition(k) for k in collected_tensors.keys() if k in full_name)\n-        # Rename the tensors\n-        collected_tensors = {prefix + k + suffix: v for k, v in collected_tensors.items()}\n+\n+        try:\n+            prefix, _, suffix = next(full_name.partition(k) for k in collected_tensors.keys() if k in full_name)\n+            # Rename the tensors\n+            collected_tensors = {prefix + k + suffix: v for k, v in collected_tensors.items()}\n+        # some quantizers need to already rename in `convert` as they cannot only rely on prefix and suffix\n+        except StopIteration:\n+            pass\n+\n         if hf_quantizer is not None and self.quantization_operation is not None:\n             with log_to_misc(layer_name, misc, (collected_tensors, layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert("
        },
        {
            "sha": "23e586910ab832dbdd9b4c9ff7e6e39c249fa209",
            "filename": "src/transformers/integrations/quark.py",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/bebfab0682839718443c7bf5020567ab3b434057/src%2Ftransformers%2Fintegrations%2Fquark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bebfab0682839718443c7bf5020567ab3b434057/src%2Ftransformers%2Fintegrations%2Fquark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquark.py?ref=bebfab0682839718443c7bf5020567ab3b434057",
            "patch": "@@ -0,0 +1,55 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+from ..core_model_loading import ConversionOps\n+from ..utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class QuarkDeserialize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: torch.Tensor,\n+        model: Optional[torch.nn.Module] = None,\n+        missing_keys: Optional[list[str]] = None,\n+        full_layer_name: str | None = None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        # target_key should be in the form of weight_scale, bias_scale, input_scale, output_scale, weight_zero_point, bias_zero_point, input_zero_point, output_zero_point\n+        target_key, value = tuple(input_dict.items())[0]\n+        value = value[0] if isinstance(value, list) else value\n+        # this will get the param name : weight, input, bias or output\n+        param = target_key.split(\"_\", 1)[0]\n+        # quant_state should be in the form of scale, or zero_point\n+        quant_state = target_key.split(\"_\", 1)[-1]\n+\n+        # here we change the name for example from the form of :\n+        # model.layers.0.mlp.down_proj.weight_scale to model.layers.0.mlp.down_proj.weight_quantizer.scale to fit within\n+        # the QParamsLinear module of quark\n+        sub_module_state = full_layer_name.rsplit(\".\", 1)[0] + \".\" + param + \"_quantizer\" + \".\" + quant_state\n+\n+        # since quark module was expecting keys in the form of model.layers.0.mlp.down_proj.weight_scale\n+        # we need to remove it from the missing_keys list\n+        missing_keys.discard(full_layer_name)\n+\n+        return {sub_module_state: value}"
        },
        {
            "sha": "c11548b1416e1fdbc93004423f51f1e00b5a6430",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/bebfab0682839718443c7bf5020567ab3b434057/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bebfab0682839718443c7bf5020567ab3b434057/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=bebfab0682839718443c7bf5020567ab3b434057",
            "patch": "@@ -94,3 +94,38 @@ def is_serializable(self, safe_serialization=None):\n     @property\n     def is_trainable(self):\n         return False\n+\n+    def get_weight_conversions(self):\n+        from ..core_model_loading import WeightConverter\n+        from ..integrations.quark import QuarkDeserialize\n+        # In Quark, quantization is managed through a QParamsLinear module, which holds\n+        # separate quantizers for the weights, inputs, and biases (e.g. weight_quantizer\n+        # input_quantizer, bias_quantizer, etc.).\n+        #\n+        # When you call `module.state_dict()`, Quark automatically renames the quantizer\n+        # parameters — for example, `input_quantizer.scale` becomes `input_scale` — and\n+        # saves them directly at the parent module level.\n+        #\n+        # This means we cannot simply rename keys like `weight_scale` back to\n+        # `weight_quantizer.scale` when loading the state_dict.\n+        # Otherwise, the `missing_keys` list would still expect keys such as\n+        # `weight_scale`, `bias_scale`, etc.\n+        #\n+        # To fix this, we keep the expected state_dict keys (like `weight_scale`,\n+        # `bias_scale`, etc.) unchanged, and during the conversion step, we explicitly\n+        # assign their values into the corresponding quantizer attributes\n+        # (`weight_quantizer.scale`, `input_quantizer.scale`, and so on).\n+\n+        # You can notice here that in target_patterns we use the same key as the source_patterns,\n+        # this is because we just want to collect the tensors, and we will rename them later in the convert function.\n+        # We cannot rename directly or else the missing_keys list will not be able to find the tensors.\n+        converters = []\n+        for key in CHECKPOINT_KEYS.keys():\n+            converters.append(\n+                WeightConverter(\n+                    source_patterns=[key],\n+                    target_patterns=key,\n+                    operations=[QuarkDeserialize(self)],\n+                )\n+            )\n+        return converters"
        },
        {
            "sha": "c04b35da90cec4ca6cff7b7aad6b2c951bcbaeea",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bebfab0682839718443c7bf5020567ab3b434057/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bebfab0682839718443c7bf5020567ab3b434057/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=bebfab0682839718443c7bf5020567ab3b434057",
            "patch": "@@ -56,6 +56,7 @@ class QuarkTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am enjoying my day off! The sun is shining, the birds are\")\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I'm here to tell you about it. It's a beautiful day,\")\n     EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am not in Paris at all! I am not in Paris, but\")\n+    EXPECTED_OUTPUTS.add(\"Today I am in Paris and I am in Paris, but I am not in Paris\\nToday I am\")\n \n     EXPECTED_RELATIVE_DIFFERENCE = 1.66\n     device_map = None"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 100,
        "deletions": 3
    }
}