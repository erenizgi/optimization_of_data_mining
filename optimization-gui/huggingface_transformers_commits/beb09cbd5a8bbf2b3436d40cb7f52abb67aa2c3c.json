{
    "author": "yonigozlan",
    "message": "ðŸ”´Make `center_crop` fast equivalent to slow (#40856)\n\nmake center_crop fast equivalent to slow",
    "sha": "beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c",
    "files": [
        {
            "sha": "5fb87c345ef094a18288d588c08437bee002d4e4",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c",
            "patch": "@@ -405,10 +405,11 @@ def rescale_and_normalize(\n     def center_crop(\n         self,\n         image: \"torch.Tensor\",\n-        size: dict[str, int],\n+        size: SizeDict,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n+        Note: override torchvision's center_crop to have the same behavior as the slow processor.\n         Center crop an image to `(size[\"height\"], size[\"width\"])`. If the input size is smaller than `crop_size` along\n         any edge, the image is padded with 0's and then center cropped.\n \n@@ -423,7 +424,24 @@ def center_crop(\n         \"\"\"\n         if size.height is None or size.width is None:\n             raise ValueError(f\"The size dictionary must have keys 'height' and 'width'. Got {size.keys()}\")\n-        return F.center_crop(image, (size[\"height\"], size[\"width\"]))\n+        image_height, image_width = image.shape[-2:]\n+        crop_height, crop_width = size.height, size.width\n+\n+        if crop_width > image_width or crop_height > image_height:\n+            padding_ltrb = [\n+                (crop_width - image_width) // 2 if crop_width > image_width else 0,\n+                (crop_height - image_height) // 2 if crop_height > image_height else 0,\n+                (crop_width - image_width + 1) // 2 if crop_width > image_width else 0,\n+                (crop_height - image_height + 1) // 2 if crop_height > image_height else 0,\n+            ]\n+            image = F.pad(image, padding_ltrb, fill=0)  # PIL uses fill value 0\n+            image_height, image_width = image.shape[-2:]\n+            if crop_width == image_width and crop_height == image_height:\n+                return image\n+\n+        crop_top = int((image_height - crop_height) / 2.0)\n+        crop_left = int((image_width - crop_width) / 2.0)\n+        return F.crop(image, crop_top, crop_left, crop_height, crop_width)\n \n     def convert_to_rgb(\n         self,"
        },
        {
            "sha": "ecd7f938f56985428bd17616b5cd7bb7d25e2eb5",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c",
            "patch": "@@ -81,7 +81,7 @@ def center_crop(\n         min_dim = min(height, width)\n         cropped_height = int((size.height / crop_size.height) * min_dim)\n         cropped_width = int((size.width / crop_size.width) * min_dim)\n-        return F.center_crop(image, (cropped_height, cropped_width))\n+        return super().center_crop(image, SizeDict(height=cropped_height, width=cropped_width))\n \n     def _preprocess(\n         self,"
        },
        {
            "sha": "18670bcb4d64b61b5f4d8a0ef17328b274d24fd4",
            "filename": "tests/models/chinese_clip/test_image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py?ref=beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c",
            "patch": "@@ -141,7 +141,7 @@ class ChineseCLIPImageProcessingTestFourChannels(ImageProcessingTestMixin, unitt\n \n     def setUp(self):\n         super().setUp()\n-        self.image_processor_tester = ChineseCLIPImageProcessingTester(self, num_channels=4, do_center_crop=True)\n+        self.image_processor_tester = ChineseCLIPImageProcessingTester(self, num_channels=3, do_center_crop=True)\n         self.expected_encoded_image_num_channels = 3\n \n     @property\n@@ -160,14 +160,6 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"image_std\"))\n             self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n-    @unittest.skip(reason=\"ChineseCLIPImageProcessor does not support 4 channels yet\")  # FIXME Amy\n-    def test_call_numpy(self):\n-        return super().test_call_numpy()\n-\n-    @unittest.skip(reason=\"ChineseCLIPImageProcessor does not support 4 channels yet\")  # FIXME Amy\n-    def test_call_pytorch(self):\n-        return super().test_call_torch()\n-\n     @unittest.skip(\n         reason=\"ChineseCLIPImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\"\n     )  # FIXME Amy"
        },
        {
            "sha": "ce0bd4181be58048ebb39344ae8ddf63d9e8d4d2",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=beb09cbd5a8bbf2b3436d40cb7f52abb67aa2c3c",
            "patch": "@@ -200,11 +200,6 @@ def test_slow_fast_equivalence_batched(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n-            self.skipTest(\n-                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n-            )\n-\n         dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 22,
        "deletions": 17
    }
}