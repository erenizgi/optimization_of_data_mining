{
    "author": "juliendenize",
    "message": "Patch MistralCommonTokenizer (#41439)\n\n* Fix token_to_id and add add_generation_prompt\n\n* Fix spm download\n\n* Refactor spm\n\n* Try another possibly non-gated spm\n\n* Improve get_vocab\n\n* lint\n\n* Improve get_vocab\n\n* Add warn to piece_to_id\n\n* Improve from_pretrained raise and revert model spm\n\n* Revert fast",
    "sha": "0566b6f5bdf474d51a34947f0957be84c42207bf",
    "files": [
        {
            "sha": "feb3db028cf64f4f1c3f8e0691d9bc58eff9109a",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 49,
            "deletions": 40,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/0566b6f5bdf474d51a34947f0957be84c42207bf/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0566b6f5bdf474d51a34947f0957be84c42207bf/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=0566b6f5bdf474d51a34947f0957be84c42207bf",
            "patch": "@@ -115,11 +115,6 @@\n                 of returning overflowing tokens.\n             return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n                 Whether or not to return special tokens mask information.\n-            return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return `(char_start, char_end)` for each token.\n-\n-                This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n-                Python's tokenizer, this method will raise `NotImplementedError`.\n             return_length  (`bool`, *optional*, defaults to `False`):\n                 Whether or not to return the lengths of the encoded inputs.\n             verbose (`bool`, *optional*, defaults to `True`):\n@@ -176,6 +171,7 @@ class MistralCommonTokenizer(PushToHubMixin):\n     Supports the following methods from the `PreTrainedTokenizerBase` class:\n \n     - [`~MistralCommonTokenizer.get_vocab`]: Returns the vocabulary as a dictionary of token to index.\n+        This is a lossy conversion for Tekkenizer as some decoding errors are collapsed into the same token.\n     - [`~MistralCommonTokenizer.encode`]: Encode a string to a list of integers.\n     - [`~MistralCommonTokenizer.decode`]: Decode a list of integers to a string.\n     - [`~MistralCommonTokenizer.batch_decode`]: Decode a batch of list of integers to a list of strings.\n@@ -354,9 +350,13 @@ def get_vocab(self) -> dict[str, int]:\n             `Dict[str, int]`: The vocabulary.\n         \"\"\"\n         if self._cache_get_vocab is None:\n-            self._cache_get_vocab = {\n-                token: idx for idx, token in enumerate(self.tokenizer.instruct_tokenizer.tokenizer.vocab())\n-            }\n+            # We reverse the order to make sure that the first token is the one to be returned when there are multiple tokens with the same string representation.\n+            vocab = self.tokenizer.instruct_tokenizer.tokenizer.vocab()\n+            self._cache_get_vocab = {token: self._piece_to_id(token, False) for token in vocab}\n+            # Order the dict.\n+            self._cache_get_vocab = dict(\n+                sorted(((k, v) for k, v in self._cache_get_vocab.items()), key=lambda x: x[1])\n+            )\n         return self._cache_get_vocab\n \n     def __len__(self):\n@@ -517,7 +517,7 @@ def batch_decode(\n \n     def _is_control_token(self, token_id: int) -> bool:\n         if self._tokenizer_type == MistralTokenizerType.spm:\n-            return token_id in self.tokenizer.instruct_tokenizer.tokenizer._control_tokens()\n+            return token_id in self.tokenizer.instruct_tokenizer.tokenizer._control_tokens\n         elif self._tokenizer_type == MistralTokenizerType.tekken:\n             return token_id < self.tokenizer.instruct_tokenizer.tokenizer.num_special_tokens\n         else:\n@@ -563,15 +563,27 @@ def convert_ids_to_tokens(\n             return tokens[0]\n         return tokens\n \n-    def _piece_to_id(self, piece: str) -> int:\n+    def _tekken_piece_to_id(self, piece: str, warn: bool) -> int:\n+        tekken_tokenizer = self.tokenizer.instruct_tokenizer.tokenizer\n+        assert isinstance(tekken_tokenizer, Tekkenizer), type(tekken_tokenizer)\n+\n+        piece_bytes = piece.encode(\"utf-8\")\n+        shift = tekken_tokenizer.num_special_tokens\n+        try:\n+            return shift + tekken_tokenizer._tekken_token2id_nospecial[piece_bytes]\n+        except KeyError:\n+            piece_str = piece_bytes.decode(\"utf-8\")\n+            if piece_str in tekken_tokenizer._special_tokens_reverse_vocab:\n+                return tekken_tokenizer._special_tokens_reverse_vocab[piece_str]\n+            if warn:\n+                logger.warning(\"Failed to convert token %s to id, replacing with <unk>\", piece_bytes)\n+            return tekken_tokenizer.unk_id\n+\n+    def _piece_to_id(self, piece: str, warn: bool) -> int:\n         if self._tokenizer_type == MistralTokenizerType.spm:\n             return self.tokenizer.instruct_tokenizer.tokenizer._model.piece_to_id(piece)\n         elif self._tokenizer_type == MistralTokenizerType.tekken:\n-            pieces = self.tokenizer.instruct_tokenizer.tokenizer._model.encode(\n-                piece, allowed_special=\"all\", disallowed_special=set()\n-            )\n-            assert len(pieces) == 1, f\"Expected to decode 1 token, got {len(pieces)}\"\n-            return pieces[0]\n+            return self._tekken_piece_to_id(piece, warn)\n         else:\n             raise ValueError(f\"Unknown tokenizer type: {self._tokenizer_type}\")\n \n@@ -595,7 +607,7 @@ def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, lis\n \n         ids: list[int] = []\n         for token in tokens:\n-            ids.append(self._piece_to_id(token))\n+            ids.append(self._piece_to_id(token, True))\n \n         if one_token:\n             return ids[0]\n@@ -647,13 +659,7 @@ def _encode_plus(\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        **kwargs,\n     ) -> BatchEncoding:\n-        if kwargs:\n-            raise ValueError(\n-                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer._encode_plus`.\"\n-            )\n-\n         def get_input_ids(text):\n             if isinstance(text, str):\n                 return self._text_to_ids(text, add_special_tokens)\n@@ -699,10 +705,8 @@ def _batch_encode_plus(\n         return_attention_mask: Optional[bool] = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n         return_length: bool = False,\n         verbose: bool = True,\n-        **kwargs,\n     ) -> BatchEncoding:\n         def get_input_ids(text):\n             if isinstance(text, str):\n@@ -712,13 +716,6 @@ def get_input_ids(text):\n             else:\n                 raise ValueError(\"Input is not valid. Should be a string or a list/tuple of integers.\")\n \n-        if return_offsets_mapping:\n-            raise NotImplementedError(\n-                \"return_offset_mapping is not available when using Python tokenizers. \"\n-                \"To use this feature, change your tokenizer to one deriving from \"\n-                \"transformers.PreTrainedTokenizerFast.\"\n-            )\n-\n         input_ids = []\n         for ids in batch_text:\n             input_ids.append(get_input_ids(ids))\n@@ -746,7 +743,7 @@ def _all_special_ids(self) -> set[int]:\n         if self._tokenizer_type == MistralTokenizerType.tekken:\n             return {t[\"rank\"] for t in self.tokenizer.instruct_tokenizer.tokenizer._all_special_tokens}\n         elif self._tokenizer_type == MistralTokenizerType.spm:\n-            return self.tokenizer.instruct_tokenizer.tokenizer._control_tokens()\n+            return self.tokenizer.instruct_tokenizer.tokenizer._control_tokens\n         else:\n             raise ValueError(f\"Unknown tokenizer type: {self._tokenizer_type}\")\n \n@@ -966,9 +963,7 @@ def _get_padding_truncation_strategies(\n                     logger.warning(\n                         \"Truncation was not explicitly activated but `max_length` is provided a specific value, please\"\n                         \" use `truncation=True` to explicitly truncate examples to max length. Defaulting to\"\n-                        \" 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the\"\n-                        \" tokenizer you can select this strategy more precisely by providing a specific strategy to\"\n-                        \" `truncation`.\"\n+                        \" 'longest_first' truncation strategy.\"\n                     )\n                 self.deprecation_warnings[\"Truncation-not-explicitly-activated\"] = True\n             truncation = \"longest_first\"\n@@ -1376,6 +1371,7 @@ def apply_chat_template(\n         self,\n         conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n         tools: Optional[list[Union[dict, Callable]]] = None,\n+        add_generation_prompt: bool = False,\n         continue_final_message: bool = False,\n         tokenize: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n@@ -1398,6 +1394,9 @@ def apply_chat_template(\n                 giving the name, description and argument types for the tool. See our\n                 [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n                 for more information.\n+            add_generation_prompt (`bool`, *optional*):\n+                This argument is a no-op for `MistralCommonTokenizer`. However it cannot be used at the same time as `continue_final_message` to keep the API consistent and\n+                if any conversation ends with an assistant message, it will raise an error. In such case, use `continue_final_message` instead.\n             continue_final_message (bool, *optional*):\n                 If this is set, the chat will be formatted so that the final\n                 message in the chat is open-ended, without any EOS tokens. The model will continue this message\n@@ -1442,6 +1441,9 @@ def apply_chat_template(\n         if not isinstance(truncation, bool):\n             raise TypeError(\"`truncation` must be a boolean for `apply_chat_template` method.\")\n \n+        if add_generation_prompt and continue_final_message:\n+            raise ValueError(\"Cannot use both `add_generation_prompt` and `continue_final_message`.\")\n+\n         if isinstance(conversation, (list, tuple)) and (\n             isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"messages\")\n         ):\n@@ -1451,6 +1453,14 @@ def apply_chat_template(\n             conversations = [conversation]\n             is_batched = False\n \n+        if add_generation_prompt:\n+            for conversation in conversations:\n+                last_message = conversation[-1]\n+                if last_message.get(\"role\") == \"assistant\":\n+                    raise ValueError(\n+                        \"The last message in the conversation is already an assistant message. Consider using `continue_final_message` instead.\"\n+                    )\n+\n         def _maybe_adapt_message(message: dict[str, Any]) -> None:\n             \"\"\"Adapt message to `mistral-common` format and leave validation to `mistral-common`.\"\"\"\n             if not isinstance(message, dict):\n@@ -1543,7 +1553,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n                                 \"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\"\n                             )\n \n-                        pixel_values = torch.tensor(images)\n+                        pixel_values = torch.from_numpy(np.stack(images))\n                     elif return_tensors == \"np\":\n                         pixel_values = np.array(images)\n                     elif return_tensors is None:\n@@ -1667,7 +1677,6 @@ def _is_valid_text_input(t):\n                 return_special_tokens_mask=return_special_tokens_mask,\n                 return_length=return_length,\n                 verbose=verbose,\n-                **kwargs,\n             )\n         else:\n             return self._encode_plus(\n@@ -1685,7 +1694,6 @@ def _is_valid_text_input(t):\n                 return_special_tokens_mask=return_special_tokens_mask,\n                 return_length=return_length,\n                 verbose=verbose,\n-                **kwargs,\n             )\n \n     @classmethod\n@@ -1760,9 +1768,10 @@ def from_pretrained(\n             raise ValueError(\"`init_inputs` are not supported by `MistralCommonTokenizer.from_pretrained`.\")\n \n         # Handle kwargs and AutoTokenizer case\n-        if kwargs and not set(kwargs.keys()).issubset({\"_from_auto\", \"trust_remote_code\"}):\n+        ignore_subset = {\"_from_auto\", \"trust_remote_code\"}\n+        if kwargs and not (set_kwargs := set(kwargs.keys())).issubset(ignore_subset):\n             raise ValueError(\n-                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.from_pretrained`.\"\n+                f\"Kwargs {list(set_kwargs - ignore_subset)} are not supported by `MistralCommonTokenizer.from_pretrained`.\"\n             )\n \n         if not os.path.isdir(pretrained_model_name_or_path):"
        },
        {
            "sha": "82dba87f7d7e77649d1f77bdb30b44c2a8da02cc",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 150,
            "deletions": 7,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/0566b6f5bdf474d51a34947f0957be84c42207bf/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0566b6f5bdf474d51a34947f0957be84c42207bf/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=0566b6f5bdf474d51a34947f0957be84c42207bf",
            "patch": "@@ -34,6 +34,7 @@\n     from mistral_common.exceptions import InvalidMessageStructureException\n     from mistral_common.protocol.instruct.request import ChatCompletionRequest\n     from mistral_common.protocol.transcription.request import TranscriptionRequest\n+    from mistral_common.tokens.tokenizers.base import SpecialTokenPolicy\n     from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n     from mistral_common.tokens.tokenizers.utils import list_local_hf_repo_files\n \n@@ -82,6 +83,10 @@ def setUpClass(cls):\n         cls.ref_tokenizer: MistralTokenizer = MistralTokenizer.from_hf_hub(\n             cls.repo_id, local_files_only=cls.local_files_only\n         )\n+\n+        # Define SPM tokenizer to test the private methods that handle SPM and Tekken differencies.\n+        cls.spm_repo_id = \"mistralai/Mistral-7B-v0.3\"\n+\n         # cls.tokenizer_audio: MistralCommonTokenizer = AutoTokenizer.from_pretrained(\n         #     \"hf-internal-testing/namesspace-mistralai-repo_name-Voxtral-Mini-3B-2507\"\n         # )\n@@ -127,12 +132,53 @@ def tearDownClass(cls):\n         del cls.ref_special_ids\n         gc.collect()\n \n+    # Copy paste of `MistralCommonTokenizer._tekken_piece_to_id`\n     def _ref_piece_to_id(self, piece: str) -> int:\n-        pieces = self.ref_tokenizer.instruct_tokenizer.tokenizer._model.encode(\n-            piece, allowed_special=\"all\", disallowed_special=set()\n+        tekken_tokenizer = self.ref_tokenizer.instruct_tokenizer.tokenizer\n+\n+        piece_bytes = piece.encode(\"utf-8\")\n+        shift = tekken_tokenizer.num_special_tokens\n+        try:\n+            return shift + tekken_tokenizer._tekken_token2id_nospecial[piece_bytes]\n+        except KeyError:\n+            piece_str = piece_bytes.decode(\"utf-8\")\n+            if piece_str in tekken_tokenizer._special_tokens_reverse_vocab:\n+                return tekken_tokenizer._special_tokens_reverse_vocab[piece_str]\n+            return tekken_tokenizer.unk_id\n+\n+    def _get_spm_tokenizer(self) -> MistralCommonTokenizer:\n+        local_files_only = len(list_local_hf_repo_files(self.spm_repo_id, revision=None)) > 0\n+        return AutoTokenizer.from_pretrained(\n+            self.spm_repo_id,\n+            local_files_only=local_files_only,\n+            revision=None,\n+            tokenizer_type=\"mistral\",\n         )\n-        assert len(pieces) == 1, f\"Expected to decode 1 token, got {len(pieces)}\"\n-        return pieces[0]\n+\n+    def test_spm_vs_tekken_is_control_token(self):\n+        spm_tokenizer = self._get_spm_tokenizer()\n+        self.assertTrue(spm_tokenizer._is_control_token(1))\n+        self.assertTrue(spm_tokenizer._is_control_token(768))\n+        self.assertFalse(spm_tokenizer._is_control_token(2000))\n+\n+        self.assertTrue(self.tokenizer._is_control_token(0))\n+        self.assertTrue(self.tokenizer._is_control_token(768))\n+        self.assertTrue(self.tokenizer._is_control_token(999))\n+        self.assertFalse(self.tokenizer._is_control_token(1000))\n+\n+    def test_spm_vs_tekken_piece_to_id(self):\n+        spm_tokenizer = self._get_spm_tokenizer()\n+        self.assertEqual(spm_tokenizer._piece_to_id(\"<s>\", False), 1)\n+        self.assertEqual(spm_tokenizer._piece_to_id(\"h\", False), 29484)\n+\n+        self.assertEqual(self.tokenizer._piece_to_id(\"<s>\", False), 1)\n+        self.assertEqual(self._ref_piece_to_id(\"<s>\"), 1)\n+        self.assertEqual(self.tokenizer._piece_to_id(\"\\u0000\", False), 1000)\n+        self.assertEqual(self._ref_piece_to_id(\"\\u0000\"), 1000)\n+        self.assertEqual(self.tokenizer._piece_to_id(\" String\", False), 3000)\n+        self.assertEqual(self._ref_piece_to_id(\" String\"), 3000)\n+        self.assertEqual(self.tokenizer._piece_to_id(\"后汉书\", False), 131071)\n+        self.assertEqual(self._ref_piece_to_id(\"后汉书\"), 131071)\n \n     def test_vocab_size(self):\n         self.assertEqual(self.tokenizer.vocab_size, self.ref_tokenizer.instruct_tokenizer.tokenizer.n_words)\n@@ -785,6 +831,40 @@ def test_apply_chat_template_continue_final_message(self):\n         with self.assertRaises(InvalidMessageStructureException):\n             self.tokenizer.apply_chat_template(conversation, tokenize=False, continue_final_message=False)\n \n+    def test_apply_chat_template_with_add_generation_prompt(self):\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Hi!\"},\n+        ]\n+\n+        # Test 1:\n+        # with add_generation_prompt\n+        for add_generation_prompt in [False, True]:\n+            expected_tokenized = self.ref_tokenizer.encode_chat_completion(\n+                ChatCompletionRequest.from_openai(conversation)\n+            )\n+            token_outputs = self.tokenizer.apply_chat_template(\n+                conversation, tokenize=True, add_generation_prompt=add_generation_prompt\n+            )\n+            self.assertEqual(token_outputs, expected_tokenized.tokens)\n+\n+        # Test 2:\n+        # with continue_final_message\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.apply_chat_template(\n+                conversation, tokenize=True, add_generation_prompt=True, continue_final_message=True\n+            )\n+\n+        # Test 3:\n+        # with last message with assistant role\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Hi!\"},\n+            {\"role\": \"asistant\", \"content\": \"Hey!\"},\n+        ]\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, add_generation_prompt=True)\n+\n     def test_apply_chat_template_with_tools(self):\n         conversation = [\n             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n@@ -891,7 +971,8 @@ def test_apply_chat_template_with_image(self):\n             conversation, tokenize=True, return_dict=True, return_tensors=\"pt\"\n         )\n         self.assertEqual(output_dict[\"input_ids\"].tolist()[0], expected_tokenized.tokens)\n-        self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], torch.tensor(expected_tokenized.images)))\n+        expected_images_pt_tensor = torch.from_numpy(np.stack(expected_tokenized.images))\n+        self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], expected_images_pt_tensor))\n \n     def test_apply_chat_template_with_audio(self):\n         ref_conversation = conversation = [\n@@ -1063,7 +1144,7 @@ def test_batch_apply_chat_template(self):\n         ):\n             self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True, unk_args=\"\")\n \n-    def test_batch_apply_images(self):\n+    def test_batch_apply_chat_template_images(self):\n         conversations = [\n             [\n                 {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n@@ -1135,7 +1216,8 @@ def test_batch_apply_images(self):\n         )\n         self.assertEqual(output[\"input_ids\"].tolist(), [expected_tokenized.tokens] * 3)\n         self.assertEqual(output[\"input_ids\"].shape[0], len(expected_tokenized.images) * 3)\n-        self.assertTrue(torch.allclose(output[\"pixel_values\"], torch.tensor([expected_tokenized.images] * 3)))\n+        expected_images_pt_tensor = torch.from_numpy(np.stack([expected_tokenized.images] * 3))\n+        self.assertTrue(torch.allclose(output[\"pixel_values\"], expected_images_pt_tensor))\n \n         output = self.tokenizer.apply_chat_template(\n             conversations, tokenize=True, return_dict=True, return_tensors=\"np\"\n@@ -1194,6 +1276,54 @@ def test_batch_apply_chat_template_with_continue_final_message(self):\n                 continue_final_message=True,\n             )\n \n+    def test_batch_apply_chat_template_with_add_generation_prompt(self):\n+        conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+            ],\n+        ]\n+\n+        # Test 1:\n+        # with add_generation_prompt\n+        for add_generation_prompt in [False, True]:\n+            expected_tokenized = [\n+                self.ref_tokenizer.encode_chat_completion(ChatCompletionRequest.from_openai(conversation))\n+                for conversation in conversations\n+            ]\n+            token_outputs = self.tokenizer.apply_chat_template(\n+                conversations, tokenize=True, add_generation_prompt=add_generation_prompt\n+            )\n+            for output, expected in zip(token_outputs, expected_tokenized):\n+                self.assertEqual(output, expected.tokens)\n+\n+        # Test 2:\n+        # with continue_final_message\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.apply_chat_template(\n+                conversations, tokenize=True, add_generation_prompt=True, continue_final_message=True\n+            )\n+\n+        # Test 3:\n+        # with last message with assistant role\n+        conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+                {\"role\": \"asistant\", \"content\": \"Hey!\"},\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+            ],\n+        ]\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.apply_chat_template(conversations, tokenize=True, add_generation_prompt=True)\n+\n     def test_batch_apply_chat_template_with_truncation(\n         self,\n     ):\n@@ -1799,3 +1929,16 @@ def test_batch_call_with_padding_and_truncation(self):\n                         for i, ids in enumerate(expected_tokens)\n                     ],\n                 )\n+\n+    def test_get_vocab(self):\n+        vocab = self.tokenizer.get_vocab()\n+        # loss of some tokens due to conversion\n+        self.assertNotEqual(len(vocab), len(self.tokenizer))\n+        for token, id_token in vocab.items():\n+            # Issue during conversion\n+            if id_token == 0 and token != \"<unk>\":\n+                continue\n+            self.assertEqual(self.tokenizer.convert_tokens_to_ids(token), id_token)\n+            self.assertEqual(\n+                self.ref_tokenizer.decode([id_token], special_token_policy=SpecialTokenPolicy.KEEP), token\n+            )"
        }
    ],
    "stats": {
        "total": 246,
        "additions": 199,
        "deletions": 47
    }
}