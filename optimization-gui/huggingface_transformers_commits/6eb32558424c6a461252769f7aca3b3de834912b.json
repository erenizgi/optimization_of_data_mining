{
    "author": "gante",
    "message": "[generate] Always use decoder config to init cache (#40772)\n\n* mega derp\n\n* fix\n\n* always use the decoder",
    "sha": "6eb32558424c6a461252769f7aca3b3de834912b",
    "files": [
        {
            "sha": "d7b0fe6e1f83a7d107123fd5ec9a6be09d94f2fb",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6eb32558424c6a461252769f7aca3b3de834912b/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6eb32558424c6a461252769f7aca3b3de834912b/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=6eb32558424c6a461252769f7aca3b3de834912b",
            "patch": "@@ -1010,7 +1010,7 @@ def __init__(\n         layers = []\n         # If a config is passed, use it to infer the layer types and initialize accordingly\n         if config is not None:\n-            config = config.get_text_config()\n+            config = config.get_text_config(decoder=True)\n             sliding_window = getattr(config, \"sliding_window\", None) or getattr(config, \"attention_chunk_size\", None)\n             layer_types = getattr(config, \"layer_types\", None)\n             if layer_types is None:\n@@ -1122,7 +1122,7 @@ def __init__(\n         offload_only_non_sliding: bool = True,\n         **kwargs,\n     ):\n-        config = config.get_text_config()\n+        config = config.get_text_config(decoder=True)\n         layer_types = getattr(config, \"layer_types\", None)\n         # If `layer_types` is not explicitly provided, infer if the model is fully sliding\n         if layer_types is None:"
        },
        {
            "sha": "0163e74e77cbc31dc3d8bbfb776f8d27a25c186a",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6eb32558424c6a461252769f7aca3b3de834912b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6eb32558424c6a461252769f7aca3b3de834912b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=6eb32558424c6a461252769f7aca3b3de834912b",
            "patch": "@@ -1867,7 +1867,7 @@ def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len:\n             self._cache = StaticCache(**self_attention_cache_kwargs)\n             if requires_cross_attention_cache:\n                 cross_attention_cache_kwargs = {\n-                    \"config\": self.config.get_text_config(encoder=True),\n+                    \"config\": self.config.get_text_config(decoder=True),\n                     \"max_cache_len\": model_kwargs[\"encoder_outputs\"][0].shape[1],\n                     \"offloading\": offload_cache,\n                 }\n@@ -1972,7 +1972,7 @@ def _prepare_cache_for_generation(\n         ):\n             dynamic_cache_kwargs = {}\n         else:\n-            dynamic_cache_kwargs = {\"config\": self.config}\n+            dynamic_cache_kwargs = {\"config\": self.config.get_text_config(decoder=True)}\n         if generation_config.cache_implementation is not None:\n             if generation_config.cache_implementation in ALL_STATIC_CACHE_IMPLEMENTATIONS:\n                 if generation_config.cache_implementation in DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS:"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}