{
    "author": "gante",
    "message": "ðŸš¨ [v5] Prune `prune_heads` (#41417)\n\n* remove _prune_heads\n\n* remove prune_heads\n\n* finalize the purge\n\n* remove another patterns",
    "sha": "7e475552be13be0d6c98adf02c199709a0d7b927",
    "files": [
        {
            "sha": "6311af79fb63a63161a02ade7de52cad0e43c823",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -345,7 +345,7 @@ Skipped : 124/323 (38.4%)\n  - bit: Bit does not use inputs_embeds\n  - blip: Blip does not use inputs_embeds\n  - blip_2: Inputs_embeds is tested in individual model tests\n- - bridgetower: \n+ - bridgetower:\n  - canine: CANINE does not have a get_input_embeddings() method.\n  - ...\n "
        },
        {
            "sha": "8051b9df3fa4a5efec65c3844d07ec2f93e7e730",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -46,10 +46,4 @@ Most of those are only useful if you are studying the code of the models in the\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward\n \n-[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n-\n-[[autodoc]] pytorch_utils.prune_layer\n-\n-[[autodoc]] pytorch_utils.prune_conv1d_layer\n-\n [[autodoc]] pytorch_utils.prune_linear_layer"
        },
        {
            "sha": "3935783ddaea88ee487f2320419a26ffb885df48",
            "filename": "docs/source/en/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -22,7 +22,6 @@ file or directory, or from a pretrained model configuration provided by the libr\n [`PreTrainedModel`] also implements a few methods which are common among all the models to:\n \n - resize the input token embeddings when new tokens are added to the vocabulary\n-- prune the attention heads of the model.\n \n The other methods that are common to each model are defined in [`~modeling_utils.ModuleUtilsMixin`] and [`~generation.GenerationMixin`].\n "
        },
        {
            "sha": "1fbb00788a049b5e01fec425483726c4eb60bb23",
            "filename": "docs/source/en/philosophy.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Fphilosophy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fen%2Fphilosophy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fphilosophy.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -56,7 +56,6 @@ A few other goals:\n - Incorporate a subjective selection of promising tools for fine-tuning and investigating these models:\n \n   - A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning.\n-  - Simple ways to mask and prune Transformer heads.\n \n ## Main concepts\n "
        },
        {
            "sha": "81f3ab8d9c8e823b284250fd5edcab96cd38d7ef",
            "filename": "docs/source/ja/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fja%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fja%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Finternal%2Fmodeling_utils.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,11 +29,4 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward\n \n-[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n-\n-[[autodoc]] pytorch_utils.prune_layer\n-\n-[[autodoc]] pytorch_utils.prune_conv1d_layer\n-\n [[autodoc]] pytorch_utils.prune_linear_layer\n-"
        },
        {
            "sha": "de4d87bdd51d7610cdd6fc91358e92dcc168c793",
            "filename": "docs/source/ko/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fko%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fko%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Finternal%2Fmodeling_utils.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,10 +29,4 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward\n \n-[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n-\n-[[autodoc]] pytorch_utils.prune_layer\n-\n-[[autodoc]] pytorch_utils.prune_conv1d_layer\n-\n [[autodoc]] pytorch_utils.prune_linear_layer"
        },
        {
            "sha": "b6a49c0feabf5164736779867cb109d1347c8a03",
            "filename": "docs/source/zh/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fzh%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/docs%2Fsource%2Fzh%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Finternal%2Fmodeling_utils.md?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,11 +29,4 @@ rendered properly in your Markdown viewer.\n \n [[autodoc]] pytorch_utils.apply_chunking_to_forward\n \n-[[autodoc]] pytorch_utils.find_pruneable_heads_and_indices\n-\n-[[autodoc]] pytorch_utils.prune_layer\n-\n-[[autodoc]] pytorch_utils.prune_conv1d_layer\n-\n [[autodoc]] pytorch_utils.prune_linear_layer\n-"
        },
        {
            "sha": "471e4096f2391286ff0813d18fe30ebef1aa05b3",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -462,7 +462,6 @@\n     _import_structure[\"pytorch_utils\"] = [\n         \"Conv1D\",\n         \"apply_chunking_to_forward\",\n-        \"prune_layer\",\n         \"infer_device\",\n     ]\n     _import_structure[\"time_series_utils\"] = []\n@@ -693,7 +692,6 @@\n     from .processing_utils import ProcessorMixin as ProcessorMixin\n     from .pytorch_utils import Conv1D as Conv1D\n     from .pytorch_utils import apply_chunking_to_forward as apply_chunking_to_forward\n-    from .pytorch_utils import prune_layer as prune_layer\n \n     # Tokenization\n     from .tokenization_utils import PreTrainedTokenizer as PreTrainedTokenizer"
        },
        {
            "sha": "fc4a00476859ef9b1933c3994e0064ffd8f84b42",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -123,11 +123,6 @@ class PreTrainedConfig(PushToHubMixin):\n         tie_encoder_decoder (`bool`, *optional*, defaults to `False`):\n             Whether all encoder weights should be tied to their equivalent decoder weights. This requires the encoder\n             and decoder model to have the exact same parameter names.\n-        prune_heads (`dict[int, list[int]]`, *optional*, defaults to `{}`):\n-            Pruned heads of the model. The keys are the selected layer indices and the associated values, the list of\n-            heads to prune in said layer.\n-\n-            For instance `{1: [0, 2], 2: [2, 3]}` will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n         chunk_size_feed_forward (`int`, *optional*, defaults to `0`):\n             The chunk size of all feed forward layers in the residual attention blocks. A chunk size of `0` means that\n             the feed forward layer is not chunked. A chunk size of n means that the feed forward layer processes `n` <\n@@ -214,7 +209,6 @@ def __init__(\n         torchscript: bool = False,\n         dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n         # Common arguments\n-        pruned_heads: Optional[dict[int, list[int]]] = None,\n         tie_word_embeddings: bool = True,\n         chunk_size_feed_forward: int = 0,\n         is_encoder_decoder: bool = False,\n@@ -279,7 +273,6 @@ def __init__(\n         self._output_attentions = output_attentions  # has public property\n \n         # Less common kwargs, only used by some models\n-        self.pruned_heads = pruned_heads if pruned_heads is not None else {}\n         self.tie_word_embeddings = tie_word_embeddings\n         self.chunk_size_feed_forward = chunk_size_feed_forward\n \n@@ -796,9 +789,6 @@ def from_dict(\n \n         config = cls(**config_dict)\n \n-        if hasattr(config, \"pruned_heads\"):\n-            config.pruned_heads = {int(key): value for key, value in config.pruned_heads.items()}\n-\n         # Update config with kwargs if needed\n         if \"num_labels\" in kwargs and \"id2label\" in kwargs:\n             num_labels = kwargs[\"num_labels\"]"
        },
        {
            "sha": "8509be218751bf1030ef15cbf922b9d12754d63a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 24,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -1806,8 +1806,7 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     [`PreTrainedModel`] takes care of storing the configuration of the models and handles methods for loading,\n     downloading and saving models as well as a few methods common to all models to:\n \n-        - resize the input embeddings,\n-        - prune heads in the self-attention heads.\n+        - resize the input embeddings\n \n     Class attributes (overridden by derived classes):\n \n@@ -3523,13 +3522,9 @@ def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]:\n \n     def init_weights(self):\n         \"\"\"\n-        If needed prunes and maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n+        Maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n         initialization logic in `_init_weights`.\n         \"\"\"\n-        # Prune heads if needed\n-        if self.config.pruned_heads:\n-            self.prune_heads(self.config.pruned_heads)\n-\n         if _init_weights:\n             # Initialize weights\n             self.initialize_weights()\n@@ -3538,23 +3533,6 @@ def init_weights(self):\n             # since from_pretrained(...) calls tie weights anyways\n             self.tie_weights()\n \n-    def prune_heads(self, heads_to_prune: dict[int, list[int]]):\n-        \"\"\"\n-        Prunes heads of the base model.\n-\n-        Arguments:\n-            heads_to_prune (`dict[int, list[int]]`):\n-                Dictionary with keys being selected layer indices (`int`) and associated values being the list of heads\n-                to prune in said layer (list of `int`). For instance {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on\n-                layer 1 and heads 2 and 3 on layer 2.\n-        \"\"\"\n-        # save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads\n-        for layer, heads in heads_to_prune.items():\n-            union_heads = set(self.config.pruned_heads.get(layer, [])) | set(heads)\n-            self.config.pruned_heads[layer] = list(union_heads)  # Unfortunately we have to store it as list for JSON\n-\n-        self.base_model._prune_heads(heads_to_prune)\n-\n     def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n         \"\"\"\n         Activates gradient checkpointing for the current model."
        },
        {
            "sha": "d9d7c67ac5b0ee7a81cce59bb0e17c48fa281310",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -36,8 +36,6 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n-    find_pruneable_heads_and_indices,\n-    prune_linear_layer,\n )\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n@@ -170,28 +168,9 @@ def __init__(self, config: AlbertConfig):\n         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.pruned_heads = set()\n \n         self.is_causal = False\n \n-    def prune_heads(self, heads: list[int]) -> None:\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.query = prune_linear_layer(self.query, index)\n-        self.key = prune_linear_layer(self.key, index)\n-        self.value = prune_linear_layer(self.value, index)\n-        self.dense = prune_linear_layer(self.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.num_attention_heads = self.num_attention_heads - len(heads)\n-        self.all_head_size = self.attention_head_size * self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -401,23 +380,6 @@ def get_input_embeddings(self) -> nn.Embedding:\n     def set_input_embeddings(self, value: nn.Embedding) -> None:\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} ALBERT has\n-        a different architecture in that its layers are shared across groups, which then has inner groups. If an ALBERT\n-        model has 12 hidden layers and 2 hidden groups, with two inner groups, there is a total of 4 different layers.\n-\n-        These layers are flattened: the indices [0,1] correspond to the two inner groups of the first hidden layer,\n-        while [2,3] correspond to the two inner groups of the second hidden layer.\n-\n-        Any layer with in index other than [0,1,2,3] will result in an error. See base class PreTrainedModel for more\n-        information about head pruning\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            group_idx = int(layer / self.config.inner_group_num)\n-            inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n-            self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ca32b2b6eb116c9424ebba8e9a8adabe7a65c549",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -30,7 +30,7 @@\n     BaseModelOutputWithPoolingAndNoAttention,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging\n from .configuration_align import AlignConfig, AlignTextConfig, AlignVisionConfig\n \n@@ -666,25 +666,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = AlignTextSelfAttention(config)\n         self.output = AlignTextSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "bddf6663c784d6ab3d0b915530b00b89d63af996",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -30,7 +30,7 @@\n     BaseModelOutputWithPoolingAndProjection,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_altclip import AltCLIPConfig, AltCLIPTextConfig, AltCLIPVisionConfig\n \n@@ -278,25 +278,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = ALT_ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.output = AltRobertaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -973,14 +954,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     # Copied from transformers.models.clap.modeling_clap.ClapTextModel.forward\n     def forward("
        },
        {
            "sha": "e27d4eae9f8c1bcc4147fe60ee99aec260702167",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -24,7 +24,6 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, SequenceClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_audio_spectrogram_transformer import ASTConfig\n@@ -202,25 +201,6 @@ def __init__(self, config: ASTConfig):\n         super().__init__()\n         self.attention = ASTSelfAttention(config)\n         self.output = ASTSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -355,14 +335,6 @@ def __init__(self, config: ASTConfig) -> None:\n     def get_input_embeddings(self) -> ASTPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "699653d2e82f76c1009f879444579c37c3fd53b1",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -35,7 +35,7 @@\n     SemanticSegmenterOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import compile_compatible_method_lru_cache, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import auto_docstring, logging, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_beit import BeitConfig\n@@ -415,25 +415,6 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n         super().__init__()\n         self.attention = BEIT_SELF_ATTENTION_CLASSES[config._attn_implementation](config, window_size=window_size)\n         self.output = BeitSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -761,14 +742,6 @@ def __init__(self, config: BeitConfig, add_pooling_layer: bool = True) -> None:\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "e364ef0676dd922c564efb34528561f06eca2b82",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -42,7 +42,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_bert import BertConfig\n@@ -317,25 +317,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = BertCrossAttention if is_cross_attention else BertSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = BertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -672,14 +653,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f9fbb80b31d4ac561ba51bd6a9805567095bad9e",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n@@ -251,25 +251,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = BertGenerationCrossAttention if is_cross_attention else BertGenerationSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = BertGenerationSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -536,14 +517,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "9b59cff3621a6e796a99c15c1ffa850f6e7b16a7",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,7 @@\n     CausalLMOutputWithCrossAttentions,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import logging\n from .configuration_blip import BlipTextConfig\n \n@@ -233,25 +233,6 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         super().__init__()\n         self.self = BlipTextSelfAttention(config, is_cross_attention, layer_idx=layer_idx)\n         self.output = BlipTextSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -567,15 +548,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    # Copied from transformers.models.bert.modeling_bert.BertModel._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def get_extended_attention_mask(\n         self, attention_mask: Tensor, input_shape: tuple[int], device: device, is_decoder: bool\n     ) -> Tensor:"
        },
        {
            "sha": "7c9c3288be457fd901474bc6d46e95b5199f84a1",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -36,7 +36,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     ModelOutput,\n     TransformersKwargs,\n@@ -642,25 +642,6 @@ def __init__(self, config, is_cross_attention=False):\n         super().__init__()\n         self.attention = Blip2QFormerMultiHeadAttention(config, is_cross_attention)\n         self.output = Blip2QFormerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -917,14 +898,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def get_extended_attention_mask(\n         self,\n         attention_mask: torch.Tensor,"
        },
        {
            "sha": "bad1097128ae4d7d2fb7077bb802f9fd8a50df80",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -36,7 +36,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging, torch_int\n from ...utils.generic import can_return_tuple\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n@@ -596,25 +596,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = BridgeTowerCrossAttention if is_cross_attention else BridgeTowerSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = BridgeTowerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -1023,14 +1004,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @can_return_tuple\n     @auto_docstring\n     # NOTE: bridgetower with its multimodality has a more complicated scheme making records harder"
        },
        {
            "sha": "3e7b4b40cb84430a48eee451b88e67352a8f8688",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 1,
            "deletions": 31,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -30,7 +30,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n from .configuration_bros import BrosConfig\n \n@@ -280,28 +280,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = BrosSelfAttention(config)\n         self.output = BrosSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads,\n-            self.self.num_attention_heads,\n-            self.self.attention_head_size,\n-            self.pruned_heads,\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -578,14 +556,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "b635b5e55002aad305eea2d16e470d63863893b6",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -44,7 +44,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_camembert import CamembertConfig\n@@ -257,25 +257,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = CamembertCrossAttention if is_cross_attention else CamembertSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = CamembertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -652,14 +633,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "8965ae9a3f7cf90d27b454128a44f5eb2f172b92",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -34,7 +34,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, logging\n from .configuration_canine import CanineConfig\n \n@@ -408,7 +408,6 @@ def __init__(\n         super().__init__()\n         self.self = CanineSelfAttention(config)\n         self.output = CanineSelfOutput(config)\n-        self.pruned_heads = set()\n \n         # additional arguments related to local attention\n         self.local = local\n@@ -427,24 +426,6 @@ def __init__(\n         self.attend_to_chunk_width = attend_to_chunk_width\n         self.attend_to_chunk_stride = attend_to_chunk_stride\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def forward(\n         self,\n         hidden_states: tuple[torch.FloatTensor],\n@@ -790,14 +771,6 @@ def __init__(self, config, add_pooling_layer=True):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def _create_3d_attention_mask_from_input_mask(self, from_tensor, to_mask):\n         \"\"\"\n         Create 3D attention mask from a 2D tensor mask."
        },
        {
            "sha": "37e2848283e7b13461ae322649c6f25f99e9991c",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,7 +29,7 @@\n     BaseModelOutputWithPoolingAndCrossAttentions,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_chinese_clip import ChineseCLIPConfig, ChineseCLIPTextConfig, ChineseCLIPVisionConfig\n \n@@ -335,25 +335,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = ChineseCLIPTextSelfAttention(config)\n         self.output = ChineseCLIPTextSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -837,14 +818,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "08961883467e608e71b108fdd829aa7a4c076f20",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,7 @@\n     BaseModelOutputWithPoolingAndCrossAttentions,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward, meshgrid\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_clap import ClapAudioConfig, ClapConfig, ClapTextConfig\n \n@@ -454,25 +454,6 @@ def __init__(self, config, dim, num_heads, window_size):\n         super().__init__()\n         self.self = ClapAudioSelfAttention(config, dim, num_heads, window_size)\n         self.output = ClapAudioSelfOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -1168,25 +1149,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = ClapTextSelfAttention(config)\n         self.output = ClapTextSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "ae9df233d4d6563a7af7da4287b1a0db5d074628",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -1003,13 +1003,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.input_embeds_layer = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.layers[layer].attn.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "2a78a283bb3dd864bec34b4329a303aef2a448a6",
            "filename": "src/transformers/models/convbert/modeling_convbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_convbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -32,7 +32,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n     logging,\n@@ -296,25 +296,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = ConvBertSelfAttention(config)\n         self.output = ConvBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -644,14 +625,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "7e13aa18911653b4043e26fc2d034a48001b70c1",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -26,7 +26,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import Conv1D\n from ...utils import (\n     auto_docstring,\n     logging,\n@@ -93,24 +93,6 @@ def __init__(self, d_model_size, num_heads, layer_idx=None):\n         self.Wv = nn.Linear(d_model_size, d_model_size)\n \n         self.dense = nn.Linear(d_model_size, d_model_size)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        attention_head_size = self.d_model_size // self.num_heads\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, attention_head_size, self.pruned_heads)\n-\n-        # Prune linear layers\n-        self.Wq = prune_linear_layer(self.Wq, index)\n-        self.Wk = prune_linear_layer(self.Wk, index)\n-        self.Wv = prune_linear_layer(self.Wv, index)\n-        self.dense = prune_linear_layer(self.dense, index, dim=1)\n-\n-        # Update hyper params\n-        self.num_heads = self.num_heads - len(heads)\n-        self.d_model_size = attention_head_size * self.num_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def split_into_heads(self, x, batch_size):\n         x = x.reshape(batch_size, -1, self.num_heads, self.depth)\n@@ -251,13 +233,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.w = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.h[layer].multi_head_attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1327a410d03d5449694d8b4db8e919382c1fc45a",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -24,7 +24,6 @@\n \n from ...modeling_outputs import ImageClassifierOutputWithNoAttention, ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_cvt import CvtConfig\n \n@@ -294,25 +293,6 @@ def __init__(\n             with_cls_token,\n         )\n         self.output = CvtSelfOutput(embed_dim, drop_rate)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_state, height, width):\n         self_output = self.attention(hidden_state, height, width)\n@@ -537,14 +517,6 @@ def __init__(self, config, add_pooling_layer=True):\n         self.encoder = CvtEncoder(config)\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "53d01d99b816294cb02591b624b0acd73fa0e3bd",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -43,7 +43,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_data2vec_text import Data2VecTextConfig\n@@ -362,25 +362,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = Data2VecTextCrossAttention if is_cross_attention else Data2VecTextSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = Data2VecTextSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -612,14 +593,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "caabeb7235fc53fa34648220137fc415d684dccc",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -33,7 +33,7 @@\n     SemanticSegmenterOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import compile_compatible_method_lru_cache, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import auto_docstring, logging, torch_int\n from .configuration_data2vec_vision import Data2VecVisionConfig\n \n@@ -421,25 +421,6 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n             config, window_size=window_size\n         )\n         self.output = Data2VecVisionSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -776,14 +757,6 @@ def __init__(self, config: Data2VecVisionConfig, add_pooling_layer: bool = False\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "e5432c730404e396a5545d12c8078e0232628de7",
            "filename": "src/transformers/models/deberta/modeling_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_deberta.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -652,13 +652,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.embeddings.word_embeddings = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        raise NotImplementedError(\"The prune function is not implemented in DeBERTa model.\")\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "28e6c87c71a5fc3b94b69db4d3b6ae0356b386fb",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -729,13 +729,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.embeddings.word_embeddings = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        raise NotImplementedError(\"The prune function is not implemented in DeBERTa model.\")\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "f194d3748d322157c04d8ff70a659e1ad2b99656",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -26,7 +26,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n+from ...pytorch_utils import Conv1D\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -122,23 +122,6 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n         self.is_causal = True\n \n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n-        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n-\n-        # Prune conv1d layers\n-        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n-        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n-\n-        # Update hyper params\n-        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n-        self.num_heads = self.num_heads - len(heads)\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()"
        },
        {
            "sha": "7097a0cbd633ff6cfaf76082c79ff1b169b062bc",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,6 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_deit import DeiTConfig\n@@ -267,25 +266,6 @@ def __init__(self, config: DeiTConfig):\n         super().__init__()\n         self.attention = DeiTSelfAttention(config)\n         self.output = DeiTSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -430,14 +410,6 @@ def __init__(self, config: DeiTConfig, add_pooling_layer: bool = True, use_mask_\n     def get_input_embeddings(self) -> DeiTPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1aaccbe3f1469acd3b46e492e2d596b3b177de60",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -32,7 +32,6 @@\n     TokenClassifierOutput,\n )\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_ernie_m import ErnieMConfig\n \n@@ -220,25 +219,6 @@ def __init__(self, config, position_embedding_type=None):\n         super().__init__()\n         self.self_attn = ErnieMSelfAttention(config, position_embedding_type=position_embedding_type)\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self_attn.num_attention_heads, self.self_attn.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self_attn.q_proj = prune_linear_layer(self.self_attn.q_proj, index)\n-        self.self_attn.k_proj = prune_linear_layer(self.self_attn.k_proj, index)\n-        self.self_attn.v_proj = prune_linear_layer(self.self_attn.v_proj, index)\n-        self.out_proj = prune_linear_layer(self.out_proj, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self_attn.num_attention_heads = self.self_attn.num_attention_heads - len(heads)\n-        self.self_attn.all_head_size = self.self_attn.attention_head_size * self.self_attn.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "4f74c775a36a96a34e1f67b1d9263059d61eaf18",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,7 @@\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, CausalLMOutput\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ....pytorch_utils import apply_chunking_to_forward\n from ....utils import logging\n from .configuration_mctct import MCTCTConfig\n \n@@ -298,25 +298,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = MCTCTSelfAttention(config)\n         self.output = MCTCTSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "4f16a1bfbafd17b40394ae4a9bf92375bf18657c",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -24,7 +24,6 @@\n from ....activations import ACT2FN\n from ....modeling_outputs import BackboneOutput\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import (\n     ModelOutput,\n     OptionalDependencyNotAvailable,\n@@ -362,25 +361,6 @@ def __init__(self, config, dim, num_heads, kernel_size):\n         super().__init__()\n         self.self = NeighborhoodAttention(config, dim, num_heads, kernel_size)\n         self.output = NeighborhoodAttentionOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "0a49da44480c5b7b6510a026568cdc06acf8313a",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -37,7 +37,7 @@\n     TokenClassifierOutput,\n )\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ....pytorch_utils import apply_chunking_to_forward\n from ....utils import (\n     ModelOutput,\n     add_code_sample_docstrings,\n@@ -285,25 +285,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = NezhaSelfAttention(config)\n         self.output = NezhaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "87510e258b77f5c1b9d15096cd82a5f87501a5dc",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -38,7 +38,6 @@\n     TokenClassifierOutput,\n )\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -289,25 +288,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = QDQBertSelfAttention(config)\n         self.output = QDQBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "6b1612778c715eb8f924d50596f8eddc05bcf4d9",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -32,7 +32,7 @@\n     ModelOutput,\n )\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ....pytorch_utils import apply_chunking_to_forward\n from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from .configuration_realm import RealmConfig\n \n@@ -261,25 +261,6 @@ def __init__(self, config, position_embedding_type=None):\n             config, position_embedding_type=position_embedding_type\n         )\n         self.output = RealmSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "9cdba679bc0aef01488a8a884f578225bcfef1e2",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,6 @@\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, SequenceClassifierOutput\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import (\n     ModelOutput,\n     add_start_docstrings,\n@@ -419,25 +418,6 @@ def __init__(self, config):\n         super().__init__()\n         self.attention = TvltSelfAttention(config)\n         self.output = TvltSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         self_outputs = self.attention(hidden_states, attention_mask, output_attentions)"
        },
        {
            "sha": "efa98eada009dd3fae4cb1db8777f39bc632fe16",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -25,7 +25,6 @@\n from ....modeling_layers import GradientCheckpointingLayer\n from ....modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -307,25 +306,6 @@ def __init__(self, config: ViTHybridConfig) -> None:\n         super().__init__()\n         self.attention = ViTHybridSelfAttention(config)\n         self.output = ViTHybridSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]) -> None:\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "298e8b6786f693365a93681bf44ebb9d3223f85f",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -24,7 +24,6 @@\n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n     OptionalDependencyNotAvailable,\n@@ -328,25 +327,6 @@ def __init__(self, config, dim, num_heads, kernel_size, dilation):\n         super().__init__()\n         self.self = NeighborhoodAttention(config, dim, num_heads, kernel_size, dilation)\n         self.output = NeighborhoodAttentionOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -618,14 +598,6 @@ def __init__(self, config, add_pooling_layer=True):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "23a18adab53a542c17bc709ff241dfcc9d5bee44",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -25,7 +25,6 @@\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import can_return_tuple, check_model_inputs\n@@ -255,25 +254,6 @@ def __init__(self, config: Dinov2Config):\n         super().__init__()\n         self.attention = Dinov2SelfAttention(config)\n         self.output = Dinov2SelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -482,14 +462,6 @@ def __init__(self, config: Dinov2Config):\n     def get_input_embeddings(self) -> Dinov2PatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "a32eb78ec24bd996f39ccfce6d6520080ae7a95d",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import can_return_tuple, check_model_inputs\n@@ -272,25 +271,6 @@ def __init__(self, config: Dinov2WithRegistersConfig):\n         super().__init__()\n         self.attention = Dinov2WithRegistersSelfAttention(config)\n         self.output = Dinov2WithRegistersSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -499,14 +479,6 @@ def __init__(self, config: Dinov2WithRegistersConfig):\n     def get_input_embeddings(self) -> Dinov2WithRegistersPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "834f7599dd55a5fd706cd374f3e767b706212ff9",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -42,8 +42,6 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n-    find_pruneable_heads_and_indices,\n-    prune_linear_layer,\n )\n from ...utils import (\n     TransformersKwargs,\n@@ -178,24 +176,6 @@ def __init__(self, config: PreTrainedConfig):\n         self.dropout = nn.Dropout(p=config.attention_dropout)\n         self.is_causal = False\n \n-        self.pruned_heads: set[int] = set()\n-\n-    def prune_heads(self, heads: list[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.attention_head_size, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q_lin = prune_linear_layer(self.q_lin, index)\n-        self.k_lin = prune_linear_layer(self.k_lin, index)\n-        self.v_lin = prune_linear_layer(self.v_lin, index)\n-        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.dim = self.attention_head_size * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -407,14 +387,6 @@ def get_input_embeddings(self) -> nn.Embedding:\n     def set_input_embeddings(self, new_embeddings: nn.Embedding):\n         self.embeddings.word_embeddings = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[list[int]]]):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.transformer.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "0d45ee784e85b887d87caf1c7486f230dc343d8d",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import meshgrid\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_donut_swin import DonutSwinConfig\n \n@@ -471,25 +471,6 @@ def __init__(self, config, dim, num_heads, window_size):\n         super().__init__()\n         self.self = DonutSwinSelfAttention(config, dim, num_heads, window_size)\n         self.output = DonutSwinSelfOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -850,14 +831,6 @@ def __init__(self, config, add_pooling_layer=True, use_mask_token=False):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "468b385d4224510e4d028205b6e7c85b66f973cd",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import load_backbone\n from ...utils.generic import can_return_tuple, check_model_inputs\n@@ -373,25 +372,6 @@ def __init__(self, config: DPTConfig):\n         super().__init__()\n         self.attention = DPTSelfAttention(config)\n         self.output = DPTViTSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -793,14 +773,6 @@ def get_input_embeddings(self):\n         else:\n             return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f1d1520cb2034f77113e7281791a17dc6f63a9cd",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -39,7 +39,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     ModelOutput,\n     TransformersKwargs,\n@@ -327,25 +327,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = ElectraCrossAttention if is_cross_attention else ElectraSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = ElectraSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -612,14 +593,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "cc403910c34cde7a37c3b8afc8d2ed4356353607",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -46,7 +46,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_ernie import ErnieConfig\n@@ -333,25 +333,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = ErnieCrossAttention if is_cross_attention else ErnieSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = ErnieSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -633,14 +614,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ccf62fd5e6f42c7532d5eb22ca0b46094c1cc22c",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -33,7 +33,6 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_esm import EsmConfig\n@@ -384,26 +383,8 @@ def __init__(self, config, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.self = EsmSelfAttention(config, layer_idx=layer_idx, is_cross_attention=is_cross_attention)\n         self.output = EsmSelfOutput(config)\n-        self.pruned_heads = set()\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n \n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n     def forward(\n         self,\n@@ -636,14 +617,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f36950fbce6faa721ba428ff99824daec27580df",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n@@ -349,26 +348,8 @@ def __init__(self, config, layer_idx=None, is_cross_attention=False):\n         super().__init__()\n         self.self = EvollaSaProtSelfAttention(config, layer_idx=layer_idx, is_cross_attention=is_cross_attention)\n         self.output = EvollaSaProtSelfOutput(config)\n-        self.pruned_heads = set()\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n \n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n     def forward(\n         self,\n@@ -568,14 +549,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     def forward(\n         self,"
        },
        {
            "sha": "ca22caabe43bfa622a9343ac0fb85a4a8b5e928b",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -236,14 +236,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     def forward(\n         self,"
        },
        {
            "sha": "c001c05b1c57e718339395938a8912af585a10b2",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 33,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -35,7 +35,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_flaubert import FlaubertConfig\n \n@@ -93,22 +93,6 @@ def __init__(self, n_heads, dim, config, layer_idx: int = 0):\n         self.k_lin = nn.Linear(dim, dim)\n         self.v_lin = nn.Linear(dim, dim)\n         self.out_lin = nn.Linear(dim, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        attention_head_size = self.dim // self.n_heads\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)\n-        # Prune linear layers\n-        self.q_lin = prune_linear_layer(self.q_lin, index)\n-        self.k_lin = prune_linear_layer(self.k_lin, index)\n-        self.v_lin = prune_linear_layer(self.v_lin, index)\n-        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.dim = attention_head_size * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -766,13 +750,6 @@ def __init__(self, config):  # , dico, is_encoder, with_output):\n             self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, config=config))\n             self.layer_norm2.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n \n-        if hasattr(config, \"pruned_heads\"):\n-            pruned_heads = config.pruned_heads.copy().items()\n-            config.pruned_heads = {}\n-            for layer, heads in pruned_heads:\n-                if self.attentions[int(layer)].n_heads == config.n_heads:\n-                    self.prune_heads({int(layer): list(map(int, heads))})\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -790,15 +767,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.embeddings = new_embeddings\n \n-    # Copied from transformers.models.xlm.modeling_xlm.XLMModel._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.attentions[layer].prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "556f3b411f7dff525bee339b834059445f6fe2d8",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, filter_out_non_signature_kwargs, logging, torch_int\n from .configuration_flava import (\n     FlavaConfig,\n@@ -511,25 +510,6 @@ def __init__(self, config: FlavaPossibleConfigs) -> None:\n         super().__init__()\n         self.attention = FlavaSelfAttention(config)\n         self.output = FlavaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]) -> None:\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -741,14 +721,6 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value: nn.Module):\n         self.embeddings.patch_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -827,14 +799,6 @@ def get_input_embeddings(self) -> PatchEmbeddings:\n     def set_input_embeddings(self, value: nn.Module):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -929,14 +893,6 @@ def __init__(self, config: FlavaMultimodalConfig, add_pooling_layer=True):\n \n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "b1fa08479f4f2789c60bf64729d5031d3affb205",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -34,7 +34,7 @@\n     CausalLMOutputWithPast,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -224,26 +224,6 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.self = GIT_SELF_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n         self.output = GitSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -916,14 +896,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def _generate_future_mask(self, size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n         # Default mask is for forward direction. Flip for backward direction.\n         mask = torch.triu(torch.ones(size, size, device=device, dtype=dtype), diagonal=1)"
        },
        {
            "sha": "a28158e2c15916dfbcf3e2df8baea94d1e81efca",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -23,7 +23,6 @@\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_glpn import GLPNConfig\n \n@@ -202,25 +201,6 @@ def __init__(self, config, hidden_size, num_attention_heads, sequence_reduction_\n             sequence_reduction_ratio=sequence_reduction_ratio,\n         )\n         self.output = GLPNSelfOutput(config, hidden_size=hidden_size)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, height, width, output_attentions=False):\n         self_outputs = self.self(hidden_states, height, width, output_attentions)\n@@ -437,14 +417,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     # Copied from transformers.models.segformer.modeling_segformer.SegformerModel.forward\n     def forward("
        },
        {
            "sha": "35eacfe98457b8796720ea8e1b515bae0c2534b1",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -37,7 +37,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n+from ...pytorch_utils import Conv1D\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -131,23 +131,6 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n         self.is_causal = True\n \n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n-        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n-\n-        # Prune conv1d layers\n-        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n-        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n-\n-        # Update hyper params\n-        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n-        self.num_heads = self.num_heads - len(heads)\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n         bsz, num_heads, q_seq_len, dk = query.size()\n@@ -585,13 +568,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.wte = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.h[layer].attn.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "0672932a514398339dbb479e2ea3482180af6373",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -836,14 +836,6 @@ def __init__(self, config: HieraConfig, add_pooling_layer: bool = True, is_mae:\n     def get_input_embeddings(self) -> HieraPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "bbc86018a6eacc3c1a793c5abb5592ed228c1927",
            "filename": "src/transformers/models/ibert/modeling_ibert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fmodeling_ibert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -35,7 +35,6 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_ibert import IBertConfig\n from .quant_modules import IntGELU, IntLayerNorm, IntSoftmax, QuantAct, QuantEmbedding, QuantLinear\n@@ -349,25 +348,6 @@ def __init__(self, config):\n         self.quant_mode = config.quant_mode\n         self.self = IBertSelfAttention(config)\n         self.output = IBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -659,14 +639,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "43592d69c829e9eb84d9da811dcfab6393cf65b3",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -15,7 +15,6 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, torch_int\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_ijepa import IJepaConfig\n@@ -243,25 +242,6 @@ def __init__(self, config: IJepaConfig):\n         super().__init__()\n         self.attention = IJepaSelfAttention(config)\n         self.output = IJepaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -418,14 +398,6 @@ def __init__(self, config: IJepaConfig, add_pooling_layer: bool = False, use_mas\n     def get_input_embeddings(self) -> IJepaPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "8a4038c58478f79e969c4d77a4c161903ace7a44",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,7 @@\n     SequenceClassifierOutputWithPast,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n+from ...pytorch_utils import Conv1D\n from ...utils import (\n     auto_docstring,\n     logging,\n@@ -98,23 +98,6 @@ def __init__(self, config, is_cross_attention: Optional[bool] = False, layer_idx\n         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n \n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(heads, self.num_heads, self.head_dim, self.pruned_heads)\n-        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n-\n-        # Prune conv1d layers\n-        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n-        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n-\n-        # Update hyper params\n-        self.split_size = (self.split_size // self.num_heads) * (self.num_heads - len(heads))\n-        self.num_heads = self.num_heads - len(heads)\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def _attn(self, query, key, value, attention_mask=None):\n         attn_weights = torch.matmul(query, key.transpose(-1, -2))\n \n@@ -434,13 +417,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.wte = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.h[layer].attn.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "3534ea2bb2ea69bb4c66ca55d7019b9b599a18fe",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -33,7 +33,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n@@ -546,25 +546,6 @@ def __init__(self, config, is_cross_attention=False):\n         super().__init__()\n         self.attention = InstructBlipQFormerMultiHeadAttention(config, is_cross_attention)\n         self.output = InstructBlipQFormerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -825,14 +806,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def get_extended_attention_mask(\n         self,\n         attention_mask: torch.Tensor,"
        },
        {
            "sha": "744b80da3325461ae93f5e641517ffac36b08f8b",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -38,7 +38,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n@@ -511,25 +511,6 @@ def __init__(self, config, is_cross_attention=False):\n         super().__init__()\n         self.attention = InstructBlipVideoQFormerMultiHeadAttention(config, is_cross_attention)\n         self.output = InstructBlipVideoQFormerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -787,14 +768,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def get_extended_attention_mask(\n         self,\n         attention_mask: torch.Tensor,"
        },
        {
            "sha": "565e9105c8b00276055b2a6899a5ab21903a1430",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -1270,15 +1270,6 @@ def __init__(self, config: Kosmos2_5VisionConfig):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_projection\n \n-    # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionModel._prune_heads\n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     # Similar to transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionModel.forward without docstring\n     def forward(\n         self,"
        },
        {
            "sha": "5f108cf5df338d70008434a132e0a7135ce94e4c",
            "filename": "src/transformers/models/layoutlm/modeling_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_layoutlm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_layoutlm import LayoutLMConfig\n \n@@ -220,25 +220,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = LayoutLMSelfAttention(config)\n         self.output = LayoutLMSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -485,14 +466,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "848279495ca23e806dc1607a2d7849f0fb419699",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -606,14 +606,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def init_visual_bbox(self, image_size=(14, 14), max_len=1000):\n         \"\"\"\n         Create the bounding boxes for the visual (patch) tokens."
        },
        {
            "sha": "31157b749e94c2e9461ec3eb656b1e8924d33554",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, logging\n from .configuration_lilt import LiltConfig\n \n@@ -308,32 +308,12 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.self = LiltSelfAttention(config, layer_idx=layer_idx)\n         self.output = LiltSelfOutput(config)\n-        self.pruned_heads = set()\n \n         ori_hidden_size = config.hidden_size\n         config.hidden_size = config.hidden_size // config.channel_shrink_ratio\n         self.layout_output = LiltSelfOutput(config)\n         config.hidden_size = ori_hidden_size\n \n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -560,14 +540,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "8efb326c4c288e41a95757815e2a1fb7d73b901a",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -25,7 +25,7 @@\n from ...activations import ACT2FN, gelu\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_longformer import LongformerConfig\n \n@@ -1078,25 +1078,6 @@ def __init__(self, config, layer_id=0):\n         super().__init__()\n         self.self = LongformerSelfAttention(config, layer_id)\n         self.output = LongformerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -1383,14 +1364,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def _pad_to_window_size(\n         self,\n         input_ids: torch.Tensor,"
        },
        {
            "sha": "e2e8b0c3b2649b820e9e374a1601ea3aff687486",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 77,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -34,7 +34,6 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -352,24 +351,8 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -520,13 +503,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)\n@@ -568,25 +545,8 @@ def __init__(self, config: LongT5Config, has_relative_attention_bias: bool = Fal\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Attention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     # Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket\n@@ -758,30 +718,12 @@ def __init__(self, config: LongT5Config, has_relative_attention_bias: bool = Fal\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n \n         # Relativen attention bias & Layer norm for global attention\n         if self.has_relative_attention_bias:\n             self.global_relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n         self.global_input_layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Attention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     @staticmethod\n     # Copied from transformers.models.t5.modeling_t5.T5Attention._relative_position_bucket\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -1702,14 +1644,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2055,14 +1989,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ccc36301d35f382bf144bf0ed30d7c60cd18652a",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -529,10 +529,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = LukeSelfAttention(config)\n         self.output = LukeSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        raise NotImplementedError(\"LUKE does not support the pruning of attention heads\")\n \n     def forward(\n         self,\n@@ -823,9 +819,6 @@ def get_entity_embeddings(self):\n     def set_entity_embeddings(self, value):\n         self.entity_embeddings.entity_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        raise NotImplementedError(\"LUKE does not support the pruning of attention heads\")\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "abd98127e77e09c7ca423353b7ea12b5d248a310",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -32,7 +32,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_markuplm import MarkupLMConfig\n \n@@ -407,25 +407,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = MarkupLMSelfAttention(config)\n         self.output = MarkupLMSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -582,14 +563,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "4e02ce177efefbc9f358f0cddd57b63b87138997",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import meshgrid\n from ...utils import auto_docstring, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_maskformer_swin import MaskFormerSwinConfig\n@@ -421,25 +421,6 @@ def __init__(self, config, dim, num_heads, window_size):\n         super().__init__()\n         self.self = MaskFormerSwinSelfAttention(config, dim, num_heads, window_size)\n         self.output = MaskFormerSwinSelfOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -751,14 +732,6 @@ def __init__(self, config, add_pooling_layer=True):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def forward(\n         self,\n         pixel_values=None,"
        },
        {
            "sha": "2294881a6b983dee770b3a23c6c4127447ba5f14",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -40,7 +40,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_megatron_bert import MegatronBertConfig\n \n@@ -221,25 +221,6 @@ def __init__(self, config, layer_idx=None):\n         self.ln = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.self = MegatronBertSelfAttention(config, layer_idx=layer_idx)\n         self.output = MegatronBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -622,14 +603,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1c48fff547cfd9a8d87bc8ffde860894eff80314",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -43,7 +43,6 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_mobilebert import MobileBertConfig\n@@ -249,25 +248,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = MobileBertSelfAttention(config)\n         self.output = MobileBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -645,14 +625,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "08698fa4bd20a3b07cc83ea50f99e8089aa79ff1",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -198,9 +198,6 @@ def __init__(self, config: MobileNetV1Config, add_pooling_layer: bool = True):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        raise NotImplementedError\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "5e5fe81548ef29c34374131f67433f7494d82420",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -334,9 +334,6 @@ def __init__(self, config: MobileNetV2Config, add_pooling_layer: bool = True):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        raise NotImplementedError\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d1028d9bc61cda2f8d26f737c5438a4ec655c332",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -32,7 +32,6 @@\n     SemanticSegmenterOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging, torch_int\n from .configuration_mobilevit import MobileViTConfig\n \n@@ -262,25 +261,6 @@ def __init__(self, config: MobileViTConfig, hidden_size: int) -> None:\n         super().__init__()\n         self.attention = MobileViTSelfAttention(config, hidden_size)\n         self.output = MobileViTSelfOutput(config, hidden_size)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]) -> None:\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_outputs = self.attention(hidden_states)\n@@ -670,16 +650,6 @@ def __init__(self, config: MobileViTConfig, expand_output: bool = True):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"Prunes heads of the model.\n-        heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base class PreTrainedModel\n-        \"\"\"\n-        for layer_index, heads in heads_to_prune.items():\n-            mobilevit_layer = self.encoder.layer[layer_index]\n-            if isinstance(mobilevit_layer, MobileViTLayer):\n-                for transformer_layer in mobilevit_layer.transformer.layer:\n-                    transformer_layer.attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d2c0b599ce76b3f4812725f2f65557aabe99a079",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -614,16 +614,6 @@ def __init__(self, config: MobileViTV2Config, expand_output: bool = True):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"Prunes heads of the model.\n-        heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base class PreTrainedModel\n-        \"\"\"\n-        for layer_index, heads in heads_to_prune.items():\n-            mobilevitv2_layer = self.encoder.layer[layer_index]\n-            if isinstance(mobilevitv2_layer, MobileViTV2Layer):\n-                for transformer_layer in mobilevitv2_layer.transformer.layer:\n-                    transformer_layer.attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "dbe835ea93367b5a0e12f3348097462ce93fdd74",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -481,7 +481,6 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n-        self.pruned_heads = set()\n \n     def forward(\n         self,"
        },
        {
            "sha": "d93282bacc1030dc056960ebf54c3819e2a93072",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -678,7 +678,6 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n-        self.pruned_heads = set()\n \n     def forward(\n         self,"
        },
        {
            "sha": "233073814388ad5f8c3ce2404f2e6932c29c7fd9",
            "filename": "src/transformers/models/mpnet/modeling_mpnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Fmodeling_mpnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -33,7 +33,6 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_mpnet import MPNetConfig\n \n@@ -202,24 +201,6 @@ def __init__(self, config):\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attn.num_attention_heads, self.attn.attention_head_size, self.pruned_heads\n-        )\n-\n-        self.attn.q = prune_linear_layer(self.attn.q, index)\n-        self.attn.k = prune_linear_layer(self.attn.k, index)\n-        self.attn.v = prune_linear_layer(self.attn.v, index)\n-        self.attn.o = prune_linear_layer(self.attn.o, index, dim=1)\n-\n-        self.attn.num_attention_heads = self.attn.num_attention_heads - len(heads)\n-        self.attn.all_head_size = self.attn.attention_head_size * self.attn.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def forward(\n         self,\n         hidden_states,\n@@ -426,14 +407,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c80e362d6b930243b23e1bee8ac4cf6f882a8b75",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -34,7 +34,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, is_cuda_platform, is_ninja_available, is_torch_cuda_available, logging\n from .configuration_mra import MraConfig\n \n@@ -631,25 +631,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = MraSelfAttention(config)\n         self.output = MraSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, attention_mask=None):\n         self_outputs = self.self(hidden_states, attention_mask)\n@@ -849,14 +830,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "46735023b0626e699a5035a463030e73fabf4ef9",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 43,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -37,7 +37,6 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -189,24 +188,8 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -357,13 +340,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)\n@@ -1051,15 +1028,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Model._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5Model.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward(\n@@ -1439,15 +1407,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5EncoderModel._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward("
        },
        {
            "sha": "07902d4d1946a9eb790f4f0d2e426d517648c7e7",
            "filename": "src/transformers/models/nystromformer/modeling_nystromformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fmodeling_nystromformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -32,7 +32,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n     logging,\n@@ -253,25 +253,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = NystromformerSelfAttention(config)\n         self.output = NystromformerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         self_outputs = self.self(hidden_states, attention_mask, output_attentions)\n@@ -472,14 +453,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d60c0ff964ccd018283878c7ba9fc7a5a294d9fd",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 1,
            "deletions": 24,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -27,7 +27,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n+from ...pytorch_utils import Conv1D\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -61,22 +61,6 @@ def __init__(self, nx, n_positions, config, scale=False):\n         self.c_proj = Conv1D(n_state, nx)\n         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_head, self.split_size // self.n_head, self.pruned_heads\n-        )\n-        index_attn = torch.cat([index, index + self.split_size, index + (2 * self.split_size)])\n-        # Prune conv1d layers\n-        self.c_attn = prune_conv1d_layer(self.c_attn, index_attn, dim=1)\n-        self.c_proj = prune_conv1d_layer(self.c_proj, index, dim=0)\n-        # Update hyper params\n-        self.split_size = (self.split_size // self.n_head) * (self.n_head - len(heads))\n-        self.n_head = self.n_head - len(heads)\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def _attn(self, q, k, v, attention_mask=None, output_attentions=False):\n         w = torch.matmul(q, k)\n@@ -335,13 +319,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.tokens_embed = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.h[layer].attn.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "3dbd3f1579a937e54c82e37bb24b27987d152b9a",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -30,7 +30,7 @@\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward, meshgrid\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from .configuration_perceiver import PerceiverConfig\n \n@@ -301,25 +301,6 @@ def __init__(\n                 output_channels = v_channels\n         self.output = PerceiverSelfOutput(config, input_channels=self.self.v_channels, output_channels=output_channels)\n         self.use_query_residual = use_query_residual\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -620,14 +601,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.latents = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "62fd2451fb0b47d1f6275fd6168012ee09314b8f",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 16,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -480,14 +480,6 @@ def __init__(self, config: Pix2StructVisionConfig):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_projection\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -635,7 +627,7 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n+\n         self.gradient_checkpointing = False\n \n     @staticmethod\n@@ -789,13 +781,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)"
        },
        {
            "sha": "9e493d6462892426f04dac9a08baa1420a1ceed7",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torch_fx_proxy, is_torchdynamo_compiling, logging\n from .configuration_pop2piano import Pop2PianoConfig\n \n@@ -195,24 +194,8 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -363,13 +346,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)"
        },
        {
            "sha": "9f3a176ba7fed85f6b5446638d0387eaf0c22532",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,6 @@\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_pvt import PvtConfig\n \n@@ -241,25 +240,6 @@ def __init__(\n             sequences_reduction_ratio=sequences_reduction_ratio,\n         )\n         self.output = PvtSelfOutput(config, hidden_size=hidden_size)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self, hidden_states: torch.Tensor, height: int, width: int, output_attentions: bool = False\n@@ -478,14 +458,6 @@ def __init__(self, config: PvtConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "5c01b15a7f7e2054dc1d5444a308a9e320216574",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -26,7 +26,6 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_pvt_v2 import PvtV2Config\n@@ -120,7 +119,7 @@ class PvtV2SelfAttention(nn.Module):\n     def __init__(self, config: PvtV2Config, hidden_size: int, num_attention_heads: int, spatial_reduction_ratio: int):\n         super().__init__()\n         self.linear_attention = config.linear_attention\n-        self.pruned_heads = set()\n+\n         self.hidden_size = hidden_size\n         self.num_attention_heads = num_attention_heads\n \n@@ -202,24 +201,6 @@ def forward(\n \n         return outputs\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.num_attention_heads, self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.query = prune_linear_layer(self.query, index)\n-        self.key = prune_linear_layer(self.key, index)\n-        self.value = prune_linear_layer(self.value, index)\n-        self.proj = prune_linear_layer(self.proj, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.num_attention_heads = self.num_attention_heads - len(heads)\n-        self.all_head_size = self.attention_head_size * self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n \n class PvtV2ConvFeedForwardNetwork(nn.Module):\n     def __init__(\n@@ -417,14 +398,6 @@ def __init__(self, config: PvtV2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "636a44dba3283009a30579ee49210de209965199",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -1950,14 +1950,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "b5002d3d2a2b8495b23b0763aee94f03eddd20c6",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -36,7 +36,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, logging\n from .configuration_rembert import RemBertConfig\n \n@@ -234,26 +234,6 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.self = RemBertSelfAttention(config, layer_idx=layer_idx)\n         self.output = RemBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     # copied from transformers.models.bert.modeling_bert.BertAttention.forward\n     def forward(\n@@ -558,14 +538,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "118f6d57ed01c6081bd0b0d89d88bfe04b3b2fe6",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -44,7 +44,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roberta import RobertaConfig\n@@ -363,25 +363,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = RobertaCrossAttention if is_cross_attention else RobertaSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RobertaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -622,14 +603,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "14b0477974ed84a189890384ddd0235dd26639b2",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -39,7 +39,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roberta_prelayernorm import RobertaPreLayerNormConfig\n@@ -362,26 +362,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RobertaPreLayerNormSelfOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.pruned_heads = set()\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -636,14 +616,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "bc53270ff95d8ac1ed00a95836d2e1192624cb15",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -38,7 +38,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roc_bert import RoCBertConfig\n@@ -383,25 +383,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = RoCBertCrossAttention if is_cross_attention else RoCBertSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = RoCBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -712,15 +693,6 @@ def get_shape_embeddings(self):\n     def set_shape_embeddings(self, value):\n         self.embeddings.shape_embed = value\n \n-    # Copied from transformers.models.bert.modeling_bert.BertModel._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "4bcea0525eac2a0b2de2b2da87af7b541eb0bba8",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -36,7 +36,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import auto_docstring, logging\n from .configuration_roformer import RoFormerConfig\n \n@@ -276,26 +276,6 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.self = RoFormerSelfAttention(config, layer_idx=layer_idx)\n         self.output = RoFormerSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -715,14 +695,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "bd0d6d1e277cbd6e6b55a4fc51ee9c96840773c9",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -24,7 +24,6 @@\n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput, SemanticSegmenterOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from .configuration_segformer import SegformerConfig\n \n@@ -226,25 +225,6 @@ def __init__(self, config, hidden_size, num_attention_heads, sequence_reduction_\n             sequence_reduction_ratio=sequence_reduction_ratio,\n         )\n         self.output = SegformerSelfOutput(config, hidden_size=hidden_size)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, height, width, output_attentions=False):\n         self_outputs = self.self(hidden_states, height, width, output_attentions)\n@@ -460,14 +440,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d081a86f9cf2e9159d42d8681772344815f3e89d",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -650,14 +650,6 @@ def __init__(self, config: SegGptConfig):\n     def get_input_embeddings(self) -> SegGptPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ffa018b00266304ba63089efab80fb36be5506cc",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -25,7 +25,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput, QuestionAnsweringModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n     can_return_tuple,\n@@ -189,25 +189,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = SplinterSelfAttention(config)\n         self.output = SplinterSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -388,14 +369,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "7b2244b42b28eed4d2a99423fcbf41e1560ba155",
            "filename": "src/transformers/models/squeezebert/modeling_squeezebert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fmodeling_squeezebert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -444,14 +444,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.embeddings.word_embeddings = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "23814c5b8329d5d7501a11d327c5a1d3a3423d17",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -23,7 +23,6 @@\n from transformers import PreTrainedModel\n from transformers.models.superglue.configuration_superglue import SuperGlueConfig\n \n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n from ..auto import AutoModelForKeypointDetection\n \n@@ -334,25 +333,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = SUPERGLUE_SELF_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.output = SuperGlueSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,"
        },
        {
            "sha": "f33bf35e2de1e1e767edab391607276f965d5376",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -27,7 +27,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import meshgrid\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_swin import SwinConfig\n@@ -499,25 +499,6 @@ def __init__(self, config, dim, num_heads, window_size):\n         super().__init__()\n         self.self = SwinSelfAttention(config, dim, num_heads, window_size)\n         self.output = SwinSelfOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -873,14 +854,6 @@ def __init__(self, config, add_pooling_layer=True, use_mask_token=False):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "344344f69061d5e30e030980772f8f468674bb3c",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -26,7 +26,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ImageSuperResolutionOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import meshgrid\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_swin2sr import Swin2SRConfig\n \n@@ -387,25 +387,6 @@ def __init__(self, config, dim, num_heads, window_size, pretrained_window_size=0\n             else (pretrained_window_size, pretrained_window_size),\n         )\n         self.output = Swin2SRSelfOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -748,14 +729,6 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def pad_and_normalize(self, pixel_values):\n         _, _, height, width = pixel_values.size()\n "
        },
        {
            "sha": "d77279257dae04e67f3bfa11057739210a69a97e",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -27,7 +27,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import meshgrid\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_swinv2 import Swinv2Config\n@@ -557,25 +557,6 @@ def __init__(self, config, dim, num_heads, window_size, pretrained_window_size=0\n             else (pretrained_window_size, pretrained_window_size),\n         )\n         self.output = Swinv2SelfOutput(config, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -949,14 +930,6 @@ def __init__(self, config, add_pooling_layer=True, use_mask_token=False):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "0f4800fc45f0f28a2352480882634e671d65fd50",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 33,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -40,7 +40,6 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n@@ -268,24 +267,8 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -436,13 +419,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)\n@@ -977,14 +954,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "9f3886e60fbaf30b763dbcb77ec6e6e54a2f9b32",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -710,14 +710,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "99ffe5fe1738199cb54db14fbe53d6421ad0c6fe",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 41,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -37,7 +37,6 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -199,24 +198,8 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -367,13 +350,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)\n@@ -1039,14 +1016,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1395,14 +1364,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "ba9cd3ee05b8e23900e68e7629fed41889a58a03",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, MaskedLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_tapas import TapasConfig\n \n@@ -264,26 +264,6 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.self = TapasSelfAttention(config, layer_idx=layer_idx)\n         self.output = TapasSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     # Copied from transformers.models.rembert.modeling_rembert.RemBertAttention.forward\n     def forward(\n@@ -587,14 +567,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "cc23b62a05ebe568e45c99b25222811fe0203fad",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -485,14 +485,6 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "b8749e57a9114f86d5360d7799fe553cabed6112",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -25,7 +25,6 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import prune_linear_layer\n from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import load_backbone\n from .configuration_tvp import TvpConfig\n@@ -343,30 +342,6 @@ def __init__(self, config):\n         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n         self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        mask = torch.ones(self.num_attention_heads, self.attention_head_size)\n-        heads = set(heads) - self.pruned_heads  # Convert to set and remove already pruned heads\n-        for head in heads:\n-            # Compute how many pruned heads are before the head and move the index accordingly\n-            head = head - sum(1 if h < head else 0 for h in self.pruned_heads)\n-            mask[head] = 0\n-        mask = mask.view(-1).contiguous().eq(1)\n-        index = torch.arange(len(mask))[mask].long()\n-\n-        # Prune linear layers\n-        self.query = prune_linear_layer(self.query, index)\n-        self.key = prune_linear_layer(self.key, index)\n-        self.value = prune_linear_layer(self.value, index)\n-        self.dense = prune_linear_layer(self.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.num_attention_heads = self.num_attention_heads - len(heads)\n-        self.all_head_size = self.attention_head_size * self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def _reshape(self, tensor: torch.Tensor, sequence_length: int, batch_size: int):\n         return (\n@@ -733,13 +708,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"Prunes heads of the model.\n-        heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c0b33ed446e932b591fcec54c9e1b24bbd1f3192",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 33,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -466,24 +465,8 @@ def __init__(\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n-        self.gradient_checkpointing = False\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.n_heads, self.key_value_proj_dim, self.pruned_heads\n-        )\n-        # Prune linear layers\n-        self.q = prune_linear_layer(self.q, index)\n-        self.k = prune_linear_layer(self.k, index)\n-        self.v = prune_linear_layer(self.v, index)\n-        self.o = prune_linear_layer(self.o, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.inner_dim = self.key_value_proj_dim * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.gradient_checkpointing = False\n \n     @staticmethod\n     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):\n@@ -634,13 +617,7 @@ def forward(\n                 causal_mask = mask[:, :, :, : key_states.shape[-2]]\n                 position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)\n@@ -1850,14 +1827,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "da8b96865d8b44862c001118be620fe18cbe8d14",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -186,7 +186,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         if self.has_relative_attention_bias:\n             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)\n-        self.pruned_heads = set()\n \n     def _shape(self, projection: torch.Tensor) -> torch.Tensor:\n         new_projection_shape = projection.size()[:-1] + (self.n_heads, self.key_value_proj_dim)\n@@ -325,13 +324,7 @@ def forward(\n             causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n             position_bias = position_bias + causal_mask\n \n-        if self.pruned_heads:\n-            mask = torch.ones(position_bias.shape[1])\n-            mask[list(self.pruned_heads)] = 0\n-            position_bias_masked = position_bias[:, mask.bool()]\n-        else:\n-            position_bias_masked = position_bias\n-\n+        position_bias_masked = position_bias\n         scores += position_bias_masked\n \n         # (batch_size, n_heads, seq_length, key_length)\n@@ -969,15 +962,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Model._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1363,15 +1347,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5EncoderModel._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.block[layer].layer[0].SelfAttention.prune_heads(heads)\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.forward with T5->UMT5, google-t5/t5-small->google/umt5-small, t5#training->umt5#training\n     def forward("
        },
        {
            "sha": "e5f9b2fa709d73de842f84d8813bbc0f07dc20bc",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,7 +29,6 @@\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from ...utils.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n from ...utils.generic import can_return_tuple, check_model_inputs\n@@ -293,25 +292,6 @@ def __init__(self, config: VideoMAEConfig):\n         super().__init__()\n         self.attention = VideoMAESelfAttention(config)\n         self.output = VideoMAESelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -442,14 +422,6 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "5fc81287e97a6245eba3fe5a4ff358273c080613",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -34,7 +34,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...pytorch_utils import meshgrid\n from ...utils import auto_docstring, logging\n from .configuration_vilt import ViltConfig\n \n@@ -390,25 +390,6 @@ def __init__(self, config):\n         super().__init__()\n         self.attention = ViltSelfAttention(config)\n         self.output = ViltSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         self_outputs = self.attention(hidden_states, attention_mask, output_attentions)\n@@ -574,14 +555,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.text_embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "dc6f6a3c3bfe646846cfcafa3406379562de8eac",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,7 @@\n     SequenceClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_visual_bert import VisualBertConfig\n \n@@ -258,25 +258,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = VisualBertSelfAttention(config)\n         self.output = VisualBertSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -558,14 +539,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "35ded9b00dc4dbc19f536d852da28f1f076df838",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -31,7 +31,6 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_vit import ViTConfig\n@@ -269,25 +268,6 @@ def __init__(self, config: ViTConfig):\n         super().__init__()\n         self.attention = ViTSelfAttention(config)\n         self.output = ViTSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -437,14 +417,6 @@ def __init__(self, config: ViTConfig, add_pooling_layer: bool = True, use_mask_t\n     def get_input_embeddings(self) -> ViTPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "e5cbd3b7cf475625c71fac520446ffd61242ba2c",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 36,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -28,7 +28,6 @@\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_vit_mae import ViTMAEConfig\n@@ -432,25 +431,6 @@ def __init__(self, config: ViTMAEConfig):\n         super().__init__()\n         self.attention = ViTMAESelfAttention(config)\n         self.output = ViTMAESelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -582,14 +562,6 @@ def __init__(self, config):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward(\n@@ -774,14 +746,6 @@ def __init__(self, config: ViTMAEConfig):\n     def get_input_embeddings(self):\n         return self.vit.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     def patchify(self, pixel_values, interpolate_pos_encoding: bool = False):\n         \"\"\"\n         Args:"
        },
        {
            "sha": "2fa36b644a636202e964f313f34ffdf249d5798d",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -25,7 +25,6 @@\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_vit_msn import ViTMSNConfig\n@@ -269,25 +268,6 @@ def __init__(self, config: ViTMSNConfig):\n         super().__init__()\n         self.attention = ViTMSNSelfAttention(config)\n         self.output = ViTMSNSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -426,14 +406,6 @@ def __init__(self, config: ViTMSNConfig, use_mask_token: bool = False):\n     def get_input_embeddings(self) -> ViTMSNPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "b0919e52f82825a74ca97671d2b34c8a46550df7",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -651,14 +651,6 @@ def __init__(self, config: VitDetConfig):\n     def get_input_embeddings(self) -> VitDetEmbeddings:\n         return self.embeddings.projection\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "a9e0b222f1d10f9ccb9eef8b03c57c610cfdcafe",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -29,7 +29,6 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import check_model_inputs\n@@ -201,25 +200,6 @@ def __init__(self, config: VitPoseBackboneConfig):\n         super().__init__()\n         self.attention = VitPoseBackboneSelfAttention(config)\n         self.output = VitPoseBackboneSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)"
        },
        {
            "sha": "433b2f2b242cda495512095f0068673094248d7e",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -24,7 +24,6 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, logging, torch_int\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_vivit import VivitConfig\n@@ -262,25 +261,6 @@ def __init__(self, config: VivitConfig):\n         super().__init__()\n         self.attention = VivitSelfAttention(config)\n         self.output = VivitSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -434,17 +414,6 @@ def __init__(self, config: VivitConfig, add_pooling_layer: bool = True):\n     def get_input_embeddings(self):\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model.\n-\n-        Args:\n-            heads_to_prune:\n-                dict of {layer_num: list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "8877eb55eba6ec35599cd9b60563216cc87f7302",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -37,7 +37,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_xlm import XLMConfig\n \n@@ -507,22 +507,6 @@ def __init__(self, n_heads, dim, config, layer_idx: int = 0):\n         self.k_lin = nn.Linear(dim, dim)\n         self.v_lin = nn.Linear(dim, dim)\n         self.out_lin = nn.Linear(dim, dim)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        attention_head_size = self.dim // self.n_heads\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(heads, self.n_heads, attention_head_size, self.pruned_heads)\n-        # Prune linear layers\n-        self.q_lin = prune_linear_layer(self.q_lin, index)\n-        self.k_lin = prune_linear_layer(self.k_lin, index)\n-        self.v_lin = prune_linear_layer(self.v_lin, index)\n-        self.out_lin = prune_linear_layer(self.out_lin, index, dim=1)\n-        # Update hyper params\n-        self.n_heads = self.n_heads - len(heads)\n-        self.dim = attention_head_size * self.n_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -743,13 +727,6 @@ def __init__(self, config):\n             self.ffns.append(TransformerFFN(self.dim, self.hidden_dim, self.dim, config=config))\n             self.layer_norm2.append(nn.LayerNorm(self.dim, eps=config.layer_norm_eps))\n \n-        if hasattr(config, \"pruned_heads\"):\n-            pruned_heads = config.pruned_heads.copy().items()\n-            config.pruned_heads = {}\n-            for layer, heads in pruned_heads:\n-                if self.attentions[int(layer)].n_heads == config.n_heads:\n-                    self.prune_heads({int(layer): list(map(int, heads))})\n-\n         # Initialize weights and apply final processing\n         self.post_init()\n         self.register_buffer(\n@@ -762,14 +739,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.embeddings = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.attentions[layer].prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "488ec18a0e908391593228258362f247be39fce7",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -44,7 +44,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_xlm_roberta import XLMRobertaConfig\n@@ -257,25 +257,6 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = XLMRobertaCrossAttention if is_cross_attention else XLMRobertaSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = XLMRobertaSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(\n         self,\n@@ -641,14 +622,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f0a60d84313e76521fb3c48ee5db79d2c5095e46",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -49,7 +49,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_xlm_roberta_xl import XLMRobertaXLConfig\n@@ -364,28 +364,9 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = XLMRobertaXLCrossAttention if is_cross_attention else XLMRobertaXLSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = XLMRobertaXLSelfOutput(config)\n-        self.pruned_heads = set()\n \n         self.self_attn_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -628,14 +609,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "fe461dda0f8320beb033e32364c9fad4797ca59a",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -65,9 +65,6 @@ def __init__(self, config):\n         self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.dropout)\n \n-    def prune_heads(self, heads):\n-        raise NotImplementedError\n-\n     @staticmethod\n     def rel_shift(x, klen=-1):\n         \"\"\"perform relative shift to form the relative attention score.\"\"\"\n@@ -892,9 +889,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.word_embedding = new_embeddings\n \n-    def _prune_heads(self, heads_to_prune):\n-        raise NotImplementedError\n-\n     def create_mask(self, qlen, mlen):\n         \"\"\"\n         Creates causal attention mask. Float mask where 1.0 indicates masked, 0.0 indicates not-masked."
        },
        {
            "sha": "c3969bbcd42955a7668ed2fe7aa7c6fbf847f7f2",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 2,
            "deletions": 30,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -38,7 +38,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_xmod import XmodConfig\n@@ -362,27 +362,8 @@ def __init__(self, config, is_causal=False, layer_idx=None, is_cross_attention=F\n         attention_class = XmodCrossAttention if is_cross_attention else XmodSelfAttention\n         self.self = attention_class(config, is_causal=is_causal, layer_idx=layer_idx)\n         self.output = XmodSelfOutput(config)\n-        self.pruned_heads = set()\n-        self.pre_norm = config.pre_norm\n-\n-    # Copied from transformers.models.roberta.modeling_roberta.RobertaAttention.prune_heads\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n \n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n+        self.pre_norm = config.pre_norm\n \n     def forward(\n         self,\n@@ -737,15 +718,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel._prune_heads\n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs()\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "882d4facae61df76231cf7d8582a36f79a6c7e59",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 0,
            "deletions": 32,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -26,7 +26,6 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, logging\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_yolos import YolosConfig\n@@ -317,25 +316,6 @@ def __init__(self, config: YolosConfig):\n         super().__init__()\n         self.attention = YolosSelfAttention(config)\n         self.output = YolosSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads: set[int]):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.attention.query = prune_linear_layer(self.attention.query, index)\n-        self.attention.key = prune_linear_layer(self.attention.key, index)\n-        self.attention.value = prune_linear_layer(self.attention.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n-        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         self_attn_output, _ = self.attention(hidden_states)\n@@ -497,18 +477,6 @@ def __init__(self, config: YolosConfig, add_pooling_layer: bool = True):\n     def get_input_embeddings(self) -> YolosPatchEmbeddings:\n         return self.embeddings.patch_embeddings\n \n-    def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n-        \"\"\"\n-        Prunes heads of the model.\n-\n-        Args:\n-            heads_to_prune (`dict`):\n-                See base class `PreTrainedModel`. The input dictionary must have the following format: {layer_num:\n-                list of heads to prune in this layer}\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @check_model_inputs(tie_last_hidden_states=False)\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ec45bc542fefdfb114924a4fe872fec93f8f3d95",
            "filename": "src/transformers/models/yoso/modeling_yoso.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fmodeling_yoso.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -33,7 +33,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import (\n     auto_docstring,\n     is_ninja_available,\n@@ -449,25 +449,6 @@ def __init__(self, config):\n         super().__init__()\n         self.self = YosoSelfAttention(config)\n         self.output = YosoSelfOutput(config)\n-        self.pruned_heads = set()\n-\n-    def prune_heads(self, heads):\n-        if len(heads) == 0:\n-            return\n-        heads, index = find_pruneable_heads_and_indices(\n-            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n-        )\n-\n-        # Prune linear layers\n-        self.self.query = prune_linear_layer(self.self.query, index)\n-        self.self.key = prune_linear_layer(self.self.key, index)\n-        self.self.value = prune_linear_layer(self.self.value, index)\n-        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n-\n-        # Update hyper params and store pruned heads\n-        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n-        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n-        self.pruned_heads = self.pruned_heads.union(heads)\n \n     def forward(self, hidden_states, attention_mask=None, output_attentions=False):\n         self_outputs = self.self(hidden_states, attention_mask, output_attentions)\n@@ -671,14 +652,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _prune_heads(self, heads_to_prune):\n-        \"\"\"\n-        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n-        class PreTrainedModel\n-        \"\"\"\n-        for layer, heads in heads_to_prune.items():\n-            self.encoder.layer[layer].attention.prune_heads(heads)\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "03820e0fd0d3a56cc50339c31a4236746a405067",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 82,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -124,61 +124,6 @@ def forward(self, x):\n         return x\n \n \n-def prune_conv1d_layer(layer: Conv1D, index: torch.LongTensor, dim: int = 1) -> Conv1D:\n-    \"\"\"\n-    Prune a Conv1D layer to keep only entries in index. A Conv1D work as a Linear layer (see e.g. BERT) but the weights\n-    are transposed.\n-\n-    Used to remove heads.\n-\n-    Args:\n-        layer ([`~pytorch_utils.Conv1D`]): The layer to prune.\n-        index (`torch.LongTensor`): The indices to keep in the layer.\n-        dim (`int`, *optional*, defaults to 1): The dimension on which to keep the indices.\n-\n-    Returns:\n-        [`~pytorch_utils.Conv1D`]: The pruned layer as a new layer with `requires_grad=True`.\n-    \"\"\"\n-    index = index.to(layer.weight.device)\n-    W = layer.weight.index_select(dim, index).detach().clone()\n-    if dim == 0:\n-        b = layer.bias.detach().clone()\n-    else:\n-        b = layer.bias[index].detach().clone()\n-    new_size = list(layer.weight.size())\n-    new_size[dim] = len(index)\n-    new_layer = Conv1D(new_size[1], new_size[0]).to(layer.weight.device)\n-    new_layer.weight.requires_grad = False\n-    new_layer.weight.copy_(W.contiguous())\n-    new_layer.weight.requires_grad = True\n-    new_layer.bias.requires_grad = False\n-    new_layer.bias.copy_(b.contiguous())\n-    new_layer.bias.requires_grad = True\n-    return new_layer\n-\n-\n-def prune_layer(layer: nn.Linear | Conv1D, index: torch.LongTensor, dim: int | None = None) -> nn.Linear | Conv1D:\n-    \"\"\"\n-    Prune a Conv1D or linear layer to keep only entries in index.\n-\n-    Used to remove heads.\n-\n-    Args:\n-        layer (`Union[torch.nn.Linear, Conv1D]`): The layer to prune.\n-        index (`torch.LongTensor`): The indices to keep in the layer.\n-        dim (`int`, *optional*): The dimension on which to keep the indices.\n-\n-    Returns:\n-        `torch.nn.Linear` or [`~pytorch_utils.Conv1D`]: The pruned layer as a new layer with `requires_grad=True`.\n-    \"\"\"\n-    if isinstance(layer, nn.Linear):\n-        return prune_linear_layer(layer, index, dim=0 if dim is None else dim)\n-    elif isinstance(layer, Conv1D):\n-        return prune_conv1d_layer(layer, index, dim=1 if dim is None else dim)\n-    else:\n-        raise ValueError(f\"Can't prune layer of class {layer.__class__}\")\n-\n-\n def apply_chunking_to_forward(\n     forward_fn: Callable[..., torch.Tensor],\n     chunk_size: int,\n@@ -257,33 +202,6 @@ def forward(self, hidden_states):\n     return forward_fn(*input_tensors)\n \n \n-def find_pruneable_heads_and_indices(\n-    heads: list[int], n_heads: int, head_size: int, already_pruned_heads: set[int]\n-) -> tuple[set[int], torch.LongTensor]:\n-    \"\"\"\n-    Finds the heads and their indices taking `already_pruned_heads` into account.\n-\n-    Args:\n-        heads (`list[int]`): List of the indices of heads to prune.\n-        n_heads (`int`): The number of heads in the model.\n-        head_size (`int`): The size of each head.\n-        already_pruned_heads (`Set[int]`): A set of already pruned heads.\n-\n-    Returns:\n-        `tuple[Set[int], torch.LongTensor]`: A tuple with the indices of heads to prune taking `already_pruned_heads`\n-        into account and the indices of rows/columns to keep in the layer weight.\n-    \"\"\"\n-    mask = torch.ones(n_heads, head_size)\n-    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n-    for head in heads:\n-        # Compute how many pruned heads are before the head and move the index accordingly\n-        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n-        mask[head] = 0\n-    mask = mask.view(-1).contiguous().eq(1)\n-    index: torch.LongTensor = torch.arange(len(mask))[mask].long()\n-    return heads, index\n-\n-\n def meshgrid(*tensors: torch.Tensor | list[torch.Tensor], indexing: str | None = None) -> tuple[torch.Tensor, ...]:\n     \"\"\"\n     Wrapper around torch.meshgrid to avoid warning messages about the introduced `indexing` argument."
        },
        {
            "sha": "117b447cd23f9d5ce53dbdabe1fa516d0158345f",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -589,10 +589,6 @@ def apply_chunking_to_forward(*args, **kwargs):\n     requires_backends(apply_chunking_to_forward, [\"torch\"])\n \n \n-def prune_layer(*args, **kwargs):\n-    requires_backends(prune_layer, [\"torch\"])\n-\n-\n class Trainer(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "0aaf33460969f1bf67b8a3cff97475aceb0eccba",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -287,7 +287,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class CausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin):\n-    test_pruning = False\n     model_tester_class = None\n     all_model_classes = None\n     pipeline_model_mapping = None"
        },
        {
            "sha": "5ec0a9795fc42b1a2bbeb57e566d4bc118d0dbd0",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -180,7 +180,7 @@ class Aimv2VisionModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (Aimv2VisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torchscript = False\n \n@@ -310,7 +310,7 @@ def prepare_config_and_inputs_for_common(self):\n class Aimv2TextModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Aimv2TextModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torchscript = False\n \n@@ -389,7 +389,7 @@ class Aimv2ModelTest(Aimv2ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_attention_outputs = False"
        },
        {
            "sha": "f815ac0cd2a87619f188a05d7474bacae0ec7cf9",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -133,7 +133,7 @@ class AlignVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (AlignVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     has_attentions = False\n \n@@ -336,7 +336,6 @@ def prepare_config_and_inputs_for_common(self):\n class AlignTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (AlignTextModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = AlignTextModelTester(self)\n@@ -441,7 +440,7 @@ class AlignModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (AlignModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": AlignModel} if is_torch_available() else {}\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "9c9151fab510053c308dad3a793b0c32b47a1c27",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -135,7 +135,7 @@ class AltCLIPVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (AltCLIPVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -297,7 +297,6 @@ def prepare_config_and_inputs_for_common(self):\n class AltCLIPTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (AltCLIPTextModel,) if is_torch_available() else ()\n     fx_compatible = False  # Cannot support if `can_return_tuple`\n-    test_pruning = False\n \n     # TODO (@SunMarc): Fix me\n     @unittest.skip(reason=\"It's broken.\")\n@@ -412,7 +411,7 @@ class AltCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     all_model_classes = (AltCLIPModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": AltCLIPModel} if is_torch_available() else {}\n     fx_compatible = False  # Cannot support if `can_return_tuple`\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "fc1bd948065f04ad53ecd8dcbd7df6c43b97c513",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -190,7 +190,7 @@ class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMi\n     \"\"\"\n \n     all_model_classes = (AriaModel, AriaForConditionalGeneration) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     _is_composite = True\n "
        },
        {
            "sha": "ba7f6866ff94943375cecd845df8fdaf8b73a04f",
            "filename": "tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -161,7 +161,7 @@ class ASTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     # TODO: Fix the failed tests when this model gets more usage"
        },
        {
            "sha": "547e57db83f0c4f75119f7ee203b62b6ac16ac32",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -206,7 +206,7 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n class AutoformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (AutoformerModel, AutoformerForPrediction) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": AutoformerModel} if is_torch_available() else {}\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torchscript = False\n     test_inputs_embeds = False"
        },
        {
            "sha": "14b9bd2916a2a91e0671f4c533001f62d0b048ac",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -170,7 +170,7 @@ class AyaVisionModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_torchscript = False\n     _is_composite = True\n "
        },
        {
            "sha": "60666bd99a3d626489db7c076985744398099733",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -289,7 +289,7 @@ class BambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "1ef08b74d396b4db75478fec3181a4a3f184c849",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -522,7 +522,6 @@ class BarkSemanticModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Te\n     is_encoder_decoder = False\n     fx_compatible = False\n     test_missing_keys = False\n-    test_pruning = False\n \n     test_resize_embeddings = True\n \n@@ -610,7 +609,7 @@ class BarkCoarseModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n     is_encoder_decoder = False\n     fx_compatible = False\n     test_missing_keys = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n \n     def setUp(self):\n@@ -694,7 +693,7 @@ class BarkFineModelTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = False\n     fx_compatible = False\n     test_missing_keys = False\n-    test_pruning = False\n+\n     # torchscript disabled for now because forward with an int\n     test_torchscript = False\n "
        },
        {
            "sha": "fad475cc73a839a6c711af28b818b437cdf0f581",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -422,7 +422,6 @@ class BartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     )\n     is_encoder_decoder = True\n     fx_compatible = False  # Fix me Michael\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = BartModelTester(self)\n@@ -1506,7 +1505,7 @@ def prepare_config_and_inputs_for_common(self):\n class BartStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BartDecoder, BartForCausalLM) if is_torch_available() else ()\n     fx_comptatible = True\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n     test_missing_keys = False\n "
        },
        {
            "sha": "af2c8e65b581a956a7f40ecb8a81b1a029364339",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -260,7 +260,6 @@ class BeitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "dac9dd0464af2f38cae2765ca5a9e2ebfb6f95e9",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -409,9 +409,6 @@ def create_and_check_for_change_to_full_attn(\n \n @require_torch\n class BigBirdModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    # pruning is currently not supported for big bird\n-    test_pruning = False\n-\n     # torchscript should be possible, but takes prohibitively long to test.\n     # Also torchscript is not an important feature to have in the beginning.\n     test_torchscript = False"
        },
        {
            "sha": "81ae76e8cb47e3bb73049ea2bf415f0eb8aa53e7",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -265,7 +265,6 @@ class BigBirdPegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineT\n     )\n     is_encoder_decoder = True\n     test_missing_keys = False\n-    test_pruning = False\n \n     # torchscript tests are not passing for now.\n     # Also torchscript is not an important feature to have in the beginning.\n@@ -783,7 +782,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BigBirdPegasusStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BigBirdPegasusDecoder, BigBirdPegasusForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp("
        },
        {
            "sha": "3a1cb8c23c2a048674f952f2305de729cbcf8689",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -270,7 +270,6 @@ class BioGptModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available() and is_sacremoses_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = BioGptModelTester(self)"
        },
        {
            "sha": "b24fac437f64d547b02a8c744ccb601f8b719191",
            "filename": "tests/models/bit/test_modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -165,7 +165,7 @@ class BitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "3392d5c55943f3da5f8da759c94b890678fad8c0",
            "filename": "tests/models/bitnet/test_modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbitnet%2Ftest_modeling_bitnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -141,7 +141,7 @@ class BitNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n \n     # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146"
        },
        {
            "sha": "e83837f582ada06627ec9134ce59074cea160d4a",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -225,7 +225,7 @@ class BlenderbotModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n     )\n     is_encoder_decoder = True\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     def setUp(self):\n@@ -518,7 +518,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BlenderbotStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BlenderbotDecoder, BlenderbotForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp("
        },
        {
            "sha": "575d84149e82d1cd07d70a3c4b8aaaa6d0794b75",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -217,7 +217,7 @@ class BlenderbotSmallModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n     )\n     is_encoder_decoder = True\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests when this model gets more usage\n@@ -529,7 +529,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BlenderbotSmallStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BlenderbotSmallDecoder, BlenderbotSmallForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp("
        },
        {
            "sha": "06a1f592e56b5f7214b0d4ef0dac4c10178cee3d",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -152,7 +152,7 @@ class BlipVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (BlipVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -313,7 +313,6 @@ def prepare_config_and_inputs_for_common(self):\n class BlipTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipTextModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = BlipTextModelTester(self)\n@@ -422,7 +421,7 @@ class BlipModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n \n@@ -766,7 +765,7 @@ class BlipVQAModelTest(ModelTesterMixin, unittest.TestCase):\n     # Doesn't run generation tests due to custom generation logic -- won't fix\n     all_generative_model_classes = ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n@@ -844,7 +843,7 @@ def test_model_get_set_embeddings(self):\n class BlipTextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForImageTextRetrieval,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n@@ -1045,7 +1044,7 @@ class BlipTextImageModelTest(ModelTesterMixin, unittest.TestCase):\n     # Doesn't run generation tests due to custom generation logic -- wont fix\n     all_generative_model_classes = ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False"
        },
        {
            "sha": "6fd7121a5d0fe9c2b1856a414256777eadcc136e",
            "filename": "tests/models/blip/test_modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip_text.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -126,7 +126,6 @@ def prepare_config_and_inputs_for_common(self):\n class BlipTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipTextModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = BlipTextModelTester(self)"
        },
        {
            "sha": "8bf897eeca4c9e5b16e81955cda3c4405446ab4e",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -157,7 +157,7 @@ class Blip2VisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (Blip2VisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -462,7 +462,7 @@ class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationT\n     all_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n     additional_model_inputs = [\"input_ids\"]\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_torchscript = False\n@@ -793,7 +793,7 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n@@ -1073,7 +1073,6 @@ def create_and_check_model(self, config, input_ids, attention_mask):\n class Blip2TextModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2TextModelWithProjection,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n@@ -1232,7 +1231,6 @@ def create_and_check_model(self, config, pixel_values):\n class Blip2VisionModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2VisionModelWithProjection,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n \n     test_resize_embeddings = False\n     test_torchscript = False\n@@ -1382,7 +1380,7 @@ class Blip2TextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForImageTextRetrieval,) if is_torch_available() else ()\n     additional_model_inputs = [\"input_ids\"]\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False"
        },
        {
            "sha": "e8e22a7a0cb5b62bec4b6028f7f0c2ca0f3c56bb",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -333,7 +333,7 @@ class BloomModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     )\n     fx_compatible = True\n     test_missing_keys = False\n-    test_pruning = False\n+\n     test_torchscript = True  # torch.autograd functions seems not to be supported\n \n     def setUp(self):"
        },
        {
            "sha": "ee46ec86a143995b7107b286b84a709c186860f1",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -186,7 +186,7 @@ class BltModelTest(CausalLMModelTest, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n     model_tester_class = BltModelTester\n     rotary_embedding_layer = BltRotaryEmbedding  # Enables RoPE tests if set"
        },
        {
            "sha": "0588bcd63f6917efa57b686d940daf52bac287ac",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -307,7 +307,7 @@ class BridgeTowerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     pipeline_model_mapping = {\"feature-extraction\": BridgeTowerModel} if is_torch_available() else {}\n \n     is_training = False\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = False\n     has_attentions = False"
        },
        {
            "sha": "884bbf2ef347f6c93b9ccbe5cb537548a1a1d27f",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -270,7 +270,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class BrosModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_pruning = False\n     test_torchscript = False\n     test_mismatched_shapes = False\n "
        },
        {
            "sha": "396301513fcfa06bd0bab4257db485ec737cb3ac",
            "filename": "tests/models/canine/test_modeling_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -234,7 +234,6 @@ class CanineModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     test_mismatched_shapes = False\n     test_resize_embeddings = False\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = CanineModelTester(self)"
        },
        {
            "sha": "bab1d7aa855fec3128660a336563b83e301d0049",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -205,7 +205,7 @@ class ChameleonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     def setUp(self):\n@@ -289,7 +289,7 @@ class ChameleonVision2SeqModelTest(ModelTesterMixin, GenerationTesterMixin, unit\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     def setUp(self):"
        },
        {
            "sha": "c3eed57313d0ca0f754909c6e973ed22d18307c4",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -409,7 +409,7 @@ class ChineseCLIPVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (ChineseCLIPVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -542,7 +542,7 @@ class ChineseCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     all_model_classes = (ChineseCLIPModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": ChineseCLIPModel} if is_torch_available() else {}\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "270fc006ec9a1173db095f648954e6c5c7a12a85",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -161,7 +161,7 @@ class ClapAudioModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (ClapAudioModel, ClapAudioModelWithProjection) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -380,7 +380,6 @@ def prepare_config_and_inputs_for_common(self):\n class ClapTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ClapTextModel, ClapTextModelWithProjection) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = ClapTextModelTester(self)\n@@ -491,7 +490,7 @@ class ClapModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ClapModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": ClapModel} if is_torch_available() else {}\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "34b9c14d39e424dd1b77001d6371b237b275c6af",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -211,7 +211,7 @@ class CLIPVisionModelTest(CLIPModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (CLIPVisionModel, CLIPVisionModelWithProjection) if is_torch_available() else ()\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -399,7 +399,7 @@ def prepare_config_and_inputs_for_common(self):\n class CLIPTextModelTest(CLIPModelTesterMixin, unittest.TestCase):\n     all_model_classes = (CLIPTextModel, CLIPTextModelWithProjection) if is_torch_available() else ()\n     fx_compatible = True\n-    test_pruning = False\n+\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     def setUp(self):\n@@ -527,7 +527,7 @@ class CLIPModelTest(CLIPModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     )\n     additional_model_inputs = [\"pixel_values\"]\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True\n@@ -698,7 +698,7 @@ class CLIPForImageClassificationModelTest(CLIPModelTesterMixin, PipelineTesterMi\n     all_model_classes = (CLIPForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": CLIPForImageClassification} if is_torch_available() else {}\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True"
        },
        {
            "sha": "ddd459527b85a902a21106042e88d7ce5d96c379",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -139,7 +139,7 @@ class CLIPSegVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (CLIPSegVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -296,7 +296,7 @@ def prepare_config_and_inputs_for_common(self):\n class CLIPSegTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (CLIPSegTextModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     def setUp(self):\n@@ -423,7 +423,7 @@ class CLIPSegModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     all_model_classes = (CLIPSegModel, CLIPSegForImageSegmentation) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": CLIPSegModel} if is_torch_available() else {}\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "e627a530f7240ea9f1a70439642778d98904f5f9",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -161,7 +161,7 @@ def create_and_check_model(self, speech_config, input_ids, input_mask):\n @require_torch\n class ClvpEncoderTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ClvpEncoder,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n \n     def setUp(self):\n@@ -280,8 +280,6 @@ class ClvpDecoderTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     all_model_classes = (ClvpModel, ClvpForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": ClvpModelForConditionalGeneration} if is_torch_available() else {}\n \n-    test_pruning = False\n-\n     def setUp(self):\n         self.model_tester = ClvpDecoderTester(self)\n         self.decoder_config_tester = ConfigTester(self, config_class=ClvpDecoderConfig, hidden_size=32)\n@@ -410,7 +408,6 @@ class ClvpModelForConditionalGenerationTest(ModelTesterMixin, unittest.TestCase)\n     # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n     all_generative_model_classes = ()\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_torchscript = False"
        },
        {
            "sha": "eea326e11e168d349324048e5f02850090d72a35",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -317,7 +317,7 @@ class CodeGenModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         {\"feature-extraction\": CodeGenModel, \"text-generation\": CodeGenForCausalLM} if is_torch_available() else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     # special case for DoubleHeads model"
        },
        {
            "sha": "2312f7ac0698778293e20ae472be3ca44abf4be1",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -170,7 +170,7 @@ class CohereModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "2a11a8943cc9299c406cdad3bdaea999003c27ce",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -158,7 +158,7 @@ class Cohere2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_torchscript = False\n     _is_composite = True\n "
        },
        {
            "sha": "c6bb15be343ad5b16ab17c795fb88570ab19f351",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -187,7 +187,7 @@ class ColPaliForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ColPaliForRetrieval,) if is_torch_available() else ()\n     fx_compatible = False\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     additional_model_inputs = [\"token_type_ids\"]\n "
        },
        {
            "sha": "df0d7686a58a8eb9be2ce62a08741a88cf34ba6a",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -202,7 +202,7 @@ class ColQwen2ForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ColQwen2ForRetrieval,) if is_torch_available() else ()\n     fx_compatible = False\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n \n     def setUp(self):"
        },
        {
            "sha": "13e37f200a43c59aa3e1a37726753ba0786ebf48",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -188,7 +188,7 @@ class ConditionalDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.T\n     )\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     zero_init_hidden_state = True\n     test_torch_exportable = True"
        },
        {
            "sha": "36e14e69dc6b0a3015a4c0eb8e5d69977ed04007",
            "filename": "tests/models/convbert/test_modeling_convbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -268,7 +268,6 @@ class ConvBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = ConvBertModelTester(self)"
        },
        {
            "sha": "1caa94b3eecea2b10626e3b5a5e1a70daf660cd4",
            "filename": "tests/models/convnext/test_modeling_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -176,7 +176,7 @@ class ConvNextModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     )\n \n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "a98206a2160151373b257b3bdd2a62c0adce5c36",
            "filename": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -155,7 +155,7 @@ class ConvNextV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     )\n \n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "9e6b0382b9930e180a3f49eb41178ba4f385d93b",
            "filename": "tests/models/cpmant/test_modeling_cpmant.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -141,7 +141,6 @@ class CpmAntModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         {\"feature-extraction\": CpmAntModel, \"text-generation\": CpmAntForCausalLM} if is_torch_available() else {}\n     )\n \n-    test_pruning = False\n     test_missing_keys = False\n     test_mismatched_shapes = False\n     test_resize_embeddings = False"
        },
        {
            "sha": "4d63f447c463480f16fd162c94ad7e9d4086a5c9",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -142,7 +142,7 @@ def prepare_config_and_inputs_for_common(self):\n \n class CsmForConditionalGenerationTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (CsmForConditionalGeneration,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_resize_embeddings_untied = False\n "
        },
        {
            "sha": "c6bd43a8575c08ddbaa3fcee00426e34e355e7d5",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -189,7 +189,6 @@ class CTRLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = True\n     test_resize_embeddings = False\n \n     # TODO: Fix the failed tests"
        },
        {
            "sha": "ab613104bda464e3f158e4212605552f206a2450",
            "filename": "tests/models/cvt/test_modeling_cvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -154,7 +154,6 @@ class CvtModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_torchscript = False\n     test_resize_embeddings = False\n     has_attentions = False"
        },
        {
            "sha": "040ca327396a9f2e051aaeb49f65e95eb467ab73",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -292,7 +292,7 @@ class DFineModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "37b69132fb068e72a8111d1b30fba29ba29e19b9",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -183,7 +183,7 @@ class DabDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     )\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     zero_init_hidden_state = True\n     test_torch_exportable = True"
        },
        {
            "sha": "486ae94eeb618a7d3f74560f55c884cd23ffd018",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -118,7 +118,7 @@ def create_and_check_model_forward(self, config, inputs_dict):\n class DacModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DacModel,) if is_torch_available() else ()\n     is_encoder_decoder = True\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     pipeline_model_mapping = {\"feature-extraction\": DacModel} if is_torch_available() else {}\n "
        },
        {
            "sha": "83e5c838bc16ca577134051dd98f13817b3e2736",
            "filename": "tests/models/data2vec/test_modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_audio.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -353,7 +353,6 @@ class Data2VecAudioModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Tes\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = Data2VecAudioModelTester(self)"
        },
        {
            "sha": "c4825356e74da40fe8c15003168eceef36b3888d",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -200,7 +200,6 @@ class Data2VecVisionModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "8a255dc93e8bab9e14294e1e977578d2ba0c2f89",
            "filename": "tests/models/deberta/test_modeling_deberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -239,7 +239,7 @@ class DebertaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n \n     fx_compatible = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp(self):"
        },
        {
            "sha": "f2a551ea68532dd18ccc21170fc21cd71061047a",
            "filename": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -253,7 +253,7 @@ class DebertaV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n \n     fx_compatible = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp(self):"
        },
        {
            "sha": "c738231438526ddc4202f292dbe7a51353a41875",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -131,7 +131,7 @@ class DecisionTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unitte\n     test_generate_without_input_ids = False\n \n     # Ignoring of a failing tests from ModelTesterMixin, as the model does not implement these features\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     test_hidden_states_output = False"
        },
        {
            "sha": "36a2dcbd1fd18fb9074356c5d940dcc9283b095c",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -234,7 +234,7 @@ class DeepseekV3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "cded2b224c004a5e1378f921dc020254e4655841",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -138,7 +138,6 @@ class DeepseekVLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n         else {}\n     )\n     _is_composite = True\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = DeepseekVLModelTester(self)"
        },
        {
            "sha": "661b44ecf39030cc1a304b3307d27f00eebb7181",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -167,7 +167,6 @@ class DeepseekVLHybridModelTest(ModelTesterMixin, GenerationTesterMixin, unittes\n         else {}\n     )\n     _is_composite = True\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = DeepseekVLHybridModelTester(self)"
        },
        {
            "sha": "065fdaaaca1472105cc5c9032cb330df99b8175e",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -194,7 +194,7 @@ class DeformableDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     )\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "372b12baed18c143d47aed0f6a252367904921c8",
            "filename": "tests/models/deit/test_modeling_deit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -219,7 +219,6 @@ class DeiTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "4e5576fbb09e337f7a962290a20bffbbfacc6dc6",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -145,7 +145,6 @@ class DepthAnythingModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Tes\n     all_model_classes = (DepthAnythingForDepthEstimation,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"depth-estimation\": DepthAnythingForDepthEstimation} if is_torch_available() else {}\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n     test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\""
        },
        {
            "sha": "7a5e6e679e6b2e62d0290ee199ad69cdd1a604f5",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -210,7 +210,6 @@ class DepthProModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "bfa9575771b1f75848123a117ea5925f7d98a44e",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -188,7 +188,7 @@ class DetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     zero_init_hidden_state = True\n     test_torch_exportable = True"
        },
        {
            "sha": "67c9b6527a4f35eda15abee928c2db4b6b9289ae",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -220,7 +220,7 @@ class DiaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     # TODO: support new pipeline behavior in tests\n     pipeline_model_mapping = {}\n     # pipeline_model_mapping = {\"text-to-audio\": DiaForConditionalGeneration} if is_torch_available() else {}\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     is_encoder_decoder = True\n     # Indicates VLMs usually but there are many audio models which are also composite"
        },
        {
            "sha": "a9a64bb3b4ddf21b5cd39fefd63adc28f93e6fef",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -197,7 +197,7 @@ class DiffLlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "2663332aaf8100bef197925e7a7aa25cb9629757",
            "filename": "tests/models/dinat/test_modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -213,7 +213,7 @@ class DinatModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_torchscript = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "f8fa6d86076b1f752dcda92bc38d6962b69351af",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -229,7 +229,6 @@ class Dinov2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     fx_compatible = False  # broken by output recording refactor\n \n-    test_pruning = False\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "8f745c41bd963f191b995b69fd0740ef98c78fa0",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -234,7 +234,6 @@ class Dinov2WithRegistersModelTest(ModelTesterMixin, PipelineTesterMixin, unitte\n     )\n     fx_compatible = False\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "bd1acbcc9445bcfdaf9159e8f5497ad6c9fd4123",
            "filename": "tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -122,7 +122,7 @@ class DINOv3ConvNextModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     pipeline_model_mapping = {\"image-feature-extraction\": DINOv3ConvNextModel} if is_torch_available() else {}\n \n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "eac994a0569fee7fb5c818ff58a30cb110ea409f",
            "filename": "tests/models/dinov3_vit/test_modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_vit%2Ftest_modeling_dinov3_vit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -152,7 +152,6 @@ class Dinov3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     fx_compatible = False\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "b5e82a822a0f7cd5b26548d3a584380ddbc4a488",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -224,7 +224,6 @@ class DistilBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         else {}\n     )\n     fx_compatible = False  # won't be maintained\n-    test_pruning = True\n     test_resize_embeddings = True\n     test_resize_position_embeddings = True\n "
        },
        {
            "sha": "f045b1e7a97ef31e72eb82a4fafd778727530045",
            "filename": "tests/models/doge/test_modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -274,7 +274,7 @@ class DogeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         else {}\n     )\n     has_attentions = False\n-    test_pruning = False\n+\n     test_torchscript = False\n     fx_compatible = False\n "
        },
        {
            "sha": "3f89e0816e41fc8a21953254dc6e8a325588396e",
            "filename": "tests/models/donut/test_modeling_donut_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -168,7 +168,6 @@ class DonutSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n     )\n     fx_compatible = True\n \n-    test_pruning = False\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "791b3909318059eacb722595e667ceca35c78779",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -77,7 +77,6 @@ class Dots1ModelTest(CausalLMModelTest, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     model_tester_class = Dots1ModelTester\n \n "
        },
        {
            "sha": "22db4fd48f2d4ad54dcfe54a7497cd65ce0b0b21",
            "filename": "tests/models/dpr/test_modeling_dpr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -187,7 +187,6 @@ class DPRModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = False\n     test_missing_keys = False  # why?\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = DPRModelTester(self)"
        },
        {
            "sha": "246750893dbffee6071ef1956f115a4d5a8c87c2",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -170,7 +170,6 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "f2c8bd4b5e450db75985e23f36f2589a9c55272d",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -137,7 +137,6 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DPTForDepthEstimation,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"depth-estimation\": DPTForDepthEstimation} if is_torch_available() else {}\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n     test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\""
        },
        {
            "sha": "f54ab5484f1ae68b5fb6996d6caa2ec3501f08dd",
            "filename": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -182,7 +182,6 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "935e88eeeb31e95c25d50e53ce745133ecbc83a9",
            "filename": "tests/models/edgetam/test_modeling_edgetam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -235,7 +235,7 @@ class EdgeTamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         {\"feature-extraction\": EdgeTamModel, \"mask-generation\": EdgeTamModel} if is_torch_available() else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torchscript = False\n     _is_composite = True"
        },
        {
            "sha": "f3871e5212dec6e67dcae6252db51b158419690d",
            "filename": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -134,7 +134,6 @@ def prepare_config_and_inputs_for_common(self):\n class EfficientLoFTRModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (EfficientLoFTRForKeypointMatching, EfficientLoFTRModel) if is_torch_available() else ()\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = True\n "
        },
        {
            "sha": "3146962209bfa449e2bb18d15cbf310c4bd60121",
            "filename": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -135,7 +135,7 @@ class EfficientNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n     )\n \n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "29a87d46d7884ad921887ebdb0277ed7dbd01631",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -132,7 +132,7 @@ class Emu3Text2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTe\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     def setUp(self):\n@@ -320,7 +320,7 @@ class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n         else ()\n     )\n     pipeline_model_mapping = {}\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     def setUp(self):"
        },
        {
            "sha": "cf75839e0140050e54cbe3dab5e3592004670aed",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -145,7 +145,7 @@ def create_and_check_model_forward(self, config, inputs_dict):\n class EncodecModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (EncodecModel,) if is_torch_available() else ()\n     is_encoder_decoder = True\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     pipeline_model_mapping = {\"feature-extraction\": EncodecModel} if is_torch_available() else {}\n "
        },
        {
            "sha": "20dfd008a7a0d95421054f19a978c65d09214773",
            "filename": "tests/models/eomt/test_modeling_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -105,7 +105,7 @@ class EomtForUniversalSegmentationTest(ModelTesterMixin, PipelineTesterMixin, un\n     all_model_classes = (EomtForUniversalSegmentation,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-segmentation\": EomtForUniversalSegmentation} if is_torch_available() else {}\n     is_encoder_decoder = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torch_exportable = False\n "
        },
        {
            "sha": "04207a27b78dbf4365b2a186173cbf72904ccfb2",
            "filename": "tests/models/esm/test_modeling_esmfold.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -208,22 +208,6 @@ def test_resize_tokens_embeddings(self):\n     def test_inputs_embeds(self):\n         pass\n \n-    @unittest.skip(reason=\"ESMFold does not support head pruning.\")\n-    def test_head_pruning(self):\n-        pass\n-\n-    @unittest.skip(reason=\"ESMFold does not support head pruning.\")\n-    def test_head_pruning_integration(self):\n-        pass\n-\n-    @unittest.skip(reason=\"ESMFold does not support head pruning.\")\n-    def test_head_pruning_save_load_from_config_init(self):\n-        pass\n-\n-    @unittest.skip(reason=\"ESMFold does not support head pruning.\")\n-    def test_head_pruning_save_load_from_pretrained(self):\n-        pass\n-\n     @unittest.skip(reason=\"ESMFold does not output hidden states in the normal way.\")\n     def test_hidden_states_output(self):\n         pass"
        },
        {
            "sha": "8537f6ec8cfecba4668e9a3dd10504f1eb73a372",
            "filename": "tests/models/evolla/test_modeling_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -199,7 +199,7 @@ def prepare_config_and_inputs_for_common(self):\n class EvollaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (EvollaModel, EvollaForProteinText2Text) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": EvollaModel} if is_torch_available() else {}\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = False\n     maxDiff = None"
        },
        {
            "sha": "76fd7c9508f824ca28fae0d01dc66747941da06f",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -259,7 +259,7 @@ def create_and_check_decoder_model_past_large_inputs(\n @require_torch\n class FalconH1ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FalconH1Model, FalconH1ForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "c4a471fac961f6d3530e311002a54a88b5cf4e82",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -272,7 +272,7 @@ class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n-    test_pruning = False\n+\n     pipeline_model_mapping = (\n         {\"feature-extraction\": FalconMambaModel, \"text-generation\": FalconMambaForCausalLM}\n         if is_torch_available()"
        },
        {
            "sha": "c6c455ba222b4c59e384c76a4618b9aaf5dcf0c6",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -125,7 +125,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FastSpeech2ConformerModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FastSpeech2ConformerModel,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = False\n     is_encoder_decoder = True\n@@ -545,7 +545,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FastSpeech2ConformerWithHifiGanTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FastSpeech2ConformerWithHifiGan,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = False\n     is_encoder_decoder = True"
        },
        {
            "sha": "a517b6f236b27005bdf468613261dfcd347f64c8",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -163,7 +163,6 @@ class FlavaImageModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (FlavaImageModel,) if is_torch_available() else ()\n \n-    test_pruning = False\n     test_torchscript = False\n     test_resize_embeddings = False\n \n@@ -432,7 +431,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FlavaTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaTextModel,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n \n     def setUp(self):\n@@ -569,7 +568,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FlavaMultimodalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaMultimodalModel,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torchscript = False\n \n@@ -683,7 +682,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FlavaImageCodebookTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaImageCodebook,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torchscript = False\n     has_attentions = False\n@@ -883,7 +882,7 @@ class FlavaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": FlavaModel} if is_torch_available() else {}\n     class_for_tester = FlavaModelTester\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "2fe7ff21bd8b125ecc2a629752d0f6540eaabf7d",
            "filename": "tests/models/florence2/test_modeling_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -235,7 +235,7 @@ class Florence2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     test_attention_outputs = False\n     _is_composite = True\n "
        },
        {
            "sha": "fba6584a80c28092ea0a7f0e52e09b90e2f9ab99",
            "filename": "tests/models/fnet/test_modeling_fnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -254,7 +254,6 @@ class FNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     # Skip Tests\n-    test_pruning = False\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "be67090daba3cbd1c8f482852ded599b49193d3d",
            "filename": "tests/models/focalnet/test_modeling_focalnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -243,7 +243,6 @@ class FocalNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     )\n     fx_compatible = False\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "4385a95385120a8e056ddbed385a3e490be29cd1",
            "filename": "tests/models/fsmt/test_modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -162,7 +162,7 @@ class FSMTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "e285d7fe87ec303dbcc8b312bb9eb5b65187e3f3",
            "filename": "tests/models/funnel/test_modeling_funnel.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffunnel%2Ftest_modeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffunnel%2Ftest_modeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffunnel%2Ftest_modeling_funnel.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -352,7 +352,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class FunnelModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_pruning = False\n     all_model_classes = (\n         (\n             FunnelModel,\n@@ -430,7 +429,6 @@ def _mock_init_weights(self, module):\n \n @require_torch\n class FunnelBaseModelTest(ModelTesterMixin, unittest.TestCase):\n-    test_pruning = False\n     all_model_classes = (\n         (FunnelBaseModel, FunnelForMultipleChoice, FunnelForSequenceClassification) if is_torch_available() else ()\n     )"
        },
        {
            "sha": "d5403e5d42abf536a1ef7490776d087353785f88",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -169,7 +169,6 @@ class FuyuModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         {\"text-generation\": FuyuForCausalLM, \"image-text-to-text\": FuyuForCausalLM} if is_torch_available() else {}\n     )\n \n-    test_pruning = False\n     test_cpu_offload = False\n     test_disk_offload = False\n "
        },
        {
            "sha": "dd2893b9c716e62acb39b690ee345c0cc5d3fdf6",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -267,7 +267,7 @@ class Gemma3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unitte\n         else ()\n     )\n     all_generative_model_classes = (Gemma3ForConditionalGeneration,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_missing_keys = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]"
        },
        {
            "sha": "b5beff5fa2de9a6a6802e7dc35c335d69307d70f",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -142,7 +142,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Gemma3nAudioModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Gemma3nAudioEncoder,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_missing_keys = False\n     is_generative = False\n     _is_stateful = True\n@@ -666,7 +666,7 @@ def prepare_config_and_inputs_for_common(self):\n class Gemma3nVision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Gemma3nModel, Gemma3nForConditionalGeneration) if is_torch_available() else ()\n     all_generative_model_classes = (Gemma3nForConditionalGeneration,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_missing_keys = False\n     _is_stateful = True\n     model_split_percents = [0.5, 0.6]"
        },
        {
            "sha": "67c75ed52dba8ee8dd45a8e97ae681f5ba18128e",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -126,7 +126,7 @@ class GitVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (GitVisionModel,) if is_torch_available() else ()\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "0e8d07b1c7df8fbcd129dab52db65cf9df3b60db",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -171,7 +171,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Glm4vModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Glm4vModel, Glm4vForConditionalGeneration) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     model_split_percents = [0.7, 0.9]  # model too big to split at 0.5\n     _is_composite = True"
        },
        {
            "sha": "ad3986ce36416c271fd5dc818b6beb88fc31960e",
            "filename": "tests/models/glm4v_moe/test_modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -182,7 +182,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Glm4vMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Glm4vMoeModel, Glm4vMoeForConditionalGeneration) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     model_split_percents = [0.7, 0.9]  # model too big to split at 0.5\n     _is_composite = True"
        },
        {
            "sha": "5db8cc4c1aa4f64d9902f53d0047f234c142c8a5",
            "filename": "tests/models/glpn/test_modeling_glpn.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -148,7 +148,6 @@ class GLPNModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "79997080f1f588a7af183038fb4cf22a8c3b49aa",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -153,7 +153,6 @@ class GotOcr2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = GotOcr2VisionText2TextModelTester(self)"
        },
        {
            "sha": "c5ae11b769eb189bc3f2c538cee50bba95aac7e3",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -398,7 +398,7 @@ class GPTBigCodeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n     )\n     fx_compatible = False\n     test_missing_keys = False\n-    test_pruning = False\n+\n     test_torchscript = False\n     multi_query = True\n "
        },
        {
            "sha": "b3f4dfbe8bb35bc6e03829d83fbfe9a992c130ae",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -380,7 +380,6 @@ class GPTNeoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     )\n     fx_compatible = True\n     test_missing_keys = False\n-    test_pruning = False\n \n     # special case for DoubleHeads model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "7d359b31cd605c00ece5eac77dfd9cccce94a725",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -285,7 +285,7 @@ class GPTNeoXModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "6de7193ec08a8de719b3e4cf2c85f49e016614c8",
            "filename": "tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -202,7 +202,7 @@ class GPTNeoXModelJapaneseTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     def setUp(self):"
        },
        {
            "sha": "fc2812c224c95e2551e1838f58b4854e08b79325",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -344,7 +344,7 @@ class GPTJModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         else {}\n     )\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     def test_torch_fx(self):"
        },
        {
            "sha": "2e1223394f2014e689fd28e97c9e956675ac8548",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -179,7 +179,7 @@ class GraniteModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "a92fa8c448f32052096762983c5cf908c4ba3326",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -217,7 +217,7 @@ class GraniteSpeechForConditionalGenerationModelTest(ModelTesterMixin, Generatio\n     \"\"\"\n \n     all_model_classes = (GraniteSpeechForConditionalGeneration,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "fd5de1bcc369d884a3c376d7839225ba70c9b5a5",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -178,7 +178,7 @@ class GraniteMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "7c19e05ab339e9b6d05929362a8147d428c1f511",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -181,7 +181,7 @@ class GraniteMoeSharedModelTest(ModelTesterMixin, GenerationTesterMixin, unittes\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "ddb5f383dbfebf93547b0701b10e3db9c9f1b09e",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -248,7 +248,7 @@ class GroundingDinoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Tes\n     all_model_classes = (GroundingDinoModel, GroundingDinoForObjectDetection) if is_torch_available() else ()\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     pipeline_model_mapping = (\n         {\"image-feature-extraction\": GroundingDinoModel, \"zero-shot-object-detection\": GroundingDinoForObjectDetection}"
        },
        {
            "sha": "ca60a9a2dc1a8e2d20b25ce417cbe33e55414896",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -143,7 +143,6 @@ class GroupViTVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (GroupViTVisionModel,) if is_torch_available() else ()\n \n-    test_pruning = False\n     test_torchscript = False\n     test_resize_embeddings = False\n \n@@ -429,7 +428,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class GroupViTTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (GroupViTTextModel,) if is_torch_available() else ()\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = GroupViTTextModelTester(self)\n@@ -528,7 +526,7 @@ def prepare_config_and_inputs_for_common(self):\n class GroupViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (GroupViTModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": GroupViTModel} if is_torch_available() else {}\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "cb330a021425c33929dfa4ce99dbfae7dd621aaf",
            "filename": "tests/models/hgnet_v2/test_modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhgnet_v2%2Ftest_modeling_hgnet_v2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -179,7 +179,7 @@ class HGNetV2ForImageClassificationTest(ModelTesterMixin, PipelineTesterMixin, u\n     pipeline_model_mapping = {\"image-classification\": HGNetV2ForImageClassification} if is_torch_available() else {}\n \n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torch_exportable = True\n     has_attentions = False"
        },
        {
            "sha": "e4d1fe26217b1141b33680e584f8fffefe6c8f07",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -245,7 +245,6 @@ class HieraModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     fx_compatible = True\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "1e98603a9b5de84ace06b5afe4b55fa566a6f5d1",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -313,7 +313,6 @@ class HubertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     fx_compatible = True\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = HubertModelTester(self)\n@@ -547,7 +546,6 @@ def test_model_from_pretrained(self):\n @require_torch\n class HubertRobustModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (HubertForCTC, HubertForSequenceClassification, HubertModel) if is_torch_available() else ()\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = HubertModelTester("
        },
        {
            "sha": "226dd73627ec63ee64876ec367c20e2a2c9a47b1",
            "filename": "tests/models/ibert/test_modeling_ibert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -224,7 +224,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class IBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_pruning = False\n     test_torchscript = False\n     test_resize_embeddings = False\n "
        },
        {
            "sha": "e0977d5e788510f6f08e390dfcf90bc205fbe8ea",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -323,7 +323,7 @@ class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMi\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     test_torchscript = False\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "94a825de108a010476dcbdbc6800a3689dd37e35",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -178,7 +178,7 @@ class Idefics2ModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Idefics2Model,) if is_torch_available() else ()\n     fx_compatible = False\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     _is_composite = True\n \n@@ -369,7 +369,7 @@ class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     all_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Idefics2ForConditionalGeneration} if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_torchscript = False\n "
        },
        {
            "sha": "4c71b70a6e006a88e30c316547f760cb4abe3e7b",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -168,7 +168,7 @@ class Idefics3ModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (Idefics3Model,) if is_torch_available() else ()\n     fx_compatible = False\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n \n     def setUp(self):\n@@ -334,7 +334,7 @@ class Idefics3ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     all_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Idefics3ForConditionalGeneration} if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_torchscript = False\n "
        },
        {
            "sha": "bf2d75987d7e1cdf982c030ed4e792b1eda0965d",
            "filename": "tests/models/ijepa/test_modeling_ijepa.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -203,7 +203,6 @@ class IJepaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     fx_compatible = False  # broken by output recording refactor\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "bc6c3cfcec7b1052e28a7b4fa7c58363decf1c69",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -194,7 +194,7 @@ class InformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     all_model_classes = (InformerModel, InformerForPrediction) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": InformerModel} if is_torch_available() else {}\n     is_encoder_decoder = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torchscript = False\n     test_inputs_embeds = False"
        },
        {
            "sha": "b694d1a084a804d9d71a9a812909c2b7ff7766e3",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -149,7 +149,7 @@ class InstructBlipVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (InstructBlipVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -475,7 +475,7 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n     pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n     additional_model_inputs = [\"qformer_input_ids\", \"input_ids\"]\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False"
        },
        {
            "sha": "c411fe8c874a3b17e3b33330e9c4a6d57aa664db",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -153,7 +153,7 @@ class InstructBlipVideoVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (InstructBlipVideoVisionModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -490,7 +490,7 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     )\n     additional_model_inputs = [\"qformer_input_ids\", \"input_ids\"]\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False"
        },
        {
            "sha": "603fb2be299a1641944b37c0e5499d5967101788",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -192,7 +192,6 @@ class InternVLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = InternVLVisionText2TextModelTester(self)"
        },
        {
            "sha": "81144a59376a2766d1c698fb3c63bc1817b5133e",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -340,7 +340,6 @@ class JambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n         self.assertIsInstance(decoder_past_key_values, HybridMambaAttentionDynamicCache)"
        },
        {
            "sha": "d8f632ac091dcaaa583d50f6871e731d45808450",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -194,7 +194,7 @@ class JanusVisionText2TextModelTest(ModelTesterMixin, GenerationTesterMixin, uni\n     all_model_classes = (JanusModel, JanusForConditionalGeneration) if is_torch_available() else ()\n     all_generative_model_classes = (JanusForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     _is_composite = True\n \n     def setUp(self):\n@@ -353,7 +353,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class JanusVQModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (JanusVQVAE,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     fx_compatible = False\n     has_attentions = False\n     test_resize_embeddings = False"
        },
        {
            "sha": "7ac48f09f510585ededf905cdea816680a9d7384",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -271,7 +271,7 @@ class Kosmos2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True"
        },
        {
            "sha": "0bd51d33450d8fa2d03b0d7c54b3759eeb835dab",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -301,7 +301,7 @@ class Kosmos2_5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True"
        },
        {
            "sha": "cf57c2a658c9ed1724a7fa10374d17b7995a8a69",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -252,7 +252,7 @@ class KyutaiSpeechToTextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "a831677efd69700b7348af44338dcf5cc36c67c3",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -265,7 +265,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n @require_detectron2\n class LayoutLMv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_pruning = False\n     test_torchscript = True\n     test_mismatched_shapes = False\n "
        },
        {
            "sha": "7acd299fbe6d2fd6a4cb91ff3d15d7fa083087a1",
            "filename": "tests/models/layoutlmv3/test_modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -270,7 +270,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class LayoutLMv3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_pruning = False\n     test_torchscript = False\n     test_mismatched_shapes = False\n "
        },
        {
            "sha": "193756b8353a1c46be115d48c1cbc2f60bc058d5",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -282,7 +282,7 @@ class LEDModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torchscript = False\n "
        },
        {
            "sha": "662600828fa3c1b824896f48fdfb1103d0d9cb4a",
            "filename": "tests/models/levit/test_modeling_levit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -182,7 +182,6 @@ class LevitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_pruning = False\n     test_torchscript = False\n     test_resize_embeddings = False\n     has_attentions = False"
        },
        {
            "sha": "72ce8f71e31b9f901faae382022f4532bb1b6a2c",
            "filename": "tests/models/lfm2_vl/test_modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_modeling_lfm2_vl.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -161,7 +161,7 @@ class Lfm2VlModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     fx_compatible = False\n     model_tester_class = Lfm2VlModelTester\n     _is_composite = True"
        },
        {
            "sha": "5dc57707b4ac587970db0bd6e320aa0841e2c2b3",
            "filename": "tests/models/lightglue/test_modeling_lightglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flightglue%2Ftest_modeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flightglue%2Ftest_modeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flightglue%2Ftest_modeling_lightglue.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -127,7 +127,6 @@ class LightGlueModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (LightGlueForKeypointMatching,) if is_torch_available() else ()\n     all_generative_model_classes = () if is_torch_available() else ()\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = True\n "
        },
        {
            "sha": "957e836b2e5e9c7e4d88c65e523ade24cb309eeb",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -239,7 +239,6 @@ class LiltModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n \n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "2634bc406bd33079835861f9ecfd0a9afca79ab1",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -183,7 +183,7 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "4b40a2d2d11f24852f7a15b0956ce23b44b0a018",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -191,7 +191,7 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n         else ()\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": LlavaNextForConditionalGeneration} if is_torch_available() else {}\n-    test_pruning = False\n+\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "6d36ca5c2af0405529fd7da5dcb1365054c8710a",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -204,7 +204,7 @@ class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, Generati\n         if is_torch_available()\n         else ()\n     )\n-    test_pruning = False\n+\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "0dfa7aa6c7f09969e1b8caff636c8a51be2c7d3b",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -195,7 +195,7 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     pipeline_model_mapping = (\n         {\"image-text-to-text\": LlavaOnevisionForConditionalGeneration} if is_torch_available() else {}\n     )\n-    test_pruning = False\n+\n     # MP works but offload doesn't work when the MultiheadAttention is offloaded\n     # TODO: One potential solution would be to add to set preload_module_classes = [\"Siglip2MultiheadAttentionPoolingHead\"]\n     # in the dispatch_model function"
        },
        {
            "sha": "f7e9bb8a86e52704c5a5cf89ff401e0ff0185d8f",
            "filename": "tests/models/longformer/test_modeling_longformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -304,7 +304,6 @@ def prepare_config_and_inputs_for_question_answering(self):\n \n @require_torch\n class LongformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_pruning = False  # pruning is not supported\n     test_torchscript = False\n \n     all_model_classes = ("
        },
        {
            "sha": "8eaa70c919b82f7c4575706c4eece65621865d02",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -510,7 +510,7 @@ class LongT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_torchscript = True\n     test_resize_embeddings = True\n     is_encoder_decoder = True\n@@ -1006,7 +1006,7 @@ def prepare_config_and_inputs_for_common(self):\n \n class LongT5EncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (LongT5EncoderModel,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = True\n     test_resize_embeddings = False\n "
        },
        {
            "sha": "270906de0d1989a5e69cb09ee47a63a0cd6e96dd",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -613,7 +613,7 @@ class LukeModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = True\n "
        },
        {
            "sha": "a84fc7236ba32f18853f4d81ec2b93db573dc70b",
            "filename": "tests/models/lxmert/test_modeling_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -530,7 +530,7 @@ class LxmertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     fx_compatible = True\n-    test_pruning = False\n+\n     test_torchscript = False\n \n     # overwrite function because qa models takes different input label shape"
        },
        {
            "sha": "002b9630bf6998a72790abad77b8947e8fd6f357",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -238,7 +238,7 @@ class M2M100ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     )\n     is_encoder_decoder = True\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests"
        },
        {
            "sha": "99567c015572d096118ab03eb860f49fc3fec205",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -241,7 +241,7 @@ class MambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n-    test_pruning = False\n+\n     pipeline_model_mapping = (\n         {\"feature-extraction\": MambaModel, \"text-generation\": MambaForCausalLM} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "41a48776594217b8a570896794f125beb46f4251",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -244,7 +244,6 @@ class Mamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     fx_compatible = False  # FIXME let's try to support this @molbap\n     test_torchscript = False  # FIXME I think this should be doable @molbap @ArthurZucker\n     test_missing_keys = False\n-    test_pruning = False\n \n     pipeline_model_mapping = (\n         {\"feature-extraction\": Mamba2Model, \"text-generation\": Mamba2ForCausalLM} if is_torch_available() else {}"
        },
        {
            "sha": "e6fafbb9cbdd13489d3956c4bed604f13d90e7a5",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -230,7 +230,7 @@ class MarianModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     )\n     is_encoder_decoder = True\n     fx_compatible = True\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     def setUp(self):\n@@ -804,7 +804,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MarianStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MarianDecoder, MarianForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp("
        },
        {
            "sha": "269e9e8aa1b8cd4ef5cd7f90a13c8d763044da5d",
            "filename": "tests/models/mask2former/test_modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -203,7 +203,7 @@ class Mask2FormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     pipeline_model_mapping = {\"image-feature-extraction\": Mask2FormerModel} if is_torch_available() else {}\n \n     is_encoder_decoder = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "6d1058fa03bc541bd127dcdaf772299fe4305e9c",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -205,7 +205,7 @@ class MaskFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     )\n \n     is_encoder_decoder = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     zero_init_hidden_state = True\n     test_torch_exportable = True"
        },
        {
            "sha": "8f577339c0bfdd294bea2a28fc4f51e720149584",
            "filename": "tests/models/maskformer/test_modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -176,7 +176,7 @@ class MaskFormerSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     pipeline_model_mapping = {\"feature-extraction\": MaskFormerSwinModel} if is_torch_available() else {}\n     fx_compatible = False\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "f9da9fc086407a748d0ea969810e5cbc0acb50c5",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -232,7 +232,7 @@ class MBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     )\n     is_encoder_decoder = True\n     fx_compatible = False  # Fix me Michael\n-    test_pruning = False\n+\n     test_missing_keys = False\n \n     # TODO: Fix the failed tests\n@@ -713,7 +713,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MBartStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MBartDecoder, MBartForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     is_encoder_decoder = False\n \n     def setUp("
        },
        {
            "sha": "69b9a86eda3db3c1391a38306164647623e395f6",
            "filename": "tests/models/metaclip_2/test_modeling_metaclip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -210,7 +210,7 @@ class MetaClip2VisionModelTest(MetaClip2ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (MetaClip2VisionModel, MetaClip2VisionModelWithProjection) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -405,7 +405,7 @@ def prepare_config_and_inputs_for_common(self):\n class MetaClip2TextModelTest(MetaClip2ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (MetaClip2TextModel, MetaClip2TextModelWithProjection) if is_torch_available() else ()\n     fx_compatible = False\n-    test_pruning = False\n+\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     def setUp(self):\n@@ -537,7 +537,7 @@ class MetaClip2ModelTest(MetaClip2ModelTesterMixin, PipelineTesterMixin, unittes\n     )\n     additional_model_inputs = [\"pixel_values\"]\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True\n@@ -709,7 +709,7 @@ class MetaClip2ForImageClassificationModelTest(MetaClip2ModelTesterMixin, Pipeli\n     all_model_classes = (MetaClip2ForImageClassification,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-classification\": MetaClip2ForImageClassification} if is_torch_available() else {}\n     fx_compatible = False\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True"
        },
        {
            "sha": "d857c96d8d967553fba9056c97b15168ea4feb25",
            "filename": "tests/models/mgp_str/test_modeling_mgp_str.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -124,7 +124,6 @@ class MgpstrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     fx_compatible = False\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n "
        },
        {
            "sha": "94b580cdc13e9aea0b05bffa514821c9372361b5",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -163,7 +163,7 @@ def create_and_check_model_forward(self, config, inputs_dict):\n class MimiModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (MimiModel,) if is_torch_available() else ()\n     is_encoder_decoder = True\n-    test_pruning = False\n+\n     test_resize_embeddings = False\n     test_torchscript = False\n "
        },
        {
            "sha": "a738770b9d12ffcd325c3c57ea1cafc4ffe13776",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -176,7 +176,6 @@ class Mistral3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         else {}\n     )\n     _is_composite = True\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = Mistral3VisionText2TextModelTester(self)"
        },
        {
            "sha": "431b8cffebc3b090c4819c5a7bb8b5c6bef8417f",
            "filename": "tests/models/mlcd/test_modeling_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -123,7 +123,7 @@ class MLCDVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n \n     all_model_classes = (MLCDVisionModel,) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "8351558622e2f6aa2acb902eefaa7bc4e509d8a5",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -125,7 +125,6 @@ class MllamaForCausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, unitte\n     \"\"\"\n \n     all_model_classes = (MllamaForCausalLM,) if is_torch_available() else ()\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = MllamaText2TextModelTester(self)\n@@ -279,7 +278,7 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n         else ()\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": MllamaForConditionalGeneration} if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_torchscript = False\n     _is_composite = True\n "
        },
        {
            "sha": "807184b3ea5b5f3209e9f9c64dea6b41e3e9f870",
            "filename": "tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -250,7 +250,7 @@ class MMGroundingDinoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.T\n     all_model_classes = (MMGroundingDinoModel, MMGroundingDinoForObjectDetection) if is_torch_available() else ()\n     is_encoder_decoder = True\n     test_torchscript = False\n-    test_pruning = False\n+\n     test_missing_keys = False\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "564b1133eabee75ff01651d9dc18965e62fdaac4",
            "filename": "tests/models/mobilenet_v1/test_modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -150,7 +150,6 @@ class MobileNetV1ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "ff54c8f93453851f6279703fc6038551b2f01cb2",
            "filename": "tests/models/mobilenet_v2/test_modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -201,7 +201,6 @@ class MobileNetV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "64ba10f6423be7ae7a90c86caee9b4ae5cf5aed5",
            "filename": "tests/models/mobilevit/test_modeling_mobilevit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -194,7 +194,6 @@ class MobileViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "4e3d3355fcd58b39fd194e5a1c6ad8bc7d0c438c",
            "filename": "tests/models/mobilevitv2/test_modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -203,7 +203,6 @@ class MobileViTV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n         else {}\n     )\n \n-    test_pruning = False\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "ac14b4589d543aa222ee17f6e8f96829f7121237",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -263,7 +263,7 @@ class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         else {}\n     )\n     fx_compatible = False\n-    test_pruning = False\n+\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     # special case for ForPreTraining model"
        },
        {
            "sha": "a9227c6c679ebee31a3f5c54095324c86fde5fe9",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -143,7 +143,6 @@ class MoonshineModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n \n     def setUp(self):\n         self.model_tester = MoonshineModelTester(self)"
        },
        {
            "sha": "a4447d2a671ffd07efc5feeb02ce8c7bfb43c539",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -155,7 +155,7 @@ def prepare_config_and_inputs_for_common(self, batch_size=None):\n @require_torch\n class MoshiDecoderTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MoshiModel, MoshiForCausalLM) if is_torch_available() else ()\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n     pipeline_model_mapping = (\n         {\n@@ -531,7 +531,7 @@ def prepare_config_and_inputs_for_common(self, batch_size=None):\n @require_torch\n class MoshiTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MoshiForConditionalGeneration,) if is_torch_available() else ()\n-    test_pruning = False  # training is not supported yet for Moshi\n+    # training is not supported yet for Moshi\n     test_resize_embeddings = False\n     test_torchscript = False\n "
        },
        {
            "sha": "4f2b3f0668d8ebb362135bb4ccf23043e1cabed8",
            "filename": "tests/models/mpnet/test_modeling_mpnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e475552be13be0d6c98adf02c199709a0d7b927/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py?ref=7e475552be13be0d6c98adf02c199709a0d7b927",
            "patch": "@@ -215,7 +215,7 @@ class MPNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else {}\n     )\n-    test_pruning = False\n+\n     test_resize_embeddings = True\n \n     def setUp(self):"
        }
    ],
    "stats": {
        "total": 4044,
        "additions": 352,
        "deletions": 3692
    }
}