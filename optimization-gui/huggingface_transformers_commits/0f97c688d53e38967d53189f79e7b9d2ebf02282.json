{
    "author": "preetam1407",
    "message": "Fix BLT training_ci overfit test (#42685)\n\n* Fix BLT training_ci overfit test by disabling cache and adjusting training thresholds\n\n* Fix BLT training_ci overfit test by disabling cache and adjusting training thresholds\n\n* Fix BLT training_ci overfit test by disabling cache and adjusting training thresholds\n\n* Format BLT tests with ruff\n\n* Fix BLT training CI with custom weight initialization and overfit test\n\n* Fix BLT training CI with custom weight initialization and overfit test\n\n* Fix BLT training CI with custom weight initialization and overfit test\n\n* Fix BLT training CI with custom weight initialization and overfit test\n\n* Fix BLT training CI with custom weight initialization and overfit test\n\n* Fix BLT training CI with custom weight initialization and overfit test\n\n* Update BLT init logic and adjust repo checks for non-functional model wrappers\n\n* Fix repo/config checks by marking BLT Text/Vision models as placeholders\n\n* Fix repo/config checks by marking BLT Text/Vision models as placeholders\n\n* Fix repo/config checks by marking BLT Text/Vision models as placeholders\n\n* Document BLT weight initialization sources and restore default overfit thresholds\n\n* Align BLT weight init with nn.init\n\n* Fix BLT init weights and remove modular conversion issues\n\n* fixes circle ci failures\n\n* fix\n\n* fix\n\n* fix recurrent_gemma overfit generation with cache\n\n* Fix recurrent_gemma overfit generation with cache\n\n* rerun circleci\n\n* rerun circleci\n\n* Log RecurrentGemma cache exception in training mixin\n\n* ci: rerun\n\n* ci: rerun\n\n* ci: rerun\n\n---------\n\nCo-authored-by: Ferdinand Mom <47445085+3outeille@users.noreply.github.com>",
    "sha": "0f97c688d53e38967d53189f79e7b9d2ebf02282",
    "files": [
        {
            "sha": "d0cf94dc14d46263052ad22713606574ce42c015",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 149,
            "deletions": 0,
            "changes": 149,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f97c688d53e38967d53189f79e7b9d2ebf02282/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f97c688d53e38967d53189f79e7b9d2ebf02282/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=0f97c688d53e38967d53189f79e7b9d2ebf02282",
            "patch": "@@ -444,6 +444,155 @@ class BltPreTrainedModel(PreTrainedModel):\n         \"attentions\": OutputRecorder(BltSelfAttention, index=1, layer_name=\"local_decoder\"),\n     }\n \n+    @torch.no_grad()\n+    def _init_weights(self, module):\n+        \"\"\"\n+        Initialize BLT weights following the original ByteLatentTransformer:\n+\n+        - Most weights are drawn from a truncated normal.\n+        - Scale is ~ 1 / sqrt(model_dim) (or 1 / sqrt(hidden_dim) for FFN outputs).\n+        - Norm layers are set to weight = 1, bias = 0.\n+        \"\"\"\n+        class_name = module.__class__.__name__\n+\n+        # Norms: RMSNorm / LayerNorm\n+        if isinstance(module, (BltRMSNorm, nn.LayerNorm)) or \"RMSNorm\" in class_name or \"LayerNorm\" in class_name:\n+            if getattr(module, \"weight\", None) is not None:\n+                nn.init.ones_(module.weight)\n+            if getattr(module, \"bias\", None) is not None:\n+                nn.init.zeros_(module.bias)\n+            return\n+\n+        # Embeddings (encoder / patcher / hash embeddings)\n+        if isinstance(module, nn.Embedding):\n+            hidden_size = getattr(self.config, \"hidden_size\", None)\n+            if hidden_size is None and hasattr(self.config, \"encoder_config\"):\n+                hidden_size = getattr(self.config.encoder_config, \"hidden_size\", None)\n+            if hidden_size is None:\n+                hidden_size = module.embedding_dim\n+\n+            std = hidden_size**-0.5\n+            nn.init.trunc_normal_(\n+                module.weight,\n+                mean=0.0,\n+                std=std,\n+                a=-3 * std,\n+                b=3 * std,\n+            )\n+            if module.padding_idx is not None:\n+                nn.init.zeros_(module.weight[module.padding_idx])\n+            return\n+\n+        # Self-attention / cross-attention projections\n+        if isinstance(module, (BltSelfAttention, BltCrossAttention)) or class_name in (\n+            \"MllamaTextSelfAttention\",\n+            \"MllamaTextCrossAttention\",\n+        ):\n+            dim = getattr(self.config, \"hidden_size\", None)\n+            if dim is None and hasattr(module, \"hidden_size\"):\n+                dim = module.hidden_size\n+            if dim is None:\n+                for name in (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"dense\"):\n+                    proj = getattr(module, name, None)\n+                    if proj is not None and hasattr(proj, \"weight\"):\n+                        dim = proj.weight.shape[-1]\n+                        break\n+            if dim is None:\n+                return\n+\n+            std = dim**-0.5\n+\n+            # Input projections (q, k, v)\n+            for proj_name in (\"q_proj\", \"k_proj\", \"v_proj\"):\n+                proj = getattr(module, proj_name, None)\n+                if proj is not None and hasattr(proj, \"weight\"):\n+                    nn.init.trunc_normal_(\n+                        proj.weight,\n+                        mean=0.0,\n+                        std=std,\n+                        a=-3 * std,\n+                        b=3 * std,\n+                    )\n+                    if getattr(proj, \"bias\", None) is not None:\n+                        nn.init.zeros_(proj.bias)\n+\n+            # Output projection: o_proj or dense\n+            o_proj = getattr(module, \"o_proj\", getattr(module, \"dense\", None))\n+            if o_proj is not None and hasattr(o_proj, \"weight\"):\n+                nn.init.trunc_normal_(\n+                    o_proj.weight,\n+                    mean=0.0,\n+                    std=std,\n+                    a=-3 * std,\n+                    b=3 * std,\n+                )\n+                if getattr(o_proj, \"bias\", None) is not None:\n+                    nn.init.zeros_(o_proj.bias)\n+            return\n+\n+        # MLP / FFN blocks\n+        if isinstance(module, BltMLP) or class_name == \"MllamaTextMLP\":\n+            hidden_size = getattr(self.config, \"hidden_size\", None)\n+            if hidden_size is None and hasattr(self.config, \"decoder_config\"):\n+                hidden_size = getattr(self.config.decoder_config, \"hidden_size\", None)\n+            if hidden_size is None and hasattr(self.config, \"encoder_config\"):\n+                hidden_size = getattr(self.config.encoder_config, \"hidden_size\", None)\n+\n+            # Input-side std\n+            in_std = None\n+            if hidden_size is not None:\n+                in_std = hidden_size**-0.5\n+\n+            gate_proj = getattr(module, \"gate_proj\", getattr(module, \"fc1\", None))\n+            up_proj = getattr(module, \"up_proj\", None)\n+            down_proj = getattr(module, \"down_proj\", getattr(module, \"fc2\", None))\n+\n+            # gate / input projections\n+            for proj in (gate_proj, up_proj):\n+                if proj is not None and hasattr(proj, \"weight\"):\n+                    std = in_std or (proj.weight.shape[1] ** -0.5)\n+                    nn.init.trunc_normal_(\n+                        proj.weight,\n+                        mean=0.0,\n+                        std=std,\n+                        a=-3 * std,\n+                        b=3 * std,\n+                    )\n+                    if getattr(proj, \"bias\", None) is not None:\n+                        nn.init.zeros_(proj.bias)\n+\n+            # output/ down projections\n+            if down_proj is not None and hasattr(down_proj, \"weight\"):\n+                hidden_dim = down_proj.weight.shape[1]\n+                out_std = hidden_dim**-0.5\n+                nn.init.trunc_normal_(\n+                    down_proj.weight,\n+                    mean=0.0,\n+                    std=out_std,\n+                    a=-3 * out_std,\n+                    b=3 * out_std,\n+                )\n+                if getattr(down_proj, \"bias\", None) is not None:\n+                    nn.init.zeros_(down_proj.bias)\n+            return\n+\n+        # Generic Linear layers (projections, lm_head, etc.)\n+        if isinstance(module, nn.Linear):\n+            fan_in = module.in_features\n+            std = fan_in**-0.5\n+            nn.init.trunc_normal_(\n+                module.weight,\n+                mean=0.0,\n+                std=std,\n+                a=-3 * std,\n+                b=3 * std,\n+            )\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+            return\n+\n+        return\n+\n \n class BltLocalEncoder(BltPreTrainedModel):\n     config: BltLocalEncoderConfig"
        },
        {
            "sha": "0482e9ffaad99b396653b43a4d748aea120f46de",
            "filename": "src/transformers/models/blt/modular_blt.py",
            "status": "modified",
            "additions": 155,
            "deletions": 1,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f97c688d53e38967d53189f79e7b9d2ebf02282/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f97c688d53e38967d53189f79e7b9d2ebf02282/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodular_blt.py?ref=0f97c688d53e38967d53189f79e7b9d2ebf02282",
            "patch": "@@ -360,8 +360,162 @@ class BltPreTrainedModel(MllamaPreTrainedModel):\n         \"attentions\": OutputRecorder(BltSelfAttention, index=1, layer_name=\"local_decoder\"),\n     }\n \n+    # Weight initialization is adapted from:\n+    # - https://github.com/facebookresearch/blt/blob/main/bytelatent/model/blt.py\n+    # - https://github.com/pytorch/torchtitan/blob/main/torchtitan/experiments/transformers_modeling_backend/model/model.py\n+    #\n+    # Both implementations use truncated normal initialization with std ~ 1 / sqrt(d_model)\n+    # (or 1 / sqrt(hidden_dim) for FFN outputs), and unit initialization for normalization layers.\n+    # We follow the same scheme here, but expressed in the Transformers APIs.\n+\n+    @torch.no_grad()\n     def _init_weights(self, module):\n-        raise AttributeError(\"No need to inherit it!\")\n+        \"\"\"\n+        Initialize BLT weights following the original ByteLatentTransformer:\n+\n+        - Most weights are drawn from a truncated normal.\n+        - Scale is ~ 1 / sqrt(model_dim) (or 1 / sqrt(hidden_dim) for FFN outputs).\n+        - Norm layers are set to weight = 1, bias = 0.\n+        \"\"\"\n+        class_name = module.__class__.__name__\n+\n+        # Norms: RMSNorm / LayerNorm\n+        if isinstance(module, (BltRMSNorm, nn.LayerNorm)) or \"RMSNorm\" in class_name or \"LayerNorm\" in class_name:\n+            if getattr(module, \"weight\", None) is not None:\n+                nn.init.ones_(module.weight)\n+            if getattr(module, \"bias\", None) is not None:\n+                nn.init.zeros_(module.bias)\n+            return\n+\n+        # Embeddings (encoder / patcher / hash embeddings)\n+        if isinstance(module, nn.Embedding):\n+            hidden_size = getattr(self.config, \"hidden_size\", None)\n+            if hidden_size is None and hasattr(self.config, \"encoder_config\"):\n+                hidden_size = getattr(self.config.encoder_config, \"hidden_size\", None)\n+            if hidden_size is None:\n+                hidden_size = module.embedding_dim\n+\n+            std = hidden_size**-0.5\n+            nn.init.trunc_normal_(\n+                module.weight,\n+                mean=0.0,\n+                std=std,\n+                a=-3 * std,\n+                b=3 * std,\n+            )\n+            if module.padding_idx is not None:\n+                nn.init.zeros_(module.weight[module.padding_idx])\n+            return\n+\n+        # Self-attention / cross-attention projections\n+        if isinstance(module, (BltSelfAttention, BltCrossAttention)) or class_name in (\n+            \"MllamaTextSelfAttention\",\n+            \"MllamaTextCrossAttention\",\n+        ):\n+            dim = getattr(self.config, \"hidden_size\", None)\n+            if dim is None and hasattr(module, \"hidden_size\"):\n+                dim = module.hidden_size\n+            if dim is None:\n+                for name in (\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"dense\"):\n+                    proj = getattr(module, name, None)\n+                    if proj is not None and hasattr(proj, \"weight\"):\n+                        dim = proj.weight.shape[-1]\n+                        break\n+            if dim is None:\n+                return\n+\n+            std = dim**-0.5\n+\n+            # Input projections (q, k, v)\n+            for proj_name in (\"q_proj\", \"k_proj\", \"v_proj\"):\n+                proj = getattr(module, proj_name, None)\n+                if proj is not None and hasattr(proj, \"weight\"):\n+                    nn.init.trunc_normal_(\n+                        proj.weight,\n+                        mean=0.0,\n+                        std=std,\n+                        a=-3 * std,\n+                        b=3 * std,\n+                    )\n+                    if getattr(proj, \"bias\", None) is not None:\n+                        nn.init.zeros_(proj.bias)\n+\n+            # Output projection: o_proj or dense\n+            o_proj = getattr(module, \"o_proj\", getattr(module, \"dense\", None))\n+            if o_proj is not None and hasattr(o_proj, \"weight\"):\n+                nn.init.trunc_normal_(\n+                    o_proj.weight,\n+                    mean=0.0,\n+                    std=std,\n+                    a=-3 * std,\n+                    b=3 * std,\n+                )\n+                if getattr(o_proj, \"bias\", None) is not None:\n+                    nn.init.zeros_(o_proj.bias)\n+            return\n+\n+        # MLP / FFN blocks\n+        if isinstance(module, BltMLP) or class_name == \"MllamaTextMLP\":\n+            hidden_size = getattr(self.config, \"hidden_size\", None)\n+            if hidden_size is None and hasattr(self.config, \"decoder_config\"):\n+                hidden_size = getattr(self.config.decoder_config, \"hidden_size\", None)\n+            if hidden_size is None and hasattr(self.config, \"encoder_config\"):\n+                hidden_size = getattr(self.config.encoder_config, \"hidden_size\", None)\n+\n+            # Input-side std\n+            in_std = None\n+            if hidden_size is not None:\n+                in_std = hidden_size**-0.5\n+\n+            gate_proj = getattr(module, \"gate_proj\", getattr(module, \"fc1\", None))\n+            up_proj = getattr(module, \"up_proj\", None)\n+            down_proj = getattr(module, \"down_proj\", getattr(module, \"fc2\", None))\n+\n+            # gate / input projections\n+            for proj in (gate_proj, up_proj):\n+                if proj is not None and hasattr(proj, \"weight\"):\n+                    std = in_std or (proj.weight.shape[1] ** -0.5)\n+                    nn.init.trunc_normal_(\n+                        proj.weight,\n+                        mean=0.0,\n+                        std=std,\n+                        a=-3 * std,\n+                        b=3 * std,\n+                    )\n+                    if getattr(proj, \"bias\", None) is not None:\n+                        nn.init.zeros_(proj.bias)\n+\n+            # output/ down projections\n+            if down_proj is not None and hasattr(down_proj, \"weight\"):\n+                hidden_dim = down_proj.weight.shape[1]\n+                out_std = hidden_dim**-0.5\n+                nn.init.trunc_normal_(\n+                    down_proj.weight,\n+                    mean=0.0,\n+                    std=out_std,\n+                    a=-3 * out_std,\n+                    b=3 * out_std,\n+                )\n+                if getattr(down_proj, \"bias\", None) is not None:\n+                    nn.init.zeros_(down_proj.bias)\n+            return\n+\n+        # Generic Linear layers (projections, lm_head, etc.)\n+        if isinstance(module, nn.Linear):\n+            fan_in = module.in_features\n+            std = fan_in**-0.5\n+            nn.init.trunc_normal_(\n+                module.weight,\n+                mean=0.0,\n+                std=std,\n+                a=-3 * std,\n+                b=3 * std,\n+            )\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+            return\n+\n+        return\n \n     def _update_causal_mask(self, module):\n         raise AttributeError(\"No need to inherit it!\")"
        },
        {
            "sha": "b9c96df3f5371c077cd0569dffaedb5c23ea0067",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f97c688d53e38967d53189f79e7b9d2ebf02282/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f97c688d53e38967d53189f79e7b9d2ebf02282/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=0f97c688d53e38967d53189f79e7b9d2ebf02282",
            "patch": "@@ -177,10 +177,6 @@ class BltModelTest(CausalLMModelTest, unittest.TestCase):\n     # used in `test_torch_compile_for_training`\n     _torch_compile_train_cls = BltForCausalLM if is_torch_available() else None\n \n-    @unittest.skip(\"BLT model requires special handling for training overfit test\")\n-    def test_training_overfit(self):\n-        pass\n-\n     @pytest.mark.generate\n     @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n     @unittest.skip("
        },
        {
            "sha": "09248acf9f8e0a7bc3a2fcc4ff7230ee34d70d35",
            "filename": "tests/test_training_mixin.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f97c688d53e38967d53189f79e7b9d2ebf02282/tests%2Ftest_training_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f97c688d53e38967d53189f79e7b9d2ebf02282/tests%2Ftest_training_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_training_mixin.py?ref=0f97c688d53e38967d53189f79e7b9d2ebf02282",
            "patch": "@@ -347,13 +347,19 @@ def test_training_overfit(self):\n \n             logger.info(f\"Prompt: {self._decode_text_tokens([expected_tokens[0]])}\")\n \n+            model_type = getattr(config, \"model_type\", \"\")\n+            use_cache = model_type == \"recurrent_gemma\"\n+            if use_cache:\n+                logger.info(\"Only RecurrentGemmaModel is using use_cache=True. Other models run with use_cache=False\")\n+\n             with torch.no_grad():\n                 generated_ids = model.generate(\n                     prompt_ids,\n                     max_new_tokens=num_tokens_to_generate,\n                     do_sample=False,\n                     pad_token_id=config.pad_token_id if hasattr(config, \"pad_token_id\") else 0,\n                     eos_token_id=0,\n+                    use_cache=use_cache,\n                 )\n \n             generated_tokens = generated_ids[0].tolist()"
        }
    ],
    "stats": {
        "total": 315,
        "additions": 310,
        "deletions": 5
    }
}