{
    "author": "cyyever",
    "message": "More markdown file fixes (#41599)\n\n* Format markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Format markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Format markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "bb0c3af995385fd070083610c98130fe9341c87a",
    "files": [
        {
            "sha": "00b23c332f4b726bf47a3c82fb090f0a69f0b098",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -36,8 +36,6 @@ Explore the [Hub](https://huggingface.com/) today to find a model and use Transf\n \n Explore the [Models Timeline](./models_timeline) to discover the latest text, vision, audio and multimodal model architectures in Transformers.\n \n-\n-\n ## Features\n \n Transformers provides everything you need for inference or training with state-of-the-art pretrained models. Some of the main features include:"
        },
        {
            "sha": "2a8e918c5185de4c7373689676205180836c4ace",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -364,6 +364,7 @@ This utility analyzes code similarities between model implementations to identif\n When adding a new model to transformers, many components (attention layers, MLPs, outputs, etc.) may already exist in similar form in other models. Instead of implementing everything from scratch, model adders can identify which existing classes are similar and potentially reusable through modularization.\n \n The tool computes two similarity scores:\n+\n - **Embedding score**: Uses semantic code embeddings (via `Qwen/Qwen3-Embedding-4B`) to detect functionally similar code even with different naming\n - **Jaccard score**: Measures token set overlap to identify structurally similar code patterns\n "
        },
        {
            "sha": "f9d684e8628975502b1a991a853eddcf76afbbe1",
            "filename": "docs/source/en/model_doc/altclip.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -100,22 +100,29 @@ for label, prob in zip(labels, probs[0]):\n - [`AltCLIPProcessor`] combines [`CLIPImageProcessor`] and [`XLMRobertaTokenizer`] into a single instance to encode text and prepare images.\n \n ## AltCLIPConfig\n+\n [[autodoc]] AltCLIPConfig\n \n ## AltCLIPTextConfig\n+\n [[autodoc]] AltCLIPTextConfig\n \n ## AltCLIPVisionConfig\n+\n [[autodoc]] AltCLIPVisionConfig\n \n ## AltCLIPModel\n+\n [[autodoc]] AltCLIPModel\n \n ## AltCLIPTextModel\n+\n [[autodoc]] AltCLIPTextModel\n \n ## AltCLIPVisionModel\n+\n [[autodoc]] AltCLIPVisionModel\n \n ## AltCLIPProcessor\n+\n [[autodoc]] AltCLIPProcessor"
        },
        {
            "sha": "d9a83388991dd0916eddf6fc46932084c694b220",
            "filename": "docs/source/en/model_doc/cwm.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcwm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fcwm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcwm.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -12,12 +12,10 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n \n-\n # Code World Model (CWM)\n \n ## Overview\n@@ -53,7 +51,8 @@ CWM requires a dedicated system prompt to function optimally during inference. W\n configuration, CWM's output quality may be significantly degraded. The following serves as the default\n system prompt for reasoning tasks. For agentic workflows, append the relevant tool specifications\n after this base prompt. Checkout the original code repository for more details.\n-```\n+\n+```text\n You are a helpful AI assistant. You always reason before responding, using the following format:\n \n <think>\n@@ -110,6 +109,7 @@ generated_ids = model.generate(\n output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n print(tokenizer.decode(output_ids))\n ```\n+\n <details>\n <summary>Produces the following output:</summary>\n "
        },
        {
            "sha": "cfb7793065774c28b4ccfb35fdb8fe29b8c4fd3c",
            "filename": "docs/source/en/model_doc/detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdetr.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -105,7 +105,7 @@ DETR can be naturally extended to perform panoptic segmentation (which unifies s\n - The decoder of DETR updates the query embeddings in parallel. This is different from language models like GPT-2, which use autoregressive decoding instead of parallel. Hence, no causal attention mask is used.\n - DETR adds position embeddings to the hidden states at each self-attention and cross-attention layer before projecting to queries and keys. For the position embeddings of the image, one can choose between fixed sinusoidal or learned absolute position embeddings. By default, the parameter `position_embedding_type` of [`~transformers.DetrConfig`] is set to `\"sine\"`.\n - During training, the authors of DETR did find it helpful to use auxiliary losses in the decoder, especially to help the model output the correct number of objects of each class. If you set the parameter `auxiliary_loss` of [`~transformers.DetrConfig`] to `True`, then prediction feedforward neural networks and Hungarian losses are added after each decoder layer (with the FFNs sharing parameters).\n-- If you want to train the model in a distributed environment across multiple nodes, then one should update the _num_boxes_ variable in the _DetrLoss_ class of _modeling_detr.py_. When training on multiple nodes, this should be set to the average number of target boxes across all nodes, as can be seen in the original implementation [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232).\n+- If you want to train the model in a distributed environment across multiple nodes, then one should update the *num_boxes* variable in the *DetrLoss* class of *modeling_detr.py*. When training on multiple nodes, this should be set to the average number of target boxes across all nodes, as can be seen in the original implementation [here](https://github.com/facebookresearch/detr/blob/a54b77800eb8e64e3ad0d8237789fcbf2f8350c5/models/detr.py#L227-L232).\n - [`~transformers.DetrForObjectDetection`] and [`~transformers.DetrForSegmentation`] can be initialized with any convolutional backbone available in the [timm library](https://github.com/rwightman/pytorch-image-models). Initializing with a MobileNet backbone for example can be done by setting the `backbone` attribute of [`~transformers.DetrConfig`] to `\"tf_mobilenetv3_small_075\"`, and then initializing the model with that config.\n - DETR resizes the input images such that the shortest side is at least a certain amount of pixels while the longest is at most 1333 pixels. At training time, scale augmentation is used such that the shortest side is randomly set to at least 480 and at most 800 pixels. At inference time, the shortest side is set to 800. One can use [`~transformers.DetrImageProcessor`] to prepare images (and optional annotations in COCO format) for the model. Due to this resizing, images in a batch can have different sizes. DETR solves this by padding images up to the largest size in a batch, and by creating a pixel mask that indicates which pixels are real/which are padding. Alternatively, one can also define a custom `collate_fn` in order to batch images together, using [`~transformers.DetrImageProcessor.pad_and_create_pixel_mask`].\n - The size of the images will determine the amount of memory being used, and will thus determine the `batch_size`. It is advised to use a batch size of 2 per GPU. See [this Github thread](https://github.com/facebookresearch/detr/issues/150) for more info."
        },
        {
            "sha": "ed60c569eb97c02452a300ee39ff71a288299dc7",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -120,7 +120,7 @@ print(answer)\n     ```py\n     >>> import re\n     >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n-from accelerate import Accelerator\n+    >>> from accelerate import Accelerator\n     >>> from datasets import load_dataset\n     >>> import torch\n \n@@ -162,9 +162,9 @@ from accelerate import Accelerator\n \n     ```py\n     >>> import re\n-    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n-from accelerate import Accelerator\n+    >>> from accelerate import Accelerator\n     >>> from datasets import load_dataset\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n     >>> import torch\n \n     >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")"
        },
        {
            "sha": "7e52f74da9820ad188c3da26c25e6b55aa0e3508",
            "filename": "docs/source/en/model_doc/edgetam.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -305,7 +305,6 @@ EdgeTAM can use masks from previous predictions as input to refine segmentation:\n ...     )\n ```\n \n-\n ## EdgeTamConfig\n \n [[autodoc]] EdgeTamConfig"
        },
        {
            "sha": "733591d6565b462ca62022e9e0dd8096415f6c82",
            "filename": "docs/source/en/model_doc/edgetam_video.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam_video.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -12,13 +12,11 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n *This model was released on 2025-01-13 and added to Hugging Face Transformers on 2025-09-29.*\n \n-\n <div style=\"float: right;\">\n     <div class=\"flex flex-wrap space-x-1\">\n         <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">"
        },
        {
            "sha": "09a147e1f45ae5fe5686a0a6f933b83d8cc13054",
            "filename": "docs/source/en/model_doc/glm4_moe.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4_moe.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -37,7 +37,6 @@ We evaluated GLM-4.6 across eight public benchmarks covering agents, reasoning,\n \n For more eval results, show cases, and technical details, please visit our [technical blog](https://z.ai/blog/glm-4.6).\n \n-\n ### GLM-4.5\n \n The [**GLM-4.5**](https://huggingface.co/papers/2508.06471) series models are foundation models designed for intelligent agents, MoE variants are documented here as Glm4Moe."
        },
        {
            "sha": "02dd4cc53ba78f9c5b718541504cd3c3e32ef20d",
            "filename": "docs/source/en/model_doc/gpt_oss.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_oss.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -35,8 +35,8 @@ The abstract from the paper is the following:\n *<INSERT PAPER ABSTRACT HERE>*\n \n Tips:\n-- **Attention Sinks with Flex Attention**: When using flex attention, attention sinks require special handling. Unlike with standard attention implementations where sinks can be added directly to attention scores, flex attention `score_mod` function operates on individual score elements rather than the full attention matrix. Therefore, attention sinks renormalization have to be applied after the flex attention computations by renormalizing the outputs using the log-sum-exp (LSE) values returned by flex attention.\n \n+- **Attention Sinks with Flex Attention**: When using flex attention, attention sinks require special handling. Unlike with standard attention implementations where sinks can be added directly to attention scores, flex attention `score_mod` function operates on individual score elements rather than the full attention matrix. Therefore, attention sinks renormalization have to be applied after the flex attention computations by renormalizing the outputs using the log-sum-exp (LSE) values returned by flex attention.\n \n <INSERT TIPS ABOUT MODEL HERE>\n "
        },
        {
            "sha": "8ff6d268991803831bcb895b966cddb1faa5c866",
            "filename": "docs/source/en/model_doc/lfm2_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_moe.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -12,19 +12,17 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n \n-\n ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n \n-\n # Lfm2Moe\n \n ## Overview\n \n LFM2-MoE is a Mixture-of-Experts (MoE) variant of [LFM2](https://huggingface.co/collections/LiquidAI/lfm2-686d721927015b2ad73eaa38). The LFM2 family is optimized for on-device inference by combining short‑range, input‑aware gated convolutions with grouped‑query attention (GQA) in a layout tuned to maximize quality under strict speed and memory constraints.\n \n-LFM2‑MoE keeps this fast backbone and introduces sparse MoE feed‑forward networks to add representational capacity without significantly increasing the active compute path. The first LFM2-MoE release is LFM2-8B-A1B, with 8.3B total parameters and 1.5B active parameters. The model excels in quality (comparable to 3-4B dense models) and speed (faster than other 1.5B class models). \n+LFM2‑MoE keeps this fast backbone and introduces sparse MoE feed‑forward networks to add representational capacity without significantly increasing the active compute path. The first LFM2-MoE release is LFM2-8B-A1B, with 8.3B total parameters and 1.5B active parameters. The model excels in quality (comparable to 3-4B dense models) and speed (faster than other 1.5B class models).\n \n ## Example\n "
        },
        {
            "sha": "d93f801a1f8324a9f0d693ad61270b8b0562621a",
            "filename": "docs/source/en/model_doc/mobilevit.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmobilevit.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -99,7 +99,6 @@ print(f\"The predicted class label is:{predicted_class_label}\")\n \n [[autodoc]] MobileViTConfig\n \n-\n ## MobileViTImageProcessor\n \n [[autodoc]] MobileViTImageProcessor"
        },
        {
            "sha": "baa4c7b87b50ce34ef41e8a646fb072d9645d2bc",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -271,6 +271,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", min_pixels=min\n ```\n \n #### Prompt for audio output\n+\n If users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n \n ```python\n@@ -307,6 +308,7 @@ text_ids = model.generate(**inputs, return_audio=False)\n ```\n \n #### Change voice type of output audio\n+\n Qwen2.5-Omni supports the ability to change the voice of the output audio. Users can use the `spk` parameter of `generate` function to specify the voice type. The `\"Qwen/Qwen2.5-Omni-7B\"` checkpoint support two voice types: `Chelsie` and `Ethan`, while `Chelsie` is a female voice and `Ethan` is a male voice. By default, if `spk` is not specified, the default voice type is `Chelsie`.\n \n ```python"
        },
        {
            "sha": "2faeaeccff0213fed6d5fe8cdf75cc875426a89e",
            "filename": "docs/source/en/model_doc/qwen3_next.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_next.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -31,6 +31,7 @@ Despite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks — w\n Moreover, it delivers over **10x higher inference throughput** than Qwen3-32B when handling contexts longer than 32K tokens.\n \n For more details, please visit our blog [Qwen3-Next](qwen3_next) ([blog post](https://qwenlm.github.io/blog/qwen3_next/)).\n+\n ## Usage examples\n \n ```python"
        },
        {
            "sha": "efbea2613cf798f1666918f4ab1b8ef55abdc188",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -271,6 +271,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-Omni-30B-A3B-Instruct\", mi\n ```\n \n #### Prompt for audio output\n+\n If users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n \n ```json\n@@ -307,6 +308,7 @@ text_ids = model.generate(**inputs, return_audio=False)\n ```\n \n #### Change voice type of output audio\n+\n Qwen3-Omni-MOE supports the ability to change the voice of the output audio. Users can use the `spk` parameter of `generate` function to specify the voice type. The `\"Qwen/Qwen3-Omni-30B-A3B-Instruct\"` checkpoint support two voice types: `Chelsie` and `Ethan`, while `Chelsie` is a female voice and `Ethan` is a male voice. By default, if `spk` is not specified, the default voice type is `Chelsie`.\n \n ```python"
        },
        {
            "sha": "eefd5b489eac8dcb2840956965fe3d0d3a8c56cd",
            "filename": "docs/source/en/model_doc/roberta-prelayernorm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Froberta-prelayernorm.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -35,7 +35,7 @@ The original code can be found [here](https://github.com/princeton-nlp/DinkyTrai\n \n ## Usage tips\n \n-- The implementation is the same as [Roberta](roberta) except instead of using _Add and Norm_ it does _Norm and Add_. _Add_ and _Norm_ refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://huggingface.co/papers/1706.03762).\n+- The implementation is the same as [Roberta](roberta) except instead of using *Add and Norm* it does *Norm and Add*. *Add* and *Norm* refers to the Addition and LayerNormalization as described in [Attention Is All You Need](https://huggingface.co/papers/1706.03762).\n - This is identical to using the `--encoder-normalize-before` flag in [fairseq](https://fairseq.readthedocs.io/).\n \n ## Resources"
        },
        {
            "sha": "e59836e7387218974dba04a95f85abee7543b8d9",
            "filename": "docs/source/en/model_doc/rt_detr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -40,7 +40,8 @@ The model version was contributed by [rafaelpadilla](https://huggingface.co/rafa\n \n ## Usage tips\n \n-Initially, an image is processed using a pre-trained convolutional neural network, specifically a Resnet-D variant as referenced in the original code. This network extracts features from the final three layers of the architecture. Following this, a hybrid encoder is employed to convert the multi-scale features into a sequential array of image features. Then, a decoder, equipped with auxiliary prediction heads is used to refine the object queries. This process facilitates the direct generation of bounding boxes, eliminating the need for any additional post-processing to acquire the logits and coordinates for the bounding boxes. The model is meant to be used on images resized to a size 640x640 with the corresponding ImageProcessor. Reshaping to other sizes will generally degrade performance. \n+Initially, an image is processed using a pre-trained convolutional neural network, specifically a Resnet-D variant as referenced in the original code. This network extracts features from the final three layers of the architecture. Following this, a hybrid encoder is employed to convert the multi-scale features into a sequential array of image features. Then, a decoder, equipped with auxiliary prediction heads is used to refine the object queries. This process facilitates the direct generation of bounding boxes, eliminating the need for any additional post-processing to acquire the logits and coordinates for the bounding boxes. The model is meant to be used on images resized to a size 640x640 with the corresponding ImageProcessor. Reshaping to other sizes will generally degrade performance.\n+\n ```py\n >>> import torch\n >>> import requests"
        },
        {
            "sha": "cd08597d4b2bf2371e63d3bead3217ccf3cadfd2",
            "filename": "docs/source/en/model_doc/rt_detr_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -43,6 +43,7 @@ This second version of RT-DETR improves how the decoder finds objects in an imag\n - **optimized processing** – improves how attention weights mix information\n \n The model is meant to be used on images resized to a size 640x640 with the corresponding ImageProcessor. Reshaping to other sizes will generally degrade performance.\n+\n ```py\n >>> import torch\n >>> import requests"
        },
        {
            "sha": "a7d280e9531f58c2a4c6b28d6d93704ee3ad715f",
            "filename": "docs/source/en/model_doc/unispeech-sat.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech-sat.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -55,7 +55,6 @@ found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech-SAT).\n   decoded using [`Wav2Vec2CTCTokenizer`].\n - UniSpeechSat performs especially well on speaker verification, speaker identification, and speaker diarization tasks.\n \n-\n ## Resources\n \n - [Audio classification task guide](../tasks/audio_classification)"
        },
        {
            "sha": "ef5762cec30cece864594c88c97321eb773cf522",
            "filename": "docs/source/en/model_doc/unispeech.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Funispeech.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -50,7 +50,6 @@ found [here](https://github.com/microsoft/UniSpeech/tree/main/UniSpeech).\n - UniSpeech model can be fine-tuned using connectionist temporal classification (CTC) so the model output has to be\n   decoded using [`Wav2Vec2CTCTokenizer`].\n \n-\n ## Resources\n \n - [Audio classification task guide](../tasks/audio_classification)"
        },
        {
            "sha": "5fb3cbbd25a44d794f746c06819b5ef9644508b6",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -48,7 +48,7 @@ a unified visual representation, outperforming models designed specifically for\n work to provide modest insights into the multi-modal inputs\n for the LLM*\n \n-## Usage tips:\n+## Usage tips\n \n - We advise users to use padding_side=\"left\" when computing batched generation as it leads to more accurate results. Simply make sure to call processor.tokenizer.padding_side = \"left\" before generating.\n "
        },
        {
            "sha": "336d0c100a527f5c6d41774e7d746a78206cf659",
            "filename": "docs/source/en/model_doc/vipllava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -37,7 +37,7 @@ The original code can be found [here](https://github.com/mu-cai/ViP-LLaVA).\n \n This model was contributed by [Younes Belkada](https://huggingface.co/ybelkada)\n \n-## Usage tips:\n+## Usage tips\n \n - The architecture is similar than llava architecture except that the multi-modal projector takes a set of concatenated vision hidden states and has an additional layernorm layer on that module.\n "
        },
        {
            "sha": "7984901461ff82a973476fb0fc794fe2f75f26a8",
            "filename": "docs/source/en/model_doc/vision-encoder-decoder.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-encoder-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-encoder-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-encoder-decoder.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -39,7 +39,7 @@ for more information).\n An example application is image captioning, in which the encoder is used to encode the image, after which an autoregressive language model generates\n the caption. Another example is optical character recognition. Refer to [TrOCR](trocr), which is an instance of [`VisionEncoderDecoderModel`].\n \n-## Randomly initializing `VisionEncoderDecoderModel` from model configurations.\n+## Randomly initializing `VisionEncoderDecoderModel` from model configurations\n \n [`VisionEncoderDecoderModel`] can be randomly initialized from an encoder and a decoder config. In the following example, we show how to do this using the default [`ViTModel`] configuration for the encoder\n and the default [`BertForCausalLM`] configuration for the decoder.\n@@ -54,7 +54,7 @@ and the default [`BertForCausalLM`] configuration for the decoder.\n >>> model = VisionEncoderDecoderModel(config=config)\n ```\n \n-## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder.\n+## Initialising `VisionEncoderDecoderModel` from a pretrained encoder and a pretrained decoder\n \n [`VisionEncoderDecoderModel`] can be initialized from a pretrained encoder checkpoint and a pretrained decoder checkpoint. Note that any pretrained Transformer-based vision model, *e.g.* [Swin](swin), can serve as the encoder and both pretrained auto-encoding models, *e.g.* BERT, pretrained causal language models, *e.g.* GPT2, as well as the pretrained decoder part of sequence-to-sequence models, *e.g.* decoder of BART, can be used as the decoder.\n Depending on which architecture you choose as the decoder, the cross-attention layers might be randomly initialized.\n@@ -69,7 +69,7 @@ To do so, the `VisionEncoderDecoderModel` class provides a [`VisionEncoderDecode\n ... )\n ```\n \n-## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference.\n+## Loading an existing `VisionEncoderDecoderModel` checkpoint and perform inference\n \n To load fine-tuned checkpoints of the `VisionEncoderDecoderModel` class, [`VisionEncoderDecoderModel`] provides the `from_pretrained(...)` method just like any other model architecture in Transformers.\n "
        },
        {
            "sha": "6f94ba09fdfeecd88e3ab8a0be6576571aa8138e",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -164,7 +164,7 @@ image_pose_result = pose_results[0]\n \n     ```py\n     from transformers import AutoProcessor, VitPoseForPoseEstimation\n-from accelerate import Accelerator\n+    from accelerate import Accelerator\n \n     device = Accelerator().device\n "
        },
        {
            "sha": "08482064eee5139c6c207fcec4fb665a542115e5",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -52,11 +52,13 @@ For the best speedups, we recommend loading the model in half-precision (e.g. `t\n On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vivit-b-16x2-kinetics400` model, we saw the following speedups during inference.\n \n ### Training\n+\n |   num_training_steps |   batch_size |   is cuda |   Speedup (%) |   Eager peak mem (MB) |   sdpa peak mem (MB) |   Mem saving (%) |\n |---------------------:|-------------:|----------:|--------------:|----------------------:|---------------------:|-----------------:|\n |                  100 |            1 |      True |         7.122 |               2575.28 |              5932.54 |           130.364 |\n \n ### Inference\n+\n |   num_batches |   batch_size |   is cuda |   is half |   Speedup (%) |   Mem eager (MB) |   Mem BT (MB) |   Mem saved (%) |\n |---------------|--------------|-----------|-----------|---------------|------------------|---------------|-----------------|\n |            20 |             1 |   True    |   False   |      15.422   |     715.807      |    317.079    |      125.75     |"
        },
        {
            "sha": "b711bbc9dcae68cf1cf3bfe6eb83e20967af805e",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -48,7 +48,6 @@ Note: Meta (FAIR) released a new version of [Wav2Vec2-BERT 2.0](https://huggingf\n - Wav2Vec2 model was trained using connectionist temporal classification (CTC) so the model output has to be decoded\n   using [`Wav2Vec2CTCTokenizer`].\n \n-\n ## Using Flash Attention 2\n \n Flash Attention 2 is an faster, optimized version of the model."
        },
        {
            "sha": "0b63660ddaa67a8a7a15c025db8ebe0828a006b7",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -29,7 +29,6 @@ rendered properly in your Markdown viewer.\n \n You can find all the original Whisper checkpoints under the [Whisper](https://huggingface.co/collections/openai/whisper-release-6501bba2cf999715fd953013) collection.\n \n-\n > [!TIP]\n > Click on the Whisper models in the right sidebar for more examples of how to apply Whisper to different audio tasks.\n "
        },
        {
            "sha": "b62e12b25683e1bbac506e0bd3ea4b9754b326e1",
            "filename": "docs/source/en/model_doc/xmod.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxmod.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n ## Overview\n \n The X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](https://huggingface.co/papers/2205.06266) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.\n-X-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (_language adapters_) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.\n+X-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (*language adapters*) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.\n \n The abstract from the paper is the following:\n "
        },
        {
            "sha": "8bcb018311ff40da207e9cf8799788c122103114",
            "filename": "docs/source/en/open_webui.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fopen_webui.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fopen_webui.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fopen_webui.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -1,4 +1,4 @@\n-#  Audio transcriptions with WebUI and `transformers serve`\n+# Audio transcriptions with WebUI and `transformers serve`\n \n This guide shows how to do audio transcription for chat purposes, using `transformers serve` and [Open WebUI](https://openwebui.com/). This guide assumes you have Open WebUI installed on your machine and ready to run. Please refer to the examples above to use the text functionalities of `transformer serve` with Open WebUI -- the instructions are the same.\n "
        },
        {
            "sha": "e0a7d082156dd40397b7596b51e05f9f3317d426",
            "filename": "docs/source/en/philosophy.md",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fphilosophy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fphilosophy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fphilosophy.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -41,19 +41,18 @@ A longer, in-depth article with examples, visualizations and timelines is availa\n - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly\n     using a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model.\n \n-\n ## Core tenets\n \n The following tenets solidified over time, and they're detailed in our new philosophy [blog post](https://huggingface.co/spaces/transformers-community/Transformers-tenets). They guide maintainer decisions when reviewing PRs and contributions.\n \n > - **Source of Truth.** Implementations must be faithful to official results and intended behavior.\n->- **One Model, One File.** Core inference/training logic is visible top-to-bottom in the model file users read.\n->- **Code is the Product.** Optimize for reading and diff-ing. Prefer explicit names over clever indirection.\n->- **Standardize, Don’t Abstract.** Keep model-specific behavior in the model. Use shared interfaces only for generic infra.\n->- **DRY\\*** (Repeat when it helps users). End-user modeling files remain self-contained. Infra is factored out.\n->- **Minimal User API.** Few codepaths, predictable kwargs, stable methods.\n->- **Backwards Compatibility.** Public surfaces should not break. Old Hub artifacts have to keep working..\n->- **Consistent Public Surface.** Naming, outputs, and optional diagnostics are aligned and tested.\n+> - **One Model, One File.** Core inference/training logic is visible top-to-bottom in the model file users read.\n+> - **Code is the Product.** Optimize for reading and diff-ing. Prefer explicit names over clever indirection.\n+> - **Standardize, Don’t Abstract.** Keep model-specific behavior in the model. Use shared interfaces only for generic infra.\n+> - **DRY\\*** (Repeat when it helps users). End-user modeling files remain self-contained. Infra is factored out.\n+> - **Minimal User API.** Few codepaths, predictable kwargs, stable methods.\n+> - **Backwards Compatibility.** Public surfaces should not break. Old Hub artifacts have to keep working..\n+> - **Consistent Public Surface.** Naming, outputs, and optional diagnostics are aligned and tested.\n \n ## Main classes\n \n@@ -64,7 +63,6 @@ The following tenets solidified over time, and they're detailed in our new philo\n \n - **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provides methods for encoding and decoding strings in a list of token embedding indices. [Image processors](main_classes/image_processor) preprocess vision inputs, [video processors](https://huggingface.co/docs/transformers/en/main_classes/video_processor) preprocess videos inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and [processors](main_classes/processors) preprocess multimodal inputs.\n \n-\n All these classes can be instantiated from pretrained instances, saved locally, and shared on the Hub with three methods:\n \n - `from_pretrained()` lets you instantiate a model, configuration, and preprocessing class from a pretrained version either"
        },
        {
            "sha": "8b7479ff71b0989e24dae8ab8bf3d5b3f3cdb2b7",
            "filename": "docs/source/en/quantization/auto_round.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -66,6 +66,7 @@ For 2 bits, we recommend using `auto-round-best` or `auto-round`.\n <hfoption id=\"quantization auto-round api\">\n \n ### AutoRound API Usage\n+\n This setting offers a better trade-off between accuracy and tuning cost, and is recommended in all scenarios.\n \n ```python\n@@ -98,6 +99,7 @@ autoround.quantize_and_save(output_dir, format='auto_round')\n <hfoption id=\"quantization auto-round-best\">\n \n ### AutoRoundBest recipe\n+\n This setting provides the best accuracy in most scenarios but is 4–5× slower than the standard AutoRound recipe. It is especially recommended for 2-bit quantization and is a good choice if sufficient resources are available.\n \n ```python\n@@ -128,6 +130,7 @@ autoround.quantize_and_save(output_dir, format='auto_round')\n <hfoption id=\"quantization auto-round-light\">\n \n ### AutoRoundLight recipe\n+\n This setting offers the best speed (2 - 3X faster than AutoRound), but it may cause a significant accuracy drop for small models and 2-bit quantization. It is recommended for 4-bit settings and models larger than 3B.\n \n ```python\n@@ -279,8 +282,10 @@ If you encounter any issues with auto-round, please open an issue on\n the [AutoRound](https://github.com/intel/auto-round/issues) repository.\n \n ## Acknowledgement\n+\n Special thanks to open-source low precision libraries such as AutoGPTQ, AutoAWQ, GPTQModel, Triton, Marlin, and ExLLaMAV2 for providing low-precision CUDA kernels, which are leveraged in AutoRound.\n \n ## Contribution\n+\n Contributions to [AutoRound](https://github.com/intel/auto-round/pulls) are welcome and greatly appreciated!\n Whether it's fixing bugs, improving documentation, adding new features, or suggesting improvements, your help is always valued."
        },
        {
            "sha": "bc931d30fe1a770393dc110f0e70d5c8fbd2debf",
            "filename": "docs/source/en/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -41,6 +41,7 @@ pip install --upgrade transformers accelerate bitsandbytes\n To compile from source, follow the instructions in the [bitsandbytes installation guide](https://huggingface.co/docs/bitsandbytes/main/en/installation).\n \n ## Hardware Compatibility\n+\n bitsandbytes is supported on NVIDIA GPUs for CUDA versions 11.8 - 13.0, Intel XPU, Intel Gaudi (HPU), and CPU. There is an ongoing effort to support additional platforms. If you're interested in providing feedback or testing, check out the [bitsandbytes repository](https://github.com/bitsandbytes-foundation/bitsandbytes) for more information.\n \n ### NVIDIA GPUs (CUDA)"
        },
        {
            "sha": "0727e97f3b5b41af1c1aefc7039209b741343da4",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -87,6 +87,7 @@ Create a [`TorchAoConfig`] and specify the quantization type and `group_size` of\n We'll show examples for recommended quantization methods based on hardwares, e.g. A100 GPU, H100 GPU, CPU.\n \n ### H100 GPU\n+\n <hfoptions id=\"examples-H100-GPU\">\n <hfoption id=\"float8-dynamic-and-weight-only\">\n \n@@ -182,6 +183,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoptions>\n \n ### A100 GPU\n+\n <hfoptions id=\"examples-A100-GPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n \n@@ -284,6 +286,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoptions>\n \n ### Intel XPU\n+\n <hfoptions id=\"examples-Intel-XPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n \n@@ -350,6 +353,7 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoptions>\n \n ### CPU\n+\n <hfoptions id=\"examples-CPU\">\n <hfoption id=\"int8-dynamic-and-weight-only\">\n \n@@ -415,7 +419,9 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n </hfoptions>\n \n ### Per Module Quantization\n+\n #### 1. Skip quantization for certain layers\n+\n With `ModuleFqnToConfig` we can specify a default configuration for all layers while skipping quantization for certain layers.\n \n ```py\n@@ -485,6 +491,7 @@ print(output_text)\n ```\n \n #### 3. Quantizing different layers with different quantization configs (with regex)\n+\n We can also use regex to specify the config for all modules that has `module_fqn` that\n matches the regex, all regex should start with `re:`, for example `re:layers\\..*\\.gate_proj` will\n match all layers like `layers.0.gate_proj`. See [here](https://github.com/pytorch/ao/blob/2fe0ca0899c730c528efdbec8886feaa38879f39/torchao/quantization/quant_api.py#L2392) for docs."
        },
        {
            "sha": "4d35c92bfe56d79aa1b0019f5d16f9b42db5447e",
            "filename": "docs/source/en/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Frun_scripts.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -61,7 +61,7 @@ The example below fine-tunes [T5-small](https://huggingface.co/google-t5/t5-smal\n \n The example script downloads and preprocesses a dataset, and then fine-tunes it with [`Trainer`] with a supported model architecture.\n \n-Resuming training from a checkpoint is very useful if training is interrupted because you don't have to start over again: \n+Resuming training from a checkpoint is very useful if training is interrupted because you don't have to start over again:\n \n * `--resume_from_checkpoint path_to_specific_checkpoint` resumes training from a specific checkpoint folder.\n "
        },
        {
            "sha": "830386d2d3ed184a3148d27d60b5066828915450",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -24,6 +24,7 @@ Transformer models can be efficiently deployed using libraries such as vLLM, Tex\n You can also serve transformer models with the `transformers serve` CLI. With Continuous Batching, `serve` now delivers solid throughput and latency well suited for evaluation, experimentation, and moderate-load local or self-hosted deployments. While vLLM, SGLang, or other inference engines remain our recommendations for large-scale production, `serve` avoids the extra runtime and operational overhead, and is on track to gain more production-oriented features.\n \n In this document, we dive into the different supported endpoints and modalities; we also cover the setup of several user interfaces that can be used on top of `transformers serve` in the following guides:\n+\n - [Jan (text and MCP user interface)](./jan)\n - [Cursor (IDE)](./cursor)\n - [Open WebUI (text, image, speech user interface)](./open_webui)"
        },
        {
            "sha": "4c4de3cf0cea79537f43ebbd9343f7041508545f",
            "filename": "docs/source/en/tasks/idefics.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -110,6 +110,7 @@ on the fly while loading.\n Now that you have the model loaded in one of the suggested ways, let's move on to exploring tasks that you can use IDEFICS for.\n \n ## Image captioning\n+\n Image captioning is the task of predicting a caption for a given image. A common application is to aid visually impaired\n people navigate through different situations, for instance, explore image content online.\n "
        },
        {
            "sha": "e1c062c52e77de6230b9935cd2ed044979f3e3cd",
            "filename": "docs/source/en/tokenizer_summary.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb0c3af995385fd070083610c98130fe9341c87a/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftokenizer_summary.md?ref=bb0c3af995385fd070083610c98130fe9341c87a",
            "patch": "@@ -166,9 +166,9 @@ base vocabulary, we obtain:\n ```\n \n BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In\n-the example above `\"h\"` followed by `\"u\"` is present _10 + 5 = 15_ times (10 times in the 10 occurrences of\n+the example above `\"h\"` followed by `\"u\"` is present *10 + 5 = 15* times (10 times in the 10 occurrences of\n `\"hug\"`, 5 times in the 5 occurrences of `\"hugs\"`). However, the most frequent symbol pair is `\"u\"` followed by\n-`\"g\"`, occurring _10 + 5 + 5 = 20_ times in total. Thus, the first merge rule the tokenizer learns is to group all\n+`\"g\"`, occurring *10 + 5 + 5 = 20* times in total. Thus, the first merge rule the tokenizer learns is to group all\n `\"u\"` symbols followed by a `\"g\"` symbol together. Next, `\"ug\"` is added to the vocabulary. The set of words then\n becomes\n \n@@ -222,8 +222,8 @@ So what does this mean exactly? Referring to the previous example, maximizing th\n equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by\n its second symbol is the greatest among all symbol pairs. *E.g.* `\"u\"`, followed by `\"g\"` would have only been\n merged if the probability of `\"ug\"` divided by `\"u\"`, `\"g\"` would have been greater than for any other symbol\n-pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it _loses_ by merging two symbols\n-to ensure it's _worth it_.\n+pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it *loses* by merging two symbols\n+to ensure it's *worth it*.\n \n <a id='unigram'></a>\n "
        }
    ],
    "stats": {
        "total": 109,
        "additions": 63,
        "deletions": 46
    }
}