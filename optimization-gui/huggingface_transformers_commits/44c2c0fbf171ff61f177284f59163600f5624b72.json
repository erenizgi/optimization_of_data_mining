{
    "author": "Sai-Suraj-27",
    "message": "Fix failing `salesforce-ctrl`, `xlm` & `gpt-neo` model generation tests (#43180)\n\n* Fix small nit in salesforce-ctrl model test\n\n* Remove breakpoint\n\n* Fix failing XLMModelLanguageGenerationTest\n\n* Fix gpt_neo model generation tests.\n\n* Comment",
    "sha": "44c2c0fbf171ff61f177284f59163600f5624b72",
    "files": [
        {
            "sha": "b5e57329a3cf2c49e8fb5e61efebb2618a3f38f0",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/44c2c0fbf171ff61f177284f59163600f5624b72/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44c2c0fbf171ff61f177284f59163600f5624b72/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=44c2c0fbf171ff61f177284f59163600f5624b72",
            "patch": "@@ -274,5 +274,6 @@ def test_lm_generate_ctrl(self):\n             5,\n         ]  # Legal the president is a good guy and I don't want to lose my job. \\n \\n I have a\n \n-        output_ids = model.generate(input_ids, do_sample=False)\n+        # We limit the generation output to (max_length - input_length) while by default 20 new tokens will be generated.\n+        output_ids = model.generate(input_ids, do_sample=False, max_length=20)\n         self.assertListEqual(output_ids[0].tolist(), expected_output_ids)"
        },
        {
            "sha": "3a45226c5d3ba50574bc486091ea6d8d01fef9ca",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/44c2c0fbf171ff61f177284f59163600f5624b72/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44c2c0fbf171ff61f177284f59163600f5624b72/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=44c2c0fbf171ff61f177284f59163600f5624b72",
            "patch": "@@ -491,7 +491,8 @@ def test_lm_generate_gpt_neo(self):\n             input_ids = torch.tensor([[464, 3290]], dtype=torch.long, device=torch_device)  # The dog\n             # The dog-eared copy of the book, which is a collection of essays by the late author,\n             expected_output_ids = [464, 3290, 12, 3380, 4866, 286, 262, 1492, 11, 543, 318, 257, 4947, 286, 27126, 416, 262, 2739, 1772, 11]  # fmt: skip\n-            output_ids = model.generate(input_ids, do_sample=False)\n+            # We limit the generation output to (max_length - input_length) while by default 20 new tokens will be generated.\n+            output_ids = model.generate(input_ids, do_sample=False, max_length=20)\n             self.assertListEqual(output_ids[0].tolist(), expected_output_ids)\n \n     @slow\n@@ -528,17 +529,21 @@ def test_batch_generation(self):\n         inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n         input_ids = inputs[\"input_ids\"].to(torch_device)\n \n+        # We limit the generation output to (max_length - input_length) while by default 20 new tokens will be generated.\n         outputs = model.generate(\n             input_ids=input_ids,\n             attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_length=20,\n         )\n \n         inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded)\n+        # We limit the generation output to (max_length - input_length) while by default 20 new tokens will be generated.\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+        # We limit the generation output to (max_length - input_length) while by default 20 new tokens will be generated.\n+        output_padded = model.generate(input_ids=inputs_padded, max_length=20 - num_paddings)\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)"
        },
        {
            "sha": "8fae497adcbc4d50281834caea798770852b7378",
            "filename": "tests/models/xlm/test_modeling_xlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/44c2c0fbf171ff61f177284f59163600f5624b72/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44c2c0fbf171ff61f177284f59163600f5624b72/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py?ref=44c2c0fbf171ff61f177284f59163600f5624b72",
            "patch": "@@ -525,5 +525,6 @@ def test_lm_generate_xlm_mlm_en_2048(self):\n             447,\n         ]  # the president the president the president the president the president the president the president the president the president the president\n         # TODO(PVP): this and other input_ids I tried for generation give pretty bad results. Not sure why. Model might just not be made for auto-regressive inference\n-        output_ids = model.generate(input_ids, do_sample=False)\n+        # We limit the generation output to (max_length - input_length) while by default 20 new tokens will be generated.\n+        output_ids = model.generate(input_ids, do_sample=False, max_length=20)\n         self.assertListEqual(output_ids[0].tolist(), expected_output_ids)"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 12,
        "deletions": 5
    }
}