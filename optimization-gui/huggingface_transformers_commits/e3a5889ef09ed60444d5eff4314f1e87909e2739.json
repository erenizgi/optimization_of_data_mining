{
    "author": "Cyrilvallez",
    "message": "Modular fix (#34802)\n\n* Modular fix\r\n\r\n* style\r\n\r\n* remove logger warning\r\n\r\n* Update modular_model_converter.py",
    "sha": "e3a5889ef09ed60444d5eff4314f1e87909e2739",
    "files": [
        {
            "sha": "7042c586cbb636a87907e84b26ed2d22c04cc33f",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=e3a5889ef09ed60444d5eff4314f1e87909e2739",
            "patch": "@@ -130,6 +130,16 @@ class MyNewModelConfig(PretrainedConfig):\n \n     model_type = \"my_new_model\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `MyNewModelModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "eddd7fe47973ef1847177cf7b9523333569d16ea",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=e3a5889ef09ed60444d5eff4314f1e87909e2739",
            "patch": "@@ -33,6 +33,16 @@ class MyNewModel2Config(PretrainedConfig):\n \n     model_type = \"my_new_model2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `MyNewModel2Model`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "0b373d4e6eab018b1e42652a73626b3d79c6c830",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 17,
            "deletions": 54,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=e3a5889ef09ed60444d5eff4314f1e87909e2739",
            "patch": "@@ -8,7 +8,6 @@\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -150,25 +149,7 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n-        if self.config.pretraining_tp > 1:\n-            slice = self.intermediate_size // self.config.pretraining_tp\n-            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n-            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n-            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n-\n-            gate_proj = torch.cat(\n-                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n-            )\n-            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n-\n-            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n-            down_proj = [\n-                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n-            ]\n-            down_proj = sum(down_proj)\n-        else:\n-            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n         return down_proj\n \n \n@@ -264,31 +245,14 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n-        if self.config.pretraining_tp > 1:\n-            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n-            query_slices = self.q_proj.weight.split(\n-                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n-            )\n-            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n-            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n-\n-            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n-            query_states = torch.cat(query_states, dim=-1)\n-\n-            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n-            key_states = torch.cat(key_states, dim=-1)\n-\n-            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n-            value_states = torch.cat(value_states, dim=-1)\n-\n-        else:\n-            query_states = self.q_proj(hidden_states)\n-            key_states = self.k_proj(hidden_states)\n-            value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -330,12 +294,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1)\n \n-        if self.config.pretraining_tp > 1:\n-            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n-            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n-            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n-        else:\n-            attn_output = self.o_proj(attn_output)\n+        attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n@@ -508,9 +467,10 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -794,7 +754,10 @@ def __init__(self, config: DummyConfig):\n         )\n         self.norm = DummyRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = DummyRotaryEmbedding(config=config)\n+\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -874,7 +837,7 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n "
        },
        {
            "sha": "189e090094c76c9c87cc55e04d22ad2f717de8b7",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=e3a5889ef09ed60444d5eff4314f1e87909e2739",
            "patch": "@@ -667,7 +667,10 @@ def __init__(self, config: MyNewModel2Config):\n             [MyNewModel2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -752,7 +755,7 @@ def forward(\n         all_self_attns = () if output_attentions else None\n         next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n "
        },
        {
            "sha": "7ad606280dcc96102d47fb4939e91b8239e87be8",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 16,
            "deletions": 53,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3a5889ef09ed60444d5eff4314f1e87909e2739/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=e3a5889ef09ed60444d5eff4314f1e87909e2739",
            "patch": "@@ -8,7 +8,6 @@\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -150,25 +149,7 @@ def __init__(self, config):\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n-        if self.config.pretraining_tp > 1:\n-            slice = self.intermediate_size // self.config.pretraining_tp\n-            gate_proj_slices = self.gate_proj.weight.split(slice, dim=0)\n-            up_proj_slices = self.up_proj.weight.split(slice, dim=0)\n-            down_proj_slices = self.down_proj.weight.split(slice, dim=1)\n-\n-            gate_proj = torch.cat(\n-                [F.linear(x, gate_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1\n-            )\n-            up_proj = torch.cat([F.linear(x, up_proj_slices[i]) for i in range(self.config.pretraining_tp)], dim=-1)\n-\n-            intermediate_states = (self.act_fn(gate_proj) * up_proj).split(slice, dim=2)\n-            down_proj = [\n-                F.linear(intermediate_states[i], down_proj_slices[i]) for i in range(self.config.pretraining_tp)\n-            ]\n-            down_proj = sum(down_proj)\n-        else:\n-            down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n         return down_proj\n \n \n@@ -264,31 +245,14 @@ def forward(\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n \n-        if self.config.pretraining_tp > 1:\n-            key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n-            query_slices = self.q_proj.weight.split(\n-                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n-            )\n-            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n-            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n-\n-            query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n-            query_states = torch.cat(query_states, dim=-1)\n-\n-            key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n-            key_states = torch.cat(key_states, dim=-1)\n-\n-            value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n-            value_states = torch.cat(value_states, dim=-1)\n-\n-        else:\n-            query_states = self.q_proj(hidden_states)\n-            key_states = self.k_proj(hidden_states)\n-            value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -330,12 +294,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1)\n \n-        if self.config.pretraining_tp > 1:\n-            attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n-            o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n-            attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n-        else:\n-            attn_output = self.o_proj(attn_output)\n+        attn_output = self.o_proj(attn_output)\n \n         if not output_attentions:\n             attn_weights = None\n@@ -508,9 +467,10 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        # use -1 to infer num_heads and num_key_value_heads as they may vary if tensor parallel is used\n+        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n \n         if position_embeddings is None:\n             logger.warning_once(\n@@ -794,7 +754,10 @@ def __init__(self, config: SuperConfig):\n         )\n         self.norm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = SuperRotaryEmbedding(config=config)\n+\n         self.gradient_checkpointing = False\n+        if getattr(config, \"pretraining_tp\", 1) != 1:\n+            logger.warn(\"`pretraining_tp` is deprecated, please use `model.tensor_parallel` instead.\")\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "ccf15363de92578a397b7ac9e958cc1cfea21fa0",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3a5889ef09ed60444d5eff4314f1e87909e2739/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3a5889ef09ed60444d5eff4314f1e87909e2739/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=e3a5889ef09ed60444d5eff4314f1e87909e2739",
            "patch": "@@ -266,7 +266,6 @@ def update_body(self, existing_body, new_statements):\n             if m.matches(stmt, m.SimpleStatementLine(body=[m.Assign()])):\n                 target = self.python_module.code_for_node(stmt.body[0].targets[0].target)\n                 if target in self.deleted_targets:\n-                    logger.warning(f\"Deleted the assign for {target}\")\n                     continue\n                 if target in self.all_assign_target:\n                     stmt = self.all_assign_target[target]\n@@ -773,6 +772,8 @@ def _merge_functions(self, functions: dict[str, cst.CSTNode], object_mapping: di\n         self.object_dependency_mapping.update(\n             {obj: dep for obj, dep in object_mapping.items() if obj in functions.keys()}\n         )\n+        # Add them to global nodes\n+        self.global_nodes.update(self.functions)\n \n     def _merge_assignments(self, assignments: dict[str, cst.CSTNode], object_mapping: dict[str, set]):\n         \"\"\"Update the global nodes with the assignment from the modular file.\n@@ -786,6 +787,8 @@ def _merge_assignments(self, assignments: dict[str, cst.CSTNode], object_mapping\n                 self.assignments[assignment] = node\n                 if assignment in object_mapping:\n                     self.object_dependency_mapping[assignment] = object_mapping[assignment]\n+        # Add them to global nodes\n+        self.global_nodes.update(self.assignments)\n \n     def _merge_classes(self, classes: dict[str, cst.CSTNode]):\n         \"\"\"Update the global nodes with the new classes from the modular (i.e. classes which do not exist in current file, and\n@@ -813,10 +816,7 @@ def merge_modular_dependencies(self, classes, functions, assignments, object_map\n         self._merge_classes(classes)\n         self.modular_file_start_lines = start_lines\n \n-        # Correctly re-set the global nodes at this point\n-        self.global_nodes.update(self.functions)\n-        self.global_nodes.update(self.assignments)\n-        # Restrict the dependency mappings to the know entities to avoid Python's built-ins\n+        # Restrict the dependency mappings to the known entities to avoid Python's built-ins and imports\n         self._restrict_dependencies_to_known_entities()\n         # Create the global mapping of recursive dependencies for functions and assignments\n         self.object_recursive_dependency_mapping = self._compute_recursive_object_dependencies()\n@@ -1024,14 +1024,17 @@ def get_needed_imports(body: dict[str, dict], all_imports: list[cst.CSTNode]) ->\n                 import_ref_count[name] = ref_count\n \n     imports_to_keep = []\n+    existing_protected_statements = set()  # str repr of the import nodes - does not work with the nodes directly\n     for node in all_imports:\n         if m.matches(node, m.If()):  # handle safe imports\n             new_statements = []\n             for stmt_node in node.body.body:\n                 append_new_import_node(stmt_node, unused_imports, new_statements)\n+            new_statements = [stmt for stmt in new_statements if str(stmt) not in existing_protected_statements]\n             if len(new_statements) > 0:\n                 new_node = node.with_changes(body=node.body.with_changes(body=new_statements))\n                 imports_to_keep.append(new_node)\n+                existing_protected_statements.update({str(stmt) for stmt in new_statements})\n         else:\n             append_new_import_node(node, unused_imports, imports_to_keep)\n "
        }
    ],
    "stats": {
        "total": 178,
        "additions": 65,
        "deletions": 113
    }
}