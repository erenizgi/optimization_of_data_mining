{
    "author": "BenjaminBossan",
    "message": "FIX Error when trying to load non-LoRA PEFT (#42663)\n\n* FIX Error when trying to load non-LoRA PEFT\n\nThis PR fixes a bug that prevented non-LoRA PEFT adapters to be loaded\ninto a transformers model. A test for this has been added.\n\nAdditionally, also testing if a non-LoRA adapter can be added to a\ntransformers model. This was not broken but still lacked test coverage.\n\n* Reviewer feedback: Remove check completely",
    "sha": "d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2",
    "files": [
        {
            "sha": "9004f5a5b1cb136afec8de87a738d8f2de4d02a6",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2",
            "patch": "@@ -279,9 +279,6 @@ def load_adapter(\n             )\n             peft_config.inference_mode = not is_trainable\n \n-        if peft_config.peft_type != PeftType.LORA:\n-            raise ValueError(\"Hotswapping is currently only supported for LoRA, please set `hotswap=False`.\")\n-\n         if not hotswap:\n             # TODO: WE NEED TOO APPLY OUR DYNAMIC WEIGHT CONVERSION AT SOME POINT HERE!\n             # Create and add fresh new adapters into the model, unless the weights are hotswapped"
        },
        {
            "sha": "0f4e169642655e8c5ab60387598884b6a1ba7e9e",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=d3ee06b8cb5e45aab51b85aafd54f4b3f7cad2e2",
            "patch": "@@ -889,6 +889,60 @@ def test_peft_pipeline_no_warning(self):\n             # Generate text to verify pipeline works\n             _ = lora_generator(text, max_new_tokens=20)\n \n+    def test_non_lora_load_adapter(self):\n+        \"\"\"\n+        Check that loading a non-LoRA adapter works. Using LoKr as an example, not testing all possible PEFT methods.\n+        \"\"\"\n+        from peft import LoKrConfig, get_peft_model\n+\n+        inputs = torch.randint(0, 100, (1, 10)).to(torch_device)\n+        atol, rtol = 1e-4, 1e-4\n+\n+        for model_id in self.transformers_test_model_ids:\n+            for transformers_class in self.transformers_test_model_classes:\n+                model = transformers_class.from_pretrained(model_id).to(torch_device)\n+                with torch.inference_mode():\n+                    output_base = model(inputs).logits\n+\n+                peft_config = LoKrConfig(init_weights=False)\n+                peft_model = get_peft_model(model, peft_config)\n+                with torch.inference_mode():\n+                    output_peft = peft_model(inputs).logits\n+\n+                # sanity check: should be different\n+                assert not torch.allclose(output_base, output_peft, atol=atol, rtol=rtol)\n+\n+                with tempfile.TemporaryDirectory() as tmpdirname:\n+                    peft_model.save_pretrained(tmpdirname)\n+                    del model, peft_model\n+\n+                    model = transformers_class.from_pretrained(tmpdirname).to(torch_device)\n+                    with torch.inference_mode():\n+                        output_transformers = model(inputs).logits\n+                    self.assertTrue(torch.allclose(output_peft, output_transformers, atol=atol, rtol=rtol))\n+\n+    def test_non_lora_add_adapter(self):\n+        \"\"\"\n+        Check that adding a non-LoRA adapter works. Using LoKr as an example, not testing all possible PEFT methods.\n+        \"\"\"\n+        from peft import LoKrConfig\n+\n+        inputs = torch.randint(0, 100, (1, 10)).to(torch_device)\n+        atol, rtol = 1e-4, 1e-4\n+\n+        for model_id in self.transformers_test_model_ids:\n+            for transformers_class in self.transformers_test_model_classes:\n+                model = transformers_class.from_pretrained(model_id).to(torch_device)\n+                with torch.inference_mode():\n+                    output_base = model(inputs).logits\n+\n+                peft_config = LoKrConfig(init_weights=False)\n+                model.add_adapter(peft_config)\n+                with torch.inference_mode():\n+                    output_peft = model(inputs).logits\n+                # should be different\n+                assert not torch.allclose(output_base, output_peft, atol=atol, rtol=rtol)\n+\n \n @require_peft\n @require_torch"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 54,
        "deletions": 3
    }
}