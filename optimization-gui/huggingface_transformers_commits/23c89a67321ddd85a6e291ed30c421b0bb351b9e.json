{
    "author": "vasqu",
    "message": "[`Attention`] Small fix on output attentions (#38948)\n\nsmall fix",
    "sha": "23c89a67321ddd85a6e291ed30c421b0bb351b9e",
    "files": [
        {
            "sha": "155491d6d5790d96a1f02122a524ae5187c954c0",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/23c89a67321ddd85a6e291ed30c421b0bb351b9e/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23c89a67321ddd85a6e291ed30c421b0bb351b9e/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=23c89a67321ddd85a6e291ed30c421b0bb351b9e",
            "patch": "@@ -338,7 +338,7 @@ def output_attentions(self):\n \n     @output_attentions.setter\n     def output_attentions(self, value):\n-        if self._attn_implementation != \"eager\":\n+        if value is True and self._attn_implementation != \"eager\":\n             raise ValueError(\n                 \"The `output_attentions` attribute is not supported when using the `attn_implementation` set to \"\n                 f\"{self._attn_implementation}. Please set it to 'eager' instead.\""
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}