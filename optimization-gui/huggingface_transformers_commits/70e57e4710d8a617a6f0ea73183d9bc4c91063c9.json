{
    "author": "juliendenize",
    "message": "Add mistral common support (#38906)\n\n* wip: correct docstrings\n\n* Add mistral-common support.\n\n* quality\n\n* wip: add requested methods\n\n* wip: fix tests\n\n* wip: add internally some methods not being supported in mistral-common\n\n* wip\n\n* wip: add opencv dependency and update test list\n\n* wip: add mistral-common to testing dependencies\n\n* wip: revert some test changes\n\n* wip: ci\n\n* wip: ci\n\n* clean\n\n* check\n\n* check\n\n* check\n\n* wip: add hf image format to apply_chat_template and return pixel_values\n\n* wip: make mistral-common non-installed safe\n\n* wip: clean zip\n\n* fix: from_pretrained\n\n* fix: path and base64\n\n* fix: path and import root\n\n* wip: add docs\n\n* clean\n\n* clean\n\n* revert\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
    "files": [
        {
            "sha": "67e6eddfb8be937520b0b4a2f19b4630e85704ec",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -28,6 +28,7 @@\n \n NOT_DEVICE_TESTS = {\n     \"test_tokenization\",\n+    \"test_tokenization_mistral_common\",\n     \"test_processor\",\n     \"test_processing\",\n     \"test_beam_constraints\","
        },
        {
            "sha": "ba60eda429ede7e025bb4a1dd39663807f304c17",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -139,6 +139,10 @@ Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/bl\n \n [[autodoc]] MistralConfig\n \n+## MistralCommonTokenizer\n+\n+[[autodoc]] MistralCommonTokenizer\n+\n ## MistralModel\n \n [[autodoc]] MistralModel"
        },
        {
            "sha": "37cf3e8b203e321be120b63d4c8a19d761a2f7c8",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -227,6 +227,10 @@ This example also how to use `BitsAndBytes` to load the model in 4bit quantizati\n \n [[autodoc]] Mistral3Config\n \n+## MistralCommonTokenizer\n+\n+[[autodoc]] MistralCommonTokenizer\n+\n ## Mistral3Model\n \n [[autodoc]] Mistral3Model"
        },
        {
            "sha": "8b07aff7fa5e730ff1816eb13badd5b160be0552",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -197,6 +197,10 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] MixtralConfig\n \n+## MistralCommonTokenizer\n+\n+[[autodoc]] MistralCommonTokenizer\n+\n ## MixtralModel\n \n [[autodoc]] MixtralModel"
        },
        {
            "sha": "6adac0277f7dccaecac07d86400f4c689622297d",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -86,6 +86,10 @@ output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up\n \n [[autodoc]] PixtralVisionConfig\n \n+## MistralCommonTokenizer\n+\n+[[autodoc]] MistralCommonTokenizer\n+\n ## PixtralVisionModel\n \n [[autodoc]] PixtralVisionModel"
        },
        {
            "sha": "ff84d793644cfdc4f50d652925ead2b43ccf27d9",
            "filename": "setup.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -204,6 +204,7 @@\n     \"opentelemetry-api\",\n     \"opentelemetry-exporter-otlp\",\n     \"opentelemetry-sdk\",\n+    \"mistral-common[opencv]>=1.6.3\",\n ]\n \n \n@@ -334,6 +335,7 @@ def run(self):\n extras[\"num2words\"] = deps_list(\"num2words\")\n extras[\"sentencepiece\"] = deps_list(\"sentencepiece\", \"protobuf\")\n extras[\"tiktoken\"] = deps_list(\"tiktoken\", \"blobfile\")\n+extras[\"mistral-common\"] = deps_list(\"mistral-common[opencv]\")\n extras[\"testing\"] = (\n     deps_list(\n         \"pytest\",\n@@ -363,6 +365,7 @@ def run(self):\n     )\n     + extras[\"retrieval\"]\n     + extras[\"modelcreation\"]\n+    + extras[\"mistral-common\"]\n )\n \n extras[\"deepspeed-testing\"] = extras[\"deepspeed\"] + extras[\"testing\"] + extras[\"optuna\"] + extras[\"sentencepiece\"]\n@@ -384,6 +387,7 @@ def run(self):\n     + extras[\"accelerate\"]\n     + extras[\"video\"]\n     + extras[\"num2words\"]\n+    + extras[\"mistral-common\"]\n )\n \n "
        },
        {
            "sha": "9120d3201773e91b3434bc2a1d0c009945bf7c8b",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -34,6 +34,7 @@\n     is_g2p_en_available,\n     is_keras_nlp_available,\n     is_librosa_available,\n+    is_mistral_common_available,\n     is_pretty_midi_available,\n     is_scipy_available,\n     is_sentencepiece_available,\n@@ -310,6 +311,18 @@\n         \"convert_slow_tokenizer\",\n     ]\n \n+try:\n+    if not (is_mistral_common_available()):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    from .utils import dummy_mistral_common_objects\n+\n+    _import_structure[\"utils.dummy_mistral_common_objects\"] = [\n+        name for name in dir(dummy_mistral_common_objects) if not name.startswith(\"_\")\n+    ]\n+else:\n+    _import_structure[\"tokenization_mistral_common\"] = [\"MistralCommonTokenizer\"]\n+\n # Vision-specific objects\n try:\n     if not is_vision_available():"
        },
        {
            "sha": "acd8ec7fb466c92b5c5b1ace3b1ab0a70fe9f42e",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -106,4 +106,5 @@\n     \"opentelemetry-api\": \"opentelemetry-api\",\n     \"opentelemetry-exporter-otlp\": \"opentelemetry-exporter-otlp\",\n     \"opentelemetry-sdk\": \"opentelemetry-sdk\",\n+    \"mistral-common[opencv]\": \"mistral-common[opencv]>=1.6.3\",\n }"
        },
        {
            "sha": "0543bfd062afe57549a681eec00bf10ff046bedb",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 23,
            "deletions": 7,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -21,6 +21,8 @@\n from collections import OrderedDict\n from typing import Any, Optional, Union\n \n+from transformers.utils.import_utils import is_mistral_common_available\n+\n from ...configuration_utils import PretrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n from ...modeling_gguf_pytorch_utils import load_gguf_checkpoint\n@@ -387,15 +389,19 @@\n         (\n             \"mistral\",\n             (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+                \"MistralCommonTokenizer\"\n+                if is_mistral_common_available()\n+                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n             ),\n         ),\n         (\n             \"mixtral\",\n             (\n-                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+                \"MistralCommonTokenizer\"\n+                if is_mistral_common_available()\n+                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n             ),\n         ),\n         (\"mllama\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n@@ -490,7 +496,15 @@\n         (\"phimoe\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"phobert\", (\"PhobertTokenizer\", None)),\n         (\"pix2struct\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n-        (\"pixtral\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"pixtral\",\n+            (\n+                None,\n+                \"MistralCommonTokenizer\"\n+                if is_mistral_common_available()\n+                else (\"PreTrainedTokenizerFast\" if is_tokenizers_available() else None),\n+            ),\n+        ),\n         (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n         (\"qdqbert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n@@ -721,8 +735,10 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n     for module_name, tokenizers in TOKENIZER_MAPPING_NAMES.items():\n         if class_name in tokenizers:\n             module_name = model_type_to_module_name(module_name)\n-\n-            module = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n+            if module_name in [\"mistral\", \"mixtral\"] and class_name == \"MistralCommonTokenizer\":\n+                module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n+            else:\n+                module = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n             try:\n                 return getattr(module, class_name)\n             except AttributeError:"
        },
        {
            "sha": "38b4905b3775a6e29290612a467cfefba3c9361a",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -108,6 +108,7 @@\n     is_librosa_available,\n     is_liger_kernel_available,\n     is_lomo_available,\n+    is_mistral_common_available,\n     is_natten_available,\n     is_nltk_available,\n     is_onnx_available,\n@@ -1526,6 +1527,13 @@ def require_speech(test_case):\n     return unittest.skipUnless(is_speech_available(), \"test requires torchaudio\")(test_case)\n \n \n+def require_mistral_common(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires mistral-common. These tests are skipped when mistral-common isn't available.\n+    \"\"\"\n+    return unittest.skipUnless(is_mistral_common_available(), \"test requires mistral-common\")(test_case)\n+\n+\n def get_gpu_count():\n     \"\"\"\n     Return the number of available gpus (regardless of whether torch, tf or jax is used)"
        },
        {
            "sha": "bf5f61ae00a6dbbbef51277cd03b37b3900904d1",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "added",
            "additions": 1830,
            "deletions": 0,
            "changes": 1830,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -0,0 +1,1830 @@\n+# Copyright 2025 Mistral AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+import shutil\n+import warnings\n+from collections.abc import Mapping, Sized\n+from enum import Enum\n+from pathlib import Path\n+from typing import Any, Callable, Optional, Union, overload\n+\n+import numpy as np\n+\n+from transformers.tokenization_utils_base import (\n+    LARGE_INTEGER,\n+    VERY_LARGE_INTEGER,\n+    BatchEncoding,\n+    EncodedInput,\n+    PreTokenizedInput,\n+    PreTrainedTokenizerBase,\n+    TextInput,\n+    TruncationStrategy,\n+)\n+from transformers.utils import PaddingStrategy, TensorType, add_end_docstrings, logging, to_py_obj\n+from transformers.utils.generic import is_torch_tensor\n+from transformers.utils.hub import PushToHubMixin\n+from transformers.utils.import_utils import is_mistral_common_available, is_torch_available, requires\n+\n+\n+if is_mistral_common_available():\n+    from mistral_common.protocol.instruct.request import ChatCompletionRequest\n+    from mistral_common.protocol.instruct.validator import ValidationMode\n+    from mistral_common.tokens.tokenizers.base import SpecialTokenPolicy\n+    from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n+    from mistral_common.tokens.tokenizers.tekken import Tekkenizer\n+    from mistral_common.tokens.tokenizers.utils import download_tokenizer_from_hf_hub\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+ENCODE_KWARGS_DOCSTRING = r\"\"\"\n+            add_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to add special tokens when encoding the sequences. This will use the underlying\n+                `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n+                automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n+                automatically.\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n+                Activates and controls padding. Accepts the following values:\n+\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence is provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n+                Activates and controls truncation. Accepts the following values:\n+\n+                - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n+                  to the maximum acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n+                  greater than the model maximum admissible input size).\n+            max_length (`int`, *optional*):\n+                Controls the maximum length to use by one of the truncation/padding parameters.\n+\n+                If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n+                is required by one of the truncation/padding parameters. If the model has no specific maximum input\n+                length (like XLNet) truncation/padding to a maximum length will be deactivated.\n+            stride (`int`, *optional*, defaults to 0):\n+                If set to a number along with `max_length`, the overflowing tokens returned when\n+                `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n+                returned to provide some overlap between truncated and overflowing sequences. The value of this\n+                argument defines the number of overlapping tokens.\n+            pad_to_multiple_of (`int`, *optional*):\n+                If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n+                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n+                `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+\"\"\"\n+\n+ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING = r\"\"\"\n+            return_attention_mask (`bool`, *optional*):\n+                Whether to return the attention mask. If left to the default, will return the attention mask according\n+                to the specific tokenizer's default, defined by the `return_outputs` attribute.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n+                of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n+                of returning overflowing tokens.\n+            return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return special tokens mask information.\n+            return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return `(char_start, char_end)` for each token.\n+\n+                This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n+                Python's tokenizer, this method will raise `NotImplementedError`.\n+            return_length  (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the lengths of the encoded inputs.\n+            verbose (`bool`, *optional*, defaults to `True`):\n+                Whether or not to print more information and warnings.\n+            **kwargs: passed to the `self.tokenize()` method\n+\n+        Return:\n+            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model.\n+\n+              [What are input IDs?](../glossary#input-ids)\n+\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n+\n+              [What are attention masks?](../glossary#attention-mask)\n+\n+            - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n+              `return_overflowing_tokens=True`).\n+            - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n+              `return_overflowing_tokens=True`).\n+            - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n+              regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n+            - **length** -- The length of the inputs (when `return_length=True`)\n+\"\"\"\n+\n+\n+class MistralTokenizerType(str, Enum):\n+    \"\"\"Enum for the different type of tokenizer.\"\"\"\n+\n+    spm = \"spm\"\n+    tekken = \"tekken\"\n+\n+\n+@requires(backends=(\"mistral-common\",))\n+class MistralCommonTokenizer(PushToHubMixin):\n+    \"\"\"\n+    Class to wrap `mistral-common` tokenizers.\n+\n+    `mistral-common` is the official tokenizer library for Mistral AI models. To use it, you need to install it with:\n+\n+    ```bash\n+    pip install transformers[mistral-common]\n+    ```\n+\n+    Otherwise the tokenizer falls back to the Transformers implementation of the tokenizer.\n+\n+    For more info on `mistral-common`, see [mistral-common](https://github.com/mistralai/mistral-common).\n+\n+    This class is a wrapper around a `mistral_common.tokens.tokenizers.mistral.MistralTokenizer`.\n+    It provides a Hugging Face compatible interface to tokenize using the official mistral-common tokenizer.\n+\n+    Supports the following methods from the `PreTrainedTokenizerBase` class:\n+\n+    - [`~MistralCommonTokenizer.get_vocab`]: Returns the vocabulary as a dictionary of token to index.\n+    - [`~MistralCommonTokenizer.encode`]: Encode a string to a list of integers.\n+    - [`~MistralCommonTokenizer.decode`]: Decode a list of integers to a string.\n+    - [`~MistralCommonTokenizer.batch_decode`]: Decode a batch of list of integers to a list of strings.\n+    - [`~MistralCommonTokenizer.convert_tokens_to_ids`]: Convert a list of tokens to a list of integers.\n+    - [`~MistralCommonTokenizer.convert_ids_to_tokens`]: Convert a list of integers to a list of tokens.\n+    - [`~MistralCommonTokenizer.tokenize`]: Tokenize a string.\n+    - [`~MistralCommonTokenizer.get_special_tokens_mask`]: Get the special tokens mask for a list of tokens.\n+    - [`~MistralCommonTokenizer.prepare_for_model`]: Prepare a list of inputs for the model.\n+    - [`~MistralCommonTokenizer.pad`]: Pad a list of inputs to the same length.\n+    - [`~MistralCommonTokenizer.truncate_sequences`]: Truncate a list of sequences to the same length.\n+    - [`~MistralCommonTokenizer.apply_chat_template`]: Apply a chat template to a list of messages.\n+    - [`~MistralCommonTokenizer.__call__`]: Tokenize a string or a list of strings.\n+    - [`~MistralCommonTokenizer.from_pretrained`]: Download and cache a pretrained tokenizer from the Hugging Face model hub or local directory.\n+    - [`~MistralCommonTokenizer.save_pretrained`]: Save a tokenizer to a directory, so it can be reloaded using the `from_pretrained` class method.\n+    - [`~MistralCommonTokenizer.push_to_hub`]: Upload tokenizer to the Hugging Face model hub.\n+\n+    Here are the key differences with the `PreTrainedTokenizerBase` class:\n+\n+    - Pair of sequences are not supported. The signature have been kept for compatibility but all arguments related to pair of sequences are ignored. The return values of pairs are returned as `None`.\n+    - The `is_split_into_words` argument is not supported.\n+    - The `return_token_type_ids` argument is not supported.\n+    - It is not possible to add new tokens to the tokenizer. Also the special tokens are handled differently from Transformers. In `mistral-common`, special tokens are never encoded directly. This means that: `tokenizer.encode(\"<s>\")` will not return the ID of the `<s>` token. Instead, it will return a list of IDs corresponding to the tokenization of the string `\"<s>\"`. For more information, see the [mistral-common documentation](https://mistralai.github.io/mistral-common/usage/tokenizers/#special-tokens).\n+\n+    If you have suggestions to improve this class, please open an issue on the [mistral-common GitHub repository](https://github.com/mistralai/mistral-common/issues) if it is related to the tokenizer or on the [Transformers GitHub repository](https://github.com/huggingface/transformers/issues) if it is related to the Hugging Face interface.\n+    \"\"\"\n+\n+    model_input_names: list[str] = [\"input_ids\", \"attention_mask\"]\n+    padding_side: str = \"left\"\n+    truncation_side: str = \"right\"\n+\n+    def __init__(\n+        self,\n+        tokenizer_path: Union[str, os.PathLike, Path],\n+        mode: ValidationMode = ValidationMode.test,\n+        model_max_length: int = VERY_LARGE_INTEGER,\n+        padding_side: str = \"left\",\n+        truncation_side: str = \"right\",\n+        model_input_names: Optional[list[str]] = None,\n+        clean_up_tokenization_spaces: bool = False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Constructs a `MistralCommonTokenizer`.\n+\n+        - **model_input_names** (`List[str]`) -- A list of inputs expected in the forward pass of the model.\n+        - **padding_side** (`str`) -- The default value for the side on which the model should have padding applied.\n+            Should be `'right'` or `'left'`.\n+        - **truncation_side** (`str`) -- The default value for the side on which the model should have truncation\n+            applied. Should be `'right'` or `'left'`.\n+\n+        Args:\n+            tokenizer_path (`str` or `os.PathLike` or `Path`):\n+                Path to the tokenizer file to load the `MistralTokenizer`.\n+            mode (`ValidationMode`, *optional*, defaults to `ValidationMode.test`):\n+                The mode to use for the tokenizer. This will be passed to the `MistralTokenizer` constructor.\n+            model_max_length (`int`, *optional*):\n+                The maximum length (in number of tokens) for the inputs to the transformer model. When the tokenizer is\n+                loaded with [`~tokenization_utils_base.PreTrainedTokenizerBase.from_pretrained`], this will be set to the\n+                value stored for the associated model in `max_model_input_sizes` (see above). If no value is provided, will\n+                default to VERY_LARGE_INTEGER (`int(1e30)`).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n+            truncation_side (`str`, *optional*):\n+                The side on which the model should have truncation applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n+            model_input_names (`List[string]`, *optional*):\n+                The list of inputs accepted by the forward pass of the model (like `\"token_type_ids\"` or\n+                `\"attention_mask\"`). Default value is picked from the class attribute of the same name.\n+            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not the model should cleanup the spaces that were added when splitting the input text during the\n+                tokenization process.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported to init `MistralCommonTokenizer`.\")\n+\n+        self._tokenizer_path = Path(tokenizer_path)\n+        self.tokenizer: MistralTokenizer = MistralTokenizer.from_file(str(self._tokenizer_path), mode=mode)\n+        self._tokenizer_type = (\n+            MistralTokenizerType.tekken\n+            if isinstance(self.tokenizer.instruct_tokenizer.tokenizer, Tekkenizer)\n+            else MistralTokenizerType.spm\n+        )\n+        self.truncation_side = truncation_side\n+        self.padding_side = padding_side\n+        self.model_max_length = model_max_length\n+        self.cleanup_tokenization_spaces = clean_up_tokenization_spaces\n+        self.deprecation_warnings = {}  # Use to store when we have already noticed a deprecation warning (avoid overlogging).\n+\n+        if model_input_names is not None:\n+            if (\n+                not isinstance(model_input_names, (list, tuple))\n+                and len(model_input_names) == 0\n+                and not all(isinstance(i, str) for i in model_input_names)\n+            ):\n+                raise ValueError(\n+                    \"`model_input_names` should be a non-empty list or tuple of str but got an empty value.\"\n+                )\n+            self.model_input_names = model_input_names\n+\n+        self._cache_get_vocab: Optional[dict[str, int]] = None\n+\n+    @property\n+    def bos_token_id(self) -> int:\n+        \"\"\"\n+        Id of the beginning of sentence token in the vocabulary.\n+        \"\"\"\n+        return self.tokenizer.instruct_tokenizer.tokenizer.bos_id\n+\n+    @property\n+    def eos_token_id(self) -> int:\n+        \"\"\"\n+        Id of the end of sentence token in the vocabulary.\n+        \"\"\"\n+        return self.tokenizer.instruct_tokenizer.tokenizer.eos_id\n+\n+    @property\n+    def unk_token_id(self) -> int:\n+        \"\"\"\n+        Id of the unknown token in the vocabulary.\n+        \"\"\"\n+        return self.tokenizer.instruct_tokenizer.tokenizer.unk_id\n+\n+    @property\n+    def pad_token_id(self) -> int:\n+        \"\"\"\n+        Id of the padding token in the vocabulary.\n+        \"\"\"\n+        return self.tokenizer.instruct_tokenizer.tokenizer.pad_id\n+\n+    @property\n+    def bos_token(self) -> str:\n+        \"\"\"\n+        String associated to the beginning of sentence token in the vocabulary.\n+        \"\"\"\n+        return self.convert_ids_to_tokens(self.bos_token_id)\n+\n+    @property\n+    def eos_token(self) -> str:\n+        \"\"\"\n+        String associated to the end of sentence token in the vocabulary.\n+        \"\"\"\n+        return self.convert_ids_to_tokens(self.eos_token_id)\n+\n+    @property\n+    def unk_token(self) -> str:\n+        \"\"\"\n+        String associated to the unknown token in the vocabulary.\n+        \"\"\"\n+        return self.convert_ids_to_tokens(self.unk_token_id)\n+\n+    @property\n+    def pad_token(self) -> str:\n+        \"\"\"\n+        String associated to the padding token in the vocabulary.\n+        \"\"\"\n+        return self.convert_ids_to_tokens(self.pad_token_id)\n+\n+    @property\n+    def vocab_size(self) -> int:\n+        \"\"\"\n+        Returns the size of the vocabulary.\n+\n+        `int`: Size of the vocabulary.\n+        \"\"\"\n+        return self.tokenizer.instruct_tokenizer.tokenizer.n_words\n+\n+    def get_vocab(self) -> dict[str, int]:\n+        \"\"\"\n+        Returns the vocabulary as a dictionary of token to index.\n+\n+        This is a lossy conversion. There may be multiple token ids that decode to the same\n+        string due to partial UTF-8 byte sequences being converted to ï¿½.\n+\n+        Returns:\n+            `Dict[str, int]`: The vocabulary.\n+        \"\"\"\n+        if self._cache_get_vocab is None:\n+            self._cache_get_vocab = {\n+                token: idx for idx, token in enumerate(self.tokenizer.instruct_tokenizer.tokenizer.vocab())\n+            }\n+        return self._cache_get_vocab\n+\n+    def __len__(self):\n+        \"\"\"\n+        Size of the full vocabulary with the added tokens.\n+        \"\"\"\n+        return self.vocab_size\n+\n+    @add_end_docstrings(\n+        ENCODE_KWARGS_DOCSTRING,\n+        \"\"\"\n+            **kwargs: Not supported by `MistralCommonTokenizer.encode`.\n+                Will raise an error if used.\n+        \"\"\",\n+        \"\"\"\n+        Returns:\n+            `List[int]`, `torch.Tensor`: The tokenized ids of the text.\n+        \"\"\",\n+    )\n+    def encode(\n+        self,\n+        text: Union[TextInput, EncodedInput],\n+        text_pair: None = None,\n+        add_special_tokens: bool = True,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n+        max_length: Optional[int] = None,\n+        stride: int = 0,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        verbose: bool = True,\n+        **kwargs,\n+    ) -> list[int]:\n+        \"\"\"\n+        Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n+\n+        Args:\n+            text (`str` or `List[int]`):\n+                The first sequence to be encoded. This can be a string or a list of integers (tokenized string ids).\n+            text_pair (`None`, *optional*):\n+                Not supported by `MistralCommonTokenizer.encode`. Kept to match `PreTrainedTokenizerBase.encode` signature.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.encode`.\")\n+        if text_pair:\n+            raise ValueError(\"`MistralCommonTokenizer.encode` does not support `text_pair`.\")\n+\n+        padding_strategy, truncation_strategy, max_length, _ = self._get_padding_truncation_strategies(\n+            padding=padding,\n+            truncation=truncation,\n+            max_length=max_length,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            verbose=verbose,\n+        )\n+\n+        encoded_inputs = self._encode_plus(\n+            text,\n+            add_special_tokens=add_special_tokens,\n+            padding_strategy=padding_strategy,\n+            truncation_strategy=truncation_strategy,\n+            max_length=max_length,\n+            stride=stride,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n+            return_tensors=return_tensors,\n+            return_attention_mask=False,\n+            return_overflowing_tokens=False,\n+            return_special_tokens_mask=False,\n+            return_length=False,\n+            verbose=verbose,\n+        )\n+\n+        return encoded_inputs[\"input_ids\"]\n+\n+    def decode(\n+        self,\n+        token_ids: Union[int, list[int], \"np.ndarray\", \"torch.Tensor\"],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        **kwargs,\n+    ) -> str:\n+        \"\"\"\n+        Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n+        tokens and clean up tokenization spaces.\n+\n+        Args:\n+            token_ids (`Union[int, List[int], np.ndarray, torch.Tensor]`):\n+                List of tokenized input ids. Can be obtained using the `__call__` method.\n+            skip_special_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not to remove special tokens in the decoding.\n+            clean_up_tokenization_spaces (`bool`, *optional*):\n+                Whether or not to clean up the tokenization spaces. If `None`, will default to\n+                `self.clean_up_tokenization_spaces`.\n+            kwargs (additional keyword arguments, *optional*):\n+                Not supported by `MistralCommonTokenizer.decode`.\n+                Will raise an error if used.\n+\n+        Returns:\n+            `str`: The decoded sentence.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.decode`.\")\n+\n+        clean_up_tokenization_spaces = clean_up_tokenization_spaces or self.cleanup_tokenization_spaces\n+\n+        # Convert inputs to python lists\n+        token_ids = to_py_obj(token_ids)\n+\n+        special_token_policy = SpecialTokenPolicy.IGNORE if skip_special_tokens else SpecialTokenPolicy.KEEP\n+\n+        decoded_string = self.tokenizer.decode(token_ids, special_token_policy=special_token_policy)\n+        if clean_up_tokenization_spaces:\n+            decoded_string = PreTrainedTokenizerBase.clean_up_tokenization(decoded_string)\n+\n+        return decoded_string\n+\n+    def batch_decode(\n+        self,\n+        sequences: Union[list[int], list[list[int]], \"np.ndarray\", \"torch.Tensor\"],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        **kwargs,\n+    ) -> list[str]:\n+        \"\"\"\n+        Convert a list of lists of token ids into a list of strings by calling decode.\n+\n+        Args:\n+            sequences (`Union[List[int], List[List[int]], np.ndarray, torch.Tensor]`):\n+                List of tokenized input ids. Can be obtained using the `__call__` method.\n+            skip_special_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not to remove special tokens in the decoding.\n+            clean_up_tokenization_spaces (`bool`, *optional*):\n+                Whether or not to clean up the tokenization spaces. If `None`, will default to\n+                `self.clean_up_tokenization_spaces`.\n+            kwargs (additional keyword arguments, *optional*):\n+                Not supported by `MistralCommonTokenizer.batch_decode`.\n+                Will raise an error if used.\n+\n+        Returns:\n+            `List[str]`: The list of decoded sentences.\n+        \"\"\"\n+        return [\n+            self.decode(\n+                seq,\n+                skip_special_tokens=skip_special_tokens,\n+                clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+                **kwargs,\n+            )\n+            for seq in sequences\n+        ]\n+\n+    def _is_control_token(self, token_id: int) -> bool:\n+        if self._tokenizer_type == MistralTokenizerType.spm:\n+            return token_id in self.tokenizer.instruct_tokenizer.tokenizer._control_tokens()\n+        elif self._tokenizer_type == MistralTokenizerType.tekken:\n+            return token_id < self.tokenizer.instruct_tokenizer.tokenizer.num_special_tokens\n+        else:\n+            raise ValueError(f\"Unknown tokenizer type: {self._tokenizer_type}\")\n+\n+    @overload\n+    def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = False) -> str: ...\n+    @overload\n+    def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = False) -> list[str]: ...\n+    def convert_ids_to_tokens(\n+        self, ids: Union[int, list[int]], skip_special_tokens: bool = False\n+    ) -> Union[str, list[str]]:\n+        \"\"\"\n+        Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n+        added tokens.\n+\n+        Args:\n+            ids (`int` or `List[int]`):\n+                The token id (or token ids) to convert to tokens.\n+            skip_special_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not to remove special tokens in the decoding.\n+\n+        Returns:\n+            `str` or `List[str]`: The decoded token(s).\n+        \"\"\"\n+\n+        if isinstance(ids, int):\n+            one_token = True\n+            ids = [ids]\n+        else:\n+            one_token = False\n+\n+        tokens: list[str] = []\n+        for token_id in ids:\n+            if self._is_control_token(token_id) and skip_special_tokens:\n+                continue\n+            tokens.append(self.tokenizer.instruct_tokenizer.tokenizer.id_to_piece(token_id))\n+\n+        if one_token:\n+            if tokens == []:\n+                raise ValueError(f\"Invalid token id {ids}.\")\n+\n+            return tokens[0]\n+        return tokens\n+\n+    def _piece_to_id(self, piece: str) -> int:\n+        if self._tokenizer_type == MistralTokenizerType.spm:\n+            return self.tokenizer.instruct_tokenizer.tokenizer._model.piece_to_id(piece)\n+        elif self._tokenizer_type == MistralTokenizerType.tekken:\n+            pieces = self.tokenizer.instruct_tokenizer.tokenizer._model.encode(\n+                piece, allowed_special=\"all\", disallowed_special=set()\n+            )\n+            assert len(pieces) == 1, f\"Expected to decode 1 token, got {len(pieces)}\"\n+            return pieces[0]\n+        else:\n+            raise ValueError(f\"Unknown tokenizer type: {self._tokenizer_type}\")\n+\n+    def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, list[int]]:\n+        \"\"\"\n+        Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n+        vocabulary.\n+\n+        Args:\n+            tokens (`str` or `List[str]`): One or several token(s) to convert to token id(s).\n+\n+        Returns:\n+            `int` or `List[int]`: The token id or list of token ids.\n+        \"\"\"\n+\n+        if isinstance(tokens, str):\n+            one_token = True\n+            tokens = [tokens]\n+        else:\n+            one_token = False\n+\n+        ids: list[int] = []\n+        for token in tokens:\n+            ids.append(self._piece_to_id(token))\n+\n+        if one_token:\n+            return ids[0]\n+        return ids\n+\n+    def _text_to_ids(self, text: TextInput, add_special_tokens: bool) -> list[int]:\n+        \"\"\"\n+        Converts a string into a sequence of tokens ids, using the tokenizer.\n+        \"\"\"\n+        tokens_ids = self.tokenizer.instruct_tokenizer.tokenizer.encode(\n+            text, bos=add_special_tokens, eos=add_special_tokens\n+        )\n+        return tokens_ids\n+\n+    def tokenize(self, text: TextInput, **kwargs) -> list[str]:\n+        \"\"\"\n+        Converts a string into a sequence of tokens, using the tokenizer.\n+\n+        Split in words for word-based vocabulary or sub-words for sub-word-based vocabularies.\n+\n+        Args:\n+            text (`str`):\n+                The sequence to be encoded.\n+            **kwargs (additional keyword arguments):\n+                Not supported by `MistralCommonTokenizer.tokenize`.\n+                Will raise an error if used.\n+\n+        Returns:\n+            `List[str]`: The list of tokens.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.tokenize`.\")\n+\n+        return self.convert_ids_to_tokens(self._text_to_ids(text, add_special_tokens=False), skip_special_tokens=False)\n+\n+    def _encode_plus(\n+        self,\n+        text: Union[TextInput, EncodedInput],\n+        add_special_tokens: bool = True,\n+        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n+        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n+        max_length: Optional[int] = None,\n+        stride: int = 0,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        return_overflowing_tokens: bool = False,\n+        return_special_tokens_mask: bool = False,\n+        return_length: bool = False,\n+        verbose: bool = True,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        if kwargs:\n+            raise ValueError(\n+                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer._encode_plus`.\"\n+            )\n+\n+        def get_input_ids(text):\n+            if isinstance(text, str):\n+                return self._text_to_ids(text, add_special_tokens)\n+            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n+                return text\n+            else:\n+                raise ValueError(f\"Input {text} is not valid. Should be a string, or a list/tuple of integers.\")\n+\n+        ids = get_input_ids(text)\n+\n+        return self.prepare_for_model(\n+            ids,\n+            add_special_tokens=add_special_tokens,\n+            padding=padding_strategy.value,\n+            truncation=truncation_strategy.value,\n+            max_length=max_length,\n+            stride=stride,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n+            return_tensors=return_tensors,\n+            prepend_batch_axis=True,\n+            return_attention_mask=return_attention_mask,\n+            return_overflowing_tokens=return_overflowing_tokens,\n+            return_special_tokens_mask=return_special_tokens_mask,\n+            return_length=return_length,\n+            verbose=verbose,\n+        )\n+\n+    def _batch_encode_plus(\n+        self,\n+        batch_text: Union[\n+            list[TextInput],\n+            list[EncodedInput],\n+        ],\n+        add_special_tokens: bool = True,\n+        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n+        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n+        max_length: Optional[int] = None,\n+        stride: int = 0,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        return_overflowing_tokens: bool = False,\n+        return_special_tokens_mask: bool = False,\n+        return_offsets_mapping: bool = False,\n+        return_length: bool = False,\n+        verbose: bool = True,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        def get_input_ids(text):\n+            if isinstance(text, str):\n+                return self._text_to_ids(text, add_special_tokens)\n+            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n+                return text\n+            else:\n+                raise ValueError(\"Input is not valid. Should be a string or a list/tuple of integers.\")\n+\n+        if return_offsets_mapping:\n+            raise NotImplementedError(\n+                \"return_offset_mapping is not available when using Python tokenizers. \"\n+                \"To use this feature, change your tokenizer to one deriving from \"\n+                \"transformers.PreTrainedTokenizerFast.\"\n+            )\n+\n+        input_ids = []\n+        for ids in batch_text:\n+            input_ids.append(get_input_ids(ids))\n+\n+        batch_outputs = self._batch_prepare_for_model(\n+            input_ids,\n+            add_special_tokens=add_special_tokens,\n+            padding_strategy=padding_strategy,\n+            truncation_strategy=truncation_strategy,\n+            max_length=max_length,\n+            stride=stride,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n+            return_attention_mask=return_attention_mask,\n+            return_overflowing_tokens=return_overflowing_tokens,\n+            return_special_tokens_mask=return_special_tokens_mask,\n+            return_length=return_length,\n+            return_tensors=return_tensors,\n+            verbose=verbose,\n+        )\n+\n+        return BatchEncoding(batch_outputs)\n+\n+    def _all_special_ids(self) -> set[int]:\n+        if self._tokenizer_type == MistralTokenizerType.tekken:\n+            return {t[\"rank\"] for t in self.tokenizer.instruct_tokenizer.tokenizer._all_special_tokens}\n+        elif self._tokenizer_type == MistralTokenizerType.spm:\n+            return self.tokenizer.instruct_tokenizer.tokenizer._control_tokens()\n+        else:\n+            raise ValueError(f\"Unknown tokenizer type: {self._tokenizer_type}\")\n+\n+    def get_special_tokens_mask(\n+        self, token_ids_0: list, token_ids_1: None = None, already_has_special_tokens: bool = False\n+    ) -> list[int]:\n+        \"\"\"\n+        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n+        special tokens using the tokenizer `prepare_for_model` or `encode_plus` methods.\n+\n+        Args:\n+            token_ids_0 (`List[int]`):\n+                List of ids of the sequence.\n+            token_ids_1 (`List[int]`, *optional*):\n+                Not supported by `MistralCommonTokenizer`. Kept to match the interface of `PreTrainedTokenizerBase`.\n+            already_has_special_tokens (`bool`, *optional*, defaults to `False`):\n+                Whether or not the token list is already formatted with special tokens for the model.\n+\n+        Returns:\n+            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n+        \"\"\"\n+        if token_ids_1 is not None:\n+            raise ValueError(\n+                \"`token_ids_1` is not supported by `MistralCommonTokenizer` and should be `None`, kept for compatibility.\"\n+            )\n+        if already_has_special_tokens:\n+            raise ValueError(\n+                \"`already_has_special_tokens` is not supported by `MistralCommonTokenizer` and should be `False`.\"\n+            )\n+\n+        all_special_ids = self._all_special_ids()  # cache the ids\n+\n+        special_tokens_mask = [1 if token in all_special_ids else 0 for token in token_ids_0]\n+        return special_tokens_mask\n+\n+    def _batch_prepare_for_model(\n+        self,\n+        batch_ids: list[Union[PreTokenizedInput, list[int]]],\n+        add_special_tokens: bool = True,\n+        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n+        truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n+        max_length: Optional[int] = None,\n+        stride: int = 0,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_tensors: Optional[str] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        return_overflowing_tokens: bool = False,\n+        return_special_tokens_mask: bool = False,\n+        return_length: bool = False,\n+        verbose: bool = True,\n+    ) -> BatchEncoding:\n+        \"\"\"\n+        Prepares a sequence of input id so that it can be used by the model. It\n+        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n+        manages a moving window (with user defined stride) for overflowing tokens.\n+\n+        Args:\n+            batch_ids: list of tokenized input ids\n+        \"\"\"\n+\n+        batch_outputs = {}\n+        for ids in batch_ids:\n+            outputs = self.prepare_for_model(\n+                ids,\n+                add_special_tokens=add_special_tokens,\n+                padding=PaddingStrategy.DO_NOT_PAD.value,  # we pad in batch afterward\n+                truncation=truncation_strategy.value,\n+                max_length=max_length,\n+                stride=stride,\n+                pad_to_multiple_of=None,  # we pad in batch afterward\n+                padding_side=None,  # we pad in batch afterward\n+                return_attention_mask=False,  # we pad in batch afterward\n+                return_overflowing_tokens=return_overflowing_tokens,\n+                return_special_tokens_mask=return_special_tokens_mask,\n+                return_length=return_length,\n+                return_tensors=None,  # We convert the whole batch to tensors at the end\n+                prepend_batch_axis=False,\n+                verbose=verbose,\n+            )\n+\n+            for key, value in outputs.items():\n+                if key not in batch_outputs:\n+                    batch_outputs[key] = []\n+                batch_outputs[key].append(value)\n+\n+        batch_outputs = self.pad(\n+            batch_outputs,\n+            padding=padding_strategy.value,\n+            max_length=max_length,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            padding_side=padding_side,\n+            return_attention_mask=return_attention_mask,\n+        )\n+\n+        batch_outputs = BatchEncoding(batch_outputs, tensor_type=return_tensors)\n+\n+        return batch_outputs\n+\n+    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n+    def prepare_for_model(\n+        self,\n+        ids: list[int],\n+        pair_ids: None = None,\n+        add_special_tokens: bool = True,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n+        max_length: Optional[int] = None,\n+        stride: int = 0,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        return_overflowing_tokens: bool = False,\n+        return_special_tokens_mask: bool = False,\n+        return_length: bool = False,\n+        verbose: bool = True,\n+        prepend_batch_axis: bool = False,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        \"\"\"\n+        Prepares a sequence of input id so that it can be used by the model. It\n+        adds special tokens, truncates sequences if overflowing while taking into account the special tokens and\n+        manages a moving window (with user defined stride) for overflowing tokens.\n+\n+        Args:\n+            ids (`List[int]`):\n+                Tokenized input ids of the first sequence.\n+            pair_ids (`None`, *optional*):\n+                Not supported by `MistralCommonTokenizer`. Kept to match the interface of `PreTrainedTokenizerBase`.\n+        \"\"\"\n+        if pair_ids is not None:\n+            raise ValueError(\n+                \"`pair_ids` is not supported by `MistralCommonTokenizer` and should be `None`, kept for compatibility.\"\n+            )\n+        if kwargs:\n+            raise ValueError(\n+                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.prepare_for_model`.\"\n+            )\n+\n+        padding_strategy, truncation_strategy, max_length, _ = self._get_padding_truncation_strategies(\n+            padding=padding,\n+            truncation=truncation,\n+            max_length=max_length,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            verbose=verbose,\n+        )\n+\n+        len_ids = len(ids)\n+\n+        # Load from model defaults\n+        if return_attention_mask is None:\n+            return_attention_mask = \"attention_mask\" in self.model_input_names\n+\n+        encoded_inputs = {}\n+\n+        # Truncation: Handle max sequence length\n+        overflowing_tokens = []\n+        if truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE and max_length and len_ids > max_length:\n+            ids, _, overflowing_tokens = self.truncate_sequences(\n+                ids,\n+                num_tokens_to_remove=len_ids - max_length,\n+                truncation_strategy=truncation_strategy,\n+                stride=stride,\n+            )\n+\n+        if return_overflowing_tokens:\n+            encoded_inputs[\"overflowing_tokens\"] = overflowing_tokens\n+            encoded_inputs[\"num_truncated_tokens\"] = len_ids - max_length\n+\n+        # Build output dictionary\n+        encoded_inputs[self.model_input_names[0]] = ids\n+        if return_special_tokens_mask:\n+            if add_special_tokens:\n+                encoded_inputs[\"special_tokens_mask\"] = self.get_special_tokens_mask(ids, None)\n+            else:\n+                encoded_inputs[\"special_tokens_mask\"] = [0] * len(ids)\n+\n+        # Padding\n+        if padding_strategy != PaddingStrategy.DO_NOT_PAD or return_attention_mask:\n+            encoded_inputs = self.pad(\n+                encoded_inputs,\n+                max_length=max_length,\n+                padding=padding_strategy.value,\n+                pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n+                return_attention_mask=return_attention_mask,\n+            )\n+\n+        if return_length:\n+            encoded_inputs[\"length\"] = len(encoded_inputs[\"input_ids\"])\n+\n+        batch_outputs = BatchEncoding(\n+            encoded_inputs, tensor_type=return_tensors, prepend_batch_axis=prepend_batch_axis\n+        )\n+\n+        return batch_outputs\n+\n+    def _get_padding_truncation_strategies(\n+        self,\n+        padding: Union[str, PaddingStrategy, bool] = False,\n+        truncation: Optional[Union[str, TruncationStrategy, bool]] = None,\n+        max_length: Optional[int] = None,\n+        pad_to_multiple_of: Optional[int] = None,\n+        verbose: bool = True,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Find the correct padding/truncation strategy.\n+        \"\"\"\n+\n+        # Backward compatibility for previous behavior, maybe we should deprecate it:\n+        # If you only set max_length, it activates truncation for max_length\n+        if max_length is not None and padding is False and truncation is None:\n+            if verbose:\n+                if not self.deprecation_warnings.get(\"Truncation-not-explicitly-activated\", False):\n+                    logger.warning(\n+                        \"Truncation was not explicitly activated but `max_length` is provided a specific value, please\"\n+                        \" use `truncation=True` to explicitly truncate examples to max length. Defaulting to\"\n+                        \" 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the\"\n+                        \" tokenizer you can select this strategy more precisely by providing a specific strategy to\"\n+                        \" `truncation`.\"\n+                    )\n+                self.deprecation_warnings[\"Truncation-not-explicitly-activated\"] = True\n+            truncation = \"longest_first\"\n+\n+        # Get padding strategy\n+        if padding is not False:\n+            if padding is True:\n+                if verbose:\n+                    if max_length is not None and (\n+                        truncation is None or truncation is False or truncation == \"do_not_truncate\"\n+                    ):\n+                        warnings.warn(\n+                            \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n+                            \"To pad to max length, use `padding='max_length'`.\"\n+                        )\n+                padding_strategy = PaddingStrategy.LONGEST  # Default to pad to the longest sequence in the batch\n+            elif not isinstance(padding, PaddingStrategy):\n+                padding_strategy = PaddingStrategy(padding)\n+            elif isinstance(padding, PaddingStrategy):\n+                padding_strategy = padding\n+        else:\n+            padding_strategy = PaddingStrategy.DO_NOT_PAD\n+\n+        # Get truncation strategy\n+        if truncation is not False and truncation is not None:\n+            if truncation is True:\n+                truncation_strategy = (\n+                    TruncationStrategy.LONGEST_FIRST\n+                )  # Default to truncate the longest sequences in pairs of inputs\n+            elif not isinstance(truncation, TruncationStrategy):\n+                truncation_strategy = TruncationStrategy(truncation)\n+            elif isinstance(truncation, TruncationStrategy):\n+                truncation_strategy = truncation\n+            if truncation in [TruncationStrategy.ONLY_FIRST, TruncationStrategy.ONLY_SECOND]:\n+                raise ValueError(\n+                    \"Truncation strategy `only_first` and `only_second` are not supported by `MistralCommonTokenizer`.\"\n+                )\n+        else:\n+            truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE\n+\n+        # Set max length if needed\n+        if max_length is None:\n+            if padding_strategy == PaddingStrategy.MAX_LENGTH:\n+                if self.model_max_length > LARGE_INTEGER:\n+                    if verbose:\n+                        if not self.deprecation_warnings.get(\"Asking-to-pad-to-max_length\", False):\n+                            logger.warning(\n+                                \"Asking to pad to max_length but no maximum length is provided and the model has no\"\n+                                \" predefined maximum length. Default to no padding.\"\n+                            )\n+                        self.deprecation_warnings[\"Asking-to-pad-to-max_length\"] = True\n+                    padding_strategy = PaddingStrategy.DO_NOT_PAD\n+                else:\n+                    max_length = self.model_max_length\n+\n+            if truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE:\n+                if self.model_max_length > LARGE_INTEGER:\n+                    if verbose:\n+                        if not self.deprecation_warnings.get(\"Asking-to-truncate-to-max_length\", False):\n+                            logger.warning(\n+                                \"Asking to truncate to max_length but no maximum length is provided and the model has\"\n+                                \" no predefined maximum length. Default to no truncation.\"\n+                            )\n+                        self.deprecation_warnings[\"Asking-to-truncate-to-max_length\"] = True\n+                    truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE\n+                else:\n+                    max_length = self.model_max_length\n+\n+        # Test if we have a padding token\n+        if padding_strategy != PaddingStrategy.DO_NOT_PAD and (self.pad_token is None or self.pad_token_id < 0):\n+            raise ValueError(\n+                \"Asking to pad but the tokenizer does not have a padding token. \"\n+                \"Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` \"\n+                \"or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\"\n+            )\n+\n+        # Check that we will truncate to a multiple of pad_to_multiple_of if both are provided\n+        if (\n+            truncation_strategy != TruncationStrategy.DO_NOT_TRUNCATE\n+            and padding_strategy != PaddingStrategy.DO_NOT_PAD\n+            and pad_to_multiple_of is not None\n+            and max_length is not None\n+            and (max_length % pad_to_multiple_of != 0)\n+        ):\n+            raise ValueError(\n+                \"Truncation and padding are both activated but \"\n+                f\"truncation length ({max_length}) is not a multiple of pad_to_multiple_of ({pad_to_multiple_of}).\"\n+            )\n+\n+        return padding_strategy, truncation_strategy, max_length, kwargs\n+\n+    def _pad(\n+        self,\n+        encoded_inputs: Union[dict[str, EncodedInput], BatchEncoding],\n+        max_length: Optional[int] = None,\n+        padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_attention_mask: Optional[bool] = None,\n+    ) -> dict:\n+        \"\"\"\n+        Pad encoded inputs (on left/right and up to predefined length or max length in the batch)\n+\n+        Args:\n+            encoded_inputs:\n+                Dictionary of tokenized inputs (`List[int]`) or batch of tokenized inputs (`List[List[int]]`).\n+            max_length: maximum length of the returned list and optionally padding length (see below).\n+                Will truncate by taking into account the special tokens.\n+            padding_strategy: PaddingStrategy to use for padding.\n+\n+                - PaddingStrategy.LONGEST Pad to the longest sequence in the batch\n+                - PaddingStrategy.MAX_LENGTH: Pad to the max length (default)\n+                - PaddingStrategy.DO_NOT_PAD: Do not pad\n+                The tokenizer padding sides are defined in `padding_side` argument:\n+\n+                    - 'left': pads on the left of the sequences\n+                    - 'right': pads on the right of the sequences\n+            pad_to_multiple_of: (optional) Integer if set will pad the sequence to a multiple of the provided value.\n+                This is especially useful to enable the use of Tensor Core on NVIDIA hardware with compute capability\n+                `>= 7.5` (Volta).\n+            padding_side:\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n+            return_attention_mask:\n+                (optional) Set to False to avoid returning attention mask (default: set to model specifics)\n+        \"\"\"\n+        # Load from model defaults\n+        if return_attention_mask is None:\n+            return_attention_mask = \"attention_mask\" in self.model_input_names\n+\n+        required_input = encoded_inputs[self.model_input_names[0]]\n+\n+        if padding_strategy == PaddingStrategy.LONGEST:\n+            max_length = len(required_input)\n+\n+        if max_length is not None and pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n+            max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n+\n+        needs_to_be_padded = padding_strategy != PaddingStrategy.DO_NOT_PAD and len(required_input) != max_length\n+\n+        # Initialize attention mask if not present.\n+        if return_attention_mask and \"attention_mask\" not in encoded_inputs:\n+            encoded_inputs[\"attention_mask\"] = [1] * len(required_input)\n+\n+        if needs_to_be_padded:\n+            difference = max_length - len(required_input)\n+            padding_side = padding_side if padding_side is not None else self.padding_side\n+\n+            if padding_side == \"right\":\n+                if return_attention_mask:\n+                    encoded_inputs[\"attention_mask\"] = encoded_inputs[\"attention_mask\"] + [0] * difference\n+                if \"special_tokens_mask\" in encoded_inputs:\n+                    encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n+                encoded_inputs[self.model_input_names[0]] = required_input + [self.pad_token_id] * difference\n+            elif padding_side == \"left\":\n+                if return_attention_mask:\n+                    encoded_inputs[\"attention_mask\"] = [0] * difference + encoded_inputs[\"attention_mask\"]\n+                if \"special_tokens_mask\" in encoded_inputs:\n+                    encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n+                encoded_inputs[self.model_input_names[0]] = [self.pad_token_id] * difference + required_input\n+            else:\n+                raise ValueError(f\"Invalid padding strategy:{padding_side}\")\n+\n+        return encoded_inputs\n+\n+    def pad(\n+        self,\n+        encoded_inputs: Union[\n+            BatchEncoding,\n+            list[BatchEncoding],\n+            dict[str, EncodedInput],\n+            dict[str, list[EncodedInput]],\n+            list[dict[str, EncodedInput]],\n+        ],\n+        padding: Union[bool, str, PaddingStrategy] = True,\n+        max_length: Optional[int] = None,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        verbose: bool = True,\n+    ) -> BatchEncoding:\n+        \"\"\"\n+        Pad a single encoded input or a batch of encoded inputs up to predefined length or to the max sequence length\n+        in the batch.\n+\n+        Padding side (left/right) padding token ids are defined at the tokenizer level (with `self.padding_side`,\n+        `self.pad_token_id`).\n+        <Tip>\n+\n+        If the `encoded_inputs` passed are dictionary of numpy arrays, PyTorch tensors, the\n+        result will use the same type unless you provide a different tensor type with `return_tensors`. In the case of\n+        PyTorch tensors, you will lose the specific device of your tensors however.\n+\n+        </Tip>\n+\n+        Args:\n+            encoded_inputs ([`BatchEncoding`], list of [`BatchEncoding`], `Dict[str, List[int]]`, `Dict[str, List[List[int]]` or `List[Dict[str, List[int]]]`):\n+                Tokenized inputs. Can represent one input ([`BatchEncoding`] or `Dict[str, List[int]]`) or a batch of\n+                tokenized inputs (list of [`BatchEncoding`], *Dict[str, List[List[int]]]* or *List[Dict[str,\n+                List[int]]]*) so you can use this method during preprocessing as well as in a PyTorch Dataloader\n+                collate function.\n+\n+                Instead of `List[int]` you can have tensors (numpy arrays, PyTorch tensors), see\n+                the note above for the return type.\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n+                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                 index) among:\n+\n+                - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            max_length (`int`, *optional*):\n+                Maximum length of the returned list and optionally padding length (see above).\n+            pad_to_multiple_of (`int`, *optional*):\n+                If set will pad the sequence to a multiple of the provided value.\n+\n+                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n+                `>= 7.5` (Volta).\n+            padding_side (`str`, *optional*):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n+            return_attention_mask (`bool`, *optional*):\n+                Whether to return the attention mask. If left to the default, will return the attention mask according\n+                to the specific tokenizer's default, defined by the `return_outputs` attribute.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+            verbose (`bool`, *optional*, defaults to `True`):\n+                Whether or not to print more information and warnings.\n+        \"\"\"\n+        # If we have a list of dicts, let's convert it in a dict of lists\n+        # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n+        if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n+\n+        # The model's main input name, usually `input_ids`, has been passed for padding\n+        if self.model_input_names[0] not in encoded_inputs:\n+            raise ValueError(\n+                \"You should supply an encoding or a list of encodings to this method \"\n+                f\"that includes {self.model_input_names[0]}, but you provided {list(encoded_inputs.keys())}\"\n+            )\n+\n+        required_input = encoded_inputs[self.model_input_names[0]]\n+\n+        if required_input is None or (isinstance(required_input, Sized) and len(required_input) == 0):\n+            if return_attention_mask:\n+                encoded_inputs[\"attention_mask\"] = []\n+            return encoded_inputs\n+\n+        # If we have PyTorch/TF/NumPy tensors/arrays as inputs, we cast them as python objects\n+        # and rebuild them afterwards if no return_tensors is specified\n+        # Note that we lose the specific device the tensor may be on for PyTorch\n+\n+        first_element = required_input[0]\n+        if isinstance(first_element, (list, tuple)):\n+            # first_element might be an empty list/tuple in some edge cases so we grab the first non empty element.\n+            for item in required_input:\n+                if len(item) != 0:\n+                    first_element = item[0]\n+                    break\n+        # At this state, if `first_element` is still a list/tuple, it's an empty one so there is nothing to do.\n+        if not isinstance(first_element, (int, list, tuple)):\n+            if is_torch_tensor(first_element):\n+                return_tensors = \"pt\" if return_tensors is None else return_tensors\n+            elif isinstance(first_element, np.ndarray):\n+                return_tensors = \"np\" if return_tensors is None else return_tensors\n+            else:\n+                raise ValueError(\n+                    f\"type of {first_element} unknown: {type(first_element)}. \"\n+                    \"Should be one of a python, numpy, pytorch or tensorflow object.\"\n+                )\n+\n+            for key, value in encoded_inputs.items():\n+                encoded_inputs[key] = to_py_obj(value)\n+\n+        # Convert padding_strategy in PaddingStrategy\n+        padding_strategy, _, max_length, _ = self._get_padding_truncation_strategies(\n+            padding=padding, max_length=max_length, verbose=verbose\n+        )\n+\n+        required_input = encoded_inputs[self.model_input_names[0]]\n+        if required_input and not isinstance(required_input[0], (list, tuple)):\n+            encoded_inputs = self._pad(\n+                encoded_inputs,\n+                max_length=max_length,\n+                padding_strategy=padding_strategy,\n+                pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n+                return_attention_mask=return_attention_mask,\n+            )\n+            return BatchEncoding(encoded_inputs, tensor_type=return_tensors)\n+\n+        batch_size = len(required_input)\n+        assert all(len(v) == batch_size for v in encoded_inputs.values()), (\n+            \"Some items in the output dictionary have a different batch size than others.\"\n+        )\n+\n+        if padding_strategy == PaddingStrategy.LONGEST:\n+            max_length = max(len(inputs) for inputs in required_input)\n+            padding_strategy = PaddingStrategy.MAX_LENGTH\n+\n+        batch_outputs = {}\n+        for i in range(batch_size):\n+            inputs = {k: v[i] for k, v in encoded_inputs.items()}\n+            outputs = self._pad(\n+                inputs,\n+                max_length=max_length,\n+                padding_strategy=padding_strategy,\n+                pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n+                return_attention_mask=return_attention_mask,\n+            )\n+\n+            for key, value in outputs.items():\n+                if key not in batch_outputs:\n+                    batch_outputs[key] = []\n+                batch_outputs[key].append(value)\n+\n+        return BatchEncoding(batch_outputs, tensor_type=return_tensors)\n+\n+    def truncate_sequences(\n+        self,\n+        ids: list[int],\n+        pair_ids: None = None,\n+        num_tokens_to_remove: int = 0,\n+        truncation_strategy: Union[str, TruncationStrategy] = \"longest_first\",\n+        stride: int = 0,\n+        **kwargs,\n+    ) -> tuple[list[int], None, list[int]]:\n+        \"\"\"\n+        Truncates a sequence pair in-place following the strategy.\n+\n+        Args:\n+            ids (`List[int]`):\n+                Tokenized input ids. Can be obtained from a string by chaining the `tokenize` and\n+                `convert_tokens_to_ids` methods.\n+            pair_ids (`None`, *optional*):\n+                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.truncate_sequences`.\n+            num_tokens_to_remove (`int`, *optional*, defaults to 0):\n+                Number of tokens to remove using the truncation strategy.\n+            truncation_strategy (`str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `'longest_first'`):\n+                The strategy to follow for truncation. Can be:\n+\n+                - `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n+                  maximum acceptable input length for the model if that argument is not provided.\n+                - `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths greater\n+                  than the model maximum admissible input size).\n+            stride (`int`, *optional*, defaults to 0):\n+                If set to a positive number, the overflowing tokens returned will contain some tokens from the main\n+                sequence returned. The value of this argument defines the number of additional tokens.\n+\n+        Returns:\n+            `Tuple[List[int], None, List[int]]`: The truncated `ids` and the list of\n+            overflowing tokens. `None` is returned to match Transformers signature.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(\n+                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.truncate_sequences`.\"\n+            )\n+        if pair_ids:\n+            raise ValueError(\"`pair_ids` is not supported by `MistralCommonTokenizer.truncate_sequences`.\")\n+\n+        if num_tokens_to_remove <= 0:\n+            return (ids, None, [])\n+\n+        if not isinstance(truncation_strategy, TruncationStrategy):\n+            truncation_strategy = TruncationStrategy(truncation_strategy)\n+\n+        if truncation_strategy in [TruncationStrategy.ONLY_FIRST, TruncationStrategy.ONLY_SECOND]:\n+            raise ValueError(\n+                f\"Only {TruncationStrategy.LONGEST_FIRST} and {TruncationStrategy.DO_NOT_TRUNCATE} are supported.\"\n+            )\n+\n+        overflowing_tokens = []\n+        if truncation_strategy == TruncationStrategy.LONGEST_FIRST:\n+            if len(ids) > num_tokens_to_remove:\n+                window_len = min(len(ids), stride + num_tokens_to_remove)\n+                if self.truncation_side == \"left\":\n+                    overflowing_tokens = ids[:window_len]\n+                    ids = ids[num_tokens_to_remove:]\n+                elif self.truncation_side == \"right\":\n+                    overflowing_tokens = ids[-window_len:]\n+                    ids = ids[:-num_tokens_to_remove]\n+                else:\n+                    raise ValueError(f\"invalid truncation strategy: {self.truncation_side}, use 'left' or 'right'.\")\n+\n+            else:\n+                error_msg = (\n+                    f\"We need to remove {num_tokens_to_remove} to truncate the input \"\n+                    f\"but the first sequence has a length {len(ids)}. \"\n+                )\n+                logger.error(error_msg)\n+\n+        return (ids, None, overflowing_tokens)\n+\n+    def apply_chat_template(\n+        self,\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        tools: Optional[list[Union[dict, Callable]]] = None,\n+        continue_final_message: bool = False,\n+        tokenize: bool = True,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: bool = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_dict: bool = False,\n+        **kwargs,\n+    ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:\n+        \"\"\"\n+        Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n+        ids.\n+\n+        Args:\n+            conversation (Union[List[Dict[str, str]], List[List[Dict[str, str]]]]): A list of dicts\n+                with \"role\" and \"content\" keys, representing the chat history so far.\n+            tools (`List[Union[Dict, Callable]]`, *optional*):\n+                A list of tools (callable functions) that will be accessible to the model. If the template does not\n+                support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n+                giving the name, description and argument types for the tool. See our\n+                [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n+                for more information.\n+            continue_final_message (bool, *optional*):\n+                If this is set, the chat will be formatted so that the final\n+                message in the chat is open-ended, without any EOS tokens. The model will continue this message\n+                rather than starting a new one. This allows you to \"prefill\" part of\n+                the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n+            tokenize (`bool`, defaults to `True`):\n+                Whether to tokenize the output. If `False`, the output will be a string.\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n+                 Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                 index) among:\n+\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            truncation (`bool`, defaults to `False`):\n+                Whether to truncate sequences at the maximum length. Has no effect if tokenize is `False`.\n+            max_length (`int`, *optional*):\n+                Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is `False`. If\n+                not specified, the tokenizer's `max_length` attribute will be used as a default.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Has no effect if tokenize is `False`. Acceptable\n+                values are:\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+            return_dict (`bool`, defaults to `False`):\n+                Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n+                If at least one conversation contains an image, its pixel values will be returned in the `pixel_values` key.\n+            kwargs (additional keyword arguments, *optional*):\n+                Not supported by `MistralCommonTokenizer.apply_chat_template`.\n+                Will raise an error if used.\n+\n+        Returns:\n+            `Union[str, List[int], List[str], List[List[int]], BatchEncoding]`: A list of token ids representing the tokenized chat so far, including control\n+            tokens. This output is ready to pass to the model, either directly or via methods like `generate()`.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(\n+                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.apply_chat_template`.\"\n+            )\n+        if not isinstance(truncation, bool):\n+            raise ValueError(\"`truncation` must be a boolean for `apply_chat_template` method.\")\n+\n+        if isinstance(conversation, (list, tuple)) and (\n+            isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"messages\")\n+        ):\n+            conversations = conversation\n+            is_batched = True\n+        else:\n+            conversations = [conversation]\n+            is_batched = False\n+\n+        def _maybe_adapt_message(message: dict[str, Any]) -> None:\n+            \"\"\"Adapt message to `mistral-common` format and leave validation to `mistral-common`.\"\"\"\n+            if not isinstance(message, dict):\n+                return\n+            maybe_list_content: Optional[Union[str, list[dict[str, Union[str, dict[str, Any]]]]]] = message.get(\n+                \"content\", None\n+            )\n+            if not maybe_list_content or isinstance(maybe_list_content, str):\n+                return\n+\n+            normalized_content: list[dict[str, Union[str, dict[str, Any]]]] = []\n+            for content in maybe_list_content:\n+                content_type = content.get(\"type\", None)\n+                if not content_type:\n+                    continue\n+                elif content_type == \"image\":\n+                    maybe_url: Optional[str] = content.get(\"url\")\n+                    maybe_path: Optional[str] = content.get(\"path\")\n+                    maybe_base64: Optional[str] = content.get(\"base64\")\n+                    if maybe_url:\n+                        image_content = maybe_url\n+                    elif maybe_path:\n+                        if not maybe_path.startswith(\"file://\"):\n+                            maybe_path = Path(maybe_path).resolve().as_uri()\n+                        image_content = maybe_path\n+                    elif maybe_base64:\n+                        if not maybe_base64.startswith(\"data:image\"):\n+                            maybe_base64 = \"data:image/unk;base64,\" + maybe_base64\n+                        image_content = maybe_base64\n+                    else:\n+                        raise ValueError(\"Image content must be specified.\")\n+                    normalized_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_content}})\n+                else:\n+                    normalized_content.append(content)\n+            message[\"content\"] = normalized_content\n+\n+        outputs = []\n+        images: list[np.ndarray] = []\n+\n+        for conversation in conversations:\n+            messages: list[dict[str, Union[str, list[dict[str, Union[str, dict[str, Any]]]]]]] = []\n+            for message in conversation:\n+                _maybe_adapt_message(message)\n+                messages.append(message)\n+\n+            chat_request = ChatCompletionRequest.from_openai(\n+                messages=messages,\n+                tools=tools,\n+                continue_final_message=continue_final_message,\n+            )\n+\n+            tokenized_request = self.tokenizer.encode_chat_completion(chat_request)\n+            if tokenize:\n+                outputs.append(tokenized_request.tokens)\n+            else:\n+                outputs.append(tokenized_request.text)\n+            images.extend(tokenized_request.images)\n+\n+        if not is_batched:\n+            outputs = outputs[0]\n+\n+        if tokenize:\n+            out = self(\n+                outputs,\n+                padding=padding,\n+                truncation=truncation,\n+                max_length=max_length,\n+                add_special_tokens=False,\n+                return_tensors=return_tensors,\n+            )\n+            if return_dict:\n+                if images:\n+                    pixel_values: Union[list[np.ndarray], np.ndarray, torch.Tensor]\n+                    if return_tensors == \"pt\":\n+                        if not is_torch_available():\n+                            raise ImportError(\n+                                \"Unable to convert output to PyTorch tensors format, PyTorch is not installed.\"\n+                            )\n+\n+                        pixel_values = torch.tensor(images)\n+                    elif return_tensors == \"np\":\n+                        pixel_values = np.array(images)\n+                    elif return_tensors is None:\n+                        pixel_values = images\n+                    else:\n+                        raise ValueError(f\"Unsupported return_tensors type: {return_tensors}\")\n+                    out.data[\"pixel_values\"] = pixel_values\n+                return out\n+            else:\n+                return out[\"input_ids\"]\n+\n+        else:\n+            logger.warning(\n+                \"`MistralCommonTokenizer.apply_chat_template(..., tokenize=False)` is unsafe and may lead to unexpected behavior.\"\n+                \" Please consider using `tokenize=True` instead and don't encode the output manually.\"\n+            )\n+            return outputs\n+\n+    @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n+    def __call__(\n+        self,\n+        text: Union[TextInput, EncodedInput, list[TextInput], list[EncodedInput], None] = None,\n+        text_pair: None = None,\n+        text_target: None = None,\n+        text_pair_target: None = None,\n+        add_special_tokens: bool = True,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n+        max_length: Optional[int] = None,\n+        stride: int = 0,\n+        pad_to_multiple_of: Optional[int] = None,\n+        padding_side: Optional[str] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        return_overflowing_tokens: bool = False,\n+        return_special_tokens_mask: bool = False,\n+        return_length: bool = False,\n+        verbose: bool = True,\n+        **kwargs,\n+    ) -> BatchEncoding:\n+        \"\"\"\n+        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n+        sequences.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of int\n+                (encoded strings).\n+            text_pair (`None`, *optional*):\n+                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.__call__`.\n+            text_target (`None`, *optional*):\n+                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.__call__`.\n+            text_pair_target (`None`, *optional*):\n+                Not supported by `MistralCommonTokenizer`. Kept to match the signature of `PreTrainedTokenizerBase.__call__`.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.__call__`.\")\n+\n+        if text_pair or text_target or text_pair_target:\n+            raise ValueError(\n+                \"`text_pair`, `text_target` and `text_pair_target` are not supported by `MistralCommonTokenizer`.\"\n+            )\n+\n+        if return_tensors in (\"tf\", \"jax\"):\n+            raise ValueError(\n+                \"`MistralCommonTokenizer` does not support `return_tensors='tf'` or `return_tensors='jax'`.\"\n+            )\n+\n+        def _is_valid_text_input(t):\n+            if isinstance(t, str):\n+                # Strings are fine\n+                return True\n+            elif isinstance(t, (list, tuple)):\n+                # List are fine as long as they are...\n+                if len(t) == 0:\n+                    # ... empty\n+                    return True\n+                elif isinstance(t[0], (str, int)):\n+                    # ... list of strings or int\n+                    return True\n+                elif isinstance(t[0], (list, tuple)):\n+                    # ... list with an empty list or with a list of strings or with a list of ints\n+                    return len(t[0]) == 0 or isinstance(t[0][0], (str, int))\n+                else:\n+                    return False\n+            else:\n+                return False\n+\n+        if not _is_valid_text_input(text):\n+            raise ValueError(\n+                \"text input must be of type `str` (single example), `List[str]` (batch or single encoded example) \"\n+                \"or `List[List[int]]` (batch of encoded examples).\"\n+            )\n+\n+        is_batched = isinstance(text, (list, tuple)) and isinstance(text[0], (str, list, tuple))\n+\n+        padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n+            padding=padding,\n+            truncation=truncation,\n+            max_length=max_length,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            verbose=verbose,\n+            **kwargs,\n+        )\n+\n+        if is_batched:\n+            return self._batch_encode_plus(\n+                batch_text=text,\n+                add_special_tokens=add_special_tokens,\n+                padding_strategy=padding_strategy,\n+                truncation_strategy=truncation_strategy,\n+                max_length=max_length,\n+                stride=stride,\n+                pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n+                return_tensors=return_tensors,\n+                return_attention_mask=return_attention_mask,\n+                return_overflowing_tokens=return_overflowing_tokens,\n+                return_special_tokens_mask=return_special_tokens_mask,\n+                return_length=return_length,\n+                verbose=verbose,\n+                **kwargs,\n+            )\n+        else:\n+            return self._encode_plus(\n+                text=text,\n+                add_special_tokens=add_special_tokens,\n+                padding_strategy=padding_strategy,\n+                truncation_strategy=truncation_strategy,\n+                max_length=max_length,\n+                stride=stride,\n+                pad_to_multiple_of=pad_to_multiple_of,\n+                padding_side=padding_side,\n+                return_tensors=return_tensors,\n+                return_attention_mask=return_attention_mask,\n+                return_overflowing_tokens=return_overflowing_tokens,\n+                return_special_tokens_mask=return_special_tokens_mask,\n+                return_length=return_length,\n+                verbose=verbose,\n+                **kwargs,\n+            )\n+\n+    @classmethod\n+    def from_pretrained(\n+        cls,\n+        pretrained_model_name_or_path: Union[str, os.PathLike],\n+        *init_inputs,\n+        mode: ValidationMode = ValidationMode.test,\n+        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        force_download: bool = False,\n+        local_files_only: bool = False,\n+        token: Optional[Union[str, bool]] = None,\n+        revision: str = \"main\",\n+        model_max_length: int = VERY_LARGE_INTEGER,\n+        padding_side: str = \"left\",\n+        truncation_side: str = \"right\",\n+        model_input_names: Optional[list[str]] = None,\n+        clean_up_tokenization_spaces: bool = False,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n+        Instantiate a `MistralCommonTokenizer` from a predefined\n+        tokenizer.\n+\n+        Args:\n+            pretrained_model_name_or_path (`str` or `os.PathLike`):\n+                Can be either:\n+\n+                - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.\n+                - A path to a *directory* containing the tokenizer config, for instance saved\n+                  using the [`MistralCommonTokenizer.tokenization_mistral_common.save_pretrained`] method, e.g.,\n+                  `./my_model_directory/`.\n+            mode (`ValidationMode`, *optional*, defaults to `ValidationMode.test`):\n+                Validation mode for the `MistralTokenizer` tokenizer.\n+            cache_dir (`str` or `os.PathLike`, *optional*):\n+                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the\n+                standard cache should not be used.\n+            force_download (`bool`, *optional*, defaults to `False`):\n+                Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n+                exist.\n+            token (`str` or *bool*, *optional*):\n+                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n+                when running `huggingface-cli login` (stored in `~/.huggingface`).\n+            local_files_only (`bool`, *optional*, defaults to `False`):\n+                Whether or not to only rely on local files and not to attempt to download any files.\n+            revision (`str`, *optional*, defaults to `\"main\"`):\n+                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n+                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n+                identifier allowed by git.\n+            max_length (`int`, *optional*):\n+                Controls the maximum length to use by one of the truncation/padding parameters.\n+\n+                If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n+                is required by one of the truncation/padding parameters. If the model has no specific maximum input\n+                length (like XLNet) truncation/padding to a maximum length will be deactivated.\n+            padding_side (`str`, *optional*, defaults to `\"left\"`):\n+                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n+                Default value is picked from the class attribute of the same name.\n+            truncation_side (`str`, *optional*, defaults to `\"right\"`):\n+                The side on which the model should have truncation applied. Should be selected between ['right', 'left'].\n+            model_input_names (`List[string]`, *optional*):\n+                The list of inputs accepted by the forward pass of the model (like `\"token_type_ids\"` or\n+                `\"attention_mask\"`). Default value is picked from the class attribute of the same name.\n+            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not the model should cleanup the spaces that were added when splitting the input text during the\n+                tokenization process.\n+            kwargs (additional keyword arguments, *optional*):\n+                Not supported by `MistralCommonTokenizer.from_pretrained`.\n+                Will raise an error if used.\n+        \"\"\"\n+        if init_inputs:\n+            raise ValueError(\"`init_inputs` are not supported by `MistralCommonTokenizer.from_pretrained`.\")\n+\n+        # Handle kwargs and AutoTokenizer case\n+        if kwargs and not kwargs.keys() == {\"_from_auto\"}:\n+            raise ValueError(\n+                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.from_pretrained`.\"\n+            )\n+\n+        if not os.path.isfile(pretrained_model_name_or_path):\n+            tokenizer_path = download_tokenizer_from_hf_hub(\n+                repo_id=pretrained_model_name_or_path,\n+                cache_dir=cache_dir,\n+                token=token,\n+                revision=revision,\n+                force_download=force_download,\n+                local_files_only=local_files_only,\n+            )\n+        else:\n+            tokenizer_path = pretrained_model_name_or_path\n+\n+        return cls(\n+            tokenizer_path=tokenizer_path,\n+            mode=mode,\n+            model_max_length=model_max_length,\n+            padding_side=padding_side,\n+            truncation_side=truncation_side,\n+            model_input_names=model_input_names,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+        )\n+\n+    def save_pretrained(\n+        self,\n+        save_directory: Union[str, os.PathLike, Path],\n+        push_to_hub: bool = False,\n+        token: Optional[Union[str, bool]] = None,\n+        commit_message: Optional[str] = None,\n+        repo_id: Optional[str] = None,\n+        private: Optional[bool] = None,\n+        repo_url: Optional[str] = None,\n+        organization: Optional[str] = None,\n+        **kwargs,\n+    ) -> tuple[str]:\n+        \"\"\"\n+        Save the full tokenizer state.\n+\n+\n+        This method make sure the full tokenizer can then be re-loaded using the\n+        [`~MistralCommonTokenizer.tokenization_mistral_common.from_pretrained`] class method.\n+\n+        Args:\n+            save_directory (`str` or `os.PathLike`): The path to a directory where the tokenizer will be saved.\n+            push_to_hub (`bool`, *optional*, defaults to `False`):\n+                Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n+                repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n+                namespace).\n+            token (`str` or *bool*, *optional*, defaults to `None`):\n+                The token to use to push to the model hub. If `True`, will use the token in the `HF_TOKEN` environment\n+                variable.\n+            commit_message (`str`, *optional*): The commit message to use when pushing to the hub.\n+            repo_id (`str`, *optional*): The name of the repository to which push to the Hub.\n+            private (`bool`, *optional*): Whether the model repository is private or not.\n+            repo_url (`str`, *optional*): The URL to the Git repository to which push to the Hub.\n+            organization (`str`, *optional*): The name of the organization in which you would like to push your model.\n+            kwargs (`Dict[str, Any]`, *optional*):\n+                Not supported by `MistralCommonTokenizer.save_pretrained`.\n+                Will raise an error if used.\n+\n+        Returns:\n+            A tuple of `str`: The files saved.\n+        \"\"\"\n+        if kwargs:\n+            raise ValueError(\n+                f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.save_pretrained`.\"\n+            )\n+\n+        save_directory = Path(save_directory)\n+        save_directory.mkdir(parents=True, exist_ok=True)\n+\n+        shutil.copy(self._tokenizer_path, save_directory)\n+\n+        if push_to_hub:\n+            repo_id = repo_id or str(save_directory).split(os.path.sep)[-1]\n+            repo_id = self._create_repo(\n+                repo_id, token=token, private=private, repo_url=repo_url, organization=organization\n+            )\n+            files_timestamps = self._get_files_timestamps(save_directory)\n+\n+            self._upload_modified_files(\n+                save_directory,\n+                repo_id,\n+                files_timestamps,\n+                commit_message=commit_message,\n+                token=token,\n+            )\n+\n+        return (str(save_directory / self._tokenizer_path.name),)"
        },
        {
            "sha": "2220281cdb888d5b1fbd130eda403683202049b2",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -182,6 +182,7 @@\n     is_liger_kernel_available,\n     is_lomo_available,\n     is_matplotlib_available,\n+    is_mistral_common_available,\n     is_mlx_available,\n     is_natten_available,\n     is_ninja_available,"
        },
        {
            "sha": "0c9a5a3a9db883b38cc5a0929e116543ce506543",
            "filename": "src/transformers/utils/dummy_mistral_common_objects.py",
            "status": "added",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Futils%2Fdummy_mistral_common_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Futils%2Fdummy_mistral_common_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_mistral_common_objects.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -0,0 +1,9 @@\n+# This file is autogenerated by the command `make fix-copies`, do not edit.\n+from ..utils import DummyObject, requires_backends\n+\n+\n+class MistralCommonTokenizer(metaclass=DummyObject):\n+    _backends = [\"mistral-common\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"mistral-common\"])"
        },
        {
            "sha": "93622f6c3c0331e80256fe14f838aa5feba15cc6",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -227,6 +227,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _rich_available = _is_package_available(\"rich\")\n _kernels_available = _is_package_available(\"kernels\")\n _matplotlib_available = _is_package_available(\"matplotlib\")\n+_mistral_common_available = _is_package_available(\"mistral_common\")\n \n _torch_version = \"N/A\"\n _torch_available = False\n@@ -1575,6 +1576,10 @@ def is_matplotlib_available():\n     return _matplotlib_available\n \n \n+def is_mistral_common_available():\n+    return _mistral_common_available\n+\n+\n def check_torch_load_is_safe():\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError(\n@@ -1979,6 +1984,11 @@ def check_torch_load_is_safe():\n rich`. Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n+MISTRAL_COMMON_IMPORT_ERROR = \"\"\"\n+{0} requires the mistral-common library but it was not found in your environment. You can install it with pip: `pip install mistral-common`. Please note that you may need to restart your runtime after installation.\n+\"\"\"\n+\n+\n BACKENDS_MAPPING = OrderedDict(\n     [\n         (\"av\", (is_av_available, AV_IMPORT_ERROR)),\n@@ -2031,6 +2041,7 @@ def check_torch_load_is_safe():\n         (\"pydantic\", (is_pydantic_available, PYDANTIC_IMPORT_ERROR)),\n         (\"fastapi\", (is_fastapi_available, FASTAPI_IMPORT_ERROR)),\n         (\"uvicorn\", (is_uvicorn_available, UVICORN_IMPORT_ERROR)),\n+        (\"mistral-common\", (is_mistral_common_available, MISTRAL_COMMON_IMPORT_ERROR)),\n     ]\n )\n "
        },
        {
            "sha": "d225ec55703a57bf24d3e52546d7e6aad1c0dc76",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "added",
            "additions": 1655,
            "deletions": 0,
            "changes": 1655,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -0,0 +1,1655 @@\n+# Copyright 2025 Mistral AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import torch\n+\n+from transformers.models.auto.tokenization_auto import AutoTokenizer\n+from transformers.testing_utils import require_mistral_common\n+from transformers.tokenization_mistral_common import MistralCommonTokenizer\n+from transformers.tokenization_utils_base import BatchEncoding, TruncationStrategy\n+from transformers.utils import PaddingStrategy, is_mistral_common_available\n+\n+\n+if is_mistral_common_available():\n+    from mistral_common.exceptions import InvalidMessageStructureException\n+    from mistral_common.protocol.instruct.request import ChatCompletionRequest\n+    from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n+\n+\n+IMG_URL = \"https://picsum.photos/id/237/200/300\"\n+IMG_BASE_64 = \"\"\"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\"\"\"\n+\n+\n+@require_mistral_common\n+class TestMistralCommonTokenizer(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        cls.tokenizer: MistralCommonTokenizer = AutoTokenizer.from_pretrained(\n+            \"hf-internal-testing/namespace-mistralai-repo_name-Mistral-Small-3.1-24B-Instruct-2503\",\n+            tokenizer_type=\"mistral\",\n+        )\n+        cls.ref_tokenizer: MistralTokenizer = MistralTokenizer.from_hf_hub(\n+            \"hf-internal-testing/namespace-mistralai-repo_name-Mistral-Small-3.1-24B-Instruct-2503\"\n+        )\n+        cls.fixture_conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+                {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+                {\"role\": \"user\", \"content\": \"What is the temperature in Paris?\"},\n+            ],\n+        ]\n+        cls.tokenized_fixture_conversations = [\n+            cls.ref_tokenizer.encode_chat_completion(ChatCompletionRequest.from_openai(conversation))\n+            for conversation in cls.fixture_conversations\n+        ]\n+\n+        cls.ref_special_ids = {t[\"rank\"] for t in cls.ref_tokenizer.instruct_tokenizer.tokenizer._all_special_tokens}\n+\n+    def _ref_piece_to_id(self, piece: str) -> int:\n+        pieces = self.ref_tokenizer.instruct_tokenizer.tokenizer._model.encode(\n+            piece, allowed_special=\"all\", disallowed_special=set()\n+        )\n+        assert len(pieces) == 1, f\"Expected to decode 1 token, got {len(pieces)}\"\n+        return pieces[0]\n+\n+    def test_vocab_size(self):\n+        self.assertEqual(self.tokenizer.vocab_size, self.ref_tokenizer.instruct_tokenizer.tokenizer.n_words)\n+\n+    def test_save_pretrained(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            tmp_file = self.tokenizer.save_pretrained(tmp_dir)[0]\n+            loaded_tokenizer = MistralCommonTokenizer.from_pretrained(tmp_file)\n+\n+        self.assertIsNotNone(loaded_tokenizer)\n+        self.assertEqual(self.tokenizer.get_vocab(), loaded_tokenizer.get_vocab())\n+        self.assertEqual(\n+            self.tokenizer.tokenizer.instruct_tokenizer.tokenizer.version,\n+            loaded_tokenizer.tokenizer.instruct_tokenizer.tokenizer.version,\n+        )\n+\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.save_pretrained`.\"\n+        ):\n+            with tempfile.TemporaryDirectory() as tmp_dir:\n+                self.tokenizer.save_pretrained(tmp_dir, unk_args=\"\")\n+\n+    def test_encode(self):\n+        string = \"Hello, world!\"\n+\n+        # Test 1:\n+        # encode with add_special_tokens\n+        expected_with_special = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=True)\n+        tokens_with_special = self.tokenizer.encode(string, add_special_tokens=True)\n+        self.assertEqual(tokens_with_special, expected_with_special)\n+\n+        # Test 2:\n+        # encode without add_special_tokens\n+        expected_without_special = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=False, eos=False)\n+        tokens_without_special = self.tokenizer.encode(string, add_special_tokens=False)\n+        self.assertEqual(tokens_without_special, expected_without_special)\n+\n+        # Test 3:\n+        # encode with return_tensors\n+        tokens_with_return_tensors = self.tokenizer.encode(string, add_special_tokens=False, return_tensors=\"pt\")\n+        self.assertIsInstance(tokens_with_return_tensors, torch.Tensor)\n+        self.assertEqual(tokens_with_return_tensors.tolist()[0], expected_without_special)\n+\n+        # Test 4:\n+        # encode with max_length\n+        tokens_with_max_length = self.tokenizer.encode(string, add_special_tokens=False, max_length=3)\n+        self.assertEqual(tokens_with_max_length, expected_without_special[:3])\n+\n+        # Test 5:\n+        # encode with padding\n+        tokens_with_padding = self.tokenizer.encode(\n+            string, add_special_tokens=False, padding=True, pad_to_multiple_of=6\n+        )\n+        expected_padding = [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (\n+            6 - len(expected_without_special) % 6\n+        ) + expected_without_special\n+        self.assertEqual(tokens_with_padding, expected_padding)\n+\n+        for padding in [\n+            False,\n+            True,\n+            \"longest\",\n+            \"max_length\",\n+            \"do_not_pad\",\n+            PaddingStrategy.LONGEST,\n+            PaddingStrategy.MAX_LENGTH,\n+            PaddingStrategy.DO_NOT_PAD,\n+        ]:\n+            tokens_with_padding = self.tokenizer.encode(string, add_special_tokens=False, padding=padding)\n+            self.assertEqual(tokens_with_padding, expected_without_special)\n+\n+        # For truncation, we use a longer string\n+        string_long = (\n+            \"Hello world! It is a beautiful day today. The sun is shining brightly and the birds are singing.\"\n+        )\n+        expected_long = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string_long, bos=False, eos=False)\n+\n+        # Test 6:\n+        # encode with truncation\n+        tokens_with_truncation = self.tokenizer.encode(\n+            string_long, add_special_tokens=False, truncation=True, max_length=12\n+        )\n+        self.assertEqual(tokens_with_truncation, expected_long[:12])\n+\n+        # Test 7:\n+        # encode with padding and truncation\n+        tokens_with_padding_and_truncation = self.tokenizer.encode(\n+            string_long, add_special_tokens=False, padding=True, pad_to_multiple_of=12, truncation=True, max_length=36\n+        )\n+        expected_long_padding = [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (\n+            12 - len(expected_long) % 12\n+        ) + expected_long\n+        self.assertEqual(tokens_with_padding_and_truncation, expected_long_padding)\n+\n+        # Test encode with unsupported kwargs\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.encode`.\"\n+        ):\n+            self.tokenizer.encode(\"Hello, world!\", add_special_tokens=True, unk_args=\"\")\n+\n+    def test_decode(self):\n+        string = \"Hello, world!\"\n+        string_with_space = \"Hello, world !\"\n+\n+        tokens_ids = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=True)\n+        tokens_ids_with_space = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(\n+            string_with_space, bos=True, eos=True\n+        )\n+\n+        # Test 1:\n+        # decode with and without skip_special_tokens\n+        self.assertEqual(self.tokenizer.decode(tokens_ids, skip_special_tokens=True), string)\n+        self.assertEqual(self.tokenizer.decode(tokens_ids, skip_special_tokens=False), \"<s>\" + string + \"</s>\")\n+        self.assertEqual(self.tokenizer.decode(tokens_ids_with_space, skip_special_tokens=True), string_with_space)\n+\n+        # Test 2:\n+        # decode with clean_up_tokenization_spaces\n+        self.assertEqual(\n+            self.tokenizer.decode(tokens_ids_with_space, skip_special_tokens=True, clean_up_tokenization_spaces=True),\n+            \"Hello, world!\",\n+        )\n+\n+        # Test 3:\n+        # decode with unsupported kwargs\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.decode`.\"\n+        ):\n+            self.tokenizer.decode(tokens_ids, skip_special_tokens=False, unk_args=\"\")\n+\n+    def test_batch_decode(self):\n+        string = \"Hello, world!\"\n+        string_with_space = \"Hello, world !\"\n+\n+        batch_tokens_ids = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=True, eos=True),\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string_with_space, bos=True, eos=True),\n+        ]\n+\n+        # Test 1:\n+        # batch_decode with and without skip_special_tokens\n+        self.assertEqual(\n+            self.tokenizer.batch_decode(batch_tokens_ids, skip_special_tokens=True),\n+            [string, string_with_space],\n+        )\n+        self.assertEqual(\n+            self.tokenizer.batch_decode(batch_tokens_ids, skip_special_tokens=False),\n+            [\"<s>\" + string + \"</s>\", \"<s>\" + string_with_space + \"</s>\"],\n+        )\n+        self.assertEqual(\n+            self.tokenizer.batch_decode(batch_tokens_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True),\n+            [\"Hello, world!\", \"Hello, world!\"],\n+        )\n+\n+        # Test 2:\n+        # batch_decode with unsupported kwargs\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.batch_decode`.\"\n+        ):\n+            self.tokenizer.batch_decode(batch_tokens_ids, skip_special_tokens=False, unk_args=\"\")\n+\n+    def test_convert_ids_to_tokens(self):\n+        # Test 1:\n+        # with skip_special_tokens=False\n+        ids = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(\"Hello world!\", bos=True, eos=True)\n+        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.id_to_piece(id) for id in ids]\n+\n+        tokens = self.tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=False)\n+        self.assertEqual(tokens, expected_tokens)\n+\n+        token = self.tokenizer.convert_ids_to_tokens(ids[0], skip_special_tokens=False)\n+        self.assertEqual(token, expected_tokens[0])\n+\n+        # Test 2:\n+        # with skip_special_tokens=True\n+        expected_tokens = expected_tokens[1:-1]\n+        tokens = self.tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n+        self.assertEqual(tokens, expected_tokens)\n+\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.convert_ids_to_tokens(ids[0], skip_special_tokens=True)\n+        token = self.tokenizer.convert_ids_to_tokens(ids[1], skip_special_tokens=True)\n+        self.assertEqual(token, expected_tokens[0])\n+\n+    def test_convert_tokens_to_ids(self):\n+        tokens = [\"Hello\", \"world\", \"!\"]\n+        expected_ids = [self._ref_piece_to_id(token) for token in tokens]\n+        # Test 1:\n+        # list of tokens\n+        ids = self.tokenizer.convert_tokens_to_ids(tokens)\n+        self.assertEqual(ids, expected_ids)\n+\n+        # Test 2:\n+        # single token\n+        id = self.tokenizer.convert_tokens_to_ids(tokens[0])\n+        self.assertEqual(id, expected_ids[0])\n+        self.assertEqual(id, self.tokenizer.convert_tokens_to_ids(tokens[0]))\n+\n+    def test_tokenize(self):\n+        string = \"Hello world!\"\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.id_to_piece(id)\n+            for id in self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(string, bos=False, eos=False)\n+        ]\n+        tokens = self.tokenizer.tokenize(string)\n+        self.assertEqual(tokens, expected_tokens)\n+\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [add_special_tokens] are not supported by `MistralCommonTokenizer.tokenize`.\"\n+        ):\n+            self.tokenizer.tokenize(string, add_special_tokens=True)\n+\n+    def test_get_special_tokens_mask(self):\n+        # Test 1:\n+        # with skip_special_tokens=False\n+        ids = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(\"Hello world!\", bos=True, eos=True)\n+        expected_mask = [1 if id in self.ref_special_ids else 0 for id in ids]\n+\n+        mask = self.tokenizer.get_special_tokens_mask(ids)\n+        self.assertEqual(mask, expected_mask)\n+\n+        # Test 2:\n+        # already_has_special_tokens=True should raise an error\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.get_special_tokens_mask(ids, already_has_special_tokens=True)\n+\n+        # Test 3:\n+        # token_ids_1 not None should raise an error\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.get_special_tokens_mask(ids, token_ids_1=ids)\n+\n+    def test_pad_batch_encoding_input(self):\n+        # Test 1:\n+        # padding and default values\n+\n+        def get_batch_encoding():\n+            return self.tokenizer(\"Hello world!\", return_special_tokens_mask=True)\n+\n+        batch_encoding = get_batch_encoding()\n+\n+        for padding in [\n+            False,\n+            True,\n+            \"longest\",\n+            \"max_length\",\n+            \"do_not_pad\",\n+            PaddingStrategy.LONGEST,\n+            PaddingStrategy.MAX_LENGTH,\n+            PaddingStrategy.DO_NOT_PAD,\n+        ]:\n+            padded_batch_encoding = self.tokenizer.pad(get_batch_encoding(), padding=padding)\n+            self.assertEqual(padded_batch_encoding, batch_encoding)\n+\n+        # Test 2:\n+        # padding_strategy=\"max_length\" or PaddingStrategy.MAX_LENGTH and max_length\n+        for padding in [\"max_length\", PaddingStrategy.MAX_LENGTH]:\n+            padded_batch_encoding = self.tokenizer.pad(get_batch_encoding(), padding=padding, max_length=12)\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (12 - len(batch_encoding[\"input_ids\"]))\n+                + batch_encoding[\"input_ids\"],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                [0] * (12 - len(batch_encoding[\"input_ids\"])) + batch_encoding[\"attention_mask\"],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                [1] * (12 - len(batch_encoding[\"input_ids\"])) + batch_encoding[\"special_tokens_mask\"],\n+            )\n+\n+        # Test 3:\n+        # padding_strategy=True or \"longest\" or PaddingStrategy.LONGEST or \"max_length\" or PaddingStrategy.MAX_LENGTH and pad_to_multiple_of 16\n+        for padding in [True, \"longest\", PaddingStrategy.LONGEST]:\n+            padded_batch_encoding = self.tokenizer.pad(get_batch_encoding(), padding=padding, pad_to_multiple_of=16)\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (16 - len(batch_encoding[\"input_ids\"]))\n+                + batch_encoding[\"input_ids\"],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                [0] * (16 - len(batch_encoding[\"input_ids\"])) + batch_encoding[\"attention_mask\"],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                [1] * (16 - len(batch_encoding[\"input_ids\"])) + batch_encoding[\"special_tokens_mask\"],\n+            )\n+\n+        # Test 4:\n+        # padding_side=\"right\"\n+        right_tokenizer = MistralCommonTokenizer.from_pretrained(\n+            \"hf-internal-testing/namespace-mistralai-repo_name-Mistral-Small-3.1-24B-Instruct-2503\",\n+            padding_side=\"right\",\n+        )\n+        right_paddings = [\n+            right_tokenizer.pad(get_batch_encoding(), padding=\"max_length\", max_length=12),\n+            self.tokenizer.pad(get_batch_encoding(), padding=\"max_length\", max_length=12, padding_side=\"right\"),\n+        ]\n+        for padded_batch_encoding in right_paddings:\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                batch_encoding[\"input_ids\"]\n+                + [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (12 - len(batch_encoding[\"input_ids\"])),\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                batch_encoding[\"attention_mask\"] + [0] * (12 - len(batch_encoding[\"input_ids\"])),\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                batch_encoding[\"special_tokens_mask\"] + [1] * (12 - len(batch_encoding[\"input_ids\"])),\n+            )\n+\n+        # Test 5:\n+        # return_attention_mask=False\n+        padded_batch_encoding = self.tokenizer.pad(\n+            get_batch_encoding(), padding=\"max_length\", max_length=12, return_attention_mask=False\n+        )\n+        self.assertEqual(\n+            padded_batch_encoding[\"input_ids\"],\n+            [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (12 - len(batch_encoding[\"input_ids\"]))\n+            + batch_encoding[\"input_ids\"],\n+        )\n+        self.assertEqual(padded_batch_encoding[\"attention_mask\"], batch_encoding[\"attention_mask\"])\n+        self.assertEqual(\n+            padded_batch_encoding[\"special_tokens_mask\"],\n+            [1] * (12 - len(batch_encoding[\"input_ids\"])) + batch_encoding[\"special_tokens_mask\"],\n+        )\n+\n+        # Test 6:\n+        # return_tensors=\"pt\" or \"np\"\n+        for return_tensors in [\"pt\", \"np\"]:\n+            padded_batch_encoding = self.tokenizer.pad(\n+                get_batch_encoding(), padding=\"max_length\", max_length=12, return_tensors=return_tensors\n+            )\n+            self.assertEqual(padded_batch_encoding[\"input_ids\"].shape, torch.Size((12,)))\n+            self.assertEqual(padded_batch_encoding[\"attention_mask\"].shape, torch.Size((12,)))\n+            self.assertEqual(padded_batch_encoding[\"special_tokens_mask\"].shape, torch.Size((12,)))\n+\n+    def test_list_batch_encoding_input(self):\n+        def get_batch_encoding():\n+            return self.tokenizer([\"Hello world!\", \"Hello world! Longer sentence.\"], return_special_tokens_mask=True)\n+\n+        # Test 1:\n+        # padding=True or \"longest\" or PaddingStrategy.LONGEST\n+        batch_encoding = get_batch_encoding()\n+        for padding in [\n+            True,\n+            \"longest\",\n+            PaddingStrategy.LONGEST,\n+        ]:\n+            padded_batch_encoding = self.tokenizer.pad(get_batch_encoding(), padding=padding)\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                [\n+                    [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (len(batch_encoding[\"input_ids\"][1]) - len(batch_encoding[\"input_ids\"][0]))\n+                    + batch_encoding[\"input_ids\"][0],\n+                    batch_encoding[\"input_ids\"][1],\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                [\n+                    [0] * (len(batch_encoding[\"input_ids\"][1]) - len(batch_encoding[\"input_ids\"][0]))\n+                    + batch_encoding[\"attention_mask\"][0],\n+                    batch_encoding[\"attention_mask\"][1],\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                [\n+                    [1] * (len(batch_encoding[\"input_ids\"][1]) - len(batch_encoding[\"input_ids\"][0]))\n+                    + batch_encoding[\"special_tokens_mask\"][0],\n+                    batch_encoding[\"special_tokens_mask\"][1],\n+                ],\n+            )\n+\n+        # Test 2:\n+        # padding_strategy=\"max_length\" or PaddingStrategy.MAX_LENGTH and max_length\n+        for padding in [\"max_length\", PaddingStrategy.MAX_LENGTH]:\n+            padded_batch_encoding = self.tokenizer.pad(get_batch_encoding(), padding=padding, max_length=12)\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                [\n+                    [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (12 - len(batch_encoding[\"input_ids\"][0]))\n+                    + batch_encoding[\"input_ids\"][0],\n+                    [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (12 - len(batch_encoding[\"input_ids\"][1]))\n+                    + batch_encoding[\"input_ids\"][1],\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                [\n+                    [0] * (12 - len(batch_encoding[\"input_ids\"][0])) + batch_encoding[\"attention_mask\"][0],\n+                    [0] * (12 - len(batch_encoding[\"input_ids\"][1])) + batch_encoding[\"attention_mask\"][1],\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                [\n+                    [1] * (12 - len(batch_encoding[\"input_ids\"][0])) + batch_encoding[\"special_tokens_mask\"][0],\n+                    [1] * (12 - len(batch_encoding[\"input_ids\"][1])) + batch_encoding[\"special_tokens_mask\"][1],\n+                ],\n+            )\n+\n+        # Test 3:\n+        # padding_strategy=True or \"longest\" or PaddingStrategy.LONGEST or \"max_length\" or PaddingStrategy.MAX_LENGTH and pad_to_multiple_of 16\n+        for padding in [True, \"longest\", PaddingStrategy.LONGEST]:\n+            padded_batch_encoding = self.tokenizer.pad(get_batch_encoding(), padding=padding, pad_to_multiple_of=16)\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                [\n+                    [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (16 - len(batch_encoding[\"input_ids\"][0]))\n+                    + batch_encoding[\"input_ids\"][0],\n+                    [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (16 - len(batch_encoding[\"input_ids\"][1]))\n+                    + batch_encoding[\"input_ids\"][1],\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                [\n+                    [0] * (16 - len(batch_encoding[\"input_ids\"][0])) + batch_encoding[\"attention_mask\"][0],\n+                    [0] * (16 - len(batch_encoding[\"input_ids\"][1])) + batch_encoding[\"attention_mask\"][1],\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                [\n+                    [1] * (16 - len(batch_encoding[\"input_ids\"][0])) + batch_encoding[\"special_tokens_mask\"][0],\n+                    [1] * (16 - len(batch_encoding[\"input_ids\"][1])) + batch_encoding[\"special_tokens_mask\"][1],\n+                ],\n+            )\n+\n+        # Test 4:\n+        # padding_side=\"right\"\n+        right_tokenizer = MistralCommonTokenizer.from_pretrained(\n+            \"hf-internal-testing/namespace-mistralai-repo_name-Mistral-Small-3.1-24B-Instruct-2503\",\n+            padding_side=\"right\",\n+        )\n+        right_paddings = [\n+            right_tokenizer.pad(get_batch_encoding(), padding=\"max_length\", max_length=12),\n+            self.tokenizer.pad(get_batch_encoding(), padding=\"max_length\", max_length=12, padding_side=\"right\"),\n+        ]\n+        for padded_batch_encoding in right_paddings:\n+            self.assertEqual(\n+                padded_batch_encoding[\"input_ids\"],\n+                [\n+                    batch_encoding[\"input_ids\"][0]\n+                    + [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (12 - len(batch_encoding[\"input_ids\"][0])),\n+                    batch_encoding[\"input_ids\"][1]\n+                    + [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    * (12 - len(batch_encoding[\"input_ids\"][1])),\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"attention_mask\"],\n+                [\n+                    batch_encoding[\"attention_mask\"][0] + [0] * (12 - len(batch_encoding[\"input_ids\"][0])),\n+                    batch_encoding[\"attention_mask\"][1] + [0] * (12 - len(batch_encoding[\"input_ids\"][1])),\n+                ],\n+            )\n+            self.assertEqual(\n+                padded_batch_encoding[\"special_tokens_mask\"],\n+                [\n+                    batch_encoding[\"special_tokens_mask\"][0] + [1] * (12 - len(batch_encoding[\"input_ids\"][0])),\n+                    batch_encoding[\"special_tokens_mask\"][1] + [1] * (12 - len(batch_encoding[\"input_ids\"][1])),\n+                ],\n+            )\n+\n+        # Test 5:\n+        # return_attention_mask=False\n+        padded_batch_encoding = self.tokenizer.pad(\n+            get_batch_encoding(), padding=\"max_length\", max_length=12, return_attention_mask=False\n+        )\n+        self.assertEqual(\n+            padded_batch_encoding[\"input_ids\"],\n+            [\n+                [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (12 - len(batch_encoding[\"input_ids\"][0]))\n+                + batch_encoding[\"input_ids\"][0],\n+                [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id] * (12 - len(batch_encoding[\"input_ids\"][1]))\n+                + batch_encoding[\"input_ids\"][1],\n+            ],\n+        )\n+        self.assertEqual(padded_batch_encoding[\"attention_mask\"], batch_encoding[\"attention_mask\"])\n+        self.assertEqual(\n+            padded_batch_encoding[\"special_tokens_mask\"],\n+            [\n+                [1] * (12 - len(batch_encoding[\"input_ids\"][0])) + batch_encoding[\"special_tokens_mask\"][0],\n+                [1] * (12 - len(batch_encoding[\"input_ids\"][1])) + batch_encoding[\"special_tokens_mask\"][1],\n+            ],\n+        )\n+\n+        # Test 6:\n+        # return_tensors=\"pt\" or \"np\"\n+        for return_tensors in [\"pt\", \"np\"]:\n+            padded_batch_encoding = self.tokenizer.pad(\n+                get_batch_encoding(), padding=\"max_length\", max_length=12, return_tensors=return_tensors\n+            )\n+            self.assertEqual(padded_batch_encoding[\"input_ids\"].shape, torch.Size((2, 12)))\n+            self.assertEqual(padded_batch_encoding[\"attention_mask\"].shape, torch.Size((2, 12)))\n+            self.assertEqual(padded_batch_encoding[\"special_tokens_mask\"].shape, torch.Size((2, 12)))\n+\n+    def test_truncate_sequences(self):\n+        # Test 1:\n+        # truncation_strategy=\"longest_first\" or TruncationStrategy.LONGEST_FIRST\n+        text = \"Hello world!\"\n+        ids = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+        for truncation in [\"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+            for num_tokens_to_remove in [0, 2]:\n+                tokens, none, overflowing_tokens = self.tokenizer.truncate_sequences(\n+                    ids, truncation_strategy=truncation, num_tokens_to_remove=num_tokens_to_remove\n+                )\n+                self.assertEqual(tokens, ids[:-num_tokens_to_remove] if num_tokens_to_remove > 0 else ids)\n+                self.assertIsNone(none)\n+                self.assertEqual(overflowing_tokens, ids[-num_tokens_to_remove:] if num_tokens_to_remove > 0 else [])\n+\n+        # Test 2:\n+        # truncation_strategy=\"only_first\" or \"only_second\" or TruncationStrategy.ONLY_FIRST or TruncationStrategy.ONLY_SECOND\n+        # Should raise a ValueError\n+        for truncation in [\"only_first\", \"only_second\", TruncationStrategy.ONLY_FIRST, TruncationStrategy.ONLY_SECOND]:\n+            with self.assertRaises(ValueError):\n+                self.tokenizer.truncate_sequences(ids, truncation_strategy=truncation, num_tokens_to_remove=1)\n+\n+        # Test 3:\n+        # truncation_strategy=\"do_not_truncate\" or TruncationStrategy.DO_NOT_TRUNCATE\n+        for truncation in [\"do_not_truncate\", TruncationStrategy.DO_NOT_TRUNCATE]:\n+            tokens, none, overflowing_tokens = self.tokenizer.truncate_sequences(\n+                ids, truncation_strategy=truncation, num_tokens_to_remove=1\n+            )\n+            self.assertEqual(tokens, ids)\n+            self.assertIsNone(none)\n+            self.assertEqual(overflowing_tokens, [])\n+\n+        # Test 4:\n+        # pair_ids is not None\n+        # Should raise a ValueError\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.truncate_sequences(\n+                ids, pair_ids=ids, truncation_strategy=\"longest_first\", num_tokens_to_remove=1\n+            )\n+\n+        # Test 5:\n+        # stride\n+        for stride in [0, 2]:\n+            tokens, none, overflowing_tokens = self.tokenizer.truncate_sequences(\n+                ids, truncation_strategy=\"longest_first\", num_tokens_to_remove=2, stride=stride\n+            )\n+            self.assertEqual(tokens, ids[:-2])\n+            self.assertIsNone(none)\n+            self.assertEqual(overflowing_tokens, ids[-2 - stride :])\n+\n+        # Test 6:\n+        # truncation_side=\"left\"\n+        left_tokenizer = MistralCommonTokenizer.from_pretrained(\n+            \"hf-internal-testing/namespace-mistralai-repo_name-Mistral-Small-3.1-24B-Instruct-2503\",\n+            truncation_side=\"left\",\n+        )\n+        tokens, none, overflowing_tokens = left_tokenizer.truncate_sequences(\n+            ids, truncation_strategy=\"longest_first\", num_tokens_to_remove=2\n+        )\n+        self.assertEqual(tokens, ids[2:])\n+        self.assertIsNone(none)\n+        self.assertEqual(overflowing_tokens, ids[:2])\n+\n+    def test_apply_chat_template_basic(self):\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Hi!\"},\n+            {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+        ]\n+\n+        expected_tokenized = self.ref_tokenizer.encode_chat_completion(ChatCompletionRequest.from_openai(conversation))\n+\n+        # Test 1:\n+        # with tokenize\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=False),\n+            expected_tokenized.text,\n+        )\n+\n+        # Test 2:\n+        # without tokenize\n+        self.assertEqual(self.tokenizer.apply_chat_template(conversation, tokenize=True), expected_tokenized.tokens)\n+\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.apply_chat_template`.\"\n+        ):\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, unk_args=\"\")\n+\n+    def test_apply_chat_template_continue_final_message(self):\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Hi!\"},\n+            {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+            {\"role\": \"assistant\", \"content\": \"Paris\"},\n+        ]\n+\n+        expected_tokenized = self.ref_tokenizer.encode_chat_completion(\n+            ChatCompletionRequest.from_openai(conversation, continue_final_message=True)\n+        )\n+\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=False, continue_final_message=True),\n+            expected_tokenized.text,\n+        )\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True),\n+            expected_tokenized.tokens,\n+        )\n+\n+        with self.assertRaises(InvalidMessageStructureException):\n+            self.tokenizer.apply_chat_template(conversation, tokenize=False, continue_final_message=False)\n+\n+    def test_apply_chat_template_with_tools(self):\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Hi!\"},\n+            {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+            {\"role\": \"user\", \"content\": \"What is the temperature in Paris?\"},\n+            {\n+                \"role\": \"assistant\",\n+                \"tool_calls\": [\n+                    {\n+                        \"id\": \"azerty123\",\n+                        \"function\": {\n+                            \"name\": \"get_current_weather\",\n+                            \"arguments\": {\"location\": \"Paris\", \"format\": \"text\", \"unit\": \"celsius\"},\n+                        },\n+                    }\n+                ],\n+            },\n+            {\"role\": \"tool\", \"name\": \"get_current_weather\", \"content\": \"22\", \"tool_call_id\": \"azerty123\"},\n+        ]\n+        tools = [\n+            {\n+                \"type\": \"function\",\n+                \"function\": {\n+                    \"name\": \"get_current_weather\",\n+                    \"description\": \"Get the current weather in a given location\",\n+                    \"parameters\": {\n+                        \"type\": \"object\",\n+                        \"properties\": {\n+                            \"location\": {\n+                                \"type\": \"string\",\n+                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n+                                \"required\": [\"location\"],\n+                            },\n+                            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n+                            \"format\": {\n+                                \"type\": \"string\",\n+                                \"enum\": [\"text\", \"json\"],\n+                                \"description\": \"The format of the response\",\n+                                \"required\": [\"format\"],\n+                            },\n+                        },\n+                    },\n+                },\n+            }\n+        ]\n+\n+        expected_tokenized = self.ref_tokenizer.encode_chat_completion(\n+            ChatCompletionRequest.from_openai(conversation, tools)\n+        )\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tools=tools, tokenize=False),\n+            expected_tokenized.text,\n+        )\n+\n+    def test_apply_chat_template_with_image(self):\n+        ref_conversation = conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What is this?\"},\n+                    {\n+                        \"type\": \"image_url\",\n+                        \"image_url\": {\"url\": IMG_URL},\n+                    },\n+                ],\n+            },\n+        ]\n+\n+        expected_tokenized = self.ref_tokenizer.encode_chat_completion(\n+            ChatCompletionRequest.from_openai(ref_conversation)\n+        )\n+        image_contents = [\n+            {\n+                \"type\": \"image_url\",\n+                \"image_url\": {\"url\": IMG_URL},\n+            },\n+            {\n+                \"type\": \"image\",\n+                \"url\": IMG_URL,\n+            },\n+            {\"type\": \"image\", \"base64\": IMG_BASE_64},\n+        ]\n+        for image_content in image_contents:\n+            conversation = [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [{\"type\": \"text\", \"text\": \"What is this?\"}, image_content],\n+                },\n+            ]\n+\n+            output = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n+            self.assertEqual(output, expected_tokenized.tokens)\n+\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=True)\n+        self.assertEqual(output_dict[\"input_ids\"], expected_tokenized.tokens)\n+        self.assertEqual(len(output_dict[\"pixel_values\"]), len(expected_tokenized.images))\n+        for o, e in zip(output_dict[\"pixel_values\"], expected_tokenized.images):\n+            self.assertTrue(np.allclose(o, e))\n+\n+        output_dict = self.tokenizer.apply_chat_template(\n+            conversation, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        self.assertEqual(output_dict[\"input_ids\"].tolist()[0], expected_tokenized.tokens)\n+        self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], torch.tensor(expected_tokenized.images)))\n+\n+    def test_appsly_chat_template_with_truncation(self):\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\"role\": \"user\", \"content\": \"Hi!\"},\n+            {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+            {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n+        ]\n+\n+        expected_tokenized = self.ref_tokenizer.encode_chat_completion(ChatCompletionRequest.from_openai(conversation))\n+\n+        # Test 1:\n+        # with truncation\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20),\n+            expected_tokenized.tokens[:20],\n+        )\n+\n+        # Test 2:\n+        # without truncation\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20),\n+            expected_tokenized.tokens,\n+        )\n+\n+        # Test 3:\n+        # assert truncation is boolean\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.apply_chat_template(\n+                conversation, tokenize=True, truncation=TruncationStrategy.LONGEST_FIRST, max_length=20\n+            )\n+\n+    def test_batch_apply_chat_template(self):\n+        conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+                {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is this?\"},\n+                        {\n+                            \"type\": \"image_url\",\n+                            \"image_url\": {\"url\": IMG_URL},\n+                        },\n+                    ],\n+                },\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+                {\"role\": \"assistant\", \"content\": \"Hello! How can I help you?\"},\n+                {\"role\": \"user\", \"content\": \"What is the temperature in Paris?\"},\n+                {\n+                    \"role\": \"assistant\",\n+                    \"tool_calls\": [\n+                        {\n+                            \"id\": \"azerty123\",\n+                            \"function\": {\n+                                \"name\": \"get_current_weather\",\n+                                \"arguments\": {\"location\": \"Paris\", \"format\": \"text\", \"unit\": \"celsius\"},\n+                            },\n+                        }\n+                    ],\n+                },\n+                {\"role\": \"tool\", \"name\": \"get_current_weather\", \"content\": \"22\", \"tool_call_id\": \"azerty123\"},\n+            ],\n+        ]\n+\n+        tools = [\n+            {\n+                \"type\": \"function\",\n+                \"function\": {\n+                    \"name\": \"get_current_weather\",\n+                    \"description\": \"Get the current weather in a given location\",\n+                    \"parameters\": {\n+                        \"type\": \"object\",\n+                        \"properties\": {\n+                            \"location\": {\n+                                \"type\": \"string\",\n+                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n+                                \"required\": [\"location\"],\n+                            },\n+                            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n+                            \"format\": {\n+                                \"type\": \"string\",\n+                                \"enum\": [\"text\", \"json\"],\n+                                \"description\": \"The format of the response\",\n+                                \"required\": [\"format\"],\n+                            },\n+                        },\n+                    },\n+                },\n+            }\n+        ]\n+\n+        expected_tokenized = [\n+            self.ref_tokenizer.encode_chat_completion(ChatCompletionRequest.from_openai(conversation, tools=tools))\n+            for conversation in conversations\n+        ]\n+\n+        text_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=False)\n+        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True)\n+\n+        self.assertEqual(len(text_outputs), len(token_outputs))\n+        self.assertEqual(len(text_outputs), len(expected_tokenized))\n+        for text, token, expected in zip(text_outputs, token_outputs, expected_tokenized):\n+            self.assertEqual(text, expected.text)\n+            self.assertEqual(token, expected.tokens)\n+\n+        with self.assertRaises(\n+            ValueError,\n+            msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.batch_apply_chat_template`.\",\n+        ):\n+            self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True, unk_args=\"\")\n+\n+    def test_batch_apply_images(self):\n+        conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is this?\"},\n+                        {\n+                            \"type\": \"image_url\",\n+                            \"image_url\": {\"url\": IMG_URL},\n+                        },\n+                    ],\n+                },\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is this?\"},\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": IMG_URL,\n+                        },\n+                    ],\n+                },\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is this?\"},\n+                        {\"type\": \"image\", \"base64\": IMG_BASE_64},\n+                    ],\n+                },\n+            ],\n+        ]\n+\n+        ref_conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What is this?\"},\n+                    {\n+                        \"type\": \"image_url\",\n+                        \"image_url\": {\"url\": IMG_URL},\n+                    },\n+                ],\n+            },\n+        ]\n+\n+        expected_tokenized = self.ref_tokenizer.encode_chat_completion(\n+            ChatCompletionRequest.from_openai(ref_conversation)\n+        )\n+\n+        output = self.tokenizer.apply_chat_template(conversations, tokenize=True)\n+        self.assertEqual(output, [expected_tokenized.tokens] * 3)\n+\n+        output = self.tokenizer.apply_chat_template(conversations, tokenize=True, return_dict=True)\n+        self.assertEqual(output[\"input_ids\"], [expected_tokenized.tokens] * 3)\n+        self.assertEqual(len(output[\"pixel_values\"]), len(expected_tokenized.images) * 3)\n+        for o, e in zip(output[\"pixel_values\"], [expected_tokenized.images] * 3):\n+            self.assertTrue(np.allclose(o, e))\n+\n+        output = self.tokenizer.apply_chat_template(\n+            conversations, tokenize=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        self.assertEqual(output[\"input_ids\"].tolist(), [expected_tokenized.tokens] * 3)\n+        self.assertEqual(output[\"input_ids\"].shape[0], len(expected_tokenized.images) * 3)\n+        self.assertTrue(torch.allclose(output[\"pixel_values\"], torch.tensor([expected_tokenized.images] * 3)))\n+\n+        output = self.tokenizer.apply_chat_template(\n+            conversations, tokenize=True, return_dict=True, return_tensors=\"np\"\n+        )\n+        self.assertEqual(output[\"input_ids\"].tolist(), [expected_tokenized.tokens] * 3)\n+        self.assertTrue(np.allclose(output[\"pixel_values\"], np.array([expected_tokenized.images] * 3)))\n+\n+    def test_batch_apply_chat_template_with_continue_final_message(self):\n+        conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+                {\"role\": \"assistant\", \"content\": \"Hello! How can \"},\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                {\"role\": \"user\", \"content\": \"Hi!\"},\n+                {\"role\": \"assistant\", \"content\": \"Hello! How can I help you? Ou prÃ©fÃ©rez vous \"},\n+            ],\n+        ]\n+\n+        # Test 1:\n+        # with continue_final_message\n+        expected_tokenized = [\n+            self.ref_tokenizer.encode_chat_completion(\n+                ChatCompletionRequest.from_openai(conversation, continue_final_message=True)\n+            )\n+            for conversation in conversations\n+        ]\n+\n+        token_outputs = self.tokenizer.apply_chat_template(conversations, tokenize=True, continue_final_message=True)\n+\n+        for output, expected in zip(token_outputs, expected_tokenized):\n+            self.assertEqual(output, expected.tokens)\n+\n+        # Test 2:\n+        # without continue_final_message\n+        with self.assertRaises(InvalidMessageStructureException):\n+            self.tokenizer.apply_chat_template(\n+                conversations,\n+                tokenize=False,\n+                continue_final_message=False,\n+            )\n+\n+        # Test 3:\n+        # with continue_final_message and last role is not assistant\n+        with self.assertRaises(InvalidMessageStructureException):\n+            self.tokenizer.apply_chat_template(\n+                conversation=[\n+                    [\n+                        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n+                        {\"role\": \"user\", \"content\": \"Hi!\"},\n+                    ]\n+                ],\n+                tokenize=True,\n+                continue_final_message=True,\n+            )\n+\n+    def test_batch_apply_chat_template_with_truncation(\n+        self,\n+    ):\n+        # Test 1:\n+        # with truncation\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            self.fixture_conversations, tokenize=True, truncation=True, max_length=20\n+        )\n+\n+        for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+            self.assertEqual(output, expected.tokens[:20])\n+\n+        # Test 2:\n+        # without truncation\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            self.fixture_conversations, tokenize=True, truncation=False, max_length=20\n+        )\n+        self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+        for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+            self.assertEqual(output, expected.tokens)\n+\n+        # Test 3:\n+        # assert truncation is boolean\n+        with self.assertRaises(ValueError):\n+            self.tokenizer.apply_chat_template(\n+                self.fixture_conversations, tokenize=True, truncation=TruncationStrategy.LONGEST_FIRST, max_length=20\n+            )\n+\n+    def test_batch_apply_chat_template_with_padding(\n+        self,\n+    ):\n+        for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n+            if padding == PaddingStrategy.MAX_LENGTH:\n+                # No padding if no max length is provided\n+                token_outputs = self.tokenizer.apply_chat_template(self.fixture_conversations, padding=padding)\n+                self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+                for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+                    self.assertEqual(output, expected.tokens)\n+\n+            max_length = 20 if padding == PaddingStrategy.MAX_LENGTH else None\n+\n+            token_outputs = self.tokenizer.apply_chat_template(\n+                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length\n+            )\n+\n+            if padding != PaddingStrategy.MAX_LENGTH:\n+                longest = max(len(tokenized.tokens) for tokenized in self.tokenized_fixture_conversations)\n+                self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+                for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+                    self.assertEqual(\n+                        output,\n+                        [self.tokenizer.pad_token_id] * (longest - len(expected.tokens)) + expected.tokens,\n+                    )\n+            else:\n+                self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+                for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+                    if len(expected.tokens) < max_length:\n+                        self.assertEqual(\n+                            output,\n+                            [self.tokenizer.pad_token_id] * (20 - len(expected.tokens)) + expected.tokens,\n+                        )\n+                    else:\n+                        self.assertEqual(output, expected.tokens)\n+\n+        for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n+            token_outputs = self.tokenizer.apply_chat_template(\n+                self.fixture_conversations, tokenize=True, padding=padding\n+            )\n+            self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+            for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+                self.assertEqual(output, expected.tokens)\n+\n+    def test_batch_apply_chat_template_with_padding_and_truncation(\n+        self,\n+    ):\n+        max_length = 20\n+        for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n+            token_outputs = self.tokenizer.apply_chat_template(\n+                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+            )\n+            self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+            for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+                self.assertEqual(\n+                    output, [self.tokenizer.pad_token_id] * (20 - len(expected.tokens)) + expected.tokens[:20]\n+                )\n+        for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n+            token_outputs = self.tokenizer.apply_chat_template(\n+                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+            )\n+            self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n+            for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n+                self.assertEqual(output, expected.tokens[:20])\n+\n+    def test_batch_apply_chat_template_return_tensors(self):\n+        # Test 1:\n+        # with tokenize\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True\n+        )\n+        self.assertIsInstance(token_outputs, torch.Tensor)\n+        self.assertEqual(\n+            token_outputs.shape,\n+            (len(self.fixture_conversations), max(len(t.tokens) for t in self.tokenized_fixture_conversations)),\n+        )\n+\n+        # Test 2:\n+        # without tokenize, should ignore return_tensors\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True\n+        )\n+        self.assertEqual(token_outputs, [t.text for t in self.tokenized_fixture_conversations])\n+\n+    def test_batch_apply_chat_template_return_dict(self):\n+        # Test 1:\n+        # with tokenize\n+        token_outputs = self.tokenizer.apply_chat_template(self.fixture_conversations, tokenize=True, return_dict=True)\n+        self.assertIn(\"input_ids\", token_outputs)\n+        self.assertIn(\"attention_mask\", token_outputs)\n+        self.assertEqual(token_outputs[\"input_ids\"], [t.tokens for t in self.tokenized_fixture_conversations])\n+        self.assertEqual(\n+            token_outputs[\"attention_mask\"], [[1] * len(t.tokens) for t in self.tokenized_fixture_conversations]\n+        )\n+\n+        # Test 2:\n+        # without tokenize, should ignore return_dict\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            self.fixture_conversations, tokenize=False, return_dict=True\n+        )\n+        self.assertNotIsInstance(token_outputs, dict)\n+        self.assertEqual(token_outputs, [t.text for t in self.tokenized_fixture_conversations])\n+\n+    def test_call(self):\n+        # Test 1:\n+        # default case\n+        text = \"Hello world!\"\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+        tokens = self.tokenizer(text)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n+\n+        # Test 2:\n+        # return_attention_mask=False\n+        tokens = self.tokenizer(text, return_attention_mask=False)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertNotIn(\"attention_mask\", tokens)\n+\n+        # Test 3:\n+        # return_tensors=\"pt\"\n+        tokens = self.tokenizer(text, return_tensors=\"pt\")\n+        self.assertIsInstance(tokens[\"input_ids\"], torch.Tensor)\n+        self.assertTrue(torch.equal(tokens[\"input_ids\"], torch.Tensor(expected_tokens).unsqueeze(0)))\n+        self.assertIsInstance(tokens[\"attention_mask\"], torch.Tensor)\n+        self.assertTrue(torch.equal(tokens[\"attention_mask\"], torch.ones(1, len(expected_tokens))))\n+\n+        # Test 4:\n+        # return_special_tokens_mask=True\n+        tokens = self.tokenizer(text, return_special_tokens_mask=True)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+\n+        # Test 5:\n+        # add_special_tokens=False\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=False, eos=False)\n+        tokens = self.tokenizer(text, add_special_tokens=False, return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [0] * len(expected_tokens))\n+\n+        with self.assertRaises(\n+            ValueError, msg=\"Kwargs [wrong_kwarg] are not supported by `MistralCommonTokenizer.__call__`.\"\n+        ):\n+            self.tokenizer(text, wrong_kwarg=True)\n+\n+        with self.assertRaises(\n+            ValueError,\n+            msg=\"`text_pair`, `text_target` and `text_pair_target` are not supported by `MistralCommonTokenizer`.\",\n+        ):\n+            self.tokenizer(text, text_pair=\"Hello world!\")\n+        with self.assertRaises(\n+            ValueError,\n+            msg=\"`text_pair`, `text_target` and `text_pair_target` are not supported by `MistralCommonTokenizer`.\",\n+        ):\n+            self.tokenizer(text, text_target=\"Hello world!\")\n+        with self.assertRaises(\n+            ValueError,\n+            msg=\"`text_pair`, `text_target` and `text_pair_target` are not supported by `MistralCommonTokenizer`.\",\n+        ):\n+            self.tokenizer(text, text_pair_target=\"Hello world!\")\n+\n+    def test_call_with_truncation(self):\n+        # Test 1:\n+        # truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST\n+        text = \"Hello world!\" * 10\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+\n+        for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+            tokens = self.tokenizer(text, truncation=True, max_length=10, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            self.assertEqual(tokens[\"input_ids\"], expected_tokens[:10])\n+            self.assertEqual(tokens[\"attention_mask\"], [1] * 10)\n+            self.assertEqual(tokens[\"special_tokens_mask\"], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n+\n+        # Test 2:\n+        # truncation=False\n+        for truncation in [False, \"do_not_truncate\", TruncationStrategy.DO_NOT_TRUNCATE]:\n+            tokens = self.tokenizer(text, truncation=truncation, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+            self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n+            self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+\n+        # Test 3:\n+        # truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST with return_overflowing_tokens=True and stride\n+        for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+            for stride in [0, 2]:\n+                tokens = self.tokenizer(\n+                    text,\n+                    truncation=truncation,\n+                    max_length=10,\n+                    return_overflowing_tokens=True,\n+                    return_special_tokens_mask=True,\n+                    stride=stride,\n+                )\n+                self.assertIsInstance(tokens, BatchEncoding)\n+                self.assertEqual(tokens[\"input_ids\"], expected_tokens[:10])\n+                self.assertEqual(tokens[\"attention_mask\"], [1] * 10)\n+                self.assertEqual(tokens[\"special_tokens_mask\"], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n+                self.assertEqual(tokens[\"overflowing_tokens\"], expected_tokens[10 - stride :])\n+                self.assertEqual(tokens[\"num_truncated_tokens\"], len(expected_tokens) - 10)\n+\n+        # Test 4:\n+        # truncation=\"only_first\" or TruncationStrategy.ONLY_FIRST or \"only_second\" or TruncationStrategy.ONLY_SECOND\n+        # should raise an error\n+        for truncation in [\"only_first\", TruncationStrategy.ONLY_FIRST, \"only_second\", TruncationStrategy.ONLY_SECOND]:\n+            with self.assertRaises(\n+                ValueError,\n+                msg=\"Truncation strategy `only_first` and `only_second` are not supported by `MistralCommonTokenizer`.\",\n+            ):\n+                self.tokenizer(text, truncation=truncation)\n+\n+    def test_call_with_padding(self):\n+        text = \"Hello world!\"\n+        expected_tokens = self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(text, bos=True, eos=True)\n+\n+        # Test 1:\n+        # padding=False or padding=True or \"do_not_pad\" or PaddingStrategy.DO_NOT_PAD or padding=\"longest\" or PaddingStrategy.LONGEST\n+        for padding in [False, True, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD, \"longest\", PaddingStrategy.LONGEST]:\n+            tokens = self.tokenizer(text, padding=padding, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+            self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens))\n+            self.assertEqual(tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1])\n+\n+        # Test 2:\n+        # padding=\"max_length\" or PaddingStrategy.MAX_LENGTH\n+        for padding in [\"max_length\", PaddingStrategy.MAX_LENGTH]:\n+            tokens = self.tokenizer(text, padding=padding, max_length=20, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            num_padding = 20 - len(expected_tokens)\n+            self.assertEqual(tokens[\"input_ids\"], num_padding * [self.tokenizer.pad_token_id] + expected_tokens)\n+            self.assertEqual(tokens[\"attention_mask\"], num_padding * [0] + [1] * len(expected_tokens))\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"], num_padding * [1] + [1] + [0] * (len(expected_tokens) - 2) + [1]\n+            )\n+\n+        # Test 3:\n+        # pad_to_multiple_of\n+        tokens = self.tokenizer(\n+            text, padding=True, max_length=20, pad_to_multiple_of=16, return_special_tokens_mask=True\n+        )\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        num_padding = 16 - len(expected_tokens)\n+        self.assertEqual(tokens[\"input_ids\"], num_padding * [self.tokenizer.pad_token_id] + expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], num_padding * [0] + [1] * len(expected_tokens))\n+        self.assertEqual(\n+            tokens[\"special_tokens_mask\"], num_padding * [1] + [1] + [0] * (len(expected_tokens) - 2) + [1]\n+        )\n+\n+        # Test 4:\n+        # padding=\"max_length\" and padding_side=\"right\"\n+        tokens = self.tokenizer(\n+            text, padding=\"max_length\", max_length=20, padding_side=\"right\", return_special_tokens_mask=True\n+        )\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        num_padding = 20 - len(expected_tokens)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens + num_padding * [self.tokenizer.pad_token_id])\n+        self.assertEqual(tokens[\"attention_mask\"], [1] * len(expected_tokens) + num_padding * [0])\n+        self.assertEqual(\n+            tokens[\"special_tokens_mask\"], [1] + [0] * (len(expected_tokens) - 2) + [1] + num_padding * [1]\n+        )\n+\n+    def test_batch_call(self):\n+        # Test 1:\n+        # default case\n+        text = [\"Hello world!\", \"Hello world! Longer\"]\n+        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        tokens = self.tokenizer(text)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n+\n+        # Test 2:\n+        # return_attention_mask=False\n+        tokens = self.tokenizer(text, return_attention_mask=False)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertNotIn(\"attention_mask\", tokens)\n+\n+        # Test 3:\n+        # return_tensors=\"pt\"\n+        tokens = self.tokenizer(text, return_tensors=\"pt\", padding=\"longest\", return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens[\"input_ids\"], torch.Tensor)\n+        self.assertEqual(tokens[\"input_ids\"].shape, torch.Size([2, len(expected_tokens[1])]))\n+        self.assertTrue(\n+            torch.equal(\n+                tokens[\"input_ids\"][0],\n+                torch.Tensor(\n+                    (len(expected_tokens[1]) - len(expected_tokens[0]))\n+                    * [self.ref_tokenizer.instruct_tokenizer.tokenizer.pad_id]\n+                    + expected_tokens[0]\n+                ),\n+            )\n+        )\n+        self.assertIsInstance(tokens[\"attention_mask\"], torch.Tensor)\n+        self.assertEqual(tokens[\"attention_mask\"].shape, torch.Size([2, len(expected_tokens[1])]))\n+        self.assertTrue(\n+            torch.equal(\n+                tokens[\"attention_mask\"][0],\n+                torch.Tensor(\n+                    [0] * (len(expected_tokens[1]) - len(expected_tokens[0])) + [1] * len(expected_tokens[0])\n+                ),\n+            )\n+        )\n+        self.assertTrue(torch.equal(tokens[\"attention_mask\"][1], torch.Tensor([1] * len(expected_tokens[1]))))\n+        self.assertIsInstance(tokens[\"special_tokens_mask\"], torch.Tensor)\n+        self.assertEqual(tokens[\"special_tokens_mask\"].shape, torch.Size([2, len(expected_tokens[1])]))\n+        self.assertTrue(\n+            torch.equal(\n+                tokens[\"special_tokens_mask\"][0],\n+                torch.Tensor(\n+                    (len(expected_tokens[1]) - len(expected_tokens[0])) * [1]\n+                    + [1]\n+                    + [0] * (len(expected_tokens[0]) - 2)\n+                    + [1]\n+                ),\n+            )\n+        )\n+        self.assertTrue(\n+            torch.equal(\n+                tokens[\"special_tokens_mask\"][1], torch.Tensor([1] + [0] * (len(expected_tokens[1]) - 2) + [1])\n+            )\n+        )\n+\n+        # Test 4:\n+        # add_special_tokens=False\n+        expected_tokens = [\n+            self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=False, eos=False) for t in text\n+        ]\n+        tokens = self.tokenizer(text, add_special_tokens=False, return_special_tokens_mask=True)\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+        self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n+        self.assertEqual(tokens[\"special_tokens_mask\"], [[0] * len(t) for t in expected_tokens])\n+\n+    def test_batch_call_with_truncation(self):\n+        # Test 1:\n+        # truncation=True\n+        text = [\"Hello world!\", \"Hello world! Longer\" * 10]\n+        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+\n+        for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+            tokens = self.tokenizer(text, truncation=True, max_length=10, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            self.assertEqual(tokens[\"input_ids\"], [expected_tokens[0][:10], expected_tokens[1][:10]])\n+            self.assertEqual(tokens[\"attention_mask\"], [[1] * min(len(t), 10) for t in expected_tokens])\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"],\n+                [[1 if id in self.ref_special_ids else 0 for id in ids[:10]] for ids in expected_tokens],\n+            )\n+\n+        # Test 2:\n+        # truncation=False\n+        for truncation in [False, \"do_not_truncate\", TruncationStrategy.DO_NOT_TRUNCATE]:\n+            tokens = self.tokenizer(text, truncation=truncation, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+            self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"],\n+                [[1] + [0] * (len(t) - 2) + [1] for t in expected_tokens],\n+            )\n+\n+        # Test 3:\n+        # truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST with return_overflowing_tokens=True and stride\n+\n+        for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+            for stride in [0, 2]:\n+                tokens = self.tokenizer(\n+                    text,\n+                    truncation=truncation,\n+                    max_length=10,\n+                    return_overflowing_tokens=True,\n+                    return_special_tokens_mask=True,\n+                    stride=stride,\n+                )\n+                self.assertIsInstance(tokens, BatchEncoding)\n+                self.assertEqual(tokens[\"input_ids\"], [expected_tokens[0][:10], expected_tokens[1][:10]])\n+                self.assertEqual(tokens[\"attention_mask\"], [[1] * min(len(t), 10) for t in expected_tokens])\n+                self.assertEqual(\n+                    tokens[\"overflowing_tokens\"],\n+                    [expected_tokens[0][10 - stride :], expected_tokens[1][10 - stride :]],\n+                )\n+                self.assertEqual(\n+                    tokens[\"num_truncated_tokens\"], [len(expected_tokens[0]) - 10, len(expected_tokens[1]) - 10]\n+                )\n+                self.assertEqual(\n+                    tokens[\"special_tokens_mask\"],\n+                    [[1 if id in self.ref_special_ids else 0 for id in ids[:10]] for ids in expected_tokens],\n+                )\n+\n+    def test_batch_call_with_padding(self):\n+        # Test 1:\n+        # padding=False or padding=True or \"do_not_pad\" or PaddingStrategy.DO_NOT_PAD or padding=\"longest\" or PaddingStrategy.LONGEST\n+        text = [\"Hello world!\", \"Hello world! Longer\"]\n+        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n+            tokens = self.tokenizer(text, padding=padding, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            self.assertEqual(tokens[\"input_ids\"], expected_tokens)\n+            self.assertEqual(tokens[\"attention_mask\"], [[1] * len(t) for t in expected_tokens])\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"],\n+                [[1] + [0] * (len(t) - 2) + [1] for t in expected_tokens],\n+            )\n+\n+        # Test 2:\n+        # padding=\"max_length\" or PaddingStrategy.MAX_LENGTH\n+        for padding in [\"max_length\", PaddingStrategy.MAX_LENGTH]:\n+            tokens = self.tokenizer(text, padding=padding, max_length=20, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            num_padding = [20 - len(t) for t in expected_tokens]\n+            self.assertEqual(\n+                tokens[\"input_ids\"],\n+                [\n+                    num_padding[0] * [self.tokenizer.pad_token_id] + expected_tokens[0],\n+                    num_padding[1] * [self.tokenizer.pad_token_id] + expected_tokens[1],\n+                ],\n+            )\n+            self.assertEqual(\n+                tokens[\"attention_mask\"],\n+                [\n+                    num_padding[0] * [0] + [1] * len(expected_tokens[0]),\n+                    num_padding[1] * [0] + [1] * len(expected_tokens[1]),\n+                ],\n+            )\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"],\n+                [\n+                    num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 2) + [1],\n+                    num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 2) + [1],\n+                ],\n+            )\n+\n+        # Test 3:\n+        # padding=True or \"longest\" or PaddingStrategy.LONGEST\n+        for padding in [True, \"longest\", PaddingStrategy.LONGEST]:\n+            tokens = self.tokenizer(text, padding=padding, return_special_tokens_mask=True)\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            num_padding = [len(expected_tokens[1]) - len(t) for t in expected_tokens]\n+            self.assertEqual(\n+                tokens[\"input_ids\"],\n+                [\n+                    num_padding[0] * [self.tokenizer.pad_token_id] + expected_tokens[0],\n+                    num_padding[1] * [self.tokenizer.pad_token_id] + expected_tokens[1],\n+                ],\n+            )\n+            self.assertEqual(\n+                tokens[\"attention_mask\"],\n+                [\n+                    num_padding[0] * [0] + [1] * len(expected_tokens[0]),\n+                    num_padding[1] * [0] + [1] * len(expected_tokens[1]),\n+                ],\n+            )\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"],\n+                [\n+                    num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 2) + [1],\n+                    num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 2) + [1],\n+                ],\n+            )\n+\n+        # Test 4:\n+        # pad_to_multiple_of\n+        tokens = self.tokenizer(\n+            text, padding=True, max_length=32, pad_to_multiple_of=16, return_special_tokens_mask=True\n+        )\n+        self.assertIsInstance(tokens, BatchEncoding)\n+        num_padding = [16 - len(t) for t in expected_tokens]\n+        self.assertEqual(\n+            tokens[\"input_ids\"],\n+            [\n+                num_padding[0] * [self.tokenizer.pad_token_id] + expected_tokens[0],\n+                num_padding[1] * [self.tokenizer.pad_token_id] + expected_tokens[1],\n+            ],\n+        )\n+        self.assertEqual(\n+            tokens[\"attention_mask\"],\n+            [\n+                num_padding[0] * [0] + [1] * len(expected_tokens[0]),\n+                num_padding[1] * [0] + [1] * len(expected_tokens[1]),\n+            ],\n+        )\n+        self.assertEqual(\n+            tokens[\"special_tokens_mask\"],\n+            [\n+                num_padding[0] * [1] + [1] + [0] * (len(expected_tokens[0]) - 2) + [1],\n+                num_padding[1] * [1] + [1] + [0] * (len(expected_tokens[1]) - 2) + [1],\n+            ],\n+        )\n+\n+        # Test 5:\n+        # padding=\"max_length\" or PaddingStrategy.MAX_LENGTH and padding_side=\"right\"\n+        for padding in [\"max_length\", PaddingStrategy.MAX_LENGTH]:\n+            tokens = self.tokenizer(\n+                text, padding=padding, max_length=20, padding_side=\"right\", return_special_tokens_mask=True\n+            )\n+            self.assertIsInstance(tokens, BatchEncoding)\n+            num_padding = [20 - len(t) for t in expected_tokens]\n+            self.assertEqual(\n+                tokens[\"input_ids\"],\n+                [\n+                    expected_tokens[0] + num_padding[0] * [self.tokenizer.pad_token_id],\n+                    expected_tokens[1] + num_padding[1] * [self.tokenizer.pad_token_id],\n+                ],\n+            )\n+            self.assertEqual(\n+                tokens[\"attention_mask\"],\n+                [\n+                    [1] * len(expected_tokens[0]) + num_padding[0] * [0],\n+                    [1] * len(expected_tokens[1]) + num_padding[1] * [0],\n+                ],\n+            )\n+            self.assertEqual(\n+                tokens[\"special_tokens_mask\"],\n+                [\n+                    [1] + [0] * (len(expected_tokens[0]) - 2) + [1] + num_padding[0] * [1],\n+                    [1] + [0] * (len(expected_tokens[1]) - 2) + [1] + num_padding[1] * [1],\n+                ],\n+            )\n+\n+    def test_batch_call_with_padding_and_truncation(self):\n+        # Test 1:\n+        # padding=True or \"longest\" or PaddingStrategy.LONGEST or \"max_length\" or PaddingStragy.MAX_LENGTH\n+        # and truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST\n+        # and max_length\n+        text = [\"Hello world!\", \"Hello world! Longer\" * 10]\n+        expected_tokens = [self.ref_tokenizer.instruct_tokenizer.tokenizer.encode(t, bos=True, eos=True) for t in text]\n+        for padding in [True, \"longest\", PaddingStrategy.LONGEST, \"max_length\", PaddingStrategy.MAX_LENGTH]:\n+            for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+                tokens = self.tokenizer(\n+                    text, padding=padding, truncation=truncation, max_length=10, return_special_tokens_mask=True\n+                )\n+                num_padding = [max(0, 10 - len(t)) for t in expected_tokens]\n+                self.assertIsInstance(tokens, BatchEncoding)\n+                self.assertEqual(\n+                    tokens[\"input_ids\"],\n+                    [num_padding[i] * [self.tokenizer.pad_token_id] + t[:10] for i, t in enumerate(expected_tokens)],\n+                )\n+                self.assertEqual(\n+                    tokens[\"attention_mask\"],\n+                    [num_padding[i] * [0] + [1] * min(len(t), 10) for i, t in enumerate(expected_tokens)],\n+                )\n+                self.assertEqual(\n+                    tokens[\"special_tokens_mask\"],\n+                    [\n+                        num_padding[i] * [1] + [1 if id in self.ref_special_ids else 0 for id in ids[:10]]\n+                        for i, ids in enumerate(expected_tokens)\n+                    ],\n+                )\n+\n+        # Test 2:\n+        # padding=True or \"longest\" or PaddingStrategy.LONGEST and truncation=True or \"longest_first\" or TruncationStrategy.LONGEST_FIRST\n+        # and no max_length\n+        for padding in [\"longest\", PaddingStrategy.LONGEST]:\n+            for truncation in [True, \"longest_first\", TruncationStrategy.LONGEST_FIRST]:\n+                tokens = self.tokenizer(text, padding=padding, truncation=truncation, return_special_tokens_mask=True)\n+                self.assertIsInstance(tokens, BatchEncoding)\n+                num_padding = [max(len(t) for t in expected_tokens) - len(t) for t in expected_tokens]\n+                self.assertEqual(\n+                    tokens[\"input_ids\"],\n+                    [num_padding[i] * [self.tokenizer.pad_token_id] + t for i, t in enumerate(expected_tokens)],\n+                )\n+                self.assertEqual(\n+                    tokens[\"attention_mask\"],\n+                    [num_padding[i] * [0] + [1] * len(t) for i, t in enumerate(expected_tokens)],\n+                )\n+                self.assertEqual(\n+                    tokens[\"special_tokens_mask\"],\n+                    [\n+                        num_padding[i] * [1] + [1 if id in self.ref_special_ids else 0 for id in ids]\n+                        for i, ids in enumerate(expected_tokens)\n+                    ],\n+                )"
        },
        {
            "sha": "e5e166a3b8d39d83acd11a39a1f6b0c92f539a50",
            "filename": "utils/tests_fetcher.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/utils%2Ftests_fetcher.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70e57e4710d8a617a6f0ea73183d9bc4c91063c9/utils%2Ftests_fetcher.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ftests_fetcher.py?ref=70e57e4710d8a617a6f0ea73183d9bc4c91063c9",
            "patch": "@@ -1164,7 +1164,7 @@ def parse_commit_message(commit_message: str) -> dict[str, bool]:\n JOB_TO_TEST_FILE = {\n     \"tests_torch\": r\"tests/models/.*/test_modeling_(?!(?:flax_|tf_)).*\",\n     \"tests_generate\": r\"tests/models/.*/test_modeling_(?!(?:flax_|tf_)).*\",\n-    \"tests_tokenization\": r\"tests/models/.*/test_tokenization.*\",\n+    \"tests_tokenization\": r\"tests/(?:models/.*/test_tokenization.*|test_tokenization_mistral_common\\.py)\",\n     \"tests_processors\": r\"tests/models/.*/test_(?!(?:modeling_|tokenization_)).*\",  # takes feature extractors, image processors, processors\n     \"examples_torch\": r\"examples/pytorch/.*test_.*\",\n     \"tests_exotic_models\": r\"tests/models/.*(?=layoutlmv|nat|deta|udop|nougat).*\","
        }
    ],
    "stats": {
        "total": 3581,
        "additions": 3573,
        "deletions": 8
    }
}