{
    "author": "yonigozlan",
    "message": "Add support for args to ProcessorMixin for backward compatibility (#33479)\n\n* add check and prepare args for BC to ProcessorMixin, improve ProcessorTesterMixin\r\n\r\n* change size and crop_size in processor kwargs tests to do_rescale and rescale_factor\r\n\r\n* remove unnecessary llava processor kwargs test overwrite\r\n\r\n* nit\r\n\r\n* change data_arg_name to input_name\r\n\r\n* Remove unnecessary test override\r\n\r\n* Remove unnecessary tests Paligemma\r\n\r\n* Move test_prepare_and_validate_optional_call_args to TesterMixin, add docstring",
    "sha": "c0c6815dc98f10d60bf9927cea8d309c754474c5",
    "files": [
        {
            "sha": "2db0ba50c21042d6ebb01b6c0a281fb0de6c9f11",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -57,11 +57,11 @@ class LlavaOnevisionProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a LLaVa-Onevision processor which wraps a LLaVa-Onevision video processor, LLaVa-NeXT image processor and a LLaMa tokenizer into a single processor.\n \n-    [`LlavaNextProcessor`] offers all the functionalities of [`LlavaOnevisionVideoProcessor`], [`LlavaNextImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`LlavaNextProcessor`] offers all the functionalities of [`LlavaOnevisionVideoProcessor`], [`LlavaOnevisionImageProcessor`] and [`LlamaTokenizerFast`]. See the\n     [`~LlavaOnevisionVideoProcessor.__call__`], [`~LlavaNextProcessor.__call__`] and [`~LlavaNextProcessor.decode`] for more information.\n \n     Args:\n-        image_processor ([`LlavaNextImageProcessor`], *optional*):\n+        image_processor ([`LlavaOnevisionImageProcessor`], *optional*):\n             The image processor is a required input.\n         tokenizer ([`LlamaTokenizerFast`], *optional*):\n             The tokenizer is a required input.\n@@ -114,6 +114,7 @@ def __call__(\n         self,\n         images: ImageInput = None,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n         videos: VideoInput = None,\n         **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n     ) -> BatchFeature:"
        },
        {
            "sha": "3b8f2b0544a567a50e7c43bad11e1b45d3f20618",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -38,7 +38,9 @@\n \n from .tokenization_utils_base import (\n     PaddingStrategy,\n+    PreTokenizedInput,\n     PreTrainedTokenizerBase,\n+    TextInput,\n     TruncationStrategy,\n )\n from .utils import (\n@@ -114,6 +116,9 @@ class TextKwargs(TypedDict, total=False):\n             The side on which padding will be applied.\n     \"\"\"\n \n+    text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n+    text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]\n+    text_pair_target: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]]\n     add_special_tokens: Optional[bool]\n     padding: Union[bool, str, PaddingStrategy]\n     truncation: Union[bool, str, TruncationStrategy]\n@@ -328,6 +333,7 @@ class ProcessorMixin(PushToHubMixin):\n \n     attributes = [\"feature_extractor\", \"tokenizer\"]\n     optional_attributes = [\"chat_template\"]\n+    optional_call_args: List[str] = []\n     # Names need to be attr_class for attr in attributes\n     feature_extractor_class = None\n     tokenizer_class = None\n@@ -973,6 +979,64 @@ def validate_init_kwargs(processor_config, valid_kwargs):\n             unused_kwargs = {k: processor_config[k] for k in unused_keys}\n         return unused_kwargs\n \n+    def prepare_and_validate_optional_call_args(self, *args):\n+        \"\"\"\n+        Matches optional positional arguments to their corresponding names in `optional_call_args`\n+        in the processor class in the order they are passed to the processor call.\n+\n+        Note that this should only be used in the `__call__` method of the processors with special\n+        arguments. Special arguments are arguments that aren't `text`, `images`, `audio`, nor `videos`\n+        but also aren't passed to the tokenizer, image processor, etc. Examples of such processors are:\n+            - `CLIPSegProcessor`\n+            - `LayoutLMv2Processor`\n+            - `OwlViTProcessor`\n+\n+        Also note that passing by position to the processor call is now deprecated and will be disallowed\n+        in future versions. We only have this for backward compatibility.\n+\n+        Example:\n+            Suppose that the processor class has `optional_call_args = [\"arg_name_1\", \"arg_name_2\"]`.\n+            And we define the call method as:\n+            ```python\n+            def __call__(\n+                self,\n+                text: str,\n+                images: Optional[ImageInput] = None,\n+                *arg,\n+                audio=None,\n+                videos=None,\n+            )\n+            ```\n+\n+            Then, if we call the processor as:\n+            ```python\n+            images = [...]\n+            processor(\"What is common in these images?\", images, arg_value_1, arg_value_2)\n+            ```\n+\n+            Then, this method will return:\n+            ```python\n+            {\n+                \"arg_name_1\": arg_value_1,\n+                \"arg_name_2\": arg_value_2,\n+            }\n+            ```\n+            which we could then pass as kwargs to `self._merge_kwargs`\n+        \"\"\"\n+        if len(args):\n+            warnings.warn(\n+                \"Passing positional arguments to the processor call is now deprecated and will be disallowed in v4.47. \"\n+                \"Please pass all arguments as keyword arguments.\"\n+            )\n+        if len(args) > len(self.optional_call_args):\n+            raise ValueError(\n+                f\"Expected *at most* {len(self.optional_call_args)} optional positional arguments in processor call\"\n+                f\"which will be matched with {' '.join(self.optional_call_args)} in the order they are passed.\"\n+                f\"However, got {len(args)} positional arguments instead.\"\n+                \"Please pass all arguments as keyword arguments instead (e.g. `processor(arg_name_1=..., arg_name_2=...))`.\"\n+            )\n+        return {arg_name: arg_value for arg_value, arg_name in zip(args, self.optional_call_args)}\n+\n     def apply_chat_template(\n         self,\n         conversation: Union[List[Dict[str, str]]],"
        },
        {
            "sha": "33bff9c77ad2631da48ecb6553452f5d059ee6ae",
            "filename": "tests/models/altclip/test_processor_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 114,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -18,7 +18,7 @@\n import unittest\n \n from transformers import XLMRobertaTokenizer, XLMRobertaTokenizerFast\n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -50,116 +50,3 @@ def get_rust_tokenizer(self, **kwargs):\n \n     def get_image_processor(self, **kwargs):\n         return CLIPImageProcessor.from_pretrained(self.model_id, **kwargs)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            crop_size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 7)\n-\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            crop_size={\"height\": 214, \"width\": 214},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)"
        },
        {
            "sha": "e433c38f78910459a5e29df2f7dc584e5d9f9c79",
            "filename": "tests/models/chinese_clip/test_processor_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -206,129 +206,3 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            crop_size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n-\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            crop_size={\"height\": 214, \"width\": 214},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n-\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, crop_size=[224, 224])\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)"
        },
        {
            "sha": "06a18061579670231683e7acaed39805753f142d",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 27,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -17,7 +17,7 @@\n import unittest\n \n from transformers import AutoProcessor, AutoTokenizer, LlamaTokenizerFast, LlavaProcessor\n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -93,29 +93,3 @@ def test_chat_template(self):\n \n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            images=image_input,\n-            text=input_str,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 5)"
        },
        {
            "sha": "55f5980bfa1579d7a1f87308254de864790867e1",
            "filename": "tests/models/llava_onevision/test_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 202,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processing_llava_onevision.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -16,7 +16,7 @@\n import tempfile\n import unittest\n \n-from transformers.testing_utils import require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -100,204 +100,3 @@ def test_chat_template(self):\n \n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n-\n-    @require_torch\n-    @require_vision\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        # Rewrite as llava-next image processor return pixel values with an added dimesion for image patches\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        video_processor = self.get_component(\"video_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        # added dimension for image patches\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 234)\n-\n-    @require_torch\n-    @require_vision\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n-        video_processor = self.get_component(\"video_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n-        # added dimension for image patches\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0]), 224)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        # added dimension for image patches\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 4)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[3], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_doubly_passed_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\"]\n-        image_input = self.prepare_image_inputs()\n-        with self.assertRaises(ValueError):\n-            _ = processor(\n-                text=input_str,\n-                images=image_input,\n-                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n-                size={\"height\": 214, \"width\": 214},\n-            )\n-\n-    @require_vision\n-    @require_torch\n-    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\", max_length=112)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 2)\n-\n-    @require_vision\n-    @require_torch\n-    def test_tokenizer_defaults_preserved_by_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        video_processor = self.get_component(\"video_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n-\n-        processor = self.processor_class(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 2)"
        },
        {
            "sha": "60de913e53ae9bc9c42284aaf60daeee2f264e23",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -61,29 +61,3 @@ def test_image_seq_length(self):\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n         )\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 112 + 14)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 10)"
        },
        {
            "sha": "29575b49367268a145b5924d162e0a9fc3b7830f",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 127,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -19,7 +19,6 @@\n import torch\n \n from transformers.testing_utils import (\n-    require_torch,\n     require_vision,\n )\n from transformers.utils import is_vision_available\n@@ -248,144 +247,28 @@ def test_processor_with_multiple_images_multiple_lists(self):\n         )\n         # fmt: on\n \n-    # Override all tests requiring shape as returning tensor batches is not supported by PixtralProcessor\n-\n-    @require_torch\n-    @require_vision\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size={\"height\": 240, \"width\": 240})\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        # Added dimension by pixtral image processor\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0][0]), 240)\n-\n-    @require_torch\n-    @require_vision\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size={\"height\": 400, \"width\": 400})\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, size={\"height\": 240, \"width\": 240})\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0][0][0]), 240)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 240, \"width\": 240}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_structured_kwargs_nested_from_dict(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 240, \"width\": 240}},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n-    def test_unstructured_kwargs(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            size={\"height\": 240, \"width\": 240},\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    @require_torch\n-    @require_vision\n+    # Override as PixtralProcessor needs nested images to work properly with batched inputs\n     def test_unstructured_kwargs_batched(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = [\"lower newer\", \"upper older longer string\"]\n-        # images needs to be nested to detect multiple prompts\n         image_input = [self.prepare_image_inputs()] * 2\n         inputs = processor(\n             text=input_str,\n             images=image_input,\n             return_tensors=\"pt\",\n-            size={\"height\": 240, \"width\": 240},\n+            do_rescale=True,\n+            rescale_factor=-1,\n             padding=\"longest\",\n             max_length=76,\n         )\n \n-        self.assertEqual(inputs[\"pixel_values\"][0][0].shape[-1], 240)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 4)\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+        self.assertTrue(\n+            len(inputs[self.text_input_name][0]) == len(inputs[self.text_input_name][1])\n+            and len(inputs[self.text_input_name][1]) < 76\n+        )"
        },
        {
            "sha": "a360fc98f4c58484c560c8fe4b9198249b18520a",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 127,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -108,130 +108,3 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input, videos=video_inputs)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n-    # Qwen2-VL doesn't accept `size` and resized to an optimal size using image_processor attrbutes\n-    # defined at `init`. Therefore, all tests are overwritten and don't actually test if kwargs are passed\n-    # to image processors\n-    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n-\n-    def test_kwargs_overrides_default_image_processor_kwargs(self):\n-        image_processor = self.get_component(\n-            \"image_processor\",\n-        )\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n-\n-    def test_unstructured_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            padding=\"max_length\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_unstructured_kwargs_batched(self):\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = [\"lower newer\", \"upper older longer string\"]\n-        image_input = self.prepare_image_inputs() * 2\n-        inputs = processor(\n-            text=input_str,\n-            images=image_input,\n-            return_tensors=\"pt\",\n-            padding=\"longest\",\n-            max_length=76,\n-        )\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[0], 1600)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 4)\n-\n-    def test_structured_kwargs_nested(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_structured_kwargs_nested_from_dict(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # Define the kwargs for each modality\n-        all_kwargs = {\n-            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n-        }\n-\n-        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[0], 800)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n-\n-    def test_image_processor_defaults_preserved_by_video_kwargs(self):\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        self.skip_processor_without_typed_kwargs(processor)\n-\n-        input_str = \"lower newer\"\n-        video_input = self.prepare_video_inputs()\n-\n-        inputs = processor(text=input_str, videos=video_input)\n-        self.assertEqual(inputs[\"pixel_values_videos\"].shape[0], 9600)"
        },
        {
            "sha": "a51c1d200eb0aa46a0a670cff92c89ff1051d37d",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 93,
            "deletions": 61,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c0c6815dc98f10d60bf9927cea8d309c754474c5/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=c0c6815dc98f10d60bf9927cea8d309c754474c5",
            "patch": "@@ -18,12 +18,6 @@\n import json\n import tempfile\n \n-\n-try:\n-    from typing import Unpack\n-except ImportError:\n-    from typing_extensions import Unpack\n-\n import numpy as np\n \n from transformers.models.auto.processing_auto import processor_class_from_name\n@@ -35,6 +29,12 @@\n from transformers.utils import is_vision_available\n \n \n+try:\n+    from typing import Unpack\n+except ImportError:\n+    from typing_extensions import Unpack\n+\n+\n if is_vision_available():\n     from PIL import Image\n \n@@ -50,6 +50,9 @@ def prepare_image_inputs():\n @require_vision\n class ProcessorTesterMixin:\n     processor_class = None\n+    text_input_name = \"input_ids\"\n+    images_input_name = \"pixel_values\"\n+    videos_input_name = \"pixel_values_videos\"\n \n     def prepare_processor_dict(self):\n         return {}\n@@ -139,68 +142,77 @@ def skip_processor_without_typed_kwargs(self, processor):\n     def test_tokenizer_defaults_preserved_by_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 117)\n \n     def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        \"\"\"\n+        We use do_rescale=True, rescale_factor=-1 to ensure that image_processor kwargs are preserved in the processor.\n+        We then check that the mean of the pixel_values is less than or equal to 0 after processing.\n+        Since the original pixel_values are in [0, 255], this is a good indicator that the rescale_factor is indeed applied.\n+        \"\"\"\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\n+            \"image_processor\", do_rescale=True, rescale_factor=-1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n \n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\", padding=\"longest\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding=\"longest\")\n \n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n         inputs = processor(\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n         )\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 112)\n \n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\", size=(234, 234))\n-        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        processor_components = self.prepare_components()\n+        processor_components[\"image_processor\"] = self.get_component(\n+            \"image_processor\", do_rescale=True, rescale_factor=1\n+        )\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n \n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n \n-        inputs = processor(text=input_str, images=image_input, size=[224, 224])\n-        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n+        inputs = processor(text=input_str, images=image_input, do_rescale=True, rescale_factor=-1, return_tensors=\"pt\")\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n \n     def test_unstructured_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = \"lower newer\"\n@@ -209,21 +221,20 @@ def test_unstructured_kwargs(self):\n             text=input_str,\n             images=image_input,\n             return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n+            do_rescale=True,\n+            rescale_factor=-1,\n             padding=\"max_length\",\n             max_length=76,\n         )\n \n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_unstructured_kwargs_batched(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = [\"lower newer\", \"upper older longer string\"]\n@@ -232,21 +243,23 @@ def test_unstructured_kwargs_batched(self):\n             text=input_str,\n             images=image_input,\n             return_tensors=\"pt\",\n-            size={\"height\": 214, \"width\": 214},\n+            do_rescale=True,\n+            rescale_factor=-1,\n             padding=\"longest\",\n             max_length=76,\n         )\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n \n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+        self.assertTrue(\n+            len(inputs[self.text_input_name][0]) == len(inputs[self.text_input_name][1])\n+            and len(inputs[self.text_input_name][1]) < 76\n+        )\n \n     def test_doubly_passed_kwargs(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = [\"lower newer\"]\n@@ -255,17 +268,16 @@ def test_doubly_passed_kwargs(self):\n             _ = processor(\n                 text=input_str,\n                 images=image_input,\n-                images_kwargs={\"size\": {\"height\": 222, \"width\": 222}},\n-                size={\"height\": 214, \"width\": 214},\n+                images_kwargs={\"do_rescale\": True, \"rescale_factor\": -1},\n+                do_rescale=True,\n+                return_tensors=\"pt\",\n             )\n \n     def test_structured_kwargs_nested(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = \"lower newer\"\n@@ -274,40 +286,35 @@ def test_structured_kwargs_nested(self):\n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n \n         inputs = processor(text=input_str, images=image_input, **all_kwargs)\n         self.skip_processor_without_typed_kwargs(processor)\n \n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     def test_structured_kwargs_nested_from_dict(self):\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        image_processor = self.get_component(\"image_processor\")\n-        tokenizer = self.get_component(\"tokenizer\")\n-\n-        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        processor_components = self.prepare_components()\n+        processor = self.processor_class(**processor_components)\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n \n         # Define the kwargs for each modality\n         all_kwargs = {\n             \"common_kwargs\": {\"return_tensors\": \"pt\"},\n-            \"images_kwargs\": {\"size\": {\"height\": 214, \"width\": 214}},\n+            \"images_kwargs\": {\"do_rescale\": True, \"rescale_factor\": -1},\n             \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n         }\n \n         inputs = processor(text=input_str, images=image_input, **all_kwargs)\n-        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n-\n-        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+        self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n+        self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n     # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n     # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n@@ -335,3 +342,28 @@ def test_overlapping_text_kwargs_handling(self):\n                 padding=\"max_length\",\n                 text_kwargs={\"padding\": \"do_not_pad\"},\n             )\n+\n+    def test_prepare_and_validate_optional_call_args(self):\n+        processor = self.get_processor()\n+        optional_call_args_name = getattr(processor, \"optional_call_args\", [])\n+        num_optional_call_args = len(optional_call_args_name)\n+        if num_optional_call_args == 0:\n+            self.skipTest(\"No optional call args\")\n+        # test all optional call args are given\n+        optional_call_args = processor.prepare_and_validate_optional_call_args(\n+            *(f\"optional_{i}\" for i in range(num_optional_call_args))\n+        )\n+        self.assertEqual(\n+            optional_call_args, {arg_name: f\"optional_{i}\" for i, arg_name in enumerate(optional_call_args_name)}\n+        )\n+        # test only one optional call arg is given\n+        optional_call_args = processor.prepare_and_validate_optional_call_args(\"optional_1\")\n+        self.assertEqual(optional_call_args, {optional_call_args_name[0]: \"optional_1\"})\n+        # test no optional call arg is given\n+        optional_call_args = processor.prepare_and_validate_optional_call_args()\n+        self.assertEqual(optional_call_args, {})\n+        # test too many optional call args are given\n+        with self.assertRaises(ValueError):\n+            processor.prepare_and_validate_optional_call_args(\n+                *(f\"optional_{i}\" for i in range(num_optional_call_args + 1))\n+            )"
        }
    ],
    "stats": {
        "total": 985,
        "additions": 173,
        "deletions": 812
    }
}