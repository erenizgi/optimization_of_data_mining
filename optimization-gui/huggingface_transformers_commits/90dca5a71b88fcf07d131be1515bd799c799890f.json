{
    "author": "a-r-r-o-w",
    "message": "minor typo fix (#33784)\n\nfix typo",
    "sha": "90dca5a71b88fcf07d131be1515bd799c799890f",
    "files": [
        {
            "sha": "73cd7db80d6acdeea6eddc7906bb3ad34f0f967f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/90dca5a71b88fcf07d131be1515bd799c799890f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90dca5a71b88fcf07d131be1515bd799c799890f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=90dca5a71b88fcf07d131be1515bd799c799890f",
            "patch": "@@ -2935,7 +2935,7 @@ def cuda(self, *args, **kwargs):\n \n     @wraps(torch.nn.Module.to)\n     def to(self, *args, **kwargs):\n-        # For BNB/GPTQ models, we prevent users from casting the model to another dytpe to restrict unwanted behaviours.\n+        # For BNB/GPTQ models, we prevent users from casting the model to another dtype to restrict unwanted behaviours.\n         # the correct API should be to load the model with the desired dtype directly through `from_pretrained`.\n         dtype_present_in_args = \"dtype\" in kwargs\n "
        },
        {
            "sha": "fe35451d2a99e3ad8ed3b79199c99c21f80daa5d",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/90dca5a71b88fcf07d131be1515bd799c799890f/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/90dca5a71b88fcf07d131be1515bd799c799890f/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=90dca5a71b88fcf07d131be1515bd799c799890f",
            "patch": "@@ -1310,7 +1310,7 @@ def __init__(self, config):\n     def forward(self, residue_index, mask=None):\n         \"\"\"\n         Input:\n-          residue_index: B x L tensor of indices (dytpe=torch.long) mask: B x L tensor of booleans\n+          residue_index: B x L tensor of indices (dtype=torch.long) mask: B x L tensor of booleans\n \n         Output:\n           pairwise_state: B x L x L x pairwise_state_dim tensor of embeddings"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}