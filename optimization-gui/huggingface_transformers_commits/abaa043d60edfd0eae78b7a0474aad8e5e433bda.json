{
    "author": "DWarez",
    "message": "bad_words_ids no longer slow on mps (#39556)\n\n* fix: bad_words_ids no longer slow on mps\n\n* fix: SequenceBiasLogitsProcessor slow `_prepare_bias_variables` method\n\n* fix: re-adding a deleted comment\n\n* fix: bug in no_bad_words_logits\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "abaa043d60edfd0eae78b7a0474aad8e5e433bda",
    "files": [
        {
            "sha": "4948ad845291dba5b18b50810d4f9e95ba3cb73e",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/abaa043d60edfd0eae78b7a0474aad8e5e433bda/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abaa043d60edfd0eae78b7a0474aad8e5e433bda/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=abaa043d60edfd0eae78b7a0474aad8e5e433bda",
            "patch": "@@ -1222,10 +1222,16 @@ def _prepare_bias_variables(self, scores: torch.FloatTensor):\n         # Precompute the bias tensors to be applied. Sequences of length 1 are kept separately, as they can be applied\n         # with simpler logic.\n         self.length_1_bias = torch.zeros((vocabulary_size,), dtype=torch.float, device=scores.device)\n+        # Extract single-token sequences and their biases\n+        single_token_ids = []\n+        single_token_biases = []\n         for sequence_ids, bias in self.sequence_bias.items():\n             if len(sequence_ids) == 1:\n-                self.length_1_bias[sequence_ids[-1]] = bias\n+                single_token_ids.append(sequence_ids[0])\n+                single_token_biases.append(bias)\n \n+        if single_token_ids:  # Only if we have any single-token sequences\n+            self.length_1_bias[single_token_ids] = torch.tensor(single_token_biases, device=scores.device)\n         self.prepared_bias_variables = True\n \n     def _validate_arguments(self):\n@@ -1340,10 +1346,10 @@ def __init__(\n                     eos_token_id = [eos_token_id]\n                 eos_token_id = torch.tensor(eos_token_id)\n \n+            eos_token_id_list = eos_token_id.tolist()  # convert to python list before\n             bad_words_ids = list(\n-                filter(lambda bad_token_seq: all(bad_token_seq != [i] for i in eos_token_id), bad_words_ids)\n+                filter(lambda bad_token_seq: all(bad_token_seq != [i] for i in eos_token_id_list), bad_words_ids)\n             )\n-\n         # Forbidding a sequence is equivalent to setting its bias to -inf\n         sequence_bias = {tuple(sequence): float(\"-inf\") for sequence in bad_words_ids}\n         super().__init__(sequence_bias=sequence_bias)"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}