{
    "author": "zucchini-nlp",
    "message": "VLM: special multimodal Tokenizer (#34461)\n\n* kinda works\r\n\r\n* update\r\n\r\n* add tests\r\n\r\n* update\r\n\r\n* use special tokens in processors\r\n\r\n* typo\r\n\r\n* fix copies\r\n\r\n* fix\r\n\r\n* fix moshi after rebase\r\n\r\n* update\r\n\r\n* fix tests\r\n\r\n* update\r\n\r\n* Update docs/source/en/main_classes/tokenizer.md\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* update docs\r\n\r\n* test for load time adding tokens\r\n\r\n* fix some more tests which are now fetched better\r\n\r\n* one more fix\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "187439c3fa139b2102a874483e9f8f0cfa8e5557",
    "files": [
        {
            "sha": "83d2ae5df6a7fb1179edb400ff6ee8b616680db8",
            "filename": "docs/source/en/main_classes/tokenizer.md",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftokenizer.md?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -51,6 +51,25 @@ token space (e.g., getting the index of the token comprising a given character o\n to a given token).\n \n \n+# Multimodal Tokenizer\n+\n+Apart from that each tokenizer can be a \"multimodal\" tokenizer which means that the tokenizer will hold all relevant special tokens\n+as part of tokenizer attributes for easier access. For example, if the tokenizer is loaded from a vision-language model like LLaVA, you will\n+be able to access `tokenizer.image_token_id` to obtain the special image token used as a placeholder. \n+\n+To enable extra special tokens for any type of tokenizer, you have to add the following lines and save the tokenizer. Extra special tokens do not\n+have to be modality related and can ne anything that the model often needs access to. In the below code, tokenizer at `output_dir` will have direct access\n+to three more special tokens.  \n+\n+```python\n+vision_tokenizer = AutoTokenizer.from_pretrained(\n+    \"llava-hf/llava-1.5-7b-hf\",\n+    extra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n+)\n+print(vision_tokenizer.image_token, vision_tokenizer.image_token_id)\n+(\"<image>\", 32000)\n+```\n+\n ## PreTrainedTokenizer\n \n [[autodoc]] PreTrainedTokenizer"
        },
        {
            "sha": "9e75e6fd3c38dfab38e52cb36cb5fbd595cfcf77",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -443,7 +443,7 @@ def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int]\n             return torch.stack(examples, dim=0)\n \n     # If yes, check if we have a `pad_token`.\n-    if tokenizer._pad_token is None:\n+    if tokenizer.pad_token is None:\n         raise ValueError(\n             \"You are attempting to pad samples but the tokenizer you are using\"\n             f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n@@ -477,7 +477,7 @@ def _tf_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = N\n         return tf.stack(examples, axis=0)\n \n     # If yes, check if we have a `pad_token`.\n-    if tokenizer._pad_token is None:\n+    if tokenizer.pad_token is None:\n         raise ValueError(\n             \"You are attempting to pad samples but the tokenizer you are using\"\n             f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n@@ -513,7 +513,7 @@ def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int]\n         return np.stack(examples, axis=0)\n \n     # If yes, check if we have a `pad_token`.\n-    if tokenizer._pad_token is None:\n+    if tokenizer.pad_token is None:\n         raise ValueError(\n             \"You are attempting to pad samples but the tokenizer you are using\"\n             f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n@@ -1090,7 +1090,7 @@ def torch_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n             self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n         ]\n         probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = labels.eq(self.tokenizer.pad_token_id)\n             probability_matrix.masked_fill_(padding_mask, value=0.0)\n \n@@ -1131,7 +1131,7 @@ def tf_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n             self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels\n         ]\n         masked_indices = masked_indices & ~tf.cast(special_tokens_mask, dtype=tf.bool)\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = inputs == self.tokenizer.pad_token_id\n             masked_indices = masked_indices & ~padding_mask\n \n@@ -1170,7 +1170,7 @@ def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n             self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n         ]\n         masked_indices[np.array(special_tokens_mask, dtype=bool)] = 0\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = labels == self.tokenizer.pad_token_id\n             masked_indices[padding_mask] = 0\n \n@@ -1251,13 +1251,13 @@ def mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any]:\n             self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n         ]\n         probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = labels.eq(self.tokenizer.pad_token_id)\n             probability_matrix.masked_fill_(padding_mask, value=0.0)\n         masked_indices = torch.bernoulli(probability_matrix).bool()\n         # probability be `1` (masked), however in albert model attention mask `0` means masked, revert the value\n         attention_mask = (~masked_indices).float()\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             attention_padding_mask = labels.eq(self.tokenizer.pad_token_id)\n             attention_mask.masked_fill_(attention_padding_mask, value=1.0)\n         labels[~masked_indices] = -100  # We only compute loss on masked tokens, -100 is default for CE compute\n@@ -1367,7 +1367,7 @@ def torch_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n             dtype=torch.bool,\n         )\n         masked_indices.masked_fill_(special_tokens_mask, value=0.0)\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = labels.eq(self.tokenizer.pad_token_id)\n             masked_indices.masked_fill_(padding_mask, value=0.0)\n \n@@ -1471,7 +1471,7 @@ def tf_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n         )\n         special_tokens_mask = tf.cast(special_tokens_mask, dtype=tf.bool)\n         masked_indices = masked_indices & ~special_tokens_mask\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = labels == self.tokenizer.pad_token_id\n             masked_indices = masked_indices & ~padding_mask\n \n@@ -1571,7 +1571,7 @@ def numpy_mask_tokens(self, inputs: Any) -> Tuple[Any, Any, Any, Any]:\n             dtype=bool,\n         )\n         masked_indices[special_tokens_mask] = 0\n-        if self.tokenizer._pad_token is not None:\n+        if self.tokenizer.pad_token is not None:\n             padding_mask = labels == self.tokenizer.pad_token_id\n             masked_indices[padding_mask] = 0.0\n "
        },
        {
            "sha": "c6852378412895c64c1b50293676ff9d46da5261",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -74,8 +74,11 @@ class Blip2Processor(ProcessorMixin):\n     def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n         tokenizer.return_token_type_ids = False\n         self.current_processor = image_processor\n-        self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n-        tokenizer.add_tokens([self.image_token], special_tokens=True)\n+        if not hasattr(tokenizer, \"image_token\"):\n+            self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n+            tokenizer.add_tokens([self.image_token], special_tokens=True)\n+        else:\n+            self.image_token = tokenizer.image_token\n         self.num_query_tokens = num_query_tokens\n \n         super().__init__(image_processor, tokenizer)"
        },
        {
            "sha": "e2a50d1af51b9eb638a0330c518f8cff9e35e7aa",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -66,9 +66,12 @@ class ChameleonProcessor(ProcessorMixin):\n \n     def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, image_token: str = \"<image>\"):\n         self.image_seq_length = image_seq_length\n-        self.image_token = image_token\n-        self.image_start_token = \"<racm3:break>\"  # fixed tokens for start and end, so can hardcode\n-        self.image_end_token = \"<eoss>\"\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n+        self.image_start_token = (\n+            tokenizer.boi_token if hasattr(tokenizer, \"boi_token\") else \"<racm3:break>\"\n+        )  # fixed tokens for start and end, so can hardcode\n+        self.image_end_token = tokenizer.eoi_token if hasattr(tokenizer, \"eoi_token\") else \"<eoss>\"\n+\n         super().__init__(image_processor, tokenizer)\n \n     def __call__("
        },
        {
            "sha": "7138cafbd625fc6d5ff7b5decac7b4a57737d3d8",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -138,7 +138,7 @@ def __getstate__(self):\n         return state\n \n     def __setstate__(self, d):\n-        self.__dict__ = d\n+        self.__dict__.update(d)\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n "
        },
        {
            "sha": "ca6e4702d3173e2fb092b49ffa0efc3fa4c6dfbe",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -219,7 +219,11 @@ def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_u\n \n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n-        self.image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n+        self.image_token_id = (\n+            tokenizer.image_token_id\n+            if hasattr(tokenizer, \"image_token\")\n+            else tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n+        )\n \n         self.default_image_dims = (\n             self.image_processor.image_num_channels,"
        },
        {
            "sha": "f99c1bda47456847bf6312989acc278efb948925",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -95,16 +95,19 @@ def __init__(self, image_processor, tokenizer=None, image_seq_len: int = 64, cha\n         if tokenizer is None:\n             raise ValueError(\"You need to specify a `tokenizer`.\")\n \n-        self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True)\n-        self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n+        if not hasattr(tokenizer, \"image_token\"):\n+            self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True)\n+            self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n+            tokens_to_add = {\"additional_special_tokens\": [self.fake_image_token, self.image_token]}\n+            tokenizer.add_special_tokens(tokens_to_add)\n+        else:\n+            self.fake_image_token = tokenizer.image_boundary_token\n+            self.image_token = tokenizer.image_token\n+\n         self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True)\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [self.end_of_utterance_token]})\n         self.image_seq_len = image_seq_len\n \n-        tokens_to_add = {\n-            \"additional_special_tokens\": [self.fake_image_token, self.image_token, self.end_of_utterance_token]\n-        }\n-        tokenizer.add_special_tokens(tokens_to_add)\n-\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def _extract_images_from_prompts(self, prompts):"
        },
        {
            "sha": "3d48839d376c5cc2299c5194ae43a573be7c9486",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -78,8 +78,11 @@ class InstructBlipProcessor(ProcessorMixin):\n     qformer_tokenizer_class = \"AutoTokenizer\"\n \n     def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n-        self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n-        tokenizer.add_tokens([self.image_token], special_tokens=True)\n+        if not hasattr(tokenizer, \"image_token\"):\n+            self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n+            tokenizer.add_tokens([self.image_token], special_tokens=True)\n+        else:\n+            self.image_token = tokenizer.image_token\n         self.num_query_tokens = num_query_tokens\n         super().__init__(image_processor, tokenizer, qformer_tokenizer)\n "
        },
        {
            "sha": "1d4e59e26b46214abbf1618af5d21fd6b325b95c",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -63,8 +63,11 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n     qformer_tokenizer_class = \"AutoTokenizer\"\n \n     def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n-        self.video_token = AddedToken(\"<video>\", normalized=False, special=True)\n-        tokenizer.add_tokens([self.video_token], special_tokens=True)\n+        if not hasattr(tokenizer, \"video_token\"):\n+            self.video_token = AddedToken(\"<video>\", normalized=False, special=True)\n+            tokenizer.add_tokens([self.video_token], special_tokens=True)\n+        else:\n+            self.video_token = tokenizer.video_token\n         self.num_query_tokens = num_query_tokens\n         super().__init__(image_processor, tokenizer, qformer_tokenizer)\n "
        },
        {
            "sha": "060748ea907ad5d4fea7fe97c997cd38518f8693",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -297,7 +297,7 @@ def __getstate__(self):\n         return state\n \n     def __setstate__(self, d):\n-        self.__dict__ = d\n+        self.__dict__.update(d)\n \n         # for backward compatibility\n         if not hasattr(self, \"sp_model_kwargs\"):"
        },
        {
            "sha": "35f29bfa957806d39c14ebd26c73e186e998ad7b",
            "filename": "src/transformers/models/llama/tokenization_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Ftokenization_llama.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -214,7 +214,7 @@ def __getstate__(self):\n         return state\n \n     def __setstate__(self, d):\n-        self.__dict__ = d\n+        self.__dict__.update(d)\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.LoadFromSerializedProto(self.sp_model_proto)\n "
        },
        {
            "sha": "0ff40acc405224f9927c3fb6b27bc24e13fc6503",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -77,7 +77,7 @@ def __init__(\n     ):\n         self.patch_size = patch_size\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        self.image_token = image_token\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__("
        },
        {
            "sha": "310083c1ce53ac72146296c3f8dc99037eff50f5",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -80,7 +80,7 @@ def __init__(\n     ):\n         self.patch_size = patch_size\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        self.image_token = image_token\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__("
        },
        {
            "sha": "a42aafcadd64c6d86559b3c7120ec23290ace37c",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -82,8 +82,8 @@ def __init__(\n     ):\n         self.patch_size = patch_size\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        self.image_token = image_token\n-        self.video_token = video_token\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n+        self.video_token = tokenizer.video_token if hasattr(tokenizer, \"video_token\") else video_token\n         super().__init__(video_processor, image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__("
        },
        {
            "sha": "9ae1cf1c08b20361a7c5ee32c9994c0450096961",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -96,8 +96,8 @@ def __init__(\n     ):\n         self.num_image_tokens = num_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        self.image_token = image_token\n-        self.video_token = video_token\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n+        self.video_token = tokenizer.video_token if hasattr(tokenizer, \"video_token\") else video_token\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n     def __call__("
        },
        {
            "sha": "3d6c08c35cd258eb216cd91b814303b456657c2a",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -213,8 +213,13 @@ class MllamaProcessor(ProcessorMixin):\n     tokenizer_class = \"PreTrainedTokenizerFast\"\n \n     def __init__(self, image_processor, tokenizer):\n-        self.image_token = \"<|image|>\"\n-        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n+        if not hasattr(tokenizer, \"image_token\"):\n+            self.image_token = \"<|image|>\"\n+            self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n+        else:\n+            self.image_token = tokenizer.image_token\n+            self.image_token_id = tokenizer.image_token_id\n+\n         self.python_token = \"<|python_tag|>\"\n         self.python_token_id = tokenizer.convert_tokens_to_ids(self.python_token)\n         self.bos_token = tokenizer.bos_token"
        },
        {
            "sha": "dab0d60ad56d155533f3bf571d14ce2ab995a805",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -160,11 +160,15 @@ def __init__(\n \n         self.image_seq_length = image_processor.image_seq_length\n \n-        image_token = AddedToken(IMAGE_TOKEN, normalized=False, special=True)\n-        tokens_to_add = {\"additional_special_tokens\": [image_token]}\n-        tokenizer.add_special_tokens(tokens_to_add)\n+        if not hasattr(tokenizer, \"image_token\"):\n+            image_token = AddedToken(IMAGE_TOKEN, normalized=False, special=True)\n+            tokens_to_add = {\"additional_special_tokens\": [image_token]}\n+            tokenizer.add_special_tokens(tokens_to_add)\n+            self.image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n+        else:\n+            self.image_token_id = tokenizer.image_token_id\n+\n         tokenizer.add_tokens(EXTRA_TOKENS)\n-        self.image_token_id = tokenizer.convert_tokens_to_ids(IMAGE_TOKEN)\n         tokenizer.add_bos_token = False\n         tokenizer.add_eos_token = False\n "
        },
        {
            "sha": "f7a59aa15f250f6d75e25787e4b05eae331a09d6",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -61,6 +61,8 @@ class Qwen2VLProcessor(ProcessorMixin):\n     tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n \n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n+        self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n+        self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__(\n@@ -132,23 +134,23 @@ def __call__(\n             merge_length = self.image_processor.merge_size**2\n             index = 0\n             for i in range(len(text)):\n-                while \"<|image_pad|>\" in text[i]:\n+                while self.image_token in text[i]:\n                     text[i] = text[i].replace(\n-                        \"<|image_pad|>\", \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n+                        self.image_token, \"<|placeholder|>\" * (image_grid_thw[index].prod() // merge_length), 1\n                     )\n                     index += 1\n-                text[i] = text[i].replace(\"<|placeholder|>\", \"<|image_pad|>\")\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n \n         if video_grid_thw is not None:\n             merge_length = self.image_processor.merge_size**2\n             index = 0\n             for i in range(len(text)):\n-                while \"<|video_pad|>\" in text[i]:\n+                while self.video_token in text[i]:\n                     text[i] = text[i].replace(\n-                        \"<|video_pad|>\", \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length), 1\n+                        self.video_token, \"<|placeholder|>\" * (video_grid_thw[index].prod() // merge_length), 1\n                     )\n                     index += 1\n-                text[i] = text[i].replace(\"<|placeholder|>\", \"<|video_pad|>\")\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n \n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n "
        },
        {
            "sha": "88708e8b29a3be6d425befbc1ab6a8159c71ed34",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -412,7 +412,7 @@ def __getstate__(self):\n         return state\n \n     def __setstate__(self, d):\n-        self.__dict__ = d\n+        self.__dict__.update(d)\n         self.sp_model = spm.SentencePieceProcessor(**self.sp_model_kwargs)\n         self.sp_model.Load(self.vocab_file)\n "
        },
        {
            "sha": "d3c27ef56ca0ce54c99ba9d5888d19b8e71cc0da",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -71,8 +71,8 @@ def __init__(\n     ):\n         self.patch_size = patch_size\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n-        self.image_token = image_token\n-        self.video_token = video_token\n+        self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n+        self.video_token = tokenizer.video_token if hasattr(tokenizer, \"video_token\") else video_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n     def __call__("
        },
        {
            "sha": "1bc13020e65b66af6fdbd621aec5a466c494c68f",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -577,7 +577,7 @@ def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_to\n                 token_index = current_vocab[token.content]\n \n             if token.special and str(token) not in self.all_special_tokens:\n-                self._additional_special_tokens.append(token)\n+                self._special_tokens_map[\"additional_special_tokens\"].append(token)\n             # the setter automatically updates the reverse map\n             self._added_tokens_decoder[token_index] = token\n             self._added_tokens_encoder[token.content] = token_index"
        },
        {
            "sha": "43e13abe563115edf2e1f06bfb729005fe036bb4",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 70,
            "deletions": 254,
            "changes": 324,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -861,16 +861,10 @@ class SpecialTokensMixin:\n     ]\n \n     def __init__(self, verbose=False, **kwargs):\n-        self._bos_token = None\n-        self._eos_token = None\n-        self._unk_token = None\n-        self._sep_token = None\n-        self._pad_token = None\n-        self._cls_token = None\n-        self._mask_token = None\n         self._pad_token_type_id = 0\n-        self._additional_special_tokens = []\n         self.verbose = verbose\n+        self._special_tokens_map = {attr: None for attr in self.SPECIAL_TOKENS_ATTRIBUTES}\n+        self._special_tokens_map[\"additional_special_tokens\"] = []  # for BC where it defaults to empty list\n \n         # We directly set the hidden value to allow initialization with special tokens\n         # which are not yet in the vocabulary. Necessary for serialization/de-serialization\n@@ -932,7 +926,7 @@ def add_special_tokens(\n                 assign the index of the `unk_token` to them).\n             replace_additional_special_tokens (`bool`, *optional*,, defaults to `True`):\n                 If `True`, the existing list of additional special tokens will be replaced by the list provided in\n-                `special_tokens_dict`. Otherwise, `self._additional_special_tokens` is just extended. In the former\n+                `special_tokens_dict`. Otherwise, `self._special_tokens_map[\"additional_special_tokens\"]` is just extended. In the former\n                 case, the tokens will NOT be removed from the tokenizer's full vocabulary - they are only being flagged\n                 as non-special tokens. Remember, this only affects which tokens are skipped during decoding, not the\n                 `added_tokens_encoder` and `added_tokens_decoder`. This means that the previous\n@@ -983,7 +977,7 @@ def add_special_tokens(\n                 if replace_additional_special_tokens and len(to_add) > 0:\n                     setattr(self, key, list(to_add))\n                 else:\n-                    self._additional_special_tokens.extend(to_add)\n+                    self._special_tokens_map[\"additional_special_tokens\"].extend(to_add)\n                 added_tokens += to_add\n \n             else:\n@@ -1053,260 +1047,62 @@ def add_tokens(\n     def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n         raise NotImplementedError\n \n-    @property\n-    def bos_token(self) -> str:\n-        \"\"\"\n-        `str`: Beginning of sentence token. Log an error if used while not having been set.\n-        \"\"\"\n-        if self._bos_token is None:\n-            if self.verbose:\n-                logger.error(\"Using bos_token, but it is not set yet.\")\n-            return None\n-        return str(self._bos_token)\n-\n-    @property\n-    def eos_token(self) -> str:\n-        \"\"\"\n-        `str`: End of sentence token. Log an error if used while not having been set.\n-        \"\"\"\n-        if self._eos_token is None:\n-            if self.verbose:\n-                logger.error(\"Using eos_token, but it is not set yet.\")\n-            return None\n-        return str(self._eos_token)\n-\n-    @property\n-    def unk_token(self) -> str:\n-        \"\"\"\n-        `str`: Unknown token. Log an error if used while not having been set.\n-        \"\"\"\n-        if self._unk_token is None:\n-            if self.verbose:\n-                logger.error(\"Using unk_token, but it is not set yet.\")\n-            return None\n-        return str(self._unk_token)\n-\n-    @property\n-    def sep_token(self) -> str:\n-        \"\"\"\n-        `str`: Separation token, to separate context and query in an input sequence. Log an error if used while not\n-        having been set.\n-        \"\"\"\n-        if self._sep_token is None:\n-            if self.verbose:\n-                logger.error(\"Using sep_token, but it is not set yet.\")\n-            return None\n-        return str(self._sep_token)\n-\n-    @property\n-    def pad_token(self) -> str:\n-        \"\"\"\n-        `str`: Padding token. Log an error if used while not having been set.\n-        \"\"\"\n-        if self._pad_token is None:\n-            if self.verbose:\n-                logger.error(\"Using pad_token, but it is not set yet.\")\n-            return None\n-        return str(self._pad_token)\n-\n-    @property\n-    def cls_token(self) -> str:\n-        \"\"\"\n-        `str`: Classification token, to extract a summary of an input sequence leveraging self-attention along the full\n-        depth of the model. Log an error if used while not having been set.\n-        \"\"\"\n-        if self._cls_token is None:\n-            if self.verbose:\n-                logger.error(\"Using cls_token, but it is not set yet.\")\n-            return None\n-        return str(self._cls_token)\n-\n-    @property\n-    def mask_token(self) -> str:\n-        \"\"\"\n-        `str`: Mask token, to use when training a model with masked-language modeling. Log an error if used while not\n-        having been set.\n-        \"\"\"\n-        if self._mask_token is None:\n-            if self.verbose:\n-                logger.error(\"Using mask_token, but it is not set yet.\")\n-            return None\n-        return str(self._mask_token)\n-\n-    @property\n-    def additional_special_tokens(self) -> List[str]:\n-        \"\"\"\n-        `List[str]`: All the additional special tokens you may want to use. Log an error if used while not having been\n-        set.\n-        \"\"\"\n-        if self._additional_special_tokens is None:\n-            if self.verbose:\n-                logger.error(\"Using additional_special_tokens, but it is not set yet.\")\n-            return None\n-        return [str(tok) for tok in self._additional_special_tokens]\n-\n-    @bos_token.setter\n-    def bos_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the BOS token\")\n-        self._bos_token = value\n-\n-    @eos_token.setter\n-    def eos_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the EOS token\")\n-        self._eos_token = value\n-\n-    @unk_token.setter\n-    def unk_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the UNK token\")\n-        self._unk_token = value\n-\n-    @sep_token.setter\n-    def sep_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the SEP token\")\n-        self._sep_token = value\n-\n-    @pad_token.setter\n-    def pad_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the PAD token\")\n-        self._pad_token = value\n-\n-    @cls_token.setter\n-    def cls_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the CLS token\")\n-        self._cls_token = value\n-\n-    @mask_token.setter\n-    def mask_token(self, value):\n-        if not isinstance(value, (str, AddedToken)) and value is not None:\n-            raise ValueError(\"Cannot set a non-string value as the MASK token\")\n-        self._mask_token = value\n-\n-    @additional_special_tokens.setter\n-    def additional_special_tokens(self, value):\n-        self._additional_special_tokens = value if value is not None else None\n-\n-    @property\n-    def bos_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the beginning of sentence token in the vocabulary. Returns `None` if the token has not\n-        been set.\n-        \"\"\"\n-        if self._bos_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.bos_token)\n-\n-    @property\n-    def eos_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the end of sentence token in the vocabulary. Returns `None` if the token has not been\n-        set.\n-        \"\"\"\n-        if self._eos_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.eos_token)\n-\n-    @property\n-    def unk_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the unknown token in the vocabulary. Returns `None` if the token has not been set.\n-        \"\"\"\n-        if self._unk_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.unk_token)\n-\n-    @property\n-    def sep_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the separation token in the vocabulary, to separate context and query in an input\n-        sequence. Returns `None` if the token has not been set.\n-        \"\"\"\n-        if self._sep_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.sep_token)\n-\n-    @property\n-    def pad_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the padding token in the vocabulary. Returns `None` if the token has not been set.\n-        \"\"\"\n-        if self._pad_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.pad_token)\n-\n     @property\n     def pad_token_type_id(self) -> int:\n         \"\"\"\n         `int`: Id of the padding token type in the vocabulary.\n         \"\"\"\n         return self._pad_token_type_id\n \n-    @property\n-    def cls_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the classification token in the vocabulary, to extract a summary of an input sequence\n-        leveraging self-attention along the full depth of the model.\n+    def __setattr__(self, key, value):\n+        key_without_id = key\n+        key_is_special_id = key.endswith(\"_id\") or key.endswith(\"_ids\")\n+        if key_is_special_id:\n+            key_without_id = key[:-3] if not key.endswith(\"_ids\") else key[:-4]\n \n-        Returns `None` if the token has not been set.\n-        \"\"\"\n-        if self._cls_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.cls_token)\n-\n-    @property\n-    def mask_token_id(self) -> Optional[int]:\n-        \"\"\"\n-        `Optional[int]`: Id of the mask token in the vocabulary, used when training a model with masked-language\n-        modeling. Returns `None` if the token has not been set.\n-        \"\"\"\n-        if self._mask_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.mask_token)\n-\n-    @property\n-    def additional_special_tokens_ids(self) -> List[int]:\n-        \"\"\"\n-        `List[int]`: Ids of all the additional special tokens in the vocabulary. Log an error if used while not having\n-        been set.\n-        \"\"\"\n-        return self.convert_tokens_to_ids(self.additional_special_tokens)\n-\n-    @bos_token_id.setter\n-    def bos_token_id(self, value):\n-        self._bos_token = self.convert_ids_to_tokens(value) if value is not None else None\n-\n-    @eos_token_id.setter\n-    def eos_token_id(self, value):\n-        self._eos_token = self.convert_ids_to_tokens(value) if value is not None else None\n-\n-    @unk_token_id.setter\n-    def unk_token_id(self, value):\n-        self._unk_token = self.convert_ids_to_tokens(value) if value is not None else None\n-\n-    @sep_token_id.setter\n-    def sep_token_id(self, value):\n-        self._sep_token = self.convert_ids_to_tokens(value) if value is not None else None\n+        if self.__dict__.get(\"_special_tokens_map\", None) is not None and any(\n+            name in self.__dict__[\"_special_tokens_map\"] for name in [key, key_without_id]\n+        ):\n+            if key_is_special_id:\n+                if value is not None:\n+                    value = (\n+                        self.convert_ids_to_tokens(value)\n+                        if key != \"additional_special_tokens\"\n+                        else [self.convert_ids_to_tokens(val) for val in value]\n+                    )\n+                key = key_without_id\n \n-    @pad_token_id.setter\n-    def pad_token_id(self, value):\n-        self._pad_token = self.convert_ids_to_tokens(value) if value is not None else None\n+            if key != \"additional_special_tokens\" and not isinstance(value, (str, AddedToken)) and value is not None:\n+                raise ValueError(f\"Cannot set a non-string value as the {key}\")\n+            self._special_tokens_map[key] = value\n+        else:\n+            super().__setattr__(key, value)\n \n-    @cls_token_id.setter\n-    def cls_token_id(self, value):\n-        self._cls_token = self.convert_ids_to_tokens(value) if value is not None else None\n+    def __getattr__(self, key):\n+        key_without_id = key\n+        key_is_special_id = key.endswith(\"_id\") or key.endswith(\"_ids\")\n+        if key_is_special_id:\n+            key_without_id = key[:-3] if not key.endswith(\"_ids\") else key[:-4]\n \n-    @mask_token_id.setter\n-    def mask_token_id(self, value):\n-        self._mask_token = self.convert_ids_to_tokens(value) if value is not None else None\n+        if self.__dict__.get(\"_special_tokens_map\", None) is not None and any(\n+            name in self.__dict__[\"_special_tokens_map\"] for name in [key, key_without_id]\n+        ):\n+            _special_tokens_map = self.__dict__[\"_special_tokens_map\"]\n+            if not key_is_special_id:\n+                if _special_tokens_map[key] is None:\n+                    if self.verbose:\n+                        logger.error(f\"Using {key}, but it is not set yet.\")\n+                    return None\n+                value = _special_tokens_map[key]\n+                return str(value) if key != \"additional_special_tokens\" else [str(tok) for tok in value]\n+            else:\n+                attr_as_tokens = getattr(self, key_without_id)\n+                return self.convert_tokens_to_ids(attr_as_tokens) if attr_as_tokens is not None else None\n \n-    @additional_special_tokens_ids.setter\n-    def additional_special_tokens_ids(self, values):\n-        self._additional_special_tokens = [self.convert_ids_to_tokens(value) for value in values]\n+        if key not in self.__dict__:\n+            raise AttributeError(f\"{self.__class__.__name__} has no attribute {key}\")\n+        else:\n+            return super().__getattr__(key)\n \n     @property\n     def special_tokens_map(self) -> Dict[str, Union[str, List[str]]]:\n@@ -1334,7 +1130,7 @@ def special_tokens_map_extended(self) -> Dict[str, Union[str, AddedToken, List[U\n         \"\"\"\n         set_attr = {}\n         for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n-            attr_value = getattr(self, \"_\" + attr)\n+            attr_value = self._special_tokens_map[attr]\n             if attr_value:\n                 set_attr[attr] = attr_value\n         return set_attr\n@@ -1379,6 +1175,20 @@ def all_special_ids(self) -> List[int]:\n         all_ids = self.convert_tokens_to_ids(all_toks)\n         return all_ids\n \n+    def _set_model_specific_special_tokens(self, special_tokens: List[str]):\n+        \"\"\"\n+        Adds new special tokens to the \"SPECIAL_TOKENS_ATTRIBUTES\" list which will be part\n+        of \"self.special_tokens\" and saved as a special token in tokenizer's config.\n+        This allows us to dynamically add new model-type specific tokens after initilizing the tokenizer.\n+        For example: if the model tokenizers is multimodal, we can support special image or audio tokens.\n+        \"\"\"\n+        self.SPECIAL_TOKENS_ATTRIBUTES = self.SPECIAL_TOKENS_ATTRIBUTES + list(special_tokens.keys())\n+        for key, value in special_tokens.items():\n+            if isinstance(value, (str, AddedToken)):\n+                self._special_tokens_map[key] = value\n+            else:\n+                raise TypeError(f\"Special token {key} has to be either str or AddedToken but got: {type(value)}\")\n+\n \n ENCODE_KWARGS_DOCSTRING = r\"\"\"\n             add_special_tokens (`bool`, *optional*, defaults to `True`):\n@@ -1633,6 +1443,9 @@ def __init__(self, **kwargs):\n \n         super().__init__(**kwargs)\n \n+        self.extra_special_tokens = kwargs.pop(\"extra_special_tokens\", {})\n+        self._set_model_specific_special_tokens(special_tokens=self.extra_special_tokens)\n+\n     @property\n     def max_len_single_sentence(self) -> int:\n         \"\"\"\n@@ -2591,8 +2404,11 @@ def save_pretrained(\n             if hasattr(self, k):\n                 tokenizer_config[k] = getattr(self, k)\n \n-        # Let's make sure we properly save the special tokens.\n+        # Let's make sure we properly save the special tokens\n         tokenizer_config.update(self.special_tokens_map)\n+        if \"extra_special_tokens\" not in tokenizer_config:\n+            tokenizer_config[\"extra_special_tokens\"] = self.extra_special_tokens\n+            tokenizer_config.update(self.extra_special_tokens)\n \n         if self.chat_template is not None:\n             if isinstance(self.chat_template, dict):"
        },
        {
            "sha": "5d238a5715ffd28b33a3fed84b4541ffbbb7d1fb",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -863,13 +863,12 @@ def train_new_from_iterator(\n         special_tokens_list = SpecialTokensMixin.SPECIAL_TOKENS_ATTRIBUTES.copy()\n         special_tokens_list.remove(\"additional_special_tokens\")\n         for token in special_tokens_list:\n-            # Get the private one to avoid unnecessary warnings.\n-            if getattr(self, f\"_{token}\") is not None:\n+            if getattr(self, token) is not None:\n                 special_token = getattr(self, token)\n                 if special_tokens_map is not None and special_token in special_tokens_map:\n                     special_token = special_tokens_map[special_token]\n \n-                special_token_full = getattr(self, f\"_{token}\")\n+                special_token_full = self._special_tokens_map.get(token, None)\n                 if isinstance(special_token_full, AddedToken):\n                     # Create an added token with the same parameters except the content\n                     kwargs[token] = AddedToken("
        },
        {
            "sha": "f661372568011106b82d33d7a1f24e26f7f06cb8",
            "filename": "tests/models/camembert/test_tokenization_camembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fcamembert%2Ftest_tokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fcamembert%2Ftest_tokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcamembert%2Ftest_tokenization_camembert.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -154,7 +154,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n                 tokenizer = self.tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n                 EXPECTED_ADDED_TOKENS_DECODER = tokenizer.added_tokens_decoder\n                 with self.subTest(\"Hub -> Slow: Test loading a slow tokenizer from the hub)\"):\n-                    self.assertEqual(tokenizer._eos_token, new_eos)\n+                    self.assertEqual(tokenizer._special_tokens_map[\"eos_token\"], new_eos)\n                     self.assertIn(new_eos, list(tokenizer.added_tokens_decoder.values()))\n \n                 with tempfile.TemporaryDirectory() as tmp_dir_2:\n@@ -194,7 +194,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n                         tokenizer_fast = self.rust_tokenizer_class.from_pretrained(\n                             pretrained_name, eos_token=new_eos, from_slow=True\n                         )\n-                        self.assertEqual(tokenizer_fast._eos_token, new_eos)\n+                        self.assertEqual(tokenizer_fast._special_tokens_map[\"eos_token\"], new_eos)\n                         self.assertIn(new_eos, list(tokenizer_fast.added_tokens_decoder.values()))\n                         # We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright\n                         with self.subTest(\"Hub -> Fast == Hub -> Slow: make sure slow and fast tokenizer match\"):"
        },
        {
            "sha": "9e39cd0279d60822ef8aec6ef5e181ffff334faa",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -1659,7 +1659,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_map = {}\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -1671,7 +1671,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:"
        },
        {
            "sha": "4a218d3f21114680a534c3d65150a148f3dc9209",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -1537,7 +1537,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_map = {}\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -1549,7 +1549,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:"
        },
        {
            "sha": "9f6d65ffc5f0a191e604bd7e27ce819d25a6cbd2",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -1588,7 +1588,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_map = {}\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -1600,7 +1600,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:"
        },
        {
            "sha": "2ae9127dee3017972e7dcf867073f8924f867854",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -385,6 +385,7 @@ def test_fast_special_tokens(self):\n         assert fast == [1, 319, 4559, 1243]\n \n         fast_tokenizer.add_eos_token = True\n+        print(fast_tokenizer.add_eos_token)\n         fast = fast_tokenizer.encode(\"A sample test\", add_special_tokens=True)\n         assert fast == [1, 319, 4559, 1243, 2]\n "
        },
        {
            "sha": "60c98776b2a40747686a0f84a141d818facb1b1a",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -1435,7 +1435,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_map = {}\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -1447,7 +1447,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:"
        },
        {
            "sha": "a520cca94bbe731e17b03dd5bfb10c4780c4a56c",
            "filename": "tests/models/moshi/test_tokenization_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -237,7 +237,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_map = {}\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -249,7 +249,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:"
        },
        {
            "sha": "113d7b7676ae851191ce638bd60c8ceac858ff0b",
            "filename": "tests/models/rembert/test_tokenization_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Frembert%2Ftest_tokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Frembert%2Ftest_tokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_tokenization_rembert.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -185,7 +185,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n                 )\n                 EXPECTED_ADDED_TOKENS_DECODER = tokenizer.added_tokens_decoder\n                 with self.subTest(\"Hub -> Slow: Test loading a slow tokenizer from the hub)\"):\n-                    self.assertEqual(tokenizer._eos_token, new_eos)\n+                    self.assertEqual(tokenizer._special_tokens_map[\"eos_token\"], new_eos)\n                     self.assertIn(new_eos, list(tokenizer.added_tokens_decoder.values()))\n \n                 with tempfile.TemporaryDirectory() as tmp_dir_2:\n@@ -223,7 +223,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n                 with self.subTest(\"Hub -> Fast: Test loading a fast tokenizer from the hub)\"):\n                     if self.rust_tokenizer_class is not None:\n                         tokenizer_fast = self.rust_tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n-                        self.assertEqual(tokenizer_fast._eos_token, new_eos)\n+                        self.assertEqual(tokenizer_fast._special_tokens_map[\"eos_token\"], new_eos)\n                         self.assertIn(new_eos, list(tokenizer_fast.added_tokens_decoder.values()))\n                         # We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright\n                         with self.subTest(\"Hub -> Fast == Hub -> Slow: make sure slow and fast tokenizer match\"):"
        },
        {
            "sha": "90d669064a0fd13975bf7f32103e3d6bbea4e9d3",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -1538,7 +1538,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_map = {}\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -1550,7 +1550,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:"
        },
        {
            "sha": "dd9eb10de40ecfbeebaaddf6a5bafae27584b08f",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -4156,8 +4156,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         special_tokens_list.remove(\"additional_special_tokens\")\n         special_tokens_map = {}\n         for token in special_tokens_list:\n-            # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is not None:\n+            if getattr(tokenizer, token) is not None:\n                 special_token = getattr(tokenizer, token)\n                 special_tokens_map[special_token] = f\"{special_token}a\"\n \n@@ -4169,7 +4168,7 @@ def test_training_new_tokenizer_with_special_tokens_change(self):\n         # Check the changes\n         for token in special_tokens_list:\n             # Get the private one to avoid unnecessary warnings.\n-            if getattr(tokenizer, f\"_{token}\") is None:\n+            if getattr(tokenizer, token) is None:\n                 continue\n             special_token = getattr(tokenizer, token)\n             if special_token in special_tokens_map:\n@@ -4411,7 +4410,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n                 tokenizer = self.tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n                 EXPECTED_ADDED_TOKENS_DECODER = tokenizer.added_tokens_decoder\n                 with self.subTest(\"Hub -> Slow: Test loading a slow tokenizer from the hub)\"):\n-                    self.assertEqual(tokenizer._eos_token, new_eos)\n+                    self.assertEqual(tokenizer._special_tokens_map[\"eos_token\"], new_eos)\n                     self.assertIn(new_eos, list(tokenizer.added_tokens_decoder.values()))\n \n                 with tempfile.TemporaryDirectory() as tmp_dir_2:\n@@ -4449,7 +4448,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n                 with self.subTest(\"Hub -> Fast: Test loading a fast tokenizer from the hub)\"):\n                     if self.rust_tokenizer_class is not None:\n                         tokenizer_fast = self.rust_tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n-                        self.assertEqual(tokenizer_fast._eos_token, new_eos)\n+                        self.assertEqual(tokenizer_fast._special_tokens_map[\"eos_token\"], new_eos)\n                         self.assertIn(new_eos, list(tokenizer_fast.added_tokens_decoder.values()))\n                         # We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright\n                         with self.subTest(\"Hub -> Fast == Hub -> Slow: make sure slow and fast tokenizer match\"):"
        },
        {
            "sha": "5171af6730081314d9831cbcc893610947576ce2",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -28,6 +28,7 @@\n     BatchEncoding,\n     BertTokenizer,\n     BertTokenizerFast,\n+    LlamaTokenizerFast,\n     PreTrainedTokenizer,\n     PreTrainedTokenizerFast,\n     TensorType,\n@@ -280,6 +281,54 @@ def test_decoding_single_token(self):\n                 self.assertEqual(decoded_flat, \"##\")\n                 self.assertEqual(decoded_list, \"##\")\n \n+    def test_extra_special_tokens_multimodal(self):\n+        special_tokens_list = [\n+            \"bos_token\",\n+            \"eos_token\",\n+            \"unk_token\",\n+            \"sep_token\",\n+            \"pad_token\",\n+            \"cls_token\",\n+            \"mask_token\",\n+            \"additional_special_tokens\",\n+        ]\n+        llama_tokenizer = LlamaTokenizerFast.from_pretrained(\"huggyllama/llama-7b\")\n+        llama_tokenizer.extra_special_tokens = {\n+            \"boi_token\": \"<image_start>\",\n+            \"eoi_token\": \"<image_end>\",\n+            \"image_token\": \"<image>\",\n+        }\n+        self.assertListEqual(llama_tokenizer.SPECIAL_TOKENS_ATTRIBUTES, special_tokens_list)\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            llama_tokenizer.save_pretrained(tmpdirname)\n+\n+            # load back and check we have extra special tokens set\n+            loaded_tokenizer = LlamaTokenizerFast.from_pretrained(tmpdirname)\n+            multimodal_special_tokens_list = special_tokens_list + [\"boi_token\", \"eoi_token\", \"image_token\"]\n+            self.assertListEqual(loaded_tokenizer.SPECIAL_TOKENS_ATTRIBUTES, multimodal_special_tokens_list)\n+\n+            # We set an image_token_id before, so we can get an \"image_token\" as str that matches the id\n+            self.assertTrue(loaded_tokenizer.image_token == \"<image>\")\n+            self.assertTrue(loaded_tokenizer.image_token_id == loaded_tokenizer.convert_tokens_to_ids(\"<image>\"))\n+\n+        # save one more time and make sure the image token can get loaded back\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            loaded_tokenizer.save_pretrained(tmpdirname)\n+            loaded_tokenizer_with_extra_tokens = LlamaTokenizerFast.from_pretrained(tmpdirname)\n+            self.assertTrue(loaded_tokenizer_with_extra_tokens.image_token == \"<image>\")\n+\n+        # test that we can also indicate extra tokens during load time\n+        extra_special_tokens = {\n+            \"boi_token\": \"<image_start>\",\n+            \"eoi_token\": \"<image_end>\",\n+            \"image_token\": \"<image>\",\n+        }\n+        tokenizer = LlamaTokenizerFast.from_pretrained(\n+            \"huggyllama/llama-7b\", extra_special_tokens=extra_special_tokens\n+        )\n+        self.assertTrue(tokenizer.image_token == \"<image>\")\n+        self.assertTrue(tokenizer.image_token_id == loaded_tokenizer.convert_tokens_to_ids(\"<image>\"))\n+\n     @require_tokenizers\n     def test_decoding_skip_special_tokens(self):\n         for tokenizer_class in [BertTokenizer, BertTokenizerFast]:"
        },
        {
            "sha": "70870be7718bee9b0ec916ba3b0b1ffab2b66111",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/187439c3fa139b2102a874483e9f8f0cfa8e5557/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=187439c3fa139b2102a874483e9f8f0cfa8e5557",
            "patch": "@@ -299,7 +299,7 @@ def _test_no_pad_and_pad(self, no_pad_features, pad_features):\n         self.assertEqual(batch[\"input_ids\"].shape, torch.Size((2, 16)))\n         self.assertEqual(batch[\"labels\"].shape, torch.Size((2, 16)))\n \n-        tokenizer._pad_token = None\n+        tokenizer.pad_token = None\n         data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n         with self.assertRaises(ValueError):\n             # Expect error due to padding token missing\n@@ -978,7 +978,7 @@ def _test_no_pad_and_pad(self, no_pad_features, pad_features):\n         self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 16])\n         self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 16])\n \n-        tokenizer._pad_token = None\n+        tokenizer.pad_token = None\n         data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")\n         with self.assertRaises(ValueError):\n             # Expect error due to padding token missing\n@@ -1673,7 +1673,7 @@ def _test_no_pad_and_pad(self, no_pad_features, pad_features):\n         self.assertEqual(batch[\"input_ids\"].shape, (2, 16))\n         self.assertEqual(batch[\"labels\"].shape, (2, 16))\n \n-        tokenizer._pad_token = None\n+        tokenizer.pad_token = None\n         data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"np\")\n         with self.assertRaises(ValueError):\n             # Expect error due to padding token missing"
        }
    ],
    "stats": {
        "total": 581,
        "additions": 247,
        "deletions": 334
    }
}