{
    "author": "ydshieh",
    "message": "Fix `qwen3` tests (#38862)\n\n* fix\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* format\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "2507169bf658e39e6ffe89a04b32e3729b218b73",
    "files": [
        {
            "sha": "3f3f5bae083e045e1e375cc79051906731603b18",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 25,
            "deletions": 28,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/2507169bf658e39e6ffe89a04b32e3729b218b73/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2507169bf658e39e6ffe89a04b32e3729b218b73/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=2507169bf658e39e6ffe89a04b32e3729b218b73",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Qwen3 model.\"\"\"\n \n-import gc\n import unittest\n \n import pytest\n@@ -23,7 +22,7 @@\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     Expectations,\n-    backend_empty_cache,\n+    cleanup,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -109,6 +108,12 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n @require_torch\n class Qwen3IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_600m_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n@@ -117,15 +122,12 @@ def test_model_600m_logits(self):\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-1.4577, 1.3261, 3.8498, 3.4229, 2.9009, 1.8813, 2.1530, 2.1431]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        EXPECTED_MEAN = torch.tensor([[-1.3789, 1.3029, 3.8262, 3.4637, 2.8796, 1.8357, 2.1290, 2.1814]])\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-4, atol=1e-4)\n         # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([5.9062, 6.0938, 5.5625, 3.8594, 2.6094, 1.9531, 4.3125, 4.9375, 3.8906, 3.1094, 3.6719, 5.1562, 6.9062, 5.7500, 5.4062, 7.0625, 8.7500, 8.7500, 8.1250, 7.9375, 8.0625, 7.5312, 7.3750, 7.2188, 7.2500, 5.8750, 2.8750, 4.3438, 2.3438, 2.2500])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+        EXPECTED_SLICE = torch.tensor([4.6905, 4.9243, 4.7101, 3.2052, 2.2683, 1.6576, 3.6529, 3.9800, 3.2605, 2.6475, 3.0468, 4.2296, 5.7443, 4.8940, 4.4883, 6.0323, 7.4057, 7.3710, 6.8373, 6.6323, 6.7114, 6.3069, 6.1751, 6.0416, 6.0793, 4.6975, 2.3286, 3.6387, 2.0757, 1.9813])  # fmt: skip\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_model_600m_generation(self):\n@@ -140,10 +142,6 @@ def test_model_600m_generation(self):\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @require_bitsandbytes\n     @slow\n     @require_flash_attn\n@@ -169,33 +167,29 @@ def test_model_600m_long_prompt(self):\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n-        del assistant_model\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @slow\n     @require_torch_sdpa\n     def test_model_600m_long_prompt_sdpa(self):\n-        EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n+        EXPECTED_OUTPUT_TOKEN_IDS = [198, 198]\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n         model = Qwen3ForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\", device_map=\"auto\", attn_implementation=\"sdpa\")\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n+\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n         # Assisted generation\n         assistant_model = model\n         assistant_model.generation_config.num_assistant_tokens = 2\n         assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"\n         generated_ids = assistant_model.generate(input_ids, max_new_tokens=4, temperature=0)\n+\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n         del assistant_model\n \n-        backend_empty_cache(torch_device)\n-        gc.collect()\n+        cleanup(torch_device, gc_collect=True)\n \n         EXPECTED_TEXT_COMPLETION = \"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"\n         prompt = \"My favourite condiment is \"\n@@ -206,13 +200,19 @@ def test_model_600m_long_prompt_sdpa(self):\n         # greedy generation outputs\n         generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n     def test_speculative_generation(self):\n-        EXPECTED_TEXT_COMPLETION = (\n-            \"My favourite condiment is 100% peanut butter. I love it so much that I can't help but use it\"\n-        )\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"cuda\", 7): \"My favourite condiment is 100% natural. It's a little spicy and a little sweet, but it's the\",\n+                (\"cuda\", 8): \"My favourite condiment is 100% peanut butter. I love it so much that I can't help but use it\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n+\n         prompt = \"My favourite condiment is \"\n         tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B-Base\", use_fast=False)\n         model = Qwen3ForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B-Base\", device_map=\"auto\", torch_dtype=torch.float16)\n@@ -227,11 +227,8 @@ def test_speculative_generation(self):\n             input_ids, max_new_tokens=20, do_sample=True, temperature=0.3, assistant_model=assistant_model\n         )\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n-        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n     def test_export_static_cache(self):"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 25,
        "deletions": 28
    }
}