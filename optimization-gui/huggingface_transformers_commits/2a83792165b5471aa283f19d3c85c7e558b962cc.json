{
    "author": "cyyever",
    "message": "Remove dict branch of attention_mask in sdpa_attention_paged_forward (#40882)\n\nRemove dict branch of attention_mask\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "2a83792165b5471aa283f19d3c85c7e558b962cc",
    "files": [
        {
            "sha": "528eb6ca07735efb13379c378048779ab4c8e204",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a83792165b5471aa283f19d3c85c7e558b962cc/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a83792165b5471aa283f19d3c85c7e558b962cc/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=2a83792165b5471aa283f19d3c85c7e558b962cc",
            "patch": "@@ -39,12 +39,7 @@ def sdpa_attention_paged_forward(\n         value = repeat_kv(value, module.num_key_value_groups)\n \n     # Get the right causal mask for the current layer\n-    if isinstance(attention_mask, dict):\n-        sliding_window = getattr(module, \"sliding_window\", 1)\n-        layer_type = \"full_attention\" if sliding_window == 1 or sliding_window is None else \"sliding_attention\"\n-        causal_mask = attention_mask[layer_type]\n-    else:\n-        causal_mask = attention_mask\n+    causal_mask = attention_mask\n \n     # Run the actual attention\n     query = query.contiguous()"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 1,
        "deletions": 6
    }
}