{
    "author": "molbap",
    "message": ":broom: :broom: :broom: Get set decoder cleanup (#39509)\n\n* simplify common get/set\n\n* remove some noise\n\n* change some 5 years old modeling utils\n\n* update examples\n\n* fix copies\n\n* revert some changes\n\n* fixes, gah\n\n* format\n\n* move to Mixin\n\n* remove smolvlm specific require grad\n\n* skip\n\n* force defaults\n\n* remodularise some stuff\n\n* remodularise more stuff\n\n* add safety for audio models\n\n* style\n\n* have a correct fallback, you daft donkey\n\n* remove this argh\n\n* change heuristic for audio models\n\n* fixup\n\n* revert\n\n* this works\n\n* this should be explicit\n\n* fix Nth ESM exception\n\n* tryout decoder\n\n* this as well\n\n* revert again\n\n* ðŸ§ \n\n* aaah ESM has two modelings aaah\n\n* broom broom\n\n* format\n\n* wrong copies\n\n* copies\n\n* modular cleanups\n\n* format\n\n* modularities\n\n* wrong mergefix\n\n* seriously\n\n* align with new model\n\n* new model",
    "sha": "ba095d387dde8a5fe7574ebbb2671dbb0991b215",
    "files": [
        {
            "sha": "11e6719479a496db74dd1b031eb56ff4cc9b2c3e",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1272,9 +1272,6 @@ def __init__(self, config: TestDetrConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "68c0eb469e48d608d690e3a5bf27f57a4f9a232c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -3017,6 +3017,47 @@ def disable_input_require_grads(self):\n         \"\"\"\n         self._require_grads_hook.remove()\n \n+    def get_decoder(self):\n+        \"\"\"\n+        Best-effort lookup of the *decoder* module.\n+\n+        Order of attempts (covers ~85 % of current usages):\n+\n+        1. `self.decoder`\n+        2. `self.model`                       (many wrappers store the decoder here)\n+        3. `self.model.get_decoder()`         (nested wrappers)\n+        4. fallback: raise for the few exotic models that need a bespoke rule\n+        \"\"\"\n+        if hasattr(self, \"decoder\"):\n+            return self.decoder\n+\n+        if hasattr(self, \"model\"):\n+            inner = self.model\n+            if hasattr(inner, \"get_decoder\"):\n+                return inner.get_decoder()\n+            return inner\n+\n+        return None  # raise AttributeError(f\"{self.__class__.__name__} has no decoder; override `get_decoder()` if needed.\")\n+\n+    def set_decoder(self, decoder):\n+        \"\"\"\n+        Symmetric setter. Mirrors the lookup logic used in `get_decoder`.\n+        \"\"\"\n+\n+        if hasattr(self, \"decoder\"):\n+            self.decoder = decoder\n+            return\n+\n+        if hasattr(self, \"model\"):\n+            inner = self.model\n+            if hasattr(inner, \"set_decoder\"):\n+                inner.set_decoder(decoder)\n+            else:\n+                self.model = decoder\n+            return\n+\n+        return  # raise AttributeError(f\"{self.__class__.__name__} cannot accept a decoder; override `set_decoder()`.\")\n+\n     def _init_weights(self, module):\n         \"\"\"\n         Initialize the weights. This is quite general on purpose, in the spirit of what we usually do. For more complex"
        },
        {
            "sha": "7d9afa092def998bac2f73d06f4956e5f70bcbd7",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -420,12 +420,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "a93c363421676d616caa230fa2dd6d01e88b8ed3",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -800,12 +800,6 @@ def __init__(self, config: AriaTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "879d63b37a95f8c545fd02c289459d18f137398e",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1424,9 +1424,6 @@ def create_network_inputs(\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "f269088db5c02ef6c8041101fb1a4431db638493",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1359,12 +1359,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "03122a4ce335db1bc2274824498209485e9f8409",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1197,9 +1197,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "92c629f80cc567d83bf037530ad45960fd997788",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -889,9 +889,6 @@ def __init__(self, config: BeitConfig) -> None:\n         self.post_init()\n \n     def get_output_embeddings(self):\n-        # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n-        # Vision models like BEiT use a Conv2d patch embed layer (no `.weight`) so calling tie_weights() would fail.\n-        # See e.g. https://github.com/huggingface/transformers/pull/39339#discussion_r2219126400\n         return None\n \n     @auto_docstring"
        },
        {
            "sha": "a334cb0f8597fdefd17251eebf2af6a948e20e2c",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -2351,9 +2351,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "0af7c794b155fa989da445960376069885b30bf6",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -419,12 +419,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "22970cd85234f6b22f6f51d7273d207c44d11570",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1155,9 +1155,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d8e6f45b449b39b46564bc3b63a94d540607ae14",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1127,9 +1127,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "12b7e570ef8f89964f935f4e03dd867c190bc8d0",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1044,12 +1044,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     def get_image_tokens(self, pixel_values):\n         return self.model.get_image_tokens(pixel_values)\n "
        },
        {
            "sha": "0c2509377de9842b544c89cc7b2f0a5b85c523e2",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1187,9 +1187,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.input_embeds_layer = value\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1260,8 +1257,6 @@ def __init__(self, config):\n         self.post_init()\n \n     def get_output_embeddings(self):\n-        # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n-        # See e.g. https://github.com/huggingface/transformers/pull/39339#discussion_r2219126400\n         return None\n \n     def get_input_embeddings(self):"
        },
        {
            "sha": "c342b95994b174b42e52a98e4f920d19e6376411",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -454,12 +454,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "384e1c22088e5ab263f43e345906cfe342fb3945",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -433,12 +433,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ada06d87f7cc0c6a89b8cca9fc0e309b383d0158",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1325,9 +1325,6 @@ def __init__(self, config: ConditionalDetrConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "2318c3896e429b0020434cbc23130f9fc1fcc50f",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -542,12 +542,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "03d5cfc0f39cfe67833ae69e8ee0aa9fcd90828d",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1355,9 +1355,6 @@ def __init__(self, config: DFineConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "4b7a27e7663b62fb1fd663595e9c6b4b16e9db9e",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1203,9 +1203,6 @@ def __init__(self, config: DabDetrConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "00d729732f7a8e1c88fd3c965d83730b5b0db900",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -567,12 +567,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f82806e4412df5ed39e82702221bb84c9f511d6a",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -611,12 +611,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "04504f4bc8cae1432f0f7a6e7672d1e48812ca26",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -274,12 +274,6 @@ def set_input_embeddings(self, value):\n     def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n         raise AttributeError(\"Not needed for DeepseekVL\")\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3796a3b04af50887e344c6eda4c0bb11c31e551c",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -420,12 +420,6 @@ def set_input_embeddings(self, value):\n     def prepare_embeddings_for_image_generation(self) -> torch.Tensor:\n         raise AttributeError(\"Not needed for DeepseekVLHybrid\")\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring(custom_args=DEEPSEEK_VL_COMMON_CUSTOM_ARGS)\n     def forward("
        },
        {
            "sha": "df3499889a559f9236db60f82fc8dbc095bdc7a1",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1365,9 +1365,6 @@ def __init__(self, config: DeformableDetrConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "a05f82ed072a813713370cbf1fb527a3415bebec",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1490,9 +1490,6 @@ def __init__(self, config: DetaConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "a6c6ab449685e86bf5873384e7c30cc0983a13ff",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -677,12 +677,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "d526b7d6504858dfb831b65533c6be64ba45908e",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1733,9 +1733,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @add_start_docstrings_to_model_forward(XLM_PROPHETNET_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=XLMProphetNetSeq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "86835ca62cfc02f7e2f8f134c9824c3eda1f31e5",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1059,9 +1059,6 @@ def __init__(self, config: DetrConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "77a888e7a3c28d4b5a313b1cffdb2325a7c14917",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -727,9 +727,6 @@ def __init__(self, config: DiaConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "267c444d952cc0e1aea6277f205e6ee919cb971e",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -542,9 +542,6 @@ def __init__(self, config: DiaConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward("
        },
        {
            "sha": "094cc375057f71eb51644bf2b49c524613ed22e1",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -684,12 +684,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "b94667398507e8b8a57b451e0a4573dfa69cb9d8",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -719,12 +719,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ea500c064512ddd1813e8b34e302e38fb05cfa57",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -543,12 +543,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1add5407d336f2c32e2f72b94bebb805d258090d",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1243,12 +1243,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c3c32f5bd61d03f680fbc291323b7161244ed025",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -499,9 +499,6 @@ def __init__(self, config: EncodecConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _encode_frame(\n         self, input_values: torch.Tensor, bandwidth: float\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:"
        },
        {
            "sha": "5f1ba03e121d8bb17f3d09550abf4938ac215328",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -194,9 +194,6 @@ def _init_weights(self, module):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n "
        },
        {
            "sha": "bcb3e3dfed4897a6e76413cc87355e3a3ba3b427",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -278,9 +278,6 @@ def __init__(\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n "
        },
        {
            "sha": "13ec6fb3a3b619e2e6315745ae3a6f37038fdbcd",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -418,12 +418,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "5f6071ecc40aae2dc33b021245277b3740d6c965",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -675,12 +675,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2618a4aa3b5b6681b5cddded0cb1383fdf192a4d",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -440,12 +440,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "779cfd14bde008506d471442cd3fb8a71a9d4b48",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1478,12 +1478,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "fa523bbaf971cee793f21439eaf102ffd57dc463",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -906,9 +906,6 @@ def __init__(self, config: FSMTConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self._tie_or_clone_weights(self.decoder.embed_tokens, self.get_input_embeddings())"
        },
        {
            "sha": "39876712f1e451faf95e4fa71a9ef2a86c6acf42",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -426,12 +426,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d4427a08e23ebc1e60484bf2f6860743a7c366df",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -499,12 +499,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3bb3998183a16f7b05e6abe9bcb8a831f071112c",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -600,12 +600,6 @@ def __init__(self, config: Gemma3TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "037bcc516ecf8116938d6572e2e678781336af0f",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1771,12 +1771,6 @@ def __init__(self, config: Gemma3nTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "59c9f39da5270f9ca7cac1cd9c42e605105a743a",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -434,12 +434,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "dafab297f5667610284121a2bfe818bfcc12b3df",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -438,12 +438,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "cb695ffbe638c18dcc8295e3974980d21ee8d243",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -552,12 +552,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c71b43b6e1ed4a237c04904279592ede43d498b3",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -614,12 +614,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ed495b94e85900c3e90520bc332cbd86486ee39c",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -490,12 +490,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d40cae4063dee45c42d874416668aac9413320ca",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -889,12 +889,6 @@ def __init__(self, config: GraniteMoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "fbf2baedf809aa5f55e44b03dc027aad00e7a9f7",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1666,12 +1666,6 @@ def __init__(self, config: GraniteMoeHybridConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "7f4428b0a8556ca66f98ff8352e0466720f7bc6c",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -947,12 +947,6 @@ def __init__(self, config: GraniteMoeSharedConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "939213e78d82b3e777dcd8fd3ecae6d0d1b5c453",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1981,12 +1981,6 @@ def __init__(self, config: GroundingDinoConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "9f4a2e73affd76f079ecfafb2b51ba88e0ccf252",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -419,12 +419,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "120cc5edbd0c34ecdd201f8fcc42f7a9d2453a1a",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -441,12 +441,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "7b0bfcbc19b7ae0d6bf52cb0dd66938ef0a29100",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -512,12 +512,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3da2259e0865fc38e6f66092e112352e0a3d984f",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1342,12 +1342,6 @@ def __init__(self, config, vision_model=None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     def tie_weights(self):\n         \"\"\"\n         Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of"
        },
        {
            "sha": "0e8e75be28f89eb70f61e874b52c458a78e41de7",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1607,12 +1607,6 @@ def __init__(self, config, vision_model=None, **kwargs):\n             name=\"lm_head\",\n         )\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     def tie_weights(self):\n         \"\"\"\n         Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of"
        },
        {
            "sha": "a75d6143cd16038a8558ca63773d466fea2e1d94",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1461,9 +1461,6 @@ def create_network_inputs(\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "db7461af73d77aead7f62bdc66a0a48d62504e99",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1297,12 +1297,6 @@ def __init__(self, config: JambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "8ee43bd29184debff7d92a883da50d301a245945",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1145,12 +1145,6 @@ def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch\n         hidden_state = self.model.generation_aligner(hidden_state)\n         return hidden_state\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2ae65710b2a5289c153032949bef0c63ccb3f63a",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1005,12 +1005,6 @@ def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch\n         hidden_state = self.model.generation_aligner(hidden_state)\n         return hidden_state\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "24ff01b16a2442d989781b7001594ad1bc0dbcb7",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1132,14 +1132,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "12325301f7d94bcbc6126d901bdb97447843866d",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1089,12 +1089,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2a72cc60b20bf4cef9fcea377e6c6b6ab073091e",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1886,9 +1886,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d802bd8e81a9a4be729255cbd947338951935887",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -690,12 +690,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f43a5fc9b5237b852cbc0db5bc333711797eaa97",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -424,12 +424,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1d974fecd8467040b03801fa7c68ed39434d7143",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -490,6 +490,7 @@ def __init__(self, config: Llama4TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -578,12 +579,6 @@ def __init__(self, config: Llama4TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2576dd44e6f9ae2b2450beebd1409491a74870f5",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1730,9 +1730,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _prune_heads(self, heads_to_prune):\n         \"\"\"\n         Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n@@ -1933,9 +1930,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c547e8be4cf75fb3fe705d0603c2030b325a96b3",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1206,9 +1206,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "745eced0d08d6ba99d793cca88ab02c4c451df0c",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1165,9 +1165,6 @@ def set_decoder_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def resize_decoder_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n         if self.config.share_encoder_decoder_embeddings:\n             raise ValueError("
        },
        {
            "sha": "3865bb2cf6d8afba26c5d1da49087ab40d99285d",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1181,9 +1181,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self._tie_or_clone_weights(self.encoder.embed_tokens, self.get_input_embeddings())"
        },
        {
            "sha": "5d5a936e8219d76178debb11003c0a46a1bbad67",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1455,9 +1455,6 @@ def __init__(self, config: MimiConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _encode_frame(\n         self,\n         input_values: torch.Tensor,"
        },
        {
            "sha": "a7c6dfe35ada087db09423e71e7582818dd3c8dd",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -824,12 +824,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "5b7c7b2c17900641f6b7d540fbec1ee2faf59c33",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -398,12 +398,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d3ca7d13b6a88f27fe1957510da75b84865ae2dd",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -809,12 +809,6 @@ def __init__(self, config, *inputs, **kwargs):\n         )\n         self.config = config\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @unpack_inputs\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     def call("
        },
        {
            "sha": "b5786e910d311408a8e4575cfb74f0d3c9964ba3",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -580,12 +580,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "aa43c81dedbc442bfebaa9c75ac1abb7a648f447",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1299,12 +1299,6 @@ def __init__(self, config):\n \n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c138f910a679a37d59622dee7f07b6a98aa4f2da",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1872,12 +1872,6 @@ def __init__(self, config: MMGroundingDinoConfig):\n \n         self.post_init()\n \n-    def get_encoder(self):\n-        return self.encoder\n-\n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "64cd6eaf65d230e878b8f00a497e0fd420c17553",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -834,9 +834,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the Moonshine encoder so that its parameters will"
        },
        {
            "sha": "99c4429344a247ae67d5f32ef30acfe213ec3f5a",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1494,12 +1494,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1633,9 +1627,6 @@ def get_audio_encoder(self):\n     def get_depth_decoder(self):\n         return self.depth_decoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "52608b191bce092d28c52b4298487b20425f456c",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1376,10 +1376,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Model.get_decoder\n-    def get_decoder(self):\n-        return self.decoder\n-\n     # Copied from transformers.models.t5.modeling_t5.T5Model._prune_heads\n     def _prune_heads(self, heads_to_prune):\n         \"\"\"\n@@ -1647,10 +1643,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_decoder\n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.forward with google-t5/->google/, T5->MT5, t5->mt5\n     def forward(\n@@ -2282,10 +2274,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_decoder\n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.forward\n     def forward("
        },
        {
            "sha": "1547e7c80d9c3158e2567a2cb82554f8ec6c811f",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -741,9 +741,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1485,9 +1482,6 @@ def get_encoder(self):\n         # get the text encoder to compute the encoder hidden-states for generation\n         return self.get_text_encoder()\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.text_encoder.get_input_embeddings()\n "
        },
        {
            "sha": "6041c2bc554f7b7ec660e77b84fcd0ffa6f7b2c8",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -679,9 +679,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     # Ignore copy\n     def forward(\n@@ -1370,9 +1367,6 @@ def get_encoder(self):\n         # get the text encoder to compute the conditioning hidden-states for generation\n         return self.get_text_encoder()\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.text_encoder.get_input_embeddings()\n "
        },
        {
            "sha": "b3b4e2e38db527f35c5a75db86eab93a5842e21a",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -987,9 +987,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def set_lightweight_tuning(self):\n         assert self.use_prompt, \"If you want to use lightweight tuning, make sure that `use_prompt=True`.\"\n "
        },
        {
            "sha": "8e942966b6bb9da841c84588169e28d22e66cde1",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -868,12 +868,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "7f447a93114a85308eb836fdc86322b863faec41",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1458,9 +1458,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "2d6e6e7092cbc3c94c85b3a9680f8949ed2f9410",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -401,12 +401,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "3fe4cfaf91dea9804a96963c947bacf4e99174e4",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -406,12 +406,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "375f8b4ec3c731434d70a8d1442c91b99504a5cc",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -997,12 +997,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "739ecffec14fa3bfbb99e61f0d71285293410619",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -702,9 +702,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "97d25f07b0999704590bdba6b906c1acbd2ccccc",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1187,9 +1187,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !="
        },
        {
            "sha": "bd00dff7a51590e81f5587c937a7d218f16a125e",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1437,9 +1437,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !="
        },
        {
            "sha": "d11d07d8d34b911aea83bace5a6b0e6b0f2dc47f",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -676,14 +676,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1e9e019e7b6964eae61de817e44aef4078aec77e",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -444,12 +444,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "23820075a02056eb6b358cc25529f60b0a784364",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -430,12 +430,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "494f1fb038bd4f6253be3db11ca764cf3796ab8c",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1735,12 +1735,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ec311754efab49afb510e7ebc48626cf7dedad89",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1209,14 +1209,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "fe60e03a65e45c98dffe8845d3caca86b7e2f1ae",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1419,9 +1419,6 @@ def get_output_embeddings(self) -> nn.Module:\n     def set_output_embeddings(self, new_embeddings):\n         self.decoder.set_output_embeddings(new_embeddings)\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "39709e95c08dc84f9bf7cf63a4be6d0fe0219c0d",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1125,9 +1125,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "8d7f0022cfb4e930dd82f60f7d4e0a6175f3310e",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -299,9 +299,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1a77172dcc9f322f566ef0d258b2dd1e50530682",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1049,9 +1049,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_mel_conditioner_outputs(\n         self,\n         input_features: torch.FloatTensor,"
        },
        {
            "sha": "10d8a158a06909bff3e99bed6ffa452442304150",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1510,9 +1510,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "2fcb44372fe46c9aae160d702279fd7776c37c10",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -414,12 +414,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "fce02864bc3dfcdb20165d54c3af9909de9c5fc7",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1684,12 +1684,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):"
        },
        {
            "sha": "1ee9411cd7aa51810340d6bb7f21d9ee162f5055",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -2133,12 +2133,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     def get_video_features(\n         self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n     ):"
        },
        {
            "sha": "238ca1fee4cb0060a85e4caaba84c53dd051c800",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1064,12 +1064,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "81b16c4ee6b6eb30acc531bf8cddd31d07065ac1",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -440,12 +440,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "62790435f17b89f916ab0464a9ed30fa2cc400c5",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -603,12 +603,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "daef714ab883d379b8b4c7fd20fe0d87dc2b98b1",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -699,12 +699,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     # Ignore copy\n     def forward("
        },
        {
            "sha": "1d4b64496969acb596b037841c745b109f3d11b5",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1544,9 +1544,6 @@ def __init__(self, config: RTDetrConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "55c5419984fd666bc9cdbbfedceaa0a49a589356",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1522,9 +1522,6 @@ def __init__(self, config: RTDetrV2Config):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "f0be87883d942a56cfc593d132def63b3fc945ad",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -425,12 +425,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1e08e288193b7d39ad30a61e4ed994c415840769",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -444,12 +444,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "450f5fec9ce30eff8fc752b93e2151ef70a4f249",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -147,9 +147,6 @@ def __init__(\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.decoder.get_input_embeddings()\n "
        },
        {
            "sha": "3692c7781dff3d5b795c8e96b5a5e620a4e304c9",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1060,9 +1060,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "cb6f76cf0dda387a15fa652afe426bf1bc4f4c39",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1982,9 +1982,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_feature_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the feature encoder so that its parameter will"
        },
        {
            "sha": "c87e7a11174815231f4da03e0554bb3b256add6c",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -904,14 +904,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     # Ignore copy"
        },
        {
            "sha": "dfdfec22ca99cf1f205dcaa103ca448afe8498dd",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -407,12 +407,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "5395499ade102abd97bd2c0c498f27b4a6e8ae48",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1259,9 +1259,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _prune_heads(self, heads_to_prune):\n         \"\"\"\n         Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n@@ -1482,9 +1479,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1feee1827f2325c58fa67d48e6061f3e5c1689f7",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1371,9 +1371,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def _prune_heads(self, heads_to_prune):\n         \"\"\"\n         Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n@@ -1624,9 +1621,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2234,9 +2228,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "142a0f73115e2adc2a057cdc4911d20fa131e501",
            "filename": "src/transformers/models/t5/modeling_tf_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_tf_t5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1194,9 +1194,6 @@ def __init__(self, config, *inputs, **kwargs):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @unpack_inputs\n     @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TFSeq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n@@ -1371,9 +1368,6 @@ def set_output_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @unpack_inputs\n     @add_start_docstrings_to_model_forward(T5_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TFSeq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)"
        },
        {
            "sha": "fda1e4e7b14153200318e1e599e037d61e934214",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -899,9 +899,6 @@ def __init__(self, config: T5GemmaConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n "
        },
        {
            "sha": "c8267488747b739a7211f12bb7b0fccd79c292ed",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -761,9 +761,6 @@ def __init__(self, config: T5GemmaConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n "
        },
        {
            "sha": "d55ea67400755faec5ad051b4950eb1189eb596b",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1024,9 +1024,6 @@ def __init__(self, config: TableTransformerConfig):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_backbone(self):\n         for name, param in self.backbone.conv_encoder.model.named_parameters():\n             param.requires_grad_(False)"
        },
        {
            "sha": "6f6d943a0edf33b0e38dd0fbbd5340e7cbdc07db",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1238,9 +1238,6 @@ def create_network_inputs(\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "cda924063c9f43f144a2c4e984930ce5b73c7a1f",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1519,9 +1519,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1718,9 +1715,6 @@ def set_input_embeddings(self, new_embeddings):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "e2b296379364847610ed7672c0fb98d16879f2bc",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -995,10 +995,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Model.get_decoder\n-    def get_decoder(self):\n-        return self.decoder\n-\n     # Copied from transformers.models.t5.modeling_t5.T5Model._prune_heads\n     def _prune_heads(self, heads_to_prune):\n         \"\"\"\n@@ -1212,10 +1208,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_decoder\n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1786,10 +1778,6 @@ def _tie_weights(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_decoder\n-    def get_decoder(self):\n-        return self.decoder\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "9e0c805dc454c8a78511ab135a79229d2ab149b3",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -276,9 +276,6 @@ def input_signature(self):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.encoder.get_input_embeddings()\n "
        },
        {
            "sha": "ae74acb331e727e0e1d61dffe60bac59d2759b47",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -148,9 +148,6 @@ def __init__(\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def get_input_embeddings(self):\n         return self.decoder.get_input_embeddings()\n "
        },
        {
            "sha": "45dad7c51f7e31fba774664983b9449bd9c8e0fd",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1107,9 +1107,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     @add_start_docstrings_to_model_forward(WHISPER_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TFSeq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n     @unpack_inputs"
        },
        {
            "sha": "0dd5c1c36b46312ddaf7ebfde64e3b4ed3f5ed66",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -995,9 +995,6 @@ def set_input_embeddings(self, value):\n     def get_encoder(self):\n         return self.encoder\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def freeze_encoder(self):\n         \"\"\"\n         Calling this function will disable the gradient computation for the Whisper encoder so that its parameters will\n@@ -1360,9 +1357,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n-    def get_decoder(self):\n-        return self.decoder\n-\n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n "
        },
        {
            "sha": "9c0f86ea44892de3be584f5cfa6abcbbfcf6898c",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1046,12 +1046,6 @@ def __init__(self, config: ZambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "6881f1278e6e71e6b15dd469486e5131397ba71e",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba095d387dde8a5fe7574ebbb2671dbb0991b215/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=ba095d387dde8a5fe7574ebbb2671dbb0991b215",
            "patch": "@@ -1469,12 +1469,6 @@ def __init__(self, config: Zamba2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n-\n     @auto_docstring\n     def forward(\n         self,"
        }
    ],
    "stats": {
        "total": 736,
        "additions": 42,
        "deletions": 694
    }
}