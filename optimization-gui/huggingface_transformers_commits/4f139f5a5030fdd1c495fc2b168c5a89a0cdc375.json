{
    "author": "ydshieh",
    "message": "Send trainer/fsdp/deepspeed CI job reports to a single channel (#37411)\n\n* send trainer/fsdd/deepspeed channel\n\n* update\n\n* change name\n\n* no .\n\n* final\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "4f139f5a5030fdd1c495fc2b168c5a89a0cdc375",
    "files": [
        {
            "sha": "95584176d6ce72e4c64f41731f8c570d8cca7a6d",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=4f139f5a5030fdd1c495fc2b168c5a89a0cdc375",
            "patch": "@@ -18,6 +18,10 @@ on:\n       docker:\n         required: true\n         type: string\n+      report_name_prefix:\n+        required: false\n+        default: run_models_gpu\n+        type: string\n \n env:\n   HF_HOME: /mnt/cache\n@@ -116,23 +120,23 @@ jobs:\n \n       - name: Run all tests on GPU\n         working-directory: /transformers\n-        run: python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n+        run: python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/failures_short.txt\n \n       - name: Run test\n         shell: bash\n         run: |\n-          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n+          mkdir -p /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports\n+          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/hello.txt\n+          echo \"${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports\"\n \n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          name: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports"
        },
        {
            "sha": "8589f4a810b629ad0b70231542eb264a8e8d242f",
            "filename": ".github/workflows/self-scheduled-caller.yml",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-caller.yml?ref=4f139f5a5030fdd1c495fc2b168c5a89a0cdc375",
            "patch": "@@ -54,12 +54,23 @@ jobs:\n       ci_event: Daily CI\n     secrets: inherit\n \n+  trainer-fsdp-ci:\n+    name: Trainer/FSDP CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_trainer_and_fsdp_gpu\n+      slack_report_channel: \"#transformers-ci-daily-training\"\n+      runner: daily-ci\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+    secrets: inherit\n+\n   deepspeed-ci:\n     name: DeepSpeed CI\n     uses: ./.github/workflows/self-scheduled.yml\n     with:\n       job: run_torch_cuda_extensions_gpu\n-      slack_report_channel: \"#transformers-ci-daily-deepspeed\"\n+      slack_report_channel: \"#transformers-ci-daily-training\"\n       runner: daily-ci\n       docker: huggingface/transformers-pytorch-deepspeed-latest-gpu\n       ci_event: Daily CI"
        },
        {
            "sha": "7fce6d60800944d836a3be26158af0d24e2124ac",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 29,
            "deletions": 8,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=4f139f5a5030fdd1c495fc2b168c5a89a0cdc375",
            "patch": "@@ -45,7 +45,7 @@ env:\n \n jobs:\n   setup:\n-    if: contains(fromJSON('[\"run_models_gpu\", \"run_quantization_torch_gpu\"]'), inputs.job)\n+    if: contains(fromJSON('[\"run_models_gpu\", \"run_trainer_and_fsdp_gpu\", \"run_quantization_torch_gpu\"]'), inputs.job)\n     name: Setup\n     strategy:\n       matrix:\n@@ -77,12 +77,17 @@ jobs:\n         run: pip freeze\n \n       - id: set-matrix\n-        if: ${{ inputs.job == 'run_models_gpu' }}\n+        if: contains(fromJSON('[\"run_models_gpu\", \"run_trainer_and_fsdp_gpu\"]'), inputs.job)\n         name: Identify models to test\n         working-directory: /transformers/tests\n         run: |\n-          echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n-          echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n+          if [ \"${{ inputs.job }}\" = \"run_models_gpu\" ]; then\n+            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n+            echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n+          elif [ \"${{ inputs.job }}\" = \"run_trainer_and_fsdp_gpu\" ]; then\n+            echo \"folder_slices=[['trainer'], ['fsdp']]\" >> $GITHUB_OUTPUT\n+            echo \"slice_ids=[0, 1]\" >> $GITHUB_OUTPUT\n+          fi\n \n       - id: set-matrix-quantization\n         if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n@@ -113,6 +118,25 @@ jobs:\n       docker: ${{ inputs.docker }}\n     secrets: inherit\n \n+  run_trainer_and_fsdp_gpu:\n+    if: ${{ inputs.job == 'run_trainer_and_fsdp_gpu' }}\n+    name: \" \"\n+    needs: setup\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+        slice_id: [0, 1]\n+    uses: ./.github/workflows/model_jobs.yml\n+    with:\n+      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n+      machine_type: ${{ matrix.machine_type }}\n+      slice_id: ${{ matrix.slice_id }}\n+      runner: ${{ inputs.runner }}\n+      docker: ${{ inputs.docker }}\n+      report_name_prefix: run_trainer_and_fsdp_gpu\n+    secrets: inherit\n+\n   run_pipelines_torch_gpu:\n     if: ${{ inputs.job == 'run_pipelines_torch_gpu' }}\n     name: PyTorch pipelines\n@@ -336,10 +360,6 @@ jobs:\n         working-directory: ${{ inputs.working-directory-prefix }}/transformers\n         run: git fetch && git checkout ${{ github.sha }}\n \n-      # TODO: update the docker image instead\n-      - name: Reinstall some packages with specific versions\n-        run: python3 -m pip install numpy==1.24.3 numba==0.61.0 scipy==1.12.0 scikit-learn==1.6.1\n-\n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n         working-directory: ${{ inputs.working-directory-prefix }}/transformers\n         run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n@@ -545,6 +565,7 @@ jobs:\n     needs: [\n       setup,\n       run_models_gpu,\n+      run_trainer_and_fsdp_gpu,\n       run_pipelines_torch_gpu,\n       run_pipelines_tf_gpu,\n       run_examples_gpu,"
        },
        {
            "sha": "dd01b082f4a92452005d24056bb318bbf0cac54c",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 26,
            "deletions": 8,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f139f5a5030fdd1c495fc2b168c5a89a0cdc375/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=4f139f5a5030fdd1c495fc2b168c5a89a0cdc375",
            "patch": "@@ -942,7 +942,6 @@ def prepare_reports(title, header, reports, to_truncate=True):\n     # To find the PR number in a commit title, for example, `Add AwesomeFormer model (#99999)`\n     pr_number_re = re.compile(r\"\\(#(\\d+)\\)$\")\n \n-    title = f\"ðŸ¤— Results of {ci_event} - {os.getenv('CI_TEST_JOB')}.\"\n     # Add Commit/PR title with a link for push CI\n     # (check the title in 2 env. variables - depending on the CI is triggered via `push` or `workflow_run` event)\n     ci_title_push = os.environ.get(\"CI_TITLE_PUSH\")\n@@ -994,6 +993,8 @@ def prepare_reports(title, header, reports, to_truncate=True):\n     else:\n         ci_title = \"\"\n \n+    # `title` will be updated at the end before calling `Message()`.\n+    title = f\"ðŸ¤— Results of {ci_event}\"\n     if runner_not_available or runner_failed or setup_failed:\n         Message.error_out(title, ci_title, runner_not_available, runner_failed, setup_failed)\n         exit(0)\n@@ -1041,6 +1042,11 @@ def prepare_reports(title, header, reports, to_truncate=True):\n         \"Unclassified\",\n     ]\n \n+    job_name = os.getenv(\"CI_TEST_JOB\")\n+    report_name_prefix = \"run_models_gpu\"\n+    if job_name == \"run_trainer_and_fsdp_gpu\":\n+        report_name_prefix = job_name\n+\n     # This dict will contain all the information relative to each model:\n     # - Failures: the total, as well as the number of failures per-category defined above\n     # - Success: total\n@@ -1055,13 +1061,13 @@ def prepare_reports(title, header, reports, to_truncate=True):\n             \"job_link\": {},\n         }\n         for model in models\n-        if f\"run_models_gpu_{model}_test_reports\" in available_artifacts\n+        if f\"{report_name_prefix}_{model}_test_reports\" in available_artifacts\n     }\n \n     unclassified_model_failures = []\n \n     for model in model_results.keys():\n-        for artifact_path in available_artifacts[f\"run_models_gpu_{model}_test_reports\"].paths:\n+        for artifact_path in available_artifacts[f\"{report_name_prefix}_{model}_test_reports\"].paths:\n             artifact = retrieve_artifact(artifact_path[\"path\"], artifact_path[\"gpu\"])\n             if \"stats\" in artifact:\n                 # Link to the GitHub Action job\n@@ -1123,7 +1129,7 @@ def prepare_reports(title, header, reports, to_truncate=True):\n         \"PyTorch pipelines\": \"run_pipelines_torch_gpu_test_reports\",\n         \"TensorFlow pipelines\": \"run_pipelines_tf_gpu_test_reports\",\n         \"Examples directory\": \"run_examples_gpu_test_reports\",\n-        \"Torch CUDA extension tests\": \"run_torch_cuda_extensions_gpu_test_reports\",\n+        \"DeepSpeed\": \"run_torch_cuda_extensions_gpu_test_reports\",\n     }\n \n     if ci_event in [\"push\", \"Nightly CI\"] or ci_event.startswith(\"Past CI\"):\n@@ -1132,7 +1138,7 @@ def prepare_reports(title, header, reports, to_truncate=True):\n         del additional_files[\"TensorFlow pipelines\"]\n     elif ci_event.startswith(\"Scheduled CI (AMD)\"):\n         del additional_files[\"TensorFlow pipelines\"]\n-        del additional_files[\"Torch CUDA extension tests\"]\n+        del additional_files[\"DeepSpeed\"]\n     elif ci_event.startswith(\"Push CI (AMD)\"):\n         additional_files = {}\n \n@@ -1143,12 +1149,11 @@ def prepare_reports(title, header, reports, to_truncate=True):\n         \"run_pipelines_torch_gpu\": \"PyTorch pipelines\",\n         \"run_pipelines_tf_gpu\": \"TensorFlow pipelines\",\n         \"run_examples_gpu\": \"Examples directory\",\n-        \"run_torch_cuda_extensions_gpu\": \"Torch CUDA extension tests\",\n+        \"run_torch_cuda_extensions_gpu\": \"DeepSpeed\",\n     }\n \n     # Remove some entries in `additional_files` if they are not concerned.\n     test_name = None\n-    job_name = os.getenv(\"CI_TEST_JOB\")\n     if job_name in job_to_test_map:\n         test_name = job_to_test_map[job_name]\n     additional_files = {k: v for k, v in additional_files.items() if k == test_name}\n@@ -1243,7 +1248,7 @@ def prepare_reports(title, header, reports, to_truncate=True):\n         \"PyTorch pipelines\": \"torch_pipeline\",\n         \"TensorFlow pipelines\": \"tf_pipeline\",\n         \"Examples directory\": \"example\",\n-        \"Torch CUDA extension tests\": \"deepspeed\",\n+        \"DeepSpeed\": \"deepspeed\",\n     }\n     for job, job_result in additional_results.items():\n         with open(f\"ci_results_{job_name}/{test_to_result_name[job]}_results.json\", \"w\", encoding=\"UTF-8\") as fp:\n@@ -1270,6 +1275,19 @@ def prepare_reports(title, header, reports, to_truncate=True):\n                 artifact_names=artifact_names, output_dir=output_dir, token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"]\n             )\n \n+    job_to_test_map.update(\n+        {\n+            \"run_models_gpu\": \"Models\",\n+            \"run_trainer_and_fsdp_gpu\": \"Trainer & FSDP\",\n+        }\n+    )\n+\n+    ci_name_in_report = \"\"\n+    if job_name in job_to_test_map:\n+        ci_name_in_report = job_to_test_map[job_name]\n+\n+    title = f\"ðŸ¤— Results of {ci_event}: {ci_name_in_report}\"\n+\n     message = Message(\n         title,\n         ci_title,"
        }
    ],
    "stats": {
        "total": 104,
        "additions": 79,
        "deletions": 25
    }
}