{
    "author": "ahadnagy",
    "message": "Benchmarking v2 GH workflows (#40716)\n\n* WIP benchmark v2 workflow\n\n* Container was missing\n\n* Change to sandbox branch name\n\n* Wrong place for image name\n\n* Variable declarations\n\n* Remove references to file logging\n\n* Remove unnecessary step\n\n* Fix deps install\n\n* Syntax\n\n* Add workdir\n\n* Add upload feature\n\n* typo\n\n* No need for hf_transfer\n\n* Pass in runner\n\n* Runner config\n\n* Runner config\n\n* Runner config\n\n* Runner config\n\n* Runner config\n\n* mi325 caller\n\n* Name workflow runs properly\n\n* Copy-paste error\n\n* Add final repo IDs and schedule\n\n* Review comments\n\n* Remove wf params\n\n* Remove parametrization from worfkflow files\n\n* Fix callers\n\n* Change push trigger to pull_request + label\n\n* Add back schedule event\n\n* Push to the same dataset\n\n* Simplify parameter description",
    "sha": "61eff450d39742ac28789fe7a2a133395b133ace",
    "files": [
        {
            "sha": "350ad0144101d886d59bc1e64a4fe05534d6c5eb",
            "filename": ".github/workflows/benchmark_v2.yml",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/.github%2Fworkflows%2Fbenchmark_v2.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/.github%2Fworkflows%2Fbenchmark_v2.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark_v2.yml?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -0,0 +1,82 @@\n+name: Benchmark v2 Framework\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      runner:\n+        description: 'GH Actions runner group to use'\n+        required: true\n+        type: string\n+      commit_sha:\n+        description: 'Commit SHA to benchmark'\n+        required: false\n+        type: string\n+        default: ''\n+      upload_to_hub:\n+        description: 'Uploading results to a HuggingFace Dataset'\n+        required: false\n+        type: string\n+        default: 'false'\n+      run_id:\n+        description: 'Custom run ID for organizing results (auto-generated if not provided)'\n+        required: false\n+        type: string\n+        default: ''\n+      benchmark_repo_id:\n+        description: 'HuggingFace Dataset to upload results to (e.g., \"org/benchmark-results\")'\n+        required: false\n+        type: string\n+        default: ''\n+\n+env:\n+  HF_HOME: /mnt/cache\n+  TRANSFORMERS_IS_CI: yes\n+  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n+  # This token is created under the bot `hf-transformers-bot`.\n+  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n+\n+jobs:\n+  benchmark-v2:\n+    name: Benchmark v2\n+    runs-on: ${{ inputs.runner }}\n+    if: |\n+      (github.event_name == 'pull_request' && contains( github.event.pull_request.labels.*.name, 'run-benchmark')) ||\n+      (github.event_name == 'schedule')\n+    container:\n+      image: huggingface/transformers-pytorch-gpu\n+      options: --gpus all --privileged --ipc host --shm-size \"16gb\"\n+    steps:\n+      - name: Get repo\n+        uses: actions/checkout@v4\n+        with:\n+          ref: ${{ inputs.commit_sha || github.sha }}\n+\n+      - name: Install benchmark dependencies\n+        run: |\n+          python3 -m pip install -r benchmark_v2/requirements.txt\n+\n+      - name: Reinstall transformers in edit mode\n+        run: |\n+          python3 -m pip uninstall -y transformers\n+          python3 -m pip install -e \".[torch]\"\n+\n+      - name: Show installed libraries and their versions\n+        run: |\n+          python3 -m pip list\n+          python3 -c \"import torch; print(f'PyTorch version: {torch.__version__}')\"\n+          python3 -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\n+          python3 -c \"import torch; print(f'CUDA device count: {torch.cuda.device_count()}')\" || true\n+          nvidia-smi || true\n+\n+      - name: Run benchmark v2\n+        working-directory: benchmark_v2\n+        run: |\n+          echo \"Running benchmarks\"\n+          python3 run_benchmarks.py \\\n+          --commit-id '${{ inputs.commit_sha || github.sha }}' \\\n+          --upload-to-hub '${{ inputs.upload_to_hub || false}}' \\\n+          --run-id '${{ inputs.run_id }}' \\\n+          --benchmark-repo-id '${{ inputs.benchmark_repo_id}}' \\\n+          --log-level INFO\n+        env:\n+          HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n\\ No newline at end of file"
        },
        {
            "sha": "30b5e8be78a5c634e1a3f495c705cf0c52007307",
            "filename": ".github/workflows/benchmark_v2_a10_caller.yml",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/.github%2Fworkflows%2Fbenchmark_v2_a10_caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/.github%2Fworkflows%2Fbenchmark_v2_a10_caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark_v2_a10_caller.yml?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -0,0 +1,20 @@\n+name: Benchmark v2 Scheduled Runner - A10 Single-GPU\n+\n+on:\n+  schedule:\n+    # Run daily at 16:30 UTC\n+    - cron: \"30 16 * * *\"\n+  pull_request:\n+    types: [ opened, labeled, reopened, synchronize ]\n+\n+jobs:\n+  benchmark-v2-default:\n+    name: Benchmark v2 - Default Models\n+    uses: ./.github/workflows/benchmark_v2.yml\n+    with:\n+      runner: aws-g5-4xlarge-cache-use1-public-80\n+      commit_sha: ${{ github.sha }}\n+      upload_to_hub: true\n+      run_id: ${{ github.run_id }}\n+      benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n+    secrets: inherit\n\\ No newline at end of file"
        },
        {
            "sha": "95fbeb5e5f6af2db0f9b9c82cf4c3906870b4bf9",
            "filename": ".github/workflows/benchmark_v2_mi325_caller.yml",
            "status": "added",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/.github%2Fworkflows%2Fbenchmark_v2_mi325_caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/.github%2Fworkflows%2Fbenchmark_v2_mi325_caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark_v2_mi325_caller.yml?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -0,0 +1,20 @@\n+name: Benchmark v2 Scheduled Runner - MI325 Single-GPU\n+\n+on:\n+  schedule:\n+    # Run daily at 16:30 UTC\n+    - cron: \"30 16 * * *\"\n+  pull_request:\n+    types: [ opened, labeled, reopened, synchronize ]\n+\n+jobs:\n+  benchmark-v2-default:\n+    name: Benchmark v2 - Default Models\n+    uses: ./.github/workflows/benchmark_v2.yml\n+    with:\n+      runner: amd-mi325-ci-1gpu\n+      commit_sha: ${{ github.sha }}\n+      upload_to_hub: true\n+      run_id: ${{ github.run_id }}\n+      benchmark_repo_id: hf-internal-testing/transformers-daily-benchmarks\n+    secrets: inherit\n\\ No newline at end of file"
        },
        {
            "sha": "1d34de6408c70703962da9d199b100b4a0ee563b",
            "filename": "benchmark_v2/README.md",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2FREADME.md?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -21,6 +21,36 @@ python run_benchmarks.py \\\n     --num-tokens-to-generate 200\n ```\n \n+### Uploading Results to HuggingFace Dataset\n+\n+You can automatically upload benchmark results to a HuggingFace Dataset for tracking and analysis:\n+\n+```bash\n+# Upload to a public dataset with auto-generated run ID\n+python run_benchmarks.py --upload-to-hf username/benchmark-results\n+\n+# Upload with a custom run ID for easy identification\n+python run_benchmarks.py --upload-to-hf username/benchmark-results --run-id experiment_v1\n+```\n+\n+**Dataset Directory Structure:**\n+```\n+dataset_name/\n+├── 2025-01-15/\n+│   ├── runs/                       # Non-scheduled runs (manual, PR, etc.)\n+│   │   └── 123-1245151651/         # GitHub run number and ID\n+│   │       └── benchmark_results/\n+│   │           ├── benchmark_summary_20250115_143022.json\n+│   │           └── model-name/\n+│   │               └── model-name_benchmark_20250115_143022.json\n+│   └── benchmark_results_abc123de/ # Scheduled runs (daily CI)\n+│       ├── benchmark_summary_20250115_143022.json\n+│       └── model-name/\n+│           └── model-name_benchmark_20250115_143022.json\n+└── 2025-01-16/\n+    └── ...\n+```\n+\n ### Running Specific Benchmarks\n \n ```bash"
        },
        {
            "sha": "2349e75f1347d95717e484a2e31e7abfab5309af",
            "filename": "benchmark_v2/benches/llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fbenches%2Fllama.py?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -20,7 +20,6 @@\n from benchmark_framework import ModelBenchmark\n \n \n-os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n torch.set_float32_matmul_precision(\"high\")\n "
        },
        {
            "sha": "e4dcbb3eb7efaa2d5475213a413ac8b2086e8377",
            "filename": "benchmark_v2/requirements.txt",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frequirements.txt?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -3,4 +3,5 @@ psutil>=5.8.0\n gpustat>=1.0.0\n torch>=2.0.0\n transformers>=4.30.0\n-datasets>=2.10.0 \n\\ No newline at end of file\n+datasets>=2.10.0\n+huggingface_hub>=0.16.0 \n\\ No newline at end of file"
        },
        {
            "sha": "44f6515a2c3079a7db1a385302c4cd38e76dc63a",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 157,
            "deletions": 8,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61eff450d39742ac28789fe7a2a133395b133ace/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=61eff450d39742ac28789fe7a2a133395b133ace",
            "patch": "@@ -24,6 +24,7 @@\n import logging\n import os\n import sys\n+import uuid\n from datetime import datetime\n from pathlib import Path\n from typing import Any, Optional\n@@ -160,14 +161,20 @@ def run_single_benchmark(\n         return None\n \n \n-def generate_summary_report(output_dir: str, benchmark_results: dict[str, Any], logger: logging.Logger) -> str:\n+def generate_summary_report(\n+    output_dir: str,\n+    benchmark_results: dict[str, Any],\n+    logger: logging.Logger,\n+    benchmark_run_uuid: Optional[str] = None,\n+) -> str:\n     \"\"\"Generate a summary report of all benchmark runs.\"\"\"\n     timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n     summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.json\")\n \n     summary_data = {\n         \"run_metadata\": {\n             \"timestamp\": datetime.utcnow().isoformat(),\n+            \"benchmark_run_uuid\": benchmark_run_uuid,\n             \"total_benchmarks\": len(benchmark_results),\n             \"successful_benchmarks\": len([r for r in benchmark_results.values() if r is not None]),\n             \"failed_benchmarks\": len([r for r in benchmark_results.values() if r is None]),\n@@ -183,9 +190,115 @@ def generate_summary_report(output_dir: str, benchmark_results: dict[str, Any],\n     return summary_file\n \n \n+def upload_results_to_hf_dataset(\n+    output_dir: str,\n+    summary_file: str,\n+    dataset_name: str,\n+    run_id: Optional[str] = None,\n+    logger: Optional[logging.Logger] = None,\n+) -> Optional[str]:\n+    \"\"\"\n+    Upload benchmark results to a HuggingFace Dataset.\n+    Based on upload_collated_report() from utils/collated_reports.py\n+    Args:\n+        output_dir: Local output directory containing results\n+        summary_file: Path to the summary file\n+        dataset_name: Name of the HuggingFace dataset to upload to\n+        run_id: Unique run identifier (if None, will generate one)\n+        logger: Logger instance\n+    Returns:\n+        The run_id used for the upload, None if upload failed\n+    \"\"\"\n+    if logger is None:\n+        logger = logging.getLogger(__name__)\n+\n+    import os\n+\n+    from huggingface_hub import HfApi\n+\n+    api = HfApi()\n+\n+    if run_id is None:\n+        github_run_number = os.getenv(\"GITHUB_RUN_NUMBER\")\n+        github_run_id = os.getenv(\"GITHUB_RUN_ID\")\n+        if github_run_number and github_run_id:\n+            run_id = f\"{github_run_number}-{github_run_id}\"\n+\n+    date_folder = datetime.now().strftime(\"%Y-%m-%d\")\n+\n+    github_event_name = os.getenv(\"GITHUB_EVENT_NAME\")\n+    if github_event_name != \"schedule\":\n+        # Non-scheduled runs go under a runs subfolder\n+        repo_path = f\"{date_folder}/runs/{run_id}/benchmark_results\"\n+    else:\n+        # Scheduled runs go directly under the date\n+        repo_path = f\"{date_folder}/{run_id}/benchmark_results\"\n+\n+    logger.info(f\"Uploading benchmark results to dataset '{dataset_name}' at path '{repo_path}'\")\n+\n+    try:\n+        # Get the authentication token (prioritize specific token, fallback to HF_TOKEN)\n+        token = os.getenv(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\") or os.getenv(\"HF_TOKEN\")\n+\n+        # Upload all files in the output directory\n+        from pathlib import Path\n+\n+        output_path = Path(output_dir)\n+\n+        for file_path in output_path.rglob(\"*\"):\n+            if file_path.is_file():\n+                # Calculate relative path from output_dir\n+                relative_path = file_path.relative_to(output_path)\n+                path_in_repo = f\"{repo_path}/{relative_path}\"\n+\n+                logger.debug(f\"Uploading {file_path} to {path_in_repo}\")\n+\n+                api.upload_file(\n+                    path_or_fileobj=str(file_path),\n+                    path_in_repo=path_in_repo,\n+                    repo_id=dataset_name,\n+                    repo_type=\"dataset\",\n+                    token=token,\n+                    commit_message=f\"Upload benchmark results for run {run_id}\",\n+                )\n+\n+        logger.info(\n+            f\"Successfully uploaded results to: https://huggingface.co/datasets/{dataset_name}/tree/main/{repo_path}\"\n+        )\n+\n+        return run_id\n+\n+    except Exception as upload_error:\n+        logger.error(f\"Failed to upload results: {upload_error}\")\n+        import traceback\n+\n+        logger.debug(traceback.format_exc())\n+        return None\n+\n+\n def main():\n     \"\"\"Main entry point for the benchmarking script.\"\"\"\n-    parser = argparse.ArgumentParser(description=\"Run all benchmarks in the ./benches directory\")\n+    # Generate a unique UUID for this benchmark run\n+    benchmark_run_uuid = str(uuid.uuid4())[:8]\n+\n+    parser = argparse.ArgumentParser(\n+        description=\"Run all benchmarks in the ./benches directory\",\n+        epilog=\"\"\"\n+Examples:\n+  # Run all available benchmarks\n+  python3 run_benchmarks.py\n+  \n+  # Run with specific model and upload to HuggingFace Dataset\n+  python3 run_benchmarks.py --model-id meta-llama/Llama-2-7b-hf --upload-to-hf username/benchmark-results\n+  \n+  # Run with custom run ID and upload to HuggingFace Dataset\n+  python3 run_benchmarks.py --run-id experiment_v1 --upload-to-hf org/benchmarks\n+  \n+  # Run only specific benchmarks with file logging\n+  python3 run_benchmarks.py --include llama --enable-file-logging\n+        \"\"\",  # noqa: W293\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n \n     parser.add_argument(\n         \"--output-dir\",\n@@ -228,20 +341,29 @@ def main():\n \n     parser.add_argument(\"--exclude\", type=str, nargs=\"*\", help=\"Exclude benchmarks matching these names\")\n \n-    parser.add_argument(\"--enable-mock\", action=\"store_true\", help=\"Enable mock benchmark (skipped by default)\")\n-\n     parser.add_argument(\"--enable-file-logging\", action=\"store_true\", help=\"Enable file logging (disabled by default)\")\n \n     parser.add_argument(\n         \"--commit-id\", type=str, help=\"Git commit ID for metadata (if not provided, will auto-detect from git)\"\n     )\n \n+    parser.add_argument(\n+        \"--upload-to-hub\",\n+        type=str,\n+        help=\"Upload results to HuggingFace Dataset (provide dataset name, e.g., 'username/benchmark-results')\",\n+    )\n+\n+    parser.add_argument(\n+        \"--run-id\", type=str, help=\"Custom run ID for organizing results (if not provided, will generate a unique ID)\"\n+    )\n+\n     args = parser.parse_args()\n \n     # Setup logging\n     logger = setup_logging(args.log_level, args.enable_file_logging)\n \n     logger.info(\"Starting benchmark discovery and execution\")\n+    logger.info(f\"Benchmark run UUID: {benchmark_run_uuid}\")\n     logger.info(f\"Output directory: {args.output_dir}\")\n     logger.info(f\"Benches directory: {args.benches_dir}\")\n \n@@ -286,9 +408,6 @@ def main():\n         if args.model_id:\n             benchmark_kwargs[\"model_id\"] = args.model_id\n \n-        # Add enable_mock flag for mock benchmark\n-        benchmark_kwargs[\"enable_mock\"] = args.enable_mock\n-\n         # Add commit_id if provided\n         if args.commit_id:\n             benchmark_kwargs[\"commit_id\"] = args.commit_id\n@@ -306,7 +425,27 @@ def main():\n                 successful_count += 1\n \n         # Generate summary report\n-        summary_file = generate_summary_report(args.output_dir, benchmark_results, logger)\n+        summary_file = generate_summary_report(args.output_dir, benchmark_results, logger, benchmark_run_uuid)\n+\n+        # Upload results to HuggingFace Dataset if requested\n+        upload_run_id = None\n+        if args.upload_to_hub:\n+            logger.info(\"=\" * 60)\n+            logger.info(\"UPLOADING TO HUGGINGFACE DATASET\")\n+            logger.info(\"=\" * 60)\n+            # Use provided run_id or fallback to benchmark run UUID\n+            effective_run_id = args.run_id or benchmark_run_uuid\n+            upload_run_id = upload_results_to_hf_dataset(\n+                output_dir=args.output_dir,\n+                summary_file=summary_file,\n+                dataset_name=args.upload_to_hub,\n+                run_id=effective_run_id,\n+                logger=logger,\n+            )\n+            if upload_run_id:\n+                logger.info(f\"Upload completed with run ID: {upload_run_id}\")\n+            else:\n+                logger.warning(\"Upload failed - continuing with local results\")\n \n         # Final summary\n         total_benchmarks = len(filtered_benchmarks)\n@@ -321,6 +460,16 @@ def main():\n         logger.info(f\"Output directory: {args.output_dir}\")\n         logger.info(f\"Summary report: {summary_file}\")\n \n+        if args.upload_to_hub:\n+            if upload_run_id:\n+                logger.info(f\"HuggingFace Dataset: {args.upload_to_hub}\")\n+                logger.info(f\"Run ID: {upload_run_id}\")\n+                logger.info(\n+                    f\"View results: https://huggingface.co/datasets/{args.upload_to_hub}/tree/main/{datetime.now().strftime('%Y-%m-%d')}/runs/{upload_run_id}\"\n+                )\n+            else:\n+                logger.warning(\"Upload to HuggingFace Dataset failed\")\n+\n         if failed_count > 0:\n             logger.warning(f\"{failed_count} benchmark(s) failed. Check logs for details.\")\n             return 1"
        }
    ],
    "stats": {
        "total": 321,
        "additions": 311,
        "deletions": 10
    }
}