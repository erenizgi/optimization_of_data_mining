{
    "author": "ydshieh",
    "message": "Fix `FalconMambaIntegrationTests` (#38566)\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "5d26a387359d669d74f14effbdc859f907133647",
    "files": [
        {
            "sha": "e59787fb8c653f7d1202414cba1d8b6f0efa4c45",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 52,
            "deletions": 12,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d26a387359d669d74f14effbdc859f907133647/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d26a387359d669d74f14effbdc859f907133647/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=5d26a387359d669d74f14effbdc859f907133647",
            "patch": "@@ -19,9 +19,12 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, FalconMambaConfig, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_large_accelerator,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     slow,\n@@ -450,15 +453,30 @@ def setUp(self):\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n         self.text = \"Hello today\"\n \n-    def test_generation_bf16(self):\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    # On T4, get `NotImplementedError: Cannot copy out of meta tensor; no data!`\n+    @require_torch_large_accelerator\n+    def test_generation_fp16(self):\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16, device_map=\"auto\")\n \n         inputs = self.tokenizer(self.text, return_tensors=\"pt\").to(torch_device)\n         out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n \n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"cuda\", 7): \"Hello today I am going to show you how to make a simple and easy to make paper plane.\\nStep\",\n+                (\"cuda\", 8): 'Hello today Iava,\\n\\nI am writing to you today to discuss the importance of maintaining a healthy lifestyle',\n+            }\n+        )  # fmt: skip\n+        EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n+\n         self.assertEqual(\n             self.tokenizer.batch_decode(out, skip_special_tokens=False)[0],\n-            \"Hello today I am going to show you how to make a simple and easy to make paper plane.\\nStep\",\n+            EXPECTED_OUTPUT,\n         )\n \n     @require_bitsandbytes\n@@ -471,19 +489,19 @@ def test_generation_4bit(self):\n \n         self.assertEqual(\n             self.tokenizer.batch_decode(out, skip_special_tokens=False)[0],\n-            \"\"\"Hello today I'm going to talk about the \"C\" in the \"C-I-\"\"\",\n+            \"Hello today Iava,\\n\\nI'm sorry to hear that you're having trouble with the \",\n         )\n \n     def test_generation_torch_compile(self):\n-        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16).to(torch_device)\n+        model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16).to(torch_device)\n         model = torch.compile(model)\n \n         inputs = self.tokenizer(self.text, return_tensors=\"pt\").to(torch_device)\n         out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n \n         self.assertEqual(\n             self.tokenizer.batch_decode(out, skip_special_tokens=False)[0],\n-            \"Hello today I am going to show you how to make a simple and easy to make paper plane.\\nStep\",\n+            \"Hello today Iava,\\n\\nI am writing to you today to discuss the importance of maintaining a healthy lifestyle\",\n         )\n \n     def test_batched_generation(self):\n@@ -493,13 +511,22 @@ def test_batched_generation(self):\n \n         texts = [\"Hello today\", \"Hello my name is Younes and today\"]\n \n-        EXPECTED_OUTPUT = [\n-            \"Hello today I'm going to show you how to make a 3D model of a house.\\n\",\n-            \"Hello my name is Younes and today I will be talking about the topic of “The importance of the internet in our life”.\\n\",\n-        ]\n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    'Hello today I will be talking about the “Theory of Relativity” by Albert Einstein.\\nThe',\n+                    'Hello my name is Younes and today I will be talking about the importance of the internet in our lives.\\nThe internet is a global',\n+                ],\n+                (\"cuda\", 8): [\n+                    'Hello today I am going to talk about the “Theory of Relativity” by Albert Einstein.\\n',\n+                    'Hello my name is Younes and today I will be talking about the importance of the internet in our lives.\\nThe internet is a global',\n+                ],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n \n         inputs = tok(texts, return_tensors=\"pt\", padding=True, return_token_type_ids=False).to(torch_device)\n-        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0, torch_dtype=torch.bfloat16)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=0, torch_dtype=torch.float16)\n \n         out = model.generate(**inputs, max_new_tokens=20)\n         out = tok.batch_decode(out, skip_special_tokens=True)\n@@ -514,14 +541,27 @@ def test_batched_generation(self):\n         out = model.generate(**inputs, max_new_tokens=20)\n         out = tok.batch_decode(out, skip_special_tokens=True)\n \n+        EXPECTED_OUTPUTS = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    ' I will be talking about the “Theory of Relativity” by Albert Einstein.\\nThe',\n+                    ' I will be talking about the importance of the internet in our lives.\\nThe internet is a global',\n+                ],\n+                (\"cuda\", 8): [\n+                    ' I am going to talk about the “Theory of Relativity” by Albert Einstein.\\n',\n+                    ' I will be talking about the importance of the internet in our lives.\\nThe internet is a global'\n+                ],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_OUTPUT = EXPECTED_OUTPUTS.get_expectation()\n         self.assertListEqual(out, EXPECTED_OUTPUT)\n \n     @require_torch_multi_accelerator\n     def test_training_kernel(self):\n         model_id = \"tiiuae/falcon-mamba-7b\"\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\n+        model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.float16)\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n \n         text = \"Hello today\""
        }
    ],
    "stats": {
        "total": 64,
        "additions": 52,
        "deletions": 12
    }
}