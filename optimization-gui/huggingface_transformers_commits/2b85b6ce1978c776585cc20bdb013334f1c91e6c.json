{
    "author": "eustlb",
    "message": "[Whisper] ðŸš¨ Fix pipeline word timestamp: timestamp token is end of token time !!! (#36632)\n\n* timestamp token is end of token time !!!\n\n* ensure correct alignment between tokens and timestamp tokens\n\n* ignore input tokens for DTW computation\n\n* use num_frames to avoid token timestamp hallucinations\n\n* token timestamps test updates !\n\n* num_frames: deprecate and use attention_mask instead\n\n* avoid breaking change\n\n* fix the pipeline usage for chunk approach\n\n* make style\n\n* better logging\n\n* better logging\n\n* make style\n\n* update tests with correct values",
    "sha": "2b85b6ce1978c776585cc20bdb013334f1c91e6c",
    "files": [
        {
            "sha": "68c52c6eb3c82f7ffbf2a008b48d428584f2ed7e",
            "filename": "src/transformers/models/whisper/feature_extraction_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py?ref=2b85b6ce1978c776585cc20bdb013334f1c91e6c",
            "patch": "@@ -252,6 +252,8 @@ def __call__(\n                 Specifies the device for computation of the log-mel spectrogram of audio signals in the\n                 `_torch_extract_fbank_features` method. (e.g., \"cpu\", \"cuda\")\n             return_token_timestamps (`bool`, *optional*, defaults to `None`):\n+                Deprecated. Use `return_attention_mask` instead from which the number of frames can be inferred.\n+\n                 Whether or not to return the number of frames of the input raw_speech.\n                 These num_frames can be used by the model to compute word level timestamps.\n         \"\"\"\n@@ -327,6 +329,9 @@ def __call__(\n             padded_inputs[\"attention_mask\"] = padded_inputs[\"attention_mask\"][:, :: self.hop_length]\n \n         if return_token_timestamps is not None:\n+            logger.warning_once(\n+                f\"`return_token_timestamps` is deprecated for {self.__class__.__name__} and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\"\n+            )\n             padded_inputs[\"num_frames\"] = [len(raw_speech_i) // self.hop_length for raw_speech_i in raw_speech]\n \n         if return_tensors is not None:"
        },
        {
            "sha": "248d17cac404ef288f5eaa288f1944c6f7f4f065",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 46,
            "deletions": 11,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=2b85b6ce1978c776585cc20bdb013334f1c91e6c",
            "patch": "@@ -331,6 +331,11 @@ def _extract_token_timestamps(\n                 num_frames = num_frames.cpu() if isinstance(num_frames, (torch.Tensor)) else num_frames\n                 num_frames = np.repeat(num_frames, repeat_time)\n \n+        # let's ignore decoder_input_ids that can negatively impact the DTW while we know they have timestamps 0.0s\n+        # (they are not taken into account for the DTW in OAI implementation)\n+        if num_input_ids is not None:\n+            weights = weights[:, :, num_input_ids:, :]\n+\n         if num_frames is None or isinstance(num_frames, int):\n             # Normalize and smoothen the weights.\n             std = torch.std(weights, dim=-2, keepdim=True, unbiased=False)\n@@ -360,7 +365,13 @@ def _extract_token_timestamps(\n             text_indices, time_indices = _dynamic_time_warping(-matrix.cpu().double().numpy())\n             jumps = np.pad(np.diff(text_indices), (1, 0), constant_values=1).astype(bool)\n             jump_times = time_indices[jumps] * time_precision\n-            timestamps[batch_idx, 1:] = torch.tensor(jump_times)\n+\n+            # each predicted token has a corresponding timestamp, expect the eos token for which we don't retrieve cross attentions\n+            # 1. for decoder_input_ids, we set the timestamps to 0.0\n+            # 2. for the eos token, we simply duplicate the timestamp of the last non-eos token\n+            timestamps[batch_idx] = torch.cat(\n+                [torch.zeros(num_input_ids), torch.tensor(jump_times), torch.tensor([jump_times[-1]])]\n+            )\n \n         return timestamps\n \n@@ -632,7 +643,10 @@ def generate(\n             language=language, task=task, is_multilingual=is_multilingual, generation_config=generation_config\n         )\n         self._set_num_frames(\n-            return_token_timestamps=return_token_timestamps, generation_config=generation_config, kwargs=kwargs\n+            return_token_timestamps=return_token_timestamps,\n+            generation_config=generation_config,\n+            attention_mask=attention_mask,\n+            kwargs=kwargs,\n         )\n         self._set_thresholds_and_condition(\n             generation_config=generation_config,\n@@ -810,10 +824,8 @@ def generate(\n                 segment_input=segment_input,\n                 decoder_input_ids=decoder_input_ids,\n                 cur_bsz=cur_bsz,\n-                batch_idx_map=batch_idx_map,\n                 seek=seek,\n-                num_segment_frames=num_segment_frames,\n-                max_frames=max_frames,\n+                batch_idx_map=batch_idx_map,\n                 temperatures=temperatures,\n                 generation_config=generation_config,\n                 logits_processor=logits_processor,\n@@ -928,10 +940,8 @@ def generate_with_fallback(\n         segment_input,\n         decoder_input_ids,\n         cur_bsz,\n-        batch_idx_map,\n         seek,\n-        num_segment_frames,\n-        max_frames,\n+        batch_idx_map,\n         temperatures,\n         generation_config,\n         logits_processor,\n@@ -1003,6 +1013,8 @@ def generate_with_fallback(\n                 return_token_timestamps=return_token_timestamps,\n                 generation_config=generation_config,\n                 is_shortform=is_shortform,\n+                seek=seek,\n+                batch_idx_map=batch_idx_map,\n             )\n \n             if cur_bsz < batch_size:\n@@ -1089,6 +1101,8 @@ def _postprocess_outputs(\n         return_token_timestamps,\n         generation_config,\n         is_shortform,\n+        seek,\n+        batch_idx_map,\n     ):\n         # remove all previously passed decoder input ids\n         # should happen only if it is the first generated segment\n@@ -1098,7 +1112,11 @@ def _postprocess_outputs(\n             return seek_outputs[:, start_idx:], seek_outputs\n \n         if return_token_timestamps and hasattr(generation_config, \"alignment_heads\"):\n-            num_frames = getattr(generation_config, \"num_frames\", None)\n+            num_frames = getattr(generation_config, \"num_frames\")\n+            if num_frames is not None:\n+                num_frames = num_frames - seek\n+                num_frames = num_frames[batch_idx_map]\n+\n             seek_outputs[\"token_timestamps\"] = self._extract_token_timestamps(\n                 seek_outputs,\n                 generation_config.alignment_heads,\n@@ -1634,7 +1652,7 @@ def _check_decoder_input_ids(kwargs):\n             )\n \n     @staticmethod\n-    def _set_num_frames(return_token_timestamps, generation_config, kwargs):\n+    def _set_num_frames(return_token_timestamps, generation_config, attention_mask, kwargs):\n         if return_token_timestamps:\n             if getattr(generation_config, \"task\", None) == \"translate\":\n                 logger.warning(\"Token-level timestamps may not be reliable for task 'translate'.\")\n@@ -1643,7 +1661,24 @@ def _set_num_frames(return_token_timestamps, generation_config, kwargs):\n                     \"Model generation config has no `alignment_heads`, token-level timestamps not available. \"\n                     \"See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.\"\n                 )\n-            generation_config.num_frames = kwargs.pop(\"num_frames\", None)\n+            if \"num_frames\" in kwargs:\n+                generation_config.num_frames = kwargs.pop(\"num_frames\")\n+                if isinstance(generation_config.num_frames, torch.Tensor):\n+                    generation_config.num_frames = generation_config.num_frames.cpu()\n+                else:\n+                    generation_config.num_frames = torch.tensor(generation_config.num_frames)\n+\n+                logger.warning_once(\n+                    \"`num_frames` is deprecated and will be removed in Transformers v5. Use `attention_mask` instead, as it can be used to infer the number of frames. \"\n+                    \"You can retrieve the `attention_mask` by doing `processor(audio, ..., return_attention_mask=True\"\n+                )\n+            elif attention_mask is not None:\n+                generation_config.num_frames = attention_mask.sum(-1).cpu()\n+            else:\n+                logger.warning_once(\n+                    \"When setting `return_token_timestamps` to `True`, make sure to pass an `attention_mask` to get precise token-level timestamps. You can retrieve the `attention_mask` by doing `processor(audio, ..., return_attention_mask=True)` \"\n+                )\n+                generation_config.num_frames = None\n \n     @staticmethod\n     def _set_thresholds_and_condition("
        },
        {
            "sha": "2d9dd6845c4d22e9170a4dcb0d1199a52212fc5c",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=2b85b6ce1978c776585cc20bdb013334f1c91e6c",
            "patch": "@@ -1099,11 +1099,11 @@ def new_chunk():\n                 # merges later and decode into text.\n                 current_tokens.append(token)\n                 if return_timestamps == \"word\":\n-                    start_time = round(token_timestamps[i] + time_offset, 2)\n-                    if i + 1 < len(token_timestamps):\n-                        end_time = round(token_timestamps[i + 1] + time_offset, 2)\n+                    if i == 0:\n+                        start_time = round(0.0 + time_offset, 2)\n                     else:\n-                        end_time = None  # should never happen\n+                        start_time = round(token_timestamps[i - 1] + time_offset, 2)\n+                    end_time = round(token_timestamps[i] + time_offset, 2)\n                     current_token_timestamps.append((start_time, end_time))\n \n         if \"stride\" in output:"
        },
        {
            "sha": "232ef4463b4d13ad1610c97613782c578848eb0f",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b85b6ce1978c776585cc20bdb013334f1c91e6c/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=2b85b6ce1978c776585cc20bdb013334f1c91e6c",
            "patch": "@@ -495,19 +495,11 @@ def _forward(self, model_inputs, return_timestamps=False, **generate_kwargs):\n             # custom processing for Whisper timestamps and word-level timestamps\n             return_timestamps = return_timestamps or getattr(self.generation_config, \"return_timestamps\", False)\n             if return_timestamps and self.type == \"seq2seq_whisper\":\n-                generate_kwargs[\"return_timestamps\"] = return_timestamps\n+                generate_kwargs[\"return_timestamps\"] = bool(return_timestamps)\n                 if return_timestamps == \"word\":\n                     generate_kwargs[\"return_token_timestamps\"] = True\n                     generate_kwargs[\"return_segments\"] = True\n \n-                    if stride is not None:\n-                        if isinstance(stride, tuple):\n-                            generate_kwargs[\"num_frames\"] = stride[0] // self.feature_extractor.hop_length\n-                        else:\n-                            generate_kwargs[\"num_frames\"] = [s[0] // self.feature_extractor.hop_length for s in stride]\n-                    else:\n-                        generate_kwargs[\"num_frames\"] = num_frames\n-\n             # User-defined `generation_config` passed to the pipeline call take precedence\n             if \"generation_config\" not in generate_kwargs:\n                 generate_kwargs[\"generation_config\"] = self.generation_config"
        },
        {
            "sha": "7888d7bab8b2f705f91a17af6b434521aeec7d2f",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 21,
            "deletions": 22,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b85b6ce1978c776585cc20bdb013334f1c91e6c/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b85b6ce1978c776585cc20bdb013334f1c91e6c/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=2b85b6ce1978c776585cc20bdb013334f1c91e6c",
            "patch": "@@ -1793,7 +1793,7 @@ def test_tiny_timestamp_generation(self):\n \n         # fmt: off\n         EXPECTED_OUTPUT = torch.tensor([\n-            50364, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50692, 50692, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50926, 50926, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 51208, 51208, 949, 505, 11, 14138, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51552, 51552, 634, 575, 12525, 22618, 1968, 6144, 35617, 7354, 1292, 6, 589, 307, 534, 10281, 934, 439, 11, 293, 51836, 51836, 50364, 393, 4411, 13, 50514\n+            [50364, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50692, 50692, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50926, 50926, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 51208, 51208, 949, 505, 11, 14138, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51552, 51552, 634, 575, 12525, 22618, 1968, 6144, 35617, 7354, 1292, 6, 589, 307, 534, 10281, 934, 439, 11, 293, 51836, 51836, 50364, 393, 4411, 13, 50514]\n         ])\n         # fmt: on\n \n@@ -2109,10 +2109,10 @@ def test_tiny_token_timestamp_generation(self):\n \n         # fmt: off\n         EXPECTED_OUTPUT = torch.tensor([\n-            [0.0000, 0.4800, 0.8200, 0.9600, 1.1200, 1.1200, 1.2200, 1.5000, 1.7200, 2.0000, 2.3400, 2.5000, 2.6600, 3.1800, 3.5600, 3.6800, 3.8000, 4.1000, 4.3000, 4.5800, 4.9400, 5.3800, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200, 12.4200],\n-            [0.0000, 0.5200, 0.9000, 1.1400, 1.4200, 1.5200, 1.6800, 1.6800, 1.8800, 2.1000, 2.2200, 2.6200, 3.1400, 3.5800, 3.9600, 4.4000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000, 17.3000],\n-            [0.0000, 0.0000, 0.7600, 1.0000, 1.4200, 1.8000, 1.9400, 2.1800, 2.5200, 3.0200, 3.3200, 3.5400, 3.9400, 4.5600, 4.9200, 5.2800, 5.5600, 5.9000, 6.1600, 6.3000, 6.4800, 6.4800, 6.6400, 7.8200, 7.9600, 8.2200, 8.6000, 8.9200, 9.2200, 9.5200, 9.7200, 10.0600, 10.5400, 10.8800, 11.2600, 11.5400, 11.7400, 12.0800, 15.6800],\n-            [0.0000, 0.0000, 0.7400, 1.0400, 1.3200, 1.6800, 2.1400, 2.4800, 2.7800, 3.0800, 3.1600, 3.4000, 3.6000, 4.0200, 4.2200, 4.8600, 5.2400, 5.7400, 6.3400, 6.6200, 6.7600, 6.7600, 6.8600, 7.2400, 7.4200, 7.6800, 7.9200, 8.4800, 8.7600, 9.2000, 9.2000, 9.4200, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200]\n+            [0.0000, 0.8200, 0.9800, 1.1200, 1.1200, 1.2200, 1.5000, 1.7200, 1.9800, 2.3400, 2.5000, 2.6600, 3.2000, 3.5600, 3.6800, 3.8000, 4.1000, 4.3000, 4.5800, 4.9400, 5.3800, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000, 11.9000],\n+            [0.0000, 0.9000, 1.1400, 1.4200, 1.5200, 1.6600, 1.6600, 1.8800, 2.1000, 2.2200, 2.6200, 3.1400, 3.5800, 3.9400, 4.4000, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600, 17.9600],\n+            [0.0000, 0.7600, 1.0000, 1.4200, 1.8000, 1.9400, 2.1800, 2.5200, 3.0200, 3.3200, 3.5400, 3.9400, 4.5600, 4.9400, 5.2800, 5.5600, 5.9000, 6.1600, 6.3000, 6.4800, 6.4800, 6.6400, 7.8200, 7.9600, 8.2200, 8.6000, 8.9200, 9.2200, 9.5200, 9.7200, 10.0800, 10.5400, 10.8800, 11.2600, 11.5400, 11.7400, 12.0800, 16.6000, 16.6000],\n+            [0.0000, 0.7400, 1.0400, 1.3000, 1.6800, 2.1200, 2.4800, 2.7600, 3.0800, 3.1600, 3.4000, 3.6000, 4.0200, 4.2200, 4.8600, 5.2400, 5.7400, 6.3400, 6.6200, 6.7600, 6.7600, 6.8600, 7.2400, 7.4000, 7.6800, 7.9200, 8.4800, 8.7600, 9.2000, 9.2000, 9.4000, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200, 15.8200]\n         ])\n         # fmt: on\n \n@@ -2139,10 +2139,10 @@ def test_small_token_timestamp_generation(self):\n \n         # fmt: off\n         EXPECTED_OUTPUT = torch.tensor([\n-            [0.0000, 0.0000, 0.7400, 0.8000, 0.9800, 1.0200, 1.1400, 1.4000, 1.5200, 1.9200, 2.2600, 2.3800, 2.5400, 2.8600, 3.2600, 3.3400, 3.4400, 3.6000, 3.6800, 3.9200, 4.2000, 4.4800, 4.7800, 5.2600, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200],\n-            [0.0000, 0.0000, 0.7600, 1.0000, 1.3000, 1.3800, 1.5200, 1.5800, 1.7000, 1.8400, 2.1000, 2.5000, 3.1400, 3.4400, 3.7400, 4.1800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800],\n-            [0.0000, 0.0000, 0.6600, 0.9000, 1.2200, 1.5200, 1.7600, 2.0200, 2.4000, 2.9200, 3.1800, 3.3200, 3.6200, 4.1000, 4.3600, 4.7800, 5.1200, 5.3400, 5.7200, 6.0600, 6.2000, 6.2000, 6.2000, 6.5000, 6.9000, 7.6400, 8.0000, 8.2400, 8.5200, 8.7400, 9.0800, 9.4000, 9.5400, 9.9400, 10.4200, 10.7600, 11.1200, 11.4400, 11.5800, 11.8600, 12.4600],\n-            [0.0000, 0.0000, 0.6600, 0.8600, 1.1400, 1.5000, 1.9600, 2.3600, 2.6400, 2.9800, 3.1200, 3.2400, 3.4800, 3.7800, 4.1400, 4.6400, 5.0800, 5.4400, 6.2200, 6.2200, 6.2200, 6.4000, 6.8400, 7.1200, 7.2600, 7.4800, 7.8200, 8.1400, 8.7000, 9.0200, 9.0200, 9.2000, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800]\n+            [0.0000, 0.7400, 0.8000, 0.9800, 1.0200, 1.1400, 1.4000, 1.5200, 1.9200, 2.2600, 2.3800, 2.5400, 2.8600, 3.2600, 3.3400, 3.4400, 3.6000, 3.6800, 3.9200, 4.2000, 4.4800, 4.7800, 5.2600, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200, 5.8200],\n+            [0.0000, 0.7600, 0.9800, 1.3000, 1.3800, 1.5200, 1.5800, 1.7000, 1.8400, 2.1000, 2.5000, 3.1400, 3.4400, 3.7400, 4.2000, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800, 4.7800],\n+            [0.0000, 0.6600, 0.9000, 1.2200, 1.5200, 1.7600, 2.0200, 2.4000, 2.9200, 3.1800, 3.3200, 3.6200, 4.1000, 4.3600, 4.7800, 5.1200, 5.3400, 5.7200, 6.0600, 6.2000, 6.2000, 6.2000, 6.5000, 6.9000, 7.6400, 8.0000, 8.2400, 8.5200, 8.7400, 9.0800, 9.4000, 9.5400, 9.9400, 10.4200, 10.7600, 11.1200, 11.4400, 11.5800, 11.8600, 12.4600, 12.4600],\n+            [0.0000, 0.6600, 0.8600, 1.1400, 1.5000, 1.9600, 2.3600, 2.6400, 2.9800, 3.1200, 3.2400, 3.4800, 3.7800, 4.1600, 4.6400, 5.0800, 5.4400, 6.2200, 6.2200, 6.2200, 6.4000, 6.8400, 7.1200, 7.2600, 7.4800, 7.8200, 8.1400, 8.7000, 9.0200, 9.0200, 9.2000, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800, 9.8800]\n         ])\n         # fmt: on\n \n@@ -2173,7 +2173,6 @@ def test_tiny_token_timestamp_batch_generation(self):\n         )\n \n         # task id and lang id prompts should not have timestamp tokens\n-        self.assertEqual(generate_outputs[\"sequences\"].shape[-1] - 2, generate_outputs[\"token_timestamps\"].shape[-1])\n         self.assertEqual(len(generate_outputs[\"sequences\"]), num_return_sequences * num_samples)\n \n     @slow\n@@ -2210,18 +2209,18 @@ def test_tiny_token_timestamp_generation_longform(self):\n \n         # fmt: off\n         EXPECTED_OUTPUT = [\n-            torch.tensor([0.0000, 0.4200, 0.8200, 0.9400, 1.1200, 1.1200, 1.2200, 1.5000, 1.7200, 2.0400, 2.3400, 2.5200, 2.6600, 3.2000, 3.4400, 3.5600, 3.6800, 3.8200, 4.1000, 4.3000, 4.5800, 4.9400, 5.4000, 6.3600]),\n-            torch.tensor([6.5400, 6.5400,  6.7400,  6.9600,  7.2600,  7.3400,  7.5800,  7.5800, 7.6400,  7.8400,  8.1000,  8.5000,  9.0000,  9.4800,  9.7200, 10.2600, 11.1000]),\n-            torch.tensor([11.2200, 11.2200, 11.4200, 11.6600, 12.0800, 12.4400, 12.5800, 12.8400, 13.1800, 13.6800, 14.0000, 14.2200, 14.6200, 14.9800, 15.2200, 15.6000, 15.9400, 16.2000, 16.5600, 16.8400, 16.9800]),\n-            torch.tensor([16.9800, 16.9800, 17.3200, 18.1600, 18.6400, 18.8600, 19.2800, 19.5600, 19.8800, 20.1800, 20.3800, 20.7200, 21.1600, 21.5400, 21.9000, 22.2000, 22.4200, 22.8600, 23.7000]),\n-            torch.tensor([23.7000, 23.7000, 23.9400, 24.1800, 24.3800, 24.8400, 25.2800, 25.6600, 25.9200, 26.2600, 26.4000, 26.5800, 26.7600, 27.1400, 27.3800, 28.0400, 28.3800, 28.8200, 29.3400, 29.5200, 29.9800]),\n-            torch.tensor([29.4400, 29.4400, 29.7000, 30.0800, 30.3800, 30.5400, 30.8200, 31.0600, 31.6600, 31.9200, 32.3000, 32.4800, 32.6200, 33.6800]),\n-            torch.tensor([33.8000, 33.8000, 33.9800, 33.9800, 34.1800, 34.4400, 34.6200, 35.0000, 35.2200, 35.3200, 35.5600, 35.9200, 36.3800, 36.6200, 36.6600, 36.9600, 37.3400, 37.9800, 38.5800, 38.7200, 38.9800, 39.4400, 39.5800, 39.8000, 40.1200, 40.2600]),\n-            torch.tensor([40.5200, 40.5200, 40.6200, 41.1000, 41.5400, 41.9200, 42.1000, 42.3200, 42.3200, 43.0600, 44.6000]),\n-            torch.tensor([44.7000, 44.7000, 44.8600, 44.9400, 45.1400, 45.1400, 45.2800, 45.6200, 45.9000, 46.2600, 47.1600, 47.4800, 47.7400, 48.1000, 48.2800, 48.4000, 48.6200, 48.8400, 49.0400, 49.2800, 49.4800, 49.6600, 49.9400, 50.5400]),\n-            torch.tensor([50.5400, 50.5400, 50.6600, 50.8800, 51.2400, 51.7200, 52.8400]),\n-            torch.tensor([52.9600, 52.9600, 53.0400, 53.2600, 53.4200, 53.5800, 53.9200, 54.1200, 54.7200, 54.9400, 55.2600, 55.6200, 55.9800, 56.5600, 56.8000, 56.9200, 57.3600, 57.9200, 58.1800, 58.5000, 58.6400, 58.8200, 59.4200]),\n-            torch.tensor([58.6800, 58.6800, 59.1400, 59.5400, 59.9200, 60.1600, 60.3800, 60.8200, 61.6200, 62.2600, 75.2000]),\n+            torch.tensor([0.0000, 0.8200, 0.9400, 1.1200, 1.1200, 1.2200, 1.5000, 1.7200, 2.0400, 2.3400, 2.5000, 2.6600, 3.2000, 3.4400, 3.5600, 3.6800, 3.8200, 4.1000, 4.3000, 4.5800, 4.9400, 5.4000, 6.3600, 6.5400]),\n+            torch.tensor([6.5400, 6.7400, 6.9600, 7.2600, 7.3400, 7.5800, 7.5800, 7.6400, 7.8400, 8.1000, 8.5000, 9.0000, 9.4800, 9.7200, 10.2600, 11.1000, 11.2200]),\n+            torch.tensor([11.2200, 11.4200, 11.6600, 12.0800, 12.4400, 12.5800, 12.8400, 13.1600, 13.6800, 14.0000, 14.2200, 14.6200, 14.9800, 15.2200, 15.6000, 15.9400, 16.2000, 16.5600, 16.8400, 16.9800, 16.9800]),\n+            torch.tensor([16.9800, 17.3200, 18.1800, 18.6400, 18.8600, 19.2800, 19.5600, 19.8800, 20.1800, 20.3800, 20.7200, 21.1600, 21.5400, 21.9000, 22.2000, 22.4200, 22.8400, 23.7000, 23.7000]),\n+            torch.tensor([23.7000, 23.9400, 24.1800, 24.3800, 24.8400, 25.2800, 25.6600, 25.9200, 26.2600, 26.3800, 26.5800, 26.7600, 27.1600, 27.3800, 28.0400, 28.3800, 28.8200, 29.3400, 29.5200, 29.9800, 29.9800]),\n+            torch.tensor([29.4400, 29.7000, 30.0600, 30.3800, 30.5400, 30.8200, 31.0600, 31.6600, 31.9200, 32.3000, 32.5000, 32.6200, 33.6800, 33.8000]),\n+            torch.tensor([33.8000, 33.9800, 33.9800, 34.1800, 34.4400, 34.6200, 35.0000, 35.2200, 35.3200, 35.5600, 35.9200, 36.3800, 36.6200, 36.6600, 36.9600, 37.3400, 37.9800, 38.5800, 38.7200, 38.9800, 39.4400, 39.5800, 39.8000, 40.1200, 40.2600, 40.5200]),\n+            torch.tensor([40.5200, 40.6200, 41.1000, 41.5400, 41.9200, 42.1000, 42.3200, 42.3200, 43.0600, 44.6000, 44.7000]),\n+            torch.tensor([44.7000, 44.8600, 44.9400, 45.1400, 45.1400, 45.2800, 45.6200, 45.9000, 46.2600, 47.1600, 47.4800, 47.7400, 48.1000, 48.2800, 48.4000, 48.6200, 48.8400, 49.0400, 49.2800, 49.4800, 49.6600, 49.9400, 50.5400, 50.5400]),\n+            torch.tensor([50.5400, 50.6600, 50.8800, 51.2400, 51.7200, 52.8400, 52.9600]),\n+            torch.tensor([52.9600, 53.0400, 53.2600, 53.4200, 53.5800, 53.9200, 54.1200, 54.7200, 54.9400, 55.2600, 55.6200, 55.9800, 56.5600, 56.8000, 56.9200, 57.3600, 57.9200, 58.1600, 58.5200, 58.6400, 58.8200, 59.4200, 59.4200]),\n+            torch.tensor([58.6800, 59.1400, 59.5400, 59.9200, 60.1400, 60.3800, 60.8400, 61.6000, 62.2400, 62.4200, 62.4200])\n         ]\n         # fmt: on\n "
        },
        {
            "sha": "f31d7da0554f492c5a34ca8b2c51a17374b55d42",
            "filename": "tests/models/whisper/test_tokenization_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 14,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b85b6ce1978c776585cc20bdb013334f1c91e6c/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b85b6ce1978c776585cc20bdb013334f1c91e6c/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py?ref=2b85b6ce1978c776585cc20bdb013334f1c91e6c",
            "patch": "@@ -344,13 +344,8 @@ def test_decode_asr_with_word_level_timestamps(self):\n         model_outputs = [\n             {\n                 'stride': [10, 0, 5],\n-                'tokens': np.array([[ 50257, 50362, 3363, 11, 345, 460, 0, 2329, 466, 340, 0, 50256 ]]),\n-                'token_timestamps': np.array([[ 0, 0, 5.18, 5.56, 5.56, 5.84, 6.36, 7.12, 7.54, 7.82, 8.16, 9.48 ]])\n-            },\n-            {\n-                'stride': [10, 5, 0],\n-                'tokens': np.array([[ 50257, 50362, 2329, 466, 340, 0, 3363, 345, 460, 0, 2329, 466, 340, 50256 ]]),\n-                'token_timestamps': np.array([[ 0, 0, 0, 2.44, 4.3, 5.04, 5.06, 5.56, 5.8, 6.32, 7.12, 7.56, 7.8, 8.72 ]])\n+                'tokens': np.array([[50363, 3363, 11, 345, 460, 0, 50423]]),\n+                'token_timestamps': np.array([[0.0, 0.5, 0.52, 0.78, 1.2, 1.28, 1.28]])\n             }\n         ]\n         # fmt: on\n@@ -361,15 +356,12 @@ def test_decode_asr_with_word_level_timestamps(self):\n         )\n \n         EXPECTED_OUTPUT = (\n-            \" Yes, you can! Just do it\",\n+            \" Yes, you can!\",\n             {\n                 \"chunks\": [\n-                    {\"text\": \" Yes,\", \"timestamp\": (5.18, 5.56)},\n-                    {\"text\": \" you\", \"timestamp\": (5.56, 5.84)},\n-                    {\"text\": \" can!\", \"timestamp\": (5.84, 7.12)},\n-                    {\"text\": \" Just\", \"timestamp\": (7.12, 7.56)},\n-                    {\"text\": \" do\", \"timestamp\": (7.56, 7.8)},\n-                    {\"text\": \" it\", \"timestamp\": (7.8, 8.72)},\n+                    {\"text\": \" Yes,\", \"timestamp\": (0.0, 0.52)},\n+                    {\"text\": \" you\", \"timestamp\": (0.52, 0.78)},\n+                    {\"text\": \" can!\", \"timestamp\": (0.78, 1.28)},\n                 ]\n             },\n         )"
        }
    ],
    "stats": {
        "total": 143,
        "additions": 83,
        "deletions": 60
    }
}