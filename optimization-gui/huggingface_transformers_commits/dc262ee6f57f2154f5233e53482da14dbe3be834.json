{
    "author": "winglian",
    "message": "remove FSDP prefix when using save_pretrained with FSDP2 (#40207)\n\n* remove FSDP prefix when using save_pretrained with FSDP2\n\n* Fix: use removeprefix correctly\n\n---------\n\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>\nCo-authored-by: S1ro1 <matej.sirovatka@gmail.com>",
    "sha": "dc262ee6f57f2154f5233e53482da14dbe3be834",
    "files": [
        {
            "sha": "4eb1dca090786fde897067cb6b7edb21c8f9a1f1",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc262ee6f57f2154f5233e53482da14dbe3be834/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc262ee6f57f2154f5233e53482da14dbe3be834/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=dc262ee6f57f2154f5233e53482da14dbe3be834",
            "patch": "@@ -4022,7 +4022,8 @@ def save_pretrained(\n         model_to_save.config.dtype = str(dtype).split(\".\")[1]\n \n         # Attach architecture to the config\n-        model_to_save.config.architectures = [model_to_save.__class__.__name__]\n+        # When using FSDP2, unwrapping is a noop, so the model name doesn't change back to the original model name\n+        model_to_save.config.architectures = [model_to_save.__class__.__name__.removeprefix(\"FSDP\")]\n \n         # If we have a custom model, we copy the file defining it in the folder and set the attributes so it can be\n         # loaded from the Hub."
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}