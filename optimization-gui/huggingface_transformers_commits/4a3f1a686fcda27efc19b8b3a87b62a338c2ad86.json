{
    "author": "abuelnasr0",
    "message": "check if eigenvalues of covariance matrix are complex.  (#34037)\n\ncheck if eigenvalues of covariance complex for psd checking",
    "sha": "4a3f1a686fcda27efc19b8b3a87b62a338c2ad86",
    "files": [
        {
            "sha": "cb0d743b0a90ae02b307aca7c8db18d77e91ceef",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a3f1a686fcda27efc19b8b3a87b62a338c2ad86/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a3f1a686fcda27efc19b8b3a87b62a338c2ad86/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4a3f1a686fcda27efc19b8b3a87b62a338c2ad86",
            "patch": "@@ -2441,8 +2441,9 @@ def _init_added_embeddings_weights_with_mean(\n         covariance = old_centered_embeddings.T @ old_centered_embeddings / old_num_tokens\n \n         # Check if the covariance is positive definite.\n+        eigenvalues = torch.linalg.eigvals(covariance)\n         is_covariance_psd = bool(\n-            (covariance == covariance.T).all() and (torch.linalg.eigvals(covariance).real >= 0).all()\n+            (covariance == covariance.T).all() and not torch.is_complex(eigenvalues) and (eigenvalues > 0).all()\n         )\n         if is_covariance_psd:\n             # If covariances is positive definite, a distribution can be created. and we can sample new weights from it."
        },
        {
            "sha": "774831791fe5aad49246107f2437b31a272e352a",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4a3f1a686fcda27efc19b8b3a87b62a338c2ad86/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4a3f1a686fcda27efc19b8b3a87b62a338c2ad86/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=4a3f1a686fcda27efc19b8b3a87b62a338c2ad86",
            "patch": "@@ -694,10 +694,6 @@ def prepare_config_and_inputs_for_generate(self, *args, **kwargs):\n         self.model_tester.seq_length = original_sequence_length\n         return test_inputs\n \n-    @unittest.skip(reason=\"Resizing sometimes goes bad\")  #  not worth investigating for now (it's not a popular model)\n-    def test_resize_tokens_embeddings(self):\n-        pass\n-\n \n @require_torch\n class ReformerLSHAttnModelTest("
        }
    ],
    "stats": {
        "total": 7,
        "additions": 2,
        "deletions": 5
    }
}