{
    "author": "kashif",
    "message": "[Trainer] use output.loss when using liger-kernel (#42444)\n\n* use output.loss when using liger\n\nHandle loss computation for models using Liger-kernel.\r\nfixes #42414\n\n* Clarify Liger-kernel loss computation in comments\n\n* Both standard transformers and Liger models handle shift_labels correctly via **kwargs\n\n* removed unused shift_labels reference in loss computation\n\n* Remove unused model unwrapping",
    "sha": "6db4332171df2b4099c44c7a5c01258b91f7394a",
    "files": [
        {
            "sha": "65331ba8322cba4b77e0709c164ec9eb472ff5c2",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 11,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6db4332171df2b4099c44c7a5c01258b91f7394a/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6db4332171df2b4099c44c7a5c01258b91f7394a/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=6db4332171df2b4099c44c7a5c01258b91f7394a",
            "patch": "@@ -3909,7 +3909,7 @@ def compute_loss(\n \n     def _deepspeed_sp_compute_loss(self, model, inputs, return_outputs, pc):\n         \"\"\"\n-        How the loss is computed by Trainer under sequence parallelism with sp_backend==\"deepspeed\" and sp_size>1.\n+        How the loss is computed by the Trainer under sequence parallelism with sp_backend==\"deepspeed\" and sp_size>1.\n         Performs weighted loss aggregation across SP ranks, accounting for varying numbers of valid tokens per rank\n         (e.g., when some ranks receive only padding or prompt tokens that are masked with -100).\n \n@@ -3927,23 +3927,20 @@ def _deepspeed_sp_compute_loss(self, model, inputs, return_outputs, pc):\n             The loss of the model along with its output if return_outputs was set to True\n         \"\"\"\n \n-        unwrapped_model = self.accelerator.unwrap_model(model)\n-\n+        # DeepSpeed SP automatically injects shift_labels into inputs (pre-shifted labels for SP).\n+        # The model's forward pass receives shift_labels via **kwargs and passes it to the loss function.\n+        # Both standard transformer models and Liger-patched models handle shift_labels correctly,\n+        # so we can directly use the computed loss from the model output.\n+        # See: https://huggingface.co/docs/accelerate/en/concept_guides/sequence_parallelism\n         outputs = model(**inputs)\n-        shift_labels = inputs[\"shift_labels\"]\n-        loss = unwrapped_model.loss_function(\n-            logits=outputs.logits,\n-            labels=None,\n-            shift_labels=shift_labels,\n-            vocab_size=unwrapped_model.config.vocab_size,\n-        )\n+        loss = outputs.loss\n \n         sp_group = self.accelerator.torch_device_mesh[\"sp\"].get_group()\n         sp_world_size = pc.sp_size\n         # differentiable weighted per-shard-loss aggregation across ranks\n         losses_per_rank = torch.distributed.nn.functional.all_gather(loss, group=sp_group)\n         # special dealing with SFT that has prompt tokens that aren't used in loss computation\n-        good_tokens = (shift_labels != -100).view(-1).sum()\n+        good_tokens = (inputs[\"shift_labels\"] != -100).view(-1).sum()\n         good_tokens_per_rank = torch.distributed.nn.functional.all_gather(good_tokens, group=sp_group)\n         # Skip ranks with zero valid tokens\n         total_loss = sum("
        }
    ],
    "stats": {
        "total": 19,
        "additions": 8,
        "deletions": 11
    }
}