{
    "author": "ArthurZucker",
    "message": "MoE + vllm = ðŸ˜»  (#40132)\n\n* update modeling mixtral\n\n* oups[13;2u\n\n* fix\n\n* better naming?\n\n* compute softmax and top_k inside the experts\n\n* update minamax as well\n\n* models that will need an update\n\n* more models that need a fix\n\n* stash\n\n* fix mixtral\n\n* update olmoe\n\n* update\n\n* update\n\n* current changes\n\n* nits\n\n* molmoe is now fixed\n\n* olmoe is good to go!\n\n* refactor qwen2_moe\n\n* fixes\n\n* fixed moe\n\n* fix qwen2 modular\n\n* nit\n\n* qwen2_moie test script works\n\n* tricky rope !\n\n* fix qwen3\n\n* DeepSeek v3 MoE Standardization (#40538)\n\n* DeepSeek-v3\n\nShared\n\nShared\n\n* Dependents of DS3\n\n* Standardize GLM4V MoE (#40539)\n\n* up\n\n* Standardize VitPose's MoE (#40549)\n\n* VitPose\n\n* outside\n\n* outside\n\n* outside\n\n* fix\n\n* update dbrx\n\n* dbrx... the magix\n\n* Refactor Ernie 4.5's MoE (#40547)\n\n* Isolate Ernie fixes\n\n* fix moe\n\n---------\n\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\n\n* fix style\n\n* style\n\n* fix copies\n\n* style\n\n* latest changes\n\n* fixes\n\n* had to stage\n\n* current updaters\n\n* up\n\n* another modular\n\n* modular graniteMoe\n\n* some update\n\n* draft another modular moe\n\n* updaters\n\n* up\n\n* fix nit\n\n* q3 nit\n\n* fix phi moe\n\n* we're going up up up up its our mooooment\n\n* fix switch transformers this time around\n\n* up\n\n* gptsan japanese is deprecated forget about it\n\n* fix mixtral to not be a linear (gives us more freedom)\n\n* update\n\n* fix copies gone wrong try catch nothing\n\n* fix mixtral\n\n* new refactor again\n\n* update aria as well\n\n* up dbrx and deepseekv3\n\n* nit\n\n* fix phimoe?\n\n* fix deepseek v3\n\n* nits\n\n* don't bother with this one please\n\n* up olmoe\n\n* ??\n\n* fix olmoe\n\n* yups\n\n* fiupx\n\n* ish\n\n* hot patch\n\n* new qwen3\n\n* updates\n\n* up\n\n* nit\n\n* fix copies\n\n* fix\n\n* nits\n\n* we're going up up up\n\n* nits\n\n* switch_transformesr edge case\n\n* lol modular gptsan?\n\n* fix deepseek\n\n* finally all modeling match modular\n\n* update\n\n* up\n\n* up\n\n* dang\n\n* up\n\n* up aria\n\n* fix dbrx\n\n* nits here and there\n\n* finish fixing dbrx\n\n* fix deepseek\n\n* upd\n\n* up\n\n* fix flex olmo\n\n* updated\n\n* update jamba\n\n* JAMBA is stil a bit todo\n\n* forward forward\n\n* fix dots11\n\n* update\n\n* fix hunyuan\n\n* fix some other\n\n* update phimoe\n\n* fuck you phimoe you are now submitted\n\n* submit granitemoe as well\n\n* try to fix some other models, reduces some of the failures\n\n* fix olmoe and qwem2moe\n\n* up\n\n* up\n\n* fix qwen2_moe\n\n* update modular make it again, simpler\n\n* nits\n\n* up\n\n* up\n\n* fix\n\n* someswitch reductions\n\n* up\n\n* fix qwen3vl\n\n* some fixes to jetmo\n\n* these should be shipped to the modular to fix jetmoe\n\n* fix most of the nllb failures\n\n* more nllb fixes\n\n* fix the modular\n\n* remove nllb modular as it sucks for now\n\n* ?\n\n* fix granitemoe\n\n* granitemoehybrid don't have rope\n\n* use rope when rope, no rope when no rope\n\n* updates\n\n* finish fixing dumbgrainite\n\n* fix most of minimax\n\n* fix\n\n* update modular\n\n* ?\n\n* up\n\n* up jetmoe still broken\n\n* up\n\n* fix, now align the moe\n\n* fix jetmoe\n\n* fix styling and qwen3 repo consitency\n\n* updatge\n\n* up up\n\n* update ruff?\n\n* nits\n\n* modeling is goot now for switch\n\n* fix\n\n* more fixses to switch!\n\n* fix some siwtch test\n\n* ?\n\n* ?\n\n* up\n\n* fix switch modular!\n\n* nit?\n\n* uip\n\n* subtest\n\n* can't believe I wasted so much time on this...\n\n* fix\n\n* updates\n\n* nits\n\n* nit jamba is fucking annoying\n\n* ?\n\n* fix?\n\n* oups\n\n* good good\n\n* styling\n\n* up\n\n* make sure qwen2 sliding works!\n\n* fix dbrx small\n\n* lol\n\n* nits\n\n* fix one test\n\n* fix load balancing loss issue\n\n* fix jamba\n\n* fix nllbmoe\n\n* fix jamba consistency and doc?\n\n* up\n\n* thse are correct\n\n* up\n\n* up\n\n* up\n\n* some of the final cleanup\n\n* update\n\n* up\n\n* fix some revert in granimoe\n\n* bring back attention multipliers for the granite family we'll see later on if they need removal\n\n* small jamba fix docstring and typing\n\n* fix phimoe\n\n* yup\n\n* fix unk returndict in granitemoes\n\n* up\n\n* fix qwen config\n\n* fix phiemoe check quality\n\n* nits\n\n* update based on caught non relative imports!\n\n* fix dbrx\n\n* Apply suggestions from code review\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\n\n* fix copies\n\n* fiuxp\n\n* fix dot1 regression!\n\n* fix phimoe issue\n\n* fix phi moe\n\n* fix float() for some models\n\n* fix jamba regression\n\n* ui\n\n* more dtype issues\n\n* fix deepseek2 and 3?\n\n* proper update\n\n* fix modular deepseek!\n\n* jamba jambaaaaaa\n\n---------\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "7938e91faabb051f3a001cd39c173d4697c2d81c",
    "files": [
        {
            "sha": "ada3f2f374292aa888477e0d5d912a854090364a",
            "filename": "docs/source/en/model_doc/switch_transformers.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswitch_transformers.md?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -105,7 +105,6 @@ print(tokenizer.decode(outputs[0]))\n ## SwitchTransformersTop1Router\n \n [[autodoc]] SwitchTransformersTop1Router\n-    - _compute_router_probabilities\n     - forward\n \n ## SwitchTransformersSparseMLP"
        },
        {
            "sha": "99ec192634f15d0781325579fd183e58e3587c26",
            "filename": "src/transformers/modeling_outputs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodeling_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodeling_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_outputs.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -357,6 +357,7 @@ class MoEModelOutput(ModelOutput):\n     hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     router_probs: Optional[tuple[torch.FloatTensor]] = None\n+    router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n@@ -494,6 +495,7 @@ class MoEModelOutputWithPastAndCrossAttentions(ModelOutput):\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n     router_probs: Optional[tuple[torch.FloatTensor]] = None\n+    router_logits: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass"
        },
        {
            "sha": "789bea15ef54be6cb11df57955b947eb5e4e60b6",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 31,
            "deletions": 83,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -309,117 +309,65 @@ def forward(self, input, tokens_per_expert):\n         )\n \n \n-class AriaGroupedExpertsMLP(nn.Module):\n-    \"\"\"\n-    Grouped MLP module for Mixture of Experts.\n-\n-    Args:\n-        config (`AriaTextConfig`):\n-            Configuration object for the model.\n-    \"\"\"\n-\n+class AriaExperts(nn.Module):\n     def __init__(self, config: AriaTextConfig) -> None:\n         super().__init__()\n         self.config = config\n         self.fc1 = AriaGroupedExpertsGemm(config.hidden_size, config.intermediate_size * 2, config.moe_num_experts)\n         self.fc2 = AriaGroupedExpertsGemm(config.intermediate_size, config.hidden_size, config.moe_num_experts)\n \n-    def forward(self, permuted_tokens, tokens_per_expert):\n-        \"\"\"\n-        Forward pass of the Grouped MLP.\n-\n-        Args:\n-            permuted_tokens (torch.Tensor): Permuted input tokens.\n-            tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert.\n-\n-        Returns:\n-            torch.Tensor: Output tensor after passing through the MLP.\n-        \"\"\"\n-        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n-        projection, gate = torch.chunk(fc1_output, 2, dim=-1)\n-        fc1_output = nn.functional.silu(projection) * gate\n-        fc2_output = self.fc2(fc1_output, tokens_per_expert)\n-        return fc2_output\n-\n-\n-# Token permutation adapted from https://github.com/NVIDIA/Megatron-LM/blob/54f1f78529cbc2b9cddad313e7f9d96ac0420a27/megatron/core/transformer/moe/token_dispatcher.py#L291-L587\n-class AriaTextMoELayer(nn.Module):\n-    \"\"\"\n-    Aria Text Mixture of Experts (MoE) Layer.\n-\n-    This layer applies a gating mechanism to route input tokens to different experts.\n-\n-    Args:\n-        config (`AriaTextConfig`):\n-            Configuration object for the text component of the model.\n-    \"\"\"\n-\n-    def __init__(self, config: AriaTextConfig):\n-        super().__init__()\n-\n-        self.router = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False)\n-        self.experts = AriaGroupedExpertsMLP(config)\n-        self.shared_experts = AriaSharedExpertsMLP(config)\n-        self.config = config\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\"\n-        Forward pass of the MoE Layer.\n-\n-        Args:\n-            hidden_states (`torch.Tensor`):\n-                Input tensor of shape (batch_size, sequence_length, hidden_size).\n-\n-        Returns:\n-            torch.Tensor: Output tensor after passing through the MoE layer.\n-\n-        Process:\n-        1. Route tokens to experts using the router.\n-        2. Permute tokens based on routing decisions.\n-        3. Process tokens through experts.\n-        4. Unpermute and combine expert outputs.\n-        5. Add shared expert output to the final result.\n-        \"\"\"\n-        original_shape = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_states.size(-1))\n-\n-        # Top K Routing\n-        logits = self.router(hidden_states)\n-        top_logits, top_indices = torch.topk(logits, k=self.config.moe_topk, dim=1)\n+    def route_tokens_to_experts(self, router_logits):\n+        top_logits, top_indices = torch.topk(router_logits, k=self.config.moe_topk, dim=1)\n         scores = nn.functional.softmax(top_logits, dim=-1)\n+        return top_indices, scores\n \n-        original_dtype = top_indices.dtype\n-\n+    def forward(self, hidden_states, router_logits) -> torch.Tensor:\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(router_logits)\n+        original_dtype = top_k_index.dtype\n         tokens_per_expert = torch.histc(\n-            top_indices.flatten().to(torch.float32),\n+            top_k_index.flatten().to(torch.float32),\n             bins=self.config.moe_num_experts,\n             min=0,\n             max=self.config.moe_num_experts - 1,\n         ).to(original_dtype)\n-        indices = top_indices\n+        indices = top_k_index\n \n-        # Token permutation\n         flatten_indices = indices.view(-1)\n         sorted_indices = torch.argsort(flatten_indices)\n         permuted_tokens = hidden_states.index_select(0, sorted_indices // self.config.moe_topk)\n \n-        # Process through experts\n-        expert_output = self.experts(permuted_tokens, tokens_per_expert)\n+        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n+        projection, gate = torch.chunk(fc1_output, 2, dim=-1)\n+        fc1_output = nn.functional.silu(projection) * gate\n+        expert_output = self.fc2(fc1_output, tokens_per_expert)\n \n-        # Token unpermutation\n         unpermuted_tokens = torch.zeros(\n-            (scores.shape[0] * self.config.moe_topk, expert_output.size(1)),\n+            (top_k_weights.shape[0] * self.config.moe_topk, expert_output.size(1)),\n             dtype=expert_output.dtype,\n             device=expert_output.device,\n         )\n         unpermuted_tokens.index_copy_(0, sorted_indices, expert_output)\n         unpermuted_tokens = unpermuted_tokens.view(-1, self.config.moe_topk, expert_output.size(1))\n \n-        output = (unpermuted_tokens * scores.unsqueeze(-1)).sum(dim=1).view(original_shape)\n+        output = (unpermuted_tokens * top_k_weights.unsqueeze(-1)).sum(dim=1)\n+        return output\n \n-        # Add shared expert output\n+\n+class AriaTextMoELayer(nn.Module):\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__()\n+        self.router = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False)\n+        self.experts = AriaExperts(config)\n+        self.shared_experts = AriaSharedExpertsMLP(config)\n+        self.config = config\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        original_shape = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_states.size(-1))\n+        router_logits = self.router(hidden_states)\n+        expert_output = self.experts(hidden_states, router_logits).view(original_shape)\n         shared_expert_output = self.shared_experts(hidden_states.view(original_shape))\n-        return output + shared_expert_output\n+        return expert_output + shared_expert_output\n \n \n def rotate_half(x):"
        },
        {
            "sha": "749a4c036ed13becadc0fa0467b64db199ba68a0",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 31,
            "deletions": 83,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1120,117 +1120,65 @@ def forward(self, input, tokens_per_expert):\n         )\n \n \n-class AriaGroupedExpertsMLP(nn.Module):\n-    \"\"\"\n-    Grouped MLP module for Mixture of Experts.\n-\n-    Args:\n-        config (`AriaTextConfig`):\n-            Configuration object for the model.\n-    \"\"\"\n-\n+class AriaExperts(nn.Module):\n     def __init__(self, config: AriaTextConfig) -> None:\n         super().__init__()\n         self.config = config\n         self.fc1 = AriaGroupedExpertsGemm(config.hidden_size, config.intermediate_size * 2, config.moe_num_experts)\n         self.fc2 = AriaGroupedExpertsGemm(config.intermediate_size, config.hidden_size, config.moe_num_experts)\n \n-    def forward(self, permuted_tokens, tokens_per_expert):\n-        \"\"\"\n-        Forward pass of the Grouped MLP.\n-\n-        Args:\n-            permuted_tokens (torch.Tensor): Permuted input tokens.\n-            tokens_per_expert (torch.Tensor): Number of tokens assigned to each expert.\n-\n-        Returns:\n-            torch.Tensor: Output tensor after passing through the MLP.\n-        \"\"\"\n-        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n-        projection, gate = torch.chunk(fc1_output, 2, dim=-1)\n-        fc1_output = nn.functional.silu(projection) * gate\n-        fc2_output = self.fc2(fc1_output, tokens_per_expert)\n-        return fc2_output\n-\n-\n-# Token permutation adapted from https://github.com/NVIDIA/Megatron-LM/blob/54f1f78529cbc2b9cddad313e7f9d96ac0420a27/megatron/core/transformer/moe/token_dispatcher.py#L291-L587\n-class AriaTextMoELayer(nn.Module):\n-    \"\"\"\n-    Aria Text Mixture of Experts (MoE) Layer.\n-\n-    This layer applies a gating mechanism to route input tokens to different experts.\n-\n-    Args:\n-        config (`AriaTextConfig`):\n-            Configuration object for the text component of the model.\n-    \"\"\"\n-\n-    def __init__(self, config: AriaTextConfig):\n-        super().__init__()\n-\n-        self.router = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False)\n-        self.experts = AriaGroupedExpertsMLP(config)\n-        self.shared_experts = AriaSharedExpertsMLP(config)\n-        self.config = config\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\"\n-        Forward pass of the MoE Layer.\n-\n-        Args:\n-            hidden_states (`torch.Tensor`):\n-                Input tensor of shape (batch_size, sequence_length, hidden_size).\n-\n-        Returns:\n-            torch.Tensor: Output tensor after passing through the MoE layer.\n-\n-        Process:\n-        1. Route tokens to experts using the router.\n-        2. Permute tokens based on routing decisions.\n-        3. Process tokens through experts.\n-        4. Unpermute and combine expert outputs.\n-        5. Add shared expert output to the final result.\n-        \"\"\"\n-        original_shape = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_states.size(-1))\n-\n-        # Top K Routing\n-        logits = self.router(hidden_states)\n-        top_logits, top_indices = torch.topk(logits, k=self.config.moe_topk, dim=1)\n+    def route_tokens_to_experts(self, router_logits):\n+        top_logits, top_indices = torch.topk(router_logits, k=self.config.moe_topk, dim=1)\n         scores = nn.functional.softmax(top_logits, dim=-1)\n+        return top_indices, scores\n \n-        original_dtype = top_indices.dtype\n-\n+    def forward(self, hidden_states, router_logits) -> torch.Tensor:\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(router_logits)\n+        original_dtype = top_k_index.dtype\n         tokens_per_expert = torch.histc(\n-            top_indices.flatten().to(torch.float32),\n+            top_k_index.flatten().to(torch.float32),\n             bins=self.config.moe_num_experts,\n             min=0,\n             max=self.config.moe_num_experts - 1,\n         ).to(original_dtype)\n-        indices = top_indices\n+        indices = top_k_index\n \n-        # Token permutation\n         flatten_indices = indices.view(-1)\n         sorted_indices = torch.argsort(flatten_indices)\n         permuted_tokens = hidden_states.index_select(0, sorted_indices // self.config.moe_topk)\n \n-        # Process through experts\n-        expert_output = self.experts(permuted_tokens, tokens_per_expert)\n+        fc1_output = self.fc1(permuted_tokens, tokens_per_expert)\n+        projection, gate = torch.chunk(fc1_output, 2, dim=-1)\n+        fc1_output = nn.functional.silu(projection) * gate\n+        expert_output = self.fc2(fc1_output, tokens_per_expert)\n \n-        # Token unpermutation\n         unpermuted_tokens = torch.zeros(\n-            (scores.shape[0] * self.config.moe_topk, expert_output.size(1)),\n+            (top_k_weights.shape[0] * self.config.moe_topk, expert_output.size(1)),\n             dtype=expert_output.dtype,\n             device=expert_output.device,\n         )\n         unpermuted_tokens.index_copy_(0, sorted_indices, expert_output)\n         unpermuted_tokens = unpermuted_tokens.view(-1, self.config.moe_topk, expert_output.size(1))\n \n-        output = (unpermuted_tokens * scores.unsqueeze(-1)).sum(dim=1).view(original_shape)\n+        output = (unpermuted_tokens * top_k_weights.unsqueeze(-1)).sum(dim=1)\n+        return output\n \n-        # Add shared expert output\n+\n+class AriaTextMoELayer(nn.Module):\n+    def __init__(self, config: AriaTextConfig):\n+        super().__init__()\n+        self.router = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False)\n+        self.experts = AriaExperts(config)\n+        self.shared_experts = AriaSharedExpertsMLP(config)\n+        self.config = config\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        original_shape = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_states.size(-1))\n+        router_logits = self.router(hidden_states)\n+        expert_output = self.experts(hidden_states, router_logits).view(original_shape)\n         shared_expert_output = self.shared_experts(hidden_states.view(original_shape))\n-        return output + shared_expert_output\n+        return expert_output + shared_expert_output\n \n \n class AriaTextAttention(LlamaAttention):"
        },
        {
            "sha": "121648dd86df2f8ceb50a25c08a64f6e814fe76f",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -159,16 +159,24 @@ def update(\n \n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n-        for layer_idx in range(len(self.key_cache)):\n-            device = self.key_cache[layer_idx].device\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.value_cache[layer_idx].device\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n-\n-            device = self.conv_states[layer_idx].device\n-            self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.ssm_states[layer_idx].device\n-            self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+        if self.get_seq_length() > 0:\n+            for layer_idx in range(len(self.key_cache)):\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+                device = self.conv_states[layer_idx].device\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.ssm_states[layer_idx].device\n+                self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        kv_length = self.get_seq_length(layer_idx) + query_length\n+        return kv_length, kv_offset\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\""
        },
        {
            "sha": "dce1fbdaffb28dd2a6dc358be9cf69bec3fc2392",
            "filename": "src/transformers/models/dbrx/configuration_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fconfiguration_dbrx.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -48,20 +48,12 @@ def __init__(\n         attn_pdrop: float = 0.0,\n         clip_qkv: Optional[float] = None,\n         kv_n_heads: int = 1,\n-        rope_theta: float = 10000.0,\n         **kwargs: Any,\n     ):\n         super().__init__(**kwargs)\n         self.attn_pdrop = attn_pdrop\n         self.clip_qkv = clip_qkv\n         self.kv_n_heads = kv_n_heads\n-        self.rope_theta = rope_theta\n-\n-        for k in [\"model_type\", \"attn_implementation\", \"transformers_version\", \"_commit_hash\", \"torch_dtype\", \"dtype\"]:\n-            if k in kwargs:\n-                kwargs.pop(k)\n-        if len(kwargs) != 0:\n-            raise ValueError(f\"Found unknown {kwargs=}\")\n \n \n class DbrxFFNConfig(PretrainedConfig):\n@@ -89,6 +81,7 @@ class DbrxFFNConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n+        hidden_size=6144,\n         ffn_act_fn: Optional[dict] = None,\n         ffn_hidden_size: int = 3584,\n         moe_num_experts: int = 4,\n@@ -101,6 +94,7 @@ def __init__(\n         super().__init__()\n         if ffn_act_fn is None:\n             ffn_act_fn = {\"name\": \"silu\"}\n+        self.hidden_size = hidden_size\n         self.ffn_act_fn = ffn_act_fn\n         self.ffn_hidden_size = ffn_hidden_size\n         self.moe_num_experts = moe_num_experts\n@@ -221,7 +215,7 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.output_router_logits = output_router_logits\n         self.num_key_value_heads = self.attn_config.kv_n_heads\n-\n+        self.rope_theta: float = 10000.0\n         tie_word_embeddings = kwargs.pop(\"tie_word_embeddings\", False)\n         if tie_word_embeddings:\n             raise ValueError(\"tie_word_embeddings is not supported for DBRX models.\")"
        },
        {
            "sha": "5043f5898255229cf20960c4f071d802e714423f",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 321,
            "deletions": 811,
            "changes": 1132,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/dbrx/modular_dbrx.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dbrx.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Databricks Mosaic Research and The HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,79 +18,70 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch DBRX model.\"\"\"\n \n-import math\n-from typing import Any, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n from .configuration_dbrx import DbrxConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-logger = logging.get_logger(__name__)\n-\n-\n class DbrxRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(self, config: DbrxConfig, device=None):\n         super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n     @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -112,7 +109,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -125,138 +121,78 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-def load_balancing_loss_func(\n-    gate_probabilities: torch.Tensor,\n-    num_experts: int,\n-    top_k: int,\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n     attention_mask: Optional[torch.Tensor],\n-) -> torch.Tensor:\n-    r\"\"\"Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        gate_logits (Union[`torch.Tensor`, tuple[torch.Tensor]):\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts (`int`):\n-            Number of experts.\n-        top_k (`int`):\n-            The number of experts each token is routed to.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if gate_probabilities is None or not isinstance(gate_probabilities, tuple):\n-        return torch.tensor(0.0)\n-\n-    if isinstance(gate_probabilities, tuple):\n-        compute_device = gate_probabilities[0].device\n-        routing_weights = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_probabilities], dim=0)\n-\n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n-\n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = routing_weights.shape[0] // (batch_size * sequence_length)\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n \n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n \n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n-            .reshape(-1, num_experts)\n-            .to(compute_device)\n-        )\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n \n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n-\n-    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n-    return overall_loss * num_experts\n+    return attn_output, attn_weights\n \n \n class DbrxAttention(nn.Module):\n-    \"\"\"Multi-head self attention.\"\"\"\n+    \"\"\"Modular DBRX attention component that can be reused across different model architectures.\"\"\"\n \n-    def __init__(self, config: DbrxConfig, block_idx: Optional[int] = None):\n+    def __init__(\n+        self,\n+        config,\n+        layer_idx: Optional[int] = None,\n+        **kwargs,\n+    ):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.d_model\n         self.num_heads = config.n_heads\n         self.head_dim = self.hidden_size // self.num_heads\n         self.max_position_embeddings = config.max_seq_len\n-        self.block_idx = block_idx\n-        if block_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `block_idx` is not recommended and will \"\n-                + \"lead to errors during the forward call if caching is used. Please make sure to provide a `block_idx` \"\n-                + \"when creating this class.\"\n-            )\n+        self.layer_idx = layer_idx\n \n         attn_config = config.attn_config\n-        self.attn_pdrop = attn_config.attn_pdrop\n+        self.attention_dropout = attn_config.attn_pdrop\n         self.clip_qkv = attn_config.clip_qkv\n         self.num_key_value_heads = attn_config.kv_n_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.rope_theta = attn_config.rope_theta\n         self.is_causal = True\n \n         self.Wqkv = nn.Linear(\n             self.hidden_size, self.hidden_size + 2 * self.num_key_value_heads * self.head_dim, bias=False\n         )\n         self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n-        self.rotary_emb = DbrxRotaryEmbedding(\n-            self.head_dim,\n-            max_position_embeddings=self.max_position_embeddings,\n-            base=self.rope_theta,\n-        )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_ids: torch.LongTensor,\n         attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Any,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n-        bsz, q_len, _ = hidden_states.size()\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         qkv_states = self.Wqkv(hidden_states)\n         min_val = -self.clip_qkv if self.clip_qkv is not None else None\n-        max_val = self.clip_qkv\n-        qkv_states = qkv_states.clamp(min=min_val, max=max_val)\n+        qkv_states = qkv_states.clamp(min=min_val, max=self.clip_qkv)\n \n         query_states, key_states, value_states = qkv_states.split(\n             [\n@@ -267,311 +203,176 @@ def forward(\n             dim=2,\n         )\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.block_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attn_pdrop, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                + f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-        attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights\n-\n-\n-class DbrxFlashAttention2(DbrxAttention):\n-    \"\"\"Dbrx flash attention module.\n-\n-    This module inherits from `DbrxAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it\n-    calls the public API of flash attention.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Any,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if isinstance(past_key_values, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-        logger.info(\"Implicitly setting `output_attentions` to False as it is not supported in Flash Attention.\")\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        qkv_states = self.Wqkv(hidden_states)\n-        if self.clip_qkv is not None:\n-            qkv_states = qkv_states.clamp(min=-self.clip_qkv, max=self.clip_qkv)\n-\n-        query_states, key_states, value_states = qkv_states.split(\n-            [\n-                self.hidden_size,\n-                self.num_key_value_heads * self.head_dim,\n-                self.num_key_value_heads * self.head_dim,\n-            ],\n-            dim=2,\n-        )\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.block_idx, cache_kwargs)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout\n-        # [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attn_pdrop if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-        input_dtype = query_states.dtype\n-        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = query_states.dtype\n-\n-            logger.warning_once(\n-                \"The input hidden states seems to be silently casted in float32, this might be \"\n-                + \"related to the fact you have upcasted embedding or layer norm layers in \"\n-                + f\"float32. We will cast back the input in {target_dtype}.\"\n-            )\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-class DbrxSdpaAttention(DbrxAttention):\n-    \"\"\"\n-    Dbrx attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `DbrxAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"DbrxModel is using DbrxSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        qkv_states = self.Wqkv(hidden_states)\n-        if self.clip_qkv is not None:\n-            qkv_states = qkv_states.clamp(min=-self.clip_qkv, max=self.clip_qkv)\n-\n-        query_states, key_states, value_states = qkv_states.split(\n-            [\n-                self.hidden_size,\n-                self.num_key_value_heads * self.head_dim,\n-                self.num_key_value_heads * self.head_dim,\n-            ],\n-            dim=2,\n-        )\n+class DbrxExpertGLU(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.ffn_hidden_size = config.ffn_hidden_size\n+        self.moe_num_experts = config.moe_num_experts\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        self.w1 = nn.Parameter(torch.empty(self.moe_num_experts * self.ffn_hidden_size, self.hidden_size))\n+        self.v1 = nn.Parameter(torch.empty(self.moe_num_experts * self.ffn_hidden_size, self.hidden_size))\n+        self.w2 = nn.Parameter(torch.empty(self.moe_num_experts * self.ffn_hidden_size, self.hidden_size))\n \n-        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=None)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, None)\n+        act_fn_name = config.ffn_act_fn.get(\"name\", \"silu\")\n+        self.activation_fn = ACT2FN[act_fn_name]\n \n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.block_idx, cache_kwargs)\n+    def forward(\n+        self, x: torch.Tensor, expert_w1: torch.Tensor, expert_v1: torch.Tensor, expert_w2: torch.Tensor\n+    ) -> torch.Tensor:\n+        gate_proj = x.matmul(expert_w1)\n+        up_proj = x.matmul(expert_v1)\n+        gate_proj = self.activation_fn(gate_proj)\n+        intermediate_states = gate_proj * up_proj\n+        down_proj = intermediate_states.matmul(expert_w2.t())\n+        return down_proj\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n \n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+class DbrxExperts(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.mlp = DbrxExpertGLU(config)\n+        self.hidden_size = config.hidden_size\n+        self.ffn_hidden_size = config.ffn_hidden_size\n+        self.num_experts = config.moe_num_experts\n \n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n+    ) -> torch.Tensor:\n+        batch_size = hidden_states.shape[0]\n+        hidden_states = hidden_states.reshape(-1, self.ffn_hidden_size)\n+\n+        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+\n+        split_expert_shape = (-1, self.ffn_hidden_size, self.hidden_size)\n+        for expert_idx in expert_hit:\n+            expert_idx = expert_idx[0]\n+            with torch.no_grad():\n+                idx, token_idx = torch.where(expert_mask[expert_idx])\n+            v1 = self.mlp.v1.view(split_expert_shape)[expert_idx]\n+            w1 = self.mlp.w1.view(split_expert_shape)[expert_idx]\n+            w2 = self.mlp.w2.view(split_expert_shape)[expert_idx]\n+            states = self.mlp(hidden_states[token_idx], w1, v1, w2)\n+            states = states.view(-1, self.ffn_hidden_size) * top_k_weights[token_idx, idx, None]\n+            next_states.index_add_(0, token_idx, states)\n+\n+        next_states = next_states.view(batch_size, -1, self.ffn_hidden_size)\n+        return next_states\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = causal_mask is None and q_len > 1\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attn_pdrop if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n+class DbrxRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.ffn_hidden_size\n+        self.moe_jitter_eps = config.moe_jitter_eps\n+        self.layer = nn.Linear(self.hidden_size, config.moe_num_experts, bias=False)\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, -1)\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.LongTensor]:\n+        if self.training and self.moe_jitter_eps is not None:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(\n+                1.0 - self.moe_jitter_eps, 1.0 + self.moe_jitter_eps\n+            )\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        router_logits = self.layer(hidden_states)\n+        return router_logits\n \n-        attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None\n+class DbrxFFN(nn.Module):\n+    \"\"\"Modular DBRX MLP/FFN component with MoE support.\"\"\"\n \n+    def __init__(self, config, **kwargs):\n+        super().__init__()\n+        self.router = DbrxRouter(config.ffn_config)\n+        self.experts = DbrxExperts(config.ffn_config)\n+\n+        self.moe_normalize_expert_weights = config.ffn_config.moe_normalize_expert_weights\n+        self.top_k = config.ffn_config.moe_top_k\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = torch.nn.functional.softmax(router_logits, dim=1, dtype=router_logits.dtype)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)\n+        if self.moe_normalize_expert_weights is not None:\n+            router_top_value = router_top_value / torch.norm(\n+                router_top_value, p=self.moe_normalize_expert_weights, dim=-1, keepdim=True\n+            )\n+        return router_top_value, router_indices\n \n-DBRX_ATTENTION_CLASSES = {\n-    \"eager\": DbrxAttention,\n-    \"flash_attention_2\": DbrxFlashAttention2,\n-    \"sdpa\": DbrxSdpaAttention,\n-}\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        router_logits = self.router(hidden_states)\n+        top_k_weights, top_k_index = self.route_tokens_to_experts(router_logits)\n+        output = self.experts(hidden_states, top_k_index, top_k_weights)\n+        return output\n \n \n class DbrxNormAttentionNorm(nn.Module):\n-    def __init__(self, config: DbrxConfig, block_idx: Optional[int] = None):\n+    def __init__(self, config: DbrxConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.block_idx = block_idx\n+        self.layer_idx = layer_idx\n         self.resid_pdrop = config.resid_pdrop\n         self.norm_1 = nn.LayerNorm(config.d_model, bias=False)\n-        self.attn = DBRX_ATTENTION_CLASSES[config._attn_implementation](\n+        self.attn = DbrxAttention(\n             config=config,\n-            block_idx=block_idx,\n+            layer_idx=layer_idx,\n         )\n         self.norm_2 = nn.LayerNorm(config.d_model, bias=False)\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_ids: torch.LongTensor,\n+        position_embeddings: torch.LongTensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Any,\n-    ) -> tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor], Optional[Cache]]:\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         residual_states = hidden_states\n         hidden_states = self.norm_1(hidden_states).to(hidden_states.dtype)\n \n-        hidden_states, attn_weights = self.attn(\n+        hidden_states, _ = self.attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            position_ids=position_ids,\n+            position_embeddings=position_embeddings,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -582,162 +383,18 @@ def forward(\n         residual_states = hidden_states\n         hidden_states = self.norm_2(hidden_states).to(hidden_states.dtype)\n \n-        return residual_states, hidden_states, attn_weights\n-\n-\n-class DbrxRouter(nn.Module):\n-    def __init__(\n-        self,\n-        hidden_size: int,\n-        moe_num_experts: int,\n-        moe_top_k: int,\n-        moe_jitter_eps: Optional[float],\n-        moe_normalize_expert_weights: Optional[float],\n-    ):\n-        super().__init__()\n-        self.hidden_size = hidden_size\n-        self.moe_num_experts = moe_num_experts\n-        self.moe_top_k = moe_top_k\n-        self.moe_jitter_eps = moe_jitter_eps\n-        self.moe_normalize_expert_weights = moe_normalize_expert_weights\n-\n-        self.layer = nn.Linear(self.hidden_size, self.moe_num_experts, bias=False)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.LongTensor]:\n-        if self.training and self.moe_jitter_eps is not None:\n-            hidden_states *= torch.empty_like(hidden_states).uniform_(\n-                1.0 - self.moe_jitter_eps, 1.0 + self.moe_jitter_eps\n-            )\n-        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        weights = self.layer(hidden_states).softmax(dim=-1, dtype=torch.float32)\n-        top_weights, top_experts = torch.topk(weights, self.moe_top_k, dim=-1)\n-\n-        top_weights_scale = (\n-            torch.norm(top_weights, p=self.moe_normalize_expert_weights, dim=-1, keepdim=True)\n-            if self.moe_normalize_expert_weights is not None\n-            else 1.0\n-        )\n-        top_weights = top_weights / top_weights_scale\n-\n-        weights = weights.to(hidden_states.dtype)\n-        top_weights = top_weights.to(hidden_states.dtype)\n-        return weights, top_weights, top_experts\n-\n-\n-class DbrxExpertGLU(nn.Module):\n-    def __init__(self, hidden_size: int, ffn_hidden_size: int, moe_num_experts: int, ffn_act_fn: dict):\n-        super().__init__()\n-        self.hidden_size = hidden_size\n-        self.ffn_hidden_size = ffn_hidden_size\n-        self.moe_num_experts = moe_num_experts\n-\n-        self.w1 = nn.Parameter(torch.empty(moe_num_experts * ffn_hidden_size, hidden_size))\n-        self.v1 = nn.Parameter(torch.empty(moe_num_experts * ffn_hidden_size, hidden_size))\n-        self.w2 = nn.Parameter(torch.empty(moe_num_experts * ffn_hidden_size, hidden_size))\n-\n-        act_fn_name = ffn_act_fn.get(\"name\", \"silu\")\n-        self.activation_fn = ACT2FN[act_fn_name]\n-\n-    def forward(\n-        self, x: torch.Tensor, expert_w1: torch.Tensor, expert_v1: torch.Tensor, expert_w2: torch.Tensor\n-    ) -> torch.Tensor:\n-        gate_proj = x.matmul(expert_w1.t())\n-        up_proj = x.matmul(expert_v1.t())\n-        gate_proj = self.activation_fn(gate_proj)\n-        intermediate_states = gate_proj * up_proj\n-        down_proj = intermediate_states.matmul(expert_w2)\n-        return down_proj\n-\n-\n-class DbrxExperts(nn.Module):\n-    def __init__(self, hidden_size: int, ffn_hidden_size: int, moe_num_experts: int, ffn_act_fn: dict):\n-        super().__init__()\n-        self.moe_num_experts = moe_num_experts\n-        self.mlp = DbrxExpertGLU(\n-            hidden_size=hidden_size,\n-            ffn_hidden_size=ffn_hidden_size,\n-            moe_num_experts=moe_num_experts,\n-            ffn_act_fn=ffn_act_fn,\n-        )\n-\n-    def forward(\n-        self, x: torch.Tensor, weights: torch.Tensor, top_weights: torch.Tensor, top_experts: torch.LongTensor\n-    ) -> torch.Tensor:\n-        bsz, q_len, hidden_size = x.shape\n-        x = x.view(-1, hidden_size)\n-        out = torch.zeros_like(x)\n-\n-        expert_mask = nn.functional.one_hot(top_experts, num_classes=self.moe_num_experts).permute(2, 1, 0)\n-        # Chunk experts at once to avoid storing full parameter multiple times in autograd\n-        w1_chunked = self.mlp.w1.view(self.mlp.moe_num_experts, self.mlp.ffn_hidden_size, self.mlp.hidden_size).chunk(\n-            self.moe_num_experts, dim=0\n-        )\n-        v1_chunked = self.mlp.v1.view(self.mlp.moe_num_experts, self.mlp.ffn_hidden_size, self.mlp.hidden_size).chunk(\n-            self.moe_num_experts, dim=0\n-        )\n-        w2_chunked = self.mlp.w2.view(self.mlp.moe_num_experts, self.mlp.ffn_hidden_size, self.mlp.hidden_size).chunk(\n-            self.moe_num_experts, dim=0\n-        )\n-        w1_chunked = [w1.squeeze(dim=0) for w1 in w1_chunked]\n-        v1_chunked = [v1.squeeze(dim=0) for v1 in v1_chunked]\n-        w2_chunked = [w2.squeeze(dim=0) for w2 in w2_chunked]\n-        for expert_idx in range(0, self.moe_num_experts):\n-            # (This cause torch.compile to fail with `torch._dynamo.exc.Unsupported: dynamic shape operator: aten.nonzero.default`)\n-            # (set torch._dynamo.config.capture_dynamic_output_shape_ops = True may help but not tested)\n-            topk_idx, token_idx = torch.where(expert_mask[expert_idx])\n-            if token_idx.shape[0] == 0:\n-                continue\n-\n-            token_list = token_idx\n-            topk_list = topk_idx\n-\n-            expert_tokens = x[None, token_list].reshape(-1, hidden_size)\n-            expert_out = (\n-                self.mlp(expert_tokens, w1_chunked[expert_idx], v1_chunked[expert_idx], w2_chunked[expert_idx])\n-                * top_weights[token_list, topk_list, None]\n-            )\n-\n-            out.index_add_(0, token_idx, expert_out)\n-\n-        out = out.reshape(bsz, q_len, hidden_size)\n-        return out\n-\n-\n-class DbrxFFN(nn.Module):\n-    def __init__(self, config: DbrxConfig):\n-        super().__init__()\n-\n-        ffn_config = config.ffn_config\n-        self.router = DbrxRouter(\n-            hidden_size=config.d_model,\n-            moe_num_experts=ffn_config.moe_num_experts,\n-            moe_top_k=ffn_config.moe_top_k,\n-            moe_jitter_eps=ffn_config.moe_jitter_eps,\n-            moe_normalize_expert_weights=ffn_config.moe_normalize_expert_weights,\n-        )\n-\n-        self.experts = DbrxExperts(\n-            hidden_size=config.d_model,\n-            ffn_hidden_size=ffn_config.ffn_hidden_size,\n-            moe_num_experts=ffn_config.moe_num_experts,\n-            ffn_act_fn=ffn_config.ffn_act_fn,\n-        )\n-\n-    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n-        weights, top_weights, top_experts = self.router(x)\n-        out = self.experts(x, weights, top_weights, top_experts)\n-        return out, weights\n+        return residual_states, hidden_states\n \n \n class DbrxBlock(GradientCheckpointingLayer):\n-    def __init__(self, config: DbrxConfig, block_idx: int):\n+    def __init__(self, config: DbrxConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.d_model\n         self.resid_pdrop = config.resid_pdrop\n-        self.block_idx = block_idx\n+        self.layer_idx = layer_idx\n         self.norm_attn_norm = DbrxNormAttentionNorm(\n             config=config,\n-            block_idx=block_idx,\n+            layer_idx=layer_idx,\n         )\n         self.ffn = DbrxFFN(config=config)\n \n@@ -746,78 +403,41 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Any,\n-    ) -> Union[\n-        tuple[torch.Tensor],\n-        tuple[torch.Tensor, Optional[torch.Tensor]],\n-        tuple[torch.Tensor, Optional[Cache]],\n-        tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache]],\n-        tuple[torch.Tensor, Optional[torch.Tensor], Optional[torch.Tensor]],\n-        tuple[torch.Tensor, Optional[Cache], Optional[torch.Tensor]],\n-        tuple[torch.Tensor, Optional[torch.Tensor], Optional[Cache], Optional[torch.Tensor]],\n-    ]:\n-        \"\"\"Forward function for DbrxBlock.\n-\n-        Args:\n-            hidden_states (`torch.Tensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            position_ids (`torch.LongTensor`): position ids of shape `(batch, seq_len)`\n-            attention_mask (`torch.Tensor`, *optional*): attention mask of size (batch_size, sequence_length)\n-                if flash attention is used or (batch_size, 1, query_sequence_length, key_sequence_length)\n-                if default attention is used.\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*): Whether or not to return the attentions tensors of all\n-                attention layers. See `attentions` under returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*): Whether or not to return the router logits.\n-            use_cache (`bool`, *optional*): If set to `True`, `past_key_values` key value states are\n-                returned and can be used to speed up decoding (see `past_key_values`).\n-            cache_position (`torch.LongTensor`, *optional*): position ids of the cache\n-        \"\"\"\n-\n-        # Norm + Attention + Norm\n-        resid_states, hidden_states, self_attn_weights = self.norm_attn_norm(\n+    ):\n+        resid_states, hidden_states = self.norm_attn_norm(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            position_ids=position_ids,\n+            position_embeddings=position_embeddings,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n-        # Fully Connected\n-        hidden_states, router_logits = self.ffn(hidden_states)\n+        hidden_states = self.ffn(hidden_states)\n         hidden_states = nn.functional.dropout(hidden_states, p=self.resid_pdrop, training=self.training)\n         hidden_states = resid_states + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n-@auto_docstring\n class DbrxPreTrainedModel(PreTrainedModel):\n     config: DbrxConfig\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DbrxBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_record_outputs = {\n+        \"hidden_states\": DbrxBlock,\n+        \"attentions\": DbrxAttention,\n+    }\n \n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range\n@@ -854,9 +474,9 @@ def __init__(self, config: DbrxConfig):\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n         self.emb_pdrop = config.emb_pdrop\n-\n+        self.rotary_emb = DbrxRotaryEmbedding(config)\n         self.wte = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n-        self.blocks = nn.ModuleList([DbrxBlock(config, block_idx) for block_idx in range(config.n_layers)])\n+        self.blocks = nn.ModuleList([DbrxBlock(config, layer_idx) for layer_idx in range(config.n_layers)])\n         self.norm_f = nn.LayerNorm(config.d_model, bias=False)\n         self.gradient_checkpointing = False\n \n@@ -869,255 +489,165 @@ def get_input_embeddings(self) -> nn.Embedding:\n     def set_input_embeddings(self, value: nn.Embedding):\n         self.wte = value\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[tuple, MoeModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        inputs_embeds = nn.functional.dropout(inputs_embeds, p=self.emb_pdrop, training=self.training)\n-\n-        if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache(config=self.config)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n-\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n-        # embed positions\n         hidden_states = inputs_embeds\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for block in self.blocks:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-            block_outputs = block(\n+        for decoder_layer in self.blocks[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = block_outputs[0]\n+        hidden_states = self.norm_f(hidden_states)\n \n-            if output_attentions:\n-                all_self_attns += (block_outputs[1],)\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n \n-            if output_router_logits:\n-                all_router_logits += (block_outputs[-1],)\n \n-        hidden_states = self.norm_f(hidden_states)\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_router_logits]\n-                if v is not None\n-            )\n-        return MoeModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The DBRX Model transformer for causal language modeling.\n-    \"\"\"\n-)\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n class DbrxForCausalLM(DbrxPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n     def __init__(self, config: DbrxConfig):\n         super().__init__(config)\n         self.transformer = DbrxModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-        self.moe_loss_weight = config.ffn_config.moe_loss_weight\n+        self.router_aux_loss_coef = config.ffn_config.moe_loss_weight\n         self.num_experts = config.ffn_config.moe_num_experts\n         self.num_experts_per_tok = config.ffn_config.moe_top_k\n-\n-        # Initialize weights and apply final processing\n         self.post_init()\n \n     def get_input_embeddings(self) -> nn.Embedding:\n@@ -1138,24 +668,22 @@ def set_decoder(self, decoder: DbrxModel):\n     def get_decoder(self) -> DbrxModel:\n         return self.transformer\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n-    ) -> Union[tuple, MoeCausalLMOutputWithPast]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1179,60 +707,42 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.transformer(\n+        outputs: MoeModelOutputWithPast = self.transformer(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n-        # No upscaling to float was ever done for Dbrx\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(\n-                logits,\n-                labels,\n-                vocab_size=self.config.vocab_size,\n-                **kwargs,\n-            )\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n \n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n-            if labels is not None and loss is not None:\n-                loss += self.moe_loss_weight * aux_loss.to(loss.device)  # make sure to reside in the same device\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n         return MoeCausalLMOutputWithPast(\n             loss=loss,"
        },
        {
            "sha": "08cdf33ca67f6376eda7c183e280ee93ac4f7ab6",
            "filename": "src/transformers/models/dbrx/modular_dbrx.py",
            "status": "added",
            "additions": 575,
            "deletions": 0,
            "changes": 575,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,575 @@\n+# coding=utf-8\n+# Copyright 2024 Databricks Mosaic Research and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Modular components for DBRX model.\"\"\"\n+\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import (\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from ..llama.modeling_llama import (\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from ..mixtral.modeling_mixtral import load_balancing_loss_func\n+from .configuration_dbrx import DbrxConfig\n+\n+\n+class DbrxRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class DbrxAttention(nn.Module):\n+    \"\"\"Modular DBRX attention component that can be reused across different model architectures.\"\"\"\n+\n+    def __init__(\n+        self,\n+        config,\n+        layer_idx: Optional[int] = None,\n+        **kwargs,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.d_model\n+        self.num_heads = config.n_heads\n+        self.head_dim = self.hidden_size // self.num_heads\n+        self.max_position_embeddings = config.max_seq_len\n+        self.layer_idx = layer_idx\n+\n+        attn_config = config.attn_config\n+        self.attention_dropout = attn_config.attn_pdrop\n+        self.clip_qkv = attn_config.clip_qkv\n+        self.num_key_value_heads = attn_config.kv_n_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.rope_theta = attn_config.rope_theta\n+        self.is_causal = True\n+\n+        self.Wqkv = nn.Linear(\n+            self.hidden_size, self.hidden_size + 2 * self.num_key_value_heads * self.head_dim, bias=False\n+        )\n+        self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        qkv_states = self.Wqkv(hidden_states)\n+        min_val = -self.clip_qkv if self.clip_qkv is not None else None\n+        qkv_states = qkv_states.clamp(min=min_val, max=self.clip_qkv)\n+\n+        query_states, key_states, value_states = qkv_states.split(\n+            [\n+                self.hidden_size,\n+                self.num_key_value_heads * self.head_dim,\n+                self.num_key_value_heads * self.head_dim,\n+            ],\n+            dim=2,\n+        )\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class DbrxExpertGLU(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.ffn_hidden_size = config.ffn_hidden_size\n+        self.moe_num_experts = config.moe_num_experts\n+\n+        self.w1 = nn.Parameter(torch.empty(self.moe_num_experts * self.ffn_hidden_size, self.hidden_size))\n+        self.v1 = nn.Parameter(torch.empty(self.moe_num_experts * self.ffn_hidden_size, self.hidden_size))\n+        self.w2 = nn.Parameter(torch.empty(self.moe_num_experts * self.ffn_hidden_size, self.hidden_size))\n+\n+        act_fn_name = config.ffn_act_fn.get(\"name\", \"silu\")\n+        self.activation_fn = ACT2FN[act_fn_name]\n+\n+    def forward(\n+        self, x: torch.Tensor, expert_w1: torch.Tensor, expert_v1: torch.Tensor, expert_w2: torch.Tensor\n+    ) -> torch.Tensor:\n+        gate_proj = x.matmul(expert_w1)\n+        up_proj = x.matmul(expert_v1)\n+        gate_proj = self.activation_fn(gate_proj)\n+        intermediate_states = gate_proj * up_proj\n+        down_proj = intermediate_states.matmul(expert_w2.t())\n+        return down_proj\n+\n+\n+class DbrxExperts(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.mlp = DbrxExpertGLU(config)\n+        self.hidden_size = config.hidden_size\n+        self.ffn_hidden_size = config.ffn_hidden_size\n+        self.num_experts = config.moe_num_experts\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        top_k_index: torch.Tensor,\n+        top_k_weights: torch.Tensor,\n+    ) -> torch.Tensor:\n+        batch_size = hidden_states.shape[0]\n+        hidden_states = hidden_states.reshape(-1, self.ffn_hidden_size)\n+\n+        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n+        with torch.no_grad():\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n+            expert_mask = expert_mask.permute(2, 1, 0)\n+            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+\n+        split_expert_shape = (-1, self.ffn_hidden_size, self.hidden_size)\n+        for expert_idx in expert_hit:\n+            expert_idx = expert_idx[0]\n+            with torch.no_grad():\n+                idx, token_idx = torch.where(expert_mask[expert_idx])\n+            v1 = self.mlp.v1.view(split_expert_shape)[expert_idx]\n+            w1 = self.mlp.w1.view(split_expert_shape)[expert_idx]\n+            w2 = self.mlp.w2.view(split_expert_shape)[expert_idx]\n+            states = self.mlp(hidden_states[token_idx], w1, v1, w2)\n+            states = states.view(-1, self.ffn_hidden_size) * top_k_weights[token_idx, idx, None]\n+            next_states.index_add_(0, token_idx, states)\n+\n+        next_states = next_states.view(batch_size, -1, self.ffn_hidden_size)\n+        return next_states\n+\n+\n+class DbrxRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.ffn_hidden_size\n+        self.moe_jitter_eps = config.moe_jitter_eps\n+        self.layer = nn.Linear(self.hidden_size, config.moe_num_experts, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.LongTensor]:\n+        if self.training and self.moe_jitter_eps is not None:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(\n+                1.0 - self.moe_jitter_eps, 1.0 + self.moe_jitter_eps\n+            )\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        router_logits = self.layer(hidden_states)\n+        return router_logits\n+\n+\n+class DbrxFFN(nn.Module):\n+    \"\"\"Modular DBRX MLP/FFN component with MoE support.\"\"\"\n+\n+    def __init__(self, config, **kwargs):\n+        super().__init__()\n+        self.router = DbrxRouter(config.ffn_config)\n+        self.experts = DbrxExperts(config.ffn_config)\n+\n+        self.moe_normalize_expert_weights = config.ffn_config.moe_normalize_expert_weights\n+        self.top_k = config.ffn_config.moe_top_k\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = torch.nn.functional.softmax(router_logits, dim=1, dtype=router_logits.dtype)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)\n+        if self.moe_normalize_expert_weights is not None:\n+            router_top_value = router_top_value / torch.norm(\n+                router_top_value, p=self.moe_normalize_expert_weights, dim=-1, keepdim=True\n+            )\n+        return router_top_value, router_indices\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        router_logits = self.router(hidden_states)\n+        top_k_weights, top_k_index = self.route_tokens_to_experts(router_logits)\n+        output = self.experts(hidden_states, top_k_index, top_k_weights)\n+        return output\n+\n+\n+class DbrxNormAttentionNorm(nn.Module):\n+    def __init__(self, config: DbrxConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.layer_idx = layer_idx\n+        self.resid_pdrop = config.resid_pdrop\n+        self.norm_1 = nn.LayerNorm(config.d_model, bias=False)\n+        self.attn = DbrxAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.norm_2 = nn.LayerNorm(config.d_model, bias=False)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: torch.LongTensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Any,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        residual_states = hidden_states\n+        hidden_states = self.norm_1(hidden_states).to(hidden_states.dtype)\n+\n+        hidden_states, _ = self.attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.resid_pdrop, training=self.training)\n+        hidden_states = hidden_states + residual_states\n+\n+        residual_states = hidden_states\n+        hidden_states = self.norm_2(hidden_states).to(hidden_states.dtype)\n+\n+        return residual_states, hidden_states\n+\n+\n+class DbrxBlock(GradientCheckpointingLayer):\n+    def __init__(self, config: DbrxConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.d_model\n+        self.resid_pdrop = config.resid_pdrop\n+        self.layer_idx = layer_idx\n+        self.norm_attn_norm = DbrxNormAttentionNorm(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.ffn = DbrxFFN(config=config)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Any,\n+    ):\n+        resid_states, hidden_states = self.norm_attn_norm(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = self.ffn(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.resid_pdrop, training=self.training)\n+        hidden_states = resid_states + hidden_states\n+        return hidden_states\n+\n+\n+class DbrxPreTrainedModel(PreTrainedModel):\n+    config: DbrxConfig\n+    base_model_prefix = \"transformer\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"DbrxBlock\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _can_record_outputs = {\n+        \"hidden_states\": DbrxBlock,\n+        \"attentions\": DbrxAttention,\n+    }\n+\n+    def _init_weights(self, module: nn.Module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, DbrxExpertGLU):\n+            module.w1.data.normal_(mean=0.0, std=std)\n+            module.v1.data.normal_(mean=0.0, std=std)\n+            module.w2.data.normal_(mean=0.0, std=std)\n+\n+\n+@auto_docstring\n+class DbrxModel(DbrxPreTrainedModel):\n+    \"\"\"Transformer decoder consisting of *config.num_hidden_layers*. Each layer is a [`DbrxBlock`] layer.\n+\n+    Args:\n+        config ([`DbrxConfig`]): Model configuration class with all parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+    \"\"\"\n+\n+    def __init__(self, config: DbrxConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+        self.emb_pdrop = config.emb_pdrop\n+        self.rotary_emb = DbrxRotaryEmbedding(config)\n+        self.wte = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n+        self.blocks = nn.ModuleList([DbrxBlock(config, layer_idx) for layer_idx in range(config.n_layers)])\n+        self.norm_f = nn.LayerNorm(config.d_model, bias=False)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Embedding:\n+        return self.wte\n+\n+    def set_input_embeddings(self, value: nn.Embedding):\n+        self.wte = value\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.wte(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.blocks[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm_f(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class DbrxForCausalLM(DbrxPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config: DbrxConfig):\n+        super().__init__(config)\n+        self.transformer = DbrxModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.router_aux_loss_coef = config.ffn_config.moe_loss_weight\n+        self.num_experts = config.ffn_config.moe_num_experts\n+        self.num_experts_per_tok = config.ffn_config.moe_top_k\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Embedding:\n+        return self.transformer.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value: nn.Embedding):\n+        self.transformer.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Linear:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings: nn.Linear):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder: DbrxModel):\n+        self.transformer = decoder\n+\n+    def get_decoder(self) -> DbrxModel:\n+        return self.transformer\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >> from transformers import AutoTokenizer, DbrxForCausalLM\n+\n+        >> model = DbrxForCausalLM.from_pretrained(\"databricks/dbrx-instruct\")\n+        >> tokenizer = AutoTokenizer.from_pretrained(\"databricks/dbrx-instruct\")\n+\n+        >> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >> # Generate\n+        >> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\n+        \"\"\"\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: MoeModelOutputWithPast = self.transformer(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_router_logits=output_router_logits,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+__all__ = [\"DbrxForCausalLM\", \"DbrxModel\", \"DbrxPreTrainedModel\"]"
        },
        {
            "sha": "4cc0d07a094ae319cbd333c28cf0547fc3f0533a",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -76,8 +76,6 @@ class DeepseekV2Config(PretrainedConfig):\n             The dropout probability applied to attention weights.\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias term in the MLP layers.\n-        aux_loss_alpha (`float`, *optional*, defaults to 0.001):\n-            Weight coefficient for auxiliary loss in Mixture of Experts (MoE) models.\n         first_k_dense_replace (`int`, *optional*, defaults to 0):\n             Number of dense layers in the shallow layers before switching to MoE layers.\n         kv_lora_rank (`int`, *optional*, defaults to 512):\n@@ -98,8 +96,6 @@ class DeepseekV2Config(PretrainedConfig):\n             The head dimension for QK projections when using RoPE.\n         routed_scaling_factor (`float`, *optional*, defaults to 1.0):\n             Scaling factor for routed experts in MoE models.\n-        seq_aux (`bool`, *optional*, defaults to `True`):\n-            Whether to compute the auxiliary loss for each individual sequence.\n         topk_group (`int`, *optional*):\n             Number of selected groups per token for expert selection.\n         topk_method (`str`, *optional*, defaults to `\"greedy\"`):\n@@ -108,8 +104,6 @@ class DeepseekV2Config(PretrainedConfig):\n             The dimension of value projections in the attention layers.\n         num_experts_per_tok (`int`, *optional*):\n             The number of experts selected per token. If `None`, the model behaves as a dense Transformer.\n-        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n-            Whether to normalize the probability distribution over top-k selected experts.\n         moe_intermediate_size (`int`, *optional*, defaults to 1407):\n             Dimension of the MoE (Mixture of Experts) representations.\n \n@@ -164,7 +158,6 @@ def __init__(\n         attention_bias=False,\n         attention_dropout=0.0,\n         mlp_bias=False,\n-        aux_loss_alpha=0.001,\n         first_k_dense_replace=0,\n         kv_lora_rank=512,\n         q_lora_rank=1536,\n@@ -174,12 +167,10 @@ def __init__(\n         qk_nope_head_dim=128,\n         qk_rope_head_dim=64,\n         routed_scaling_factor=1.0,\n-        seq_aux=True,\n         topk_group=None,\n         topk_method=\"greedy\",\n         v_head_dim=128,\n         num_experts_per_tok=None,\n-        norm_topk_prob=False,\n         moe_intermediate_size=1407,\n         **kwargs,\n     ):\n@@ -217,7 +208,6 @@ def __init__(\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n-        self.aux_loss_alpha = aux_loss_alpha\n         self.first_k_dense_replace = first_k_dense_replace\n         self.kv_lora_rank = kv_lora_rank\n         self.q_lora_rank = q_lora_rank\n@@ -227,12 +217,10 @@ def __init__(\n         self.qk_nope_head_dim = qk_nope_head_dim\n         self.qk_rope_head_dim = qk_rope_head_dim\n         self.routed_scaling_factor = routed_scaling_factor\n-        self.seq_aux = seq_aux\n         self.topk_group = topk_group\n         self.topk_method = topk_method\n         self.v_head_dim = v_head_dim\n         self.num_experts_per_tok = num_experts_per_tok\n-        self.norm_topk_prob = norm_topk_prob\n         self.moe_intermediate_size = moe_intermediate_size\n \n "
        },
        {
            "sha": "1bd1e4def6fc2508cc6a3e549593cd3f328ff9e0",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 59,
            "deletions": 91,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -42,117 +42,85 @@\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n \n-class DeepseekV2MoEGate(nn.Module):\n+class DeepseekV2Experts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.n_routed_experts\n+        for _ in range(config.n_routed_experts):\n+            self.append(DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class DeepseekV2Moe(nn.Module):\n     def __init__(self, config: DeepseekV2Config):\n         super().__init__()\n         self.config = config\n-        self.top_k = config.num_experts_per_tok\n-        self.num_experts = config.n_routed_experts\n+        self.experts = DeepseekV2Experts(config)\n+        self.gate = nn.Linear(config.hidden_size, config.n_routed_experts, bias=False)\n+        if config.n_shared_experts is not None:\n+            intermediate_size = config.moe_intermediate_size * config.n_shared_experts\n+            self.shared_experts = DeepseekV2MLP(config=config, intermediate_size=intermediate_size)\n         self.routed_scaling_factor = config.routed_scaling_factor\n-        self.alpha = config.aux_loss_alpha\n-        self.seq_aux = config.seq_aux\n         self.topk_method = config.topk_method\n         self.num_group = config.n_group\n+        self.top_k = config.num_experts_per_tok\n         self.topk_group = config.topk_group\n \n-        # topk selection algorithm\n-        self.norm_topk_prob = config.norm_topk_prob\n-        self.gating_dim = config.hidden_size\n-        self.weight = nn.Parameter(torch.empty((self.num_experts, self.gating_dim)))\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        batch_size, seq_len, hidden_dim = hidden_states.shape\n-        ### compute gating score\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32), None)\n-        scores = logits.softmax(dim=-1, dtype=torch.float32)\n-\n-        # select top-k experts\n-        # greedy method is used for DeepSeek-V2-Lite\n-        # group_limited_greedy for DeepSeek-V2 and DeepSeek-V2-Chat\n+    def route_tokens_to_experts(self, router_logits):\n+        batch_size, seq_len, hidden_dim = router_logits.shape\n+        router_logits = router_logits.view(-1, hidden_dim)\n+        router_logits = router_logits.softmax(dim=-1, dtype=torch.float32)\n         if self.topk_method == \"greedy\":\n-            topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)\n+            topk_weight, topk_idx = torch.topk(router_logits, k=self.top_k, dim=-1, sorted=False)\n         elif self.topk_method == \"group_limited_greedy\":\n-            group_scores = scores.view(batch_size * seq_len, self.num_group, -1).max(dim=-1).values  # [n, num_group]\n-            group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]  # [n, top_k_group]\n-            group_mask = torch.zeros_like(group_scores)  # [n, num_group]\n-            group_mask.scatter_(1, group_idx, 1)  # [n, num_group]\n+            group_scores = router_logits.view(batch_size * seq_len, self.num_group, -1).max(dim=-1).values\n+            group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+            group_mask = torch.zeros_like(group_scores)\n+            group_mask.scatter_(1, group_idx, 1)\n             score_mask = (\n                 group_mask.unsqueeze(-1)\n                 .expand(batch_size * seq_len, self.num_group, self.num_experts // self.num_group)\n                 .reshape(batch_size * seq_len, -1)\n-            )  # [n, e]\n-            tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n+            )\n+            tmp_scores = router_logits.masked_fill(~score_mask.bool(), 0.0)\n             topk_weight, topk_idx = torch.topk(tmp_scores, k=self.top_k, dim=-1, sorted=False)\n \n         topk_weight = topk_weight * self.routed_scaling_factor\n-        ### expert-level computation auxiliary loss\n         return topk_idx, topk_weight\n \n-\n-class DeepseekV2MoE(nn.Module):\n-    \"\"\"\n-    A mixed expert module containing shared experts.\n-    \"\"\"\n-\n-    def __init__(self, config: DeepseekV2Config):\n-        super().__init__()\n-        self.config = config\n-        self.num_experts_per_tok = config.num_experts_per_tok\n-\n-        self.experts = nn.ModuleList(\n-            [\n-                (DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size))\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n-        self.gate = DeepseekV2MoEGate(config)\n-        if config.n_shared_experts is not None:\n-            intermediate_size = config.moe_intermediate_size * config.n_shared_experts\n-            self.shared_experts = DeepseekV2MLP(config=config, intermediate_size=intermediate_size)\n-        self.ep_rank = 0\n-        self.experts_per_rank = config.n_routed_experts\n-\n-    def moe(self, hidden_states: torch.Tensor, topk_ids: torch.Tensor, topk_weight: torch.Tensor) -> torch.Tensor:\n-        cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts)))\n-        cnts.scatter_(1, topk_ids, 1)\n-        tokens_per_expert = cnts.sum(dim=0)\n-        indices = topk_ids.view(-1).argsort()\n-        sorted_tokens = hidden_states[indices // topk_ids.shape[1]]\n-\n-        # Process experts\n-        outputs = []\n-        start_idx = 0\n-        for i, num_tokens in enumerate(tokens_per_expert):\n-            if num_tokens == 0:\n-                continue\n-            end_idx = start_idx + num_tokens\n-            expert = self.experts[i + self.ep_rank * self.experts_per_rank]\n-            tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n-            expert_out = expert(tokens_for_this_expert)\n-            outputs.append(expert_out)\n-            start_idx = end_idx\n-\n-        outs = torch.cat(outputs, dim=0) if outputs else sorted_tokens.new_empty(0)\n-\n-        # Reorder and combine outputs\n-        new_x = torch.empty_like(outs)\n-        new_x[indices] = outs\n-        hidden_states = (\n-            new_x.view(*topk_ids.shape, -1)\n-            .type(topk_weight.dtype)\n-            .mul_(topk_weight.unsqueeze(dim=-1))\n-            .sum(dim=1)\n-            .type(new_x.dtype)\n-        )\n-        return hidden_states\n-\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         residuals = hidden_states\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n+        router_logits = nn.functional.linear(hidden_states.type(torch.float32), self.gate.weight.type(torch.float32))\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         hidden_states = hidden_states + self.shared_experts(residuals)\n         return hidden_states\n \n@@ -410,7 +378,7 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n         self.hidden_size = config.hidden_size\n \n         self.self_attn = DeepseekV2Attention(config=config, layer_idx=layer_idx)\n-        self.mlp = DeepseekV2MoE(config) if layer_idx >= config.first_k_dense_replace else DeepseekV2MLP(config)\n+        self.mlp = DeepseekV2Moe(config) if layer_idx >= config.first_k_dense_replace else DeepseekV2MLP(config)\n \n         self.input_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -469,8 +437,8 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n-        if isinstance(module, DeepseekV2MoEGate):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, DeepseekV2Moe):\n+            module.gate.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n @auto_docstring"
        },
        {
            "sha": "d5b3d99268556e26c53081a18e303fb965a22323",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 103,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -38,6 +38,7 @@\n     eager_attention_forward,\n )\n from ..llama4.modeling_llama4 import Llama4TextRotaryEmbedding\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeExperts\n \n \n logger = logging.get_logger(__name__)\n@@ -95,8 +96,6 @@ class DeepseekV2Config(LlamaConfig):\n             The dropout probability applied to attention weights.\n         mlp_bias (`bool`, *optional*, defaults to `False`):\n             Whether to use a bias term in the MLP layers.\n-        aux_loss_alpha (`float`, *optional*, defaults to 0.001):\n-            Weight coefficient for auxiliary loss in Mixture of Experts (MoE) models.\n         first_k_dense_replace (`int`, *optional*, defaults to 0):\n             Number of dense layers in the shallow layers before switching to MoE layers.\n         kv_lora_rank (`int`, *optional*, defaults to 512):\n@@ -117,8 +116,6 @@ class DeepseekV2Config(LlamaConfig):\n             The head dimension for QK projections when using RoPE.\n         routed_scaling_factor (`float`, *optional*, defaults to 1.0):\n             Scaling factor for routed experts in MoE models.\n-        seq_aux (`bool`, *optional*, defaults to `True`):\n-            Whether to compute the auxiliary loss for each individual sequence.\n         topk_group (`int`, *optional*):\n             Number of selected groups per token for expert selection.\n         topk_method (`str`, *optional*, defaults to `\"greedy\"`):\n@@ -127,8 +124,6 @@ class DeepseekV2Config(LlamaConfig):\n             The dimension of value projections in the attention layers.\n         num_experts_per_tok (`int`, *optional*):\n             The number of experts selected per token. If `None`, the model behaves as a dense Transformer.\n-        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n-            Whether to normalize the probability distribution over top-k selected experts.\n         moe_intermediate_size (`int`, *optional*, defaults to 1407):\n             Dimension of the MoE (Mixture of Experts) representations.\n \n@@ -178,7 +173,6 @@ def __init__(\n         attention_bias=False,\n         attention_dropout=0.0,\n         mlp_bias=False,\n-        aux_loss_alpha=0.001,\n         first_k_dense_replace=0,\n         kv_lora_rank=512,\n         q_lora_rank=1536,\n@@ -188,19 +182,16 @@ def __init__(\n         qk_nope_head_dim=128,\n         qk_rope_head_dim=64,\n         routed_scaling_factor=1.0,\n-        seq_aux=True,\n         topk_group=None,\n         topk_method=\"greedy\",\n         v_head_dim=128,\n         num_experts_per_tok=None,\n-        norm_topk_prob=False,\n         moe_intermediate_size=1407,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n \n         del self.pretraining_tp\n-        self.aux_loss_alpha = aux_loss_alpha\n         self.first_k_dense_replace = first_k_dense_replace\n         self.kv_lora_rank = kv_lora_rank\n         self.q_lora_rank = q_lora_rank\n@@ -210,12 +201,10 @@ def __init__(\n         self.qk_nope_head_dim = qk_nope_head_dim\n         self.qk_rope_head_dim = qk_rope_head_dim\n         self.routed_scaling_factor = routed_scaling_factor\n-        self.seq_aux = seq_aux\n         self.topk_group = topk_group\n         self.topk_method = topk_method\n         self.v_head_dim = v_head_dim\n         self.num_experts_per_tok = num_experts_per_tok\n-        self.norm_topk_prob = norm_topk_prob\n         self.moe_intermediate_size = moe_intermediate_size\n         self.head_dim = qk_rope_head_dim\n \n@@ -236,117 +225,59 @@ def apply_rotary_emb(\n     return xq_out, xk_out\n \n \n-class DeepseekV2MoEGate(nn.Module):\n+class DeepseekV2Experts(Qwen2MoeExperts, nn.ModuleList):\n+    def __init__(self, config):\n+        nn.ModuleList.__init__(self)\n+        self.num_experts = config.n_routed_experts\n+        for _ in range(config.n_routed_experts):\n+            self.append(DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+\n+class DeepseekV2Moe(nn.Module):\n     def __init__(self, config: DeepseekV2Config):\n         super().__init__()\n         self.config = config\n-        self.top_k = config.num_experts_per_tok\n-        self.num_experts = config.n_routed_experts\n+        self.experts = DeepseekV2Experts(config)\n+        self.gate = nn.Linear(config.hidden_size, config.n_routed_experts, bias=False)\n+        if config.n_shared_experts is not None:\n+            intermediate_size = config.moe_intermediate_size * config.n_shared_experts\n+            self.shared_experts = DeepseekV2MLP(config=config, intermediate_size=intermediate_size)\n         self.routed_scaling_factor = config.routed_scaling_factor\n-        self.alpha = config.aux_loss_alpha\n-        self.seq_aux = config.seq_aux\n         self.topk_method = config.topk_method\n         self.num_group = config.n_group\n+        self.top_k = config.num_experts_per_tok\n         self.topk_group = config.topk_group\n \n-        # topk selection algorithm\n-        self.norm_topk_prob = config.norm_topk_prob\n-        self.gating_dim = config.hidden_size\n-        self.weight = nn.Parameter(torch.empty((self.num_experts, self.gating_dim)))\n-\n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        batch_size, seq_len, hidden_dim = hidden_states.shape\n-        ### compute gating score\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32), None)\n-        scores = logits.softmax(dim=-1, dtype=torch.float32)\n-\n-        # select top-k experts\n-        # greedy method is used for DeepSeek-V2-Lite\n-        # group_limited_greedy for DeepSeek-V2 and DeepSeek-V2-Chat\n+    def route_tokens_to_experts(self, router_logits):\n+        batch_size, seq_len, hidden_dim = router_logits.shape\n+        router_logits = router_logits.view(-1, hidden_dim)\n+        router_logits = router_logits.softmax(dim=-1, dtype=torch.float32)\n         if self.topk_method == \"greedy\":\n-            topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)\n+            topk_weight, topk_idx = torch.topk(router_logits, k=self.top_k, dim=-1, sorted=False)\n         elif self.topk_method == \"group_limited_greedy\":\n-            group_scores = scores.view(batch_size * seq_len, self.num_group, -1).max(dim=-1).values  # [n, num_group]\n-            group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]  # [n, top_k_group]\n-            group_mask = torch.zeros_like(group_scores)  # [n, num_group]\n-            group_mask.scatter_(1, group_idx, 1)  # [n, num_group]\n+            group_scores = router_logits.view(batch_size * seq_len, self.num_group, -1).max(dim=-1).values\n+            group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+            group_mask = torch.zeros_like(group_scores)\n+            group_mask.scatter_(1, group_idx, 1)\n             score_mask = (\n                 group_mask.unsqueeze(-1)\n                 .expand(batch_size * seq_len, self.num_group, self.num_experts // self.num_group)\n                 .reshape(batch_size * seq_len, -1)\n-            )  # [n, e]\n-            tmp_scores = scores.masked_fill(~score_mask.bool(), 0.0)  # [n, e]\n+            )\n+            tmp_scores = router_logits.masked_fill(~score_mask.bool(), 0.0)\n             topk_weight, topk_idx = torch.topk(tmp_scores, k=self.top_k, dim=-1, sorted=False)\n \n         topk_weight = topk_weight * self.routed_scaling_factor\n-        ### expert-level computation auxiliary loss\n         return topk_idx, topk_weight\n \n-\n-class DeepseekV2MoE(nn.Module):\n-    \"\"\"\n-    A mixed expert module containing shared experts.\n-    \"\"\"\n-\n-    def __init__(self, config: DeepseekV2Config):\n-        super().__init__()\n-        self.config = config\n-        self.num_experts_per_tok = config.num_experts_per_tok\n-\n-        self.experts = nn.ModuleList(\n-            [\n-                (DeepseekV2MLP(config, intermediate_size=config.moe_intermediate_size))\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n-        self.gate = DeepseekV2MoEGate(config)\n-        if config.n_shared_experts is not None:\n-            intermediate_size = config.moe_intermediate_size * config.n_shared_experts\n-            self.shared_experts = DeepseekV2MLP(config=config, intermediate_size=intermediate_size)\n-        self.ep_rank = 0\n-        self.experts_per_rank = config.n_routed_experts\n-\n-    def moe(self, hidden_states: torch.Tensor, topk_ids: torch.Tensor, topk_weight: torch.Tensor) -> torch.Tensor:\n-        cnts = topk_ids.new_zeros((topk_ids.shape[0], len(self.experts)))\n-        cnts.scatter_(1, topk_ids, 1)\n-        tokens_per_expert = cnts.sum(dim=0)\n-        indices = topk_ids.view(-1).argsort()\n-        sorted_tokens = hidden_states[indices // topk_ids.shape[1]]\n-\n-        # Process experts\n-        outputs = []\n-        start_idx = 0\n-        for i, num_tokens in enumerate(tokens_per_expert):\n-            if num_tokens == 0:\n-                continue\n-            end_idx = start_idx + num_tokens\n-            expert = self.experts[i + self.ep_rank * self.experts_per_rank]\n-            tokens_for_this_expert = sorted_tokens[start_idx:end_idx]\n-            expert_out = expert(tokens_for_this_expert)\n-            outputs.append(expert_out)\n-            start_idx = end_idx\n-\n-        outs = torch.cat(outputs, dim=0) if outputs else sorted_tokens.new_empty(0)\n-\n-        # Reorder and combine outputs\n-        new_x = torch.empty_like(outs)\n-        new_x[indices] = outs\n-        hidden_states = (\n-            new_x.view(*topk_ids.shape, -1)\n-            .type(topk_weight.dtype)\n-            .mul_(topk_weight.unsqueeze(dim=-1))\n-            .sum(dim=1)\n-            .type(new_x.dtype)\n-        )\n-        return hidden_states\n-\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         residuals = hidden_states\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n+        router_logits = nn.functional.linear(hidden_states.type(torch.float32), self.gate.weight.type(torch.float32))\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         hidden_states = hidden_states + self.shared_experts(residuals)\n         return hidden_states\n \n@@ -497,7 +428,7 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n \n         self.self_attn = DeepseekV2Attention(config=config, layer_idx=layer_idx)\n-        self.mlp = DeepseekV2MoE(config) if layer_idx >= config.first_k_dense_replace else DeepseekV2MLP(config)\n+        self.mlp = DeepseekV2Moe(config) if layer_idx >= config.first_k_dense_replace else DeepseekV2MLP(config)\n \n         self.input_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -508,8 +439,8 @@ class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         PreTrainedModel._init_weights(self, module)\n-        if isinstance(module, DeepseekV2MoEGate):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        if isinstance(module, DeepseekV2Moe):\n+            module.gate.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n \n \n class DeepseekV2Model(LlamaModel):"
        },
        {
            "sha": "54b32c7a60c99cd4a154ea9464059b83a0c0baef",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -151,6 +151,9 @@ class DeepseekV3Config(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_experts\": \"n_routed_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "629d1cf2ccad77a44b30c23c4ded0a9422a94b9d",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 69,
            "deletions": 69,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -90,12 +90,11 @@ def forward(self, x, position_ids):\n \n \n class DeepseekV3MLP(nn.Module):\n-    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+    def __init__(self, config, intermediate_size=None):\n         super().__init__()\n         self.config = config\n-        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n-\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n@@ -110,47 +109,49 @@ class DeepseekV3TopkRouter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.top_k = config.num_experts_per_tok\n         self.n_routed_experts = config.n_routed_experts\n-        self.routed_scaling_factor = config.routed_scaling_factor\n-        self.n_group = config.n_group\n-        self.topk_group = config.topk_group\n-        self.norm_topk_prob = config.norm_topk_prob\n \n         self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n         self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n \n-    @torch.no_grad()\n-    def get_topk_indices(self, scores):\n-        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n-        group_scores = (\n-            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .topk(2, dim=-1)[0]\n-            .sum(dim=-1)\n-        )\n-        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n-        group_mask = torch.zeros_like(group_scores)\n-        group_mask.scatter_(1, group_idx, 1)\n-        score_mask = (\n-            group_mask.unsqueeze(-1)\n-            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .reshape(-1, self.n_routed_experts)\n-        )\n-        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n-        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n-        return topk_indices\n-\n     def forward(self, hidden_states):\n         hidden_states = hidden_states.view(-1, self.config.hidden_size)\n         router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n-        scores = router_logits.sigmoid()\n-        topk_indices = self.get_topk_indices(scores)\n-        topk_weights = scores.gather(1, topk_indices)\n-        if self.norm_topk_prob:\n-            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n-            topk_weights /= denominator\n-        topk_weights = topk_weights * self.routed_scaling_factor\n-        return topk_indices, topk_weights\n+        return router_logits\n+\n+\n+class DeepseekV3NaiveMoe(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n \n class DeepseekV3MoE(nn.Module):\n@@ -161,49 +162,48 @@ class DeepseekV3MoE(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.experts = nn.ModuleList(\n-            [\n-                DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size)\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n+        self.experts = DeepseekV3NaiveMoe(config)\n         self.gate = DeepseekV3TopkRouter(config)\n         self.shared_experts = DeepseekV3MLP(\n             config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n         )\n+        self.n_routed_experts = config.n_routed_experts\n+        self.n_group = config.n_group\n+        self.topk_group = config.topk_group\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.top_k = config.num_experts_per_tok\n \n-    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n-        r\"\"\"\n-        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n-        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n-        \"\"\"\n-        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n-        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n-        expert_mask = expert_mask.permute(2, 0, 1)\n-\n-        for expert_idx in range(len(self.experts)):\n-            expert = self.experts[expert_idx]\n-            mask = expert_mask[expert_idx]\n-            token_indices, weight_indices = torch.where(mask)\n-\n-            if token_indices.numel() > 0:\n-                expert_weights = topk_weights[token_indices, weight_indices]\n-                expert_input = hidden_states[token_indices]\n-                expert_output = expert(expert_input)\n-                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n-                final_hidden_states.index_add_(0, token_indices, weighted_output)\n-\n-        # in original deepseek, the output of the experts are gathered once we leave this module\n-        # thus the moe module is itelsf an IsolatedParallel module\n-        # and all expert are \"local\" meaning we shard but we don't gather\n-        return final_hidden_states.type(hidden_states.dtype)\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = router_logits.sigmoid()\n+        router_logits = router_logits + self.gate.e_score_correction_bias\n+        group_scores = (\n+            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+        )\n+        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+        group_mask = torch.zeros_like(group_scores)\n+        group_mask.scatter_(1, group_idx, 1)\n+        score_mask = (\n+            group_mask.unsqueeze(-1)\n+            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .reshape(-1, self.n_routed_experts)\n+        )\n+        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        topk_weights = router_logits.gather(1, topk_indices)\n+        if self.norm_topk_prob:\n+            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n+            topk_weights /= denominator\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n \n     def forward(self, hidden_states):\n         residuals = hidden_states\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         hidden_states = hidden_states + self.shared_experts(residuals)\n         return hidden_states\n "
        },
        {
            "sha": "eb823989c9f6c280fa8e55a5d5e1aadcef1b28e7",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 44,
            "deletions": 81,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -5,7 +5,6 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification, GenericForTokenClassification\n@@ -24,6 +23,8 @@\n     eager_attention_forward,\n     rotate_half,\n )\n+from ..mixtral.modeling_mixtral import MixtralExperts\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeMLP\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n \n@@ -38,6 +39,10 @@ class DeepseekV3RotaryEmbedding(LlamaRotaryEmbedding):\n     pass\n \n \n+class DeepseekV3MLP(Qwen2MoeMLP):\n+    pass\n+\n+\n def apply_rotary_pos_emb_interleave(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     r\"\"\"\n     TODO let's just use the original freqcis computation to not have the view\n@@ -82,44 +87,54 @@ def yarn_get_mscale(scale=1, mscale=1):\n     return 0.1 * mscale * math.log(scale) + 1.0\n \n \n-class DeepseekV3MLP(nn.Module):\n-    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+class DeepseekV3TopkRouter(nn.Module):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n-        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.n_routed_experts = config.n_routed_experts\n \n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n+        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n \n-    def forward(self, x):\n-        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-        return down_proj\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n+        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n+        return router_logits\n \n \n-class DeepseekV3TopkRouter(nn.Module):\n+class DeepseekV3NaiveMoe(MixtralExperts, nn.ModuleList):\n+    def __init__(self, config):\n+        nn.ModuleList.__init__(self)\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+\n+class DeepseekV3MoE(nn.Module):\n+    \"\"\"\n+    A mixed expert module containing shared experts.\n+    \"\"\"\n+\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.top_k = config.num_experts_per_tok\n+        self.experts = DeepseekV3NaiveMoe(config)\n+        self.gate = DeepseekV3TopkRouter(config)\n+        self.shared_experts = DeepseekV3MLP(\n+            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n+        )\n         self.n_routed_experts = config.n_routed_experts\n-        self.routed_scaling_factor = config.routed_scaling_factor\n         self.n_group = config.n_group\n         self.topk_group = config.topk_group\n         self.norm_topk_prob = config.norm_topk_prob\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.top_k = config.num_experts_per_tok\n \n-        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n-        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n-\n-    @torch.no_grad()\n-    def get_topk_indices(self, scores):\n-        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = router_logits.sigmoid()\n+        router_logits = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .topk(2, dim=-1)[0]\n-            .sum(dim=-1)\n+            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -129,74 +144,22 @@ def get_topk_indices(self, scores):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n-        return topk_indices\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n-        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n-        scores = router_logits.sigmoid()\n-        topk_indices = self.get_topk_indices(scores)\n-        topk_weights = scores.gather(1, topk_indices)\n+        topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:\n             denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n             topk_weights /= denominator\n         topk_weights = topk_weights * self.routed_scaling_factor\n         return topk_indices, topk_weights\n \n-\n-class DeepseekV3MoE(nn.Module):\n-    \"\"\"\n-    A mixed expert module containing shared experts.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.experts = nn.ModuleList(\n-            [\n-                DeepseekV3MLP(config, intermediate_size=config.moe_intermediate_size)\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n-        self.gate = DeepseekV3TopkRouter(config)\n-        self.shared_experts = DeepseekV3MLP(\n-            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n-        )\n-\n-    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n-        r\"\"\"\n-        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n-        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n-        \"\"\"\n-        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n-        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n-        expert_mask = expert_mask.permute(2, 0, 1)\n-\n-        for expert_idx in range(len(self.experts)):\n-            expert = self.experts[expert_idx]\n-            mask = expert_mask[expert_idx]\n-            token_indices, weight_indices = torch.where(mask)\n-\n-            if token_indices.numel() > 0:\n-                expert_weights = topk_weights[token_indices, weight_indices]\n-                expert_input = hidden_states[token_indices]\n-                expert_output = expert(expert_input)\n-                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n-                final_hidden_states.index_add_(0, token_indices, weighted_output)\n-\n-        # in original deepseek, the output of the experts are gathered once we leave this module\n-        # thus the moe module is itelsf an IsolatedParallel module\n-        # and all expert are \"local\" meaning we shard but we don't gather\n-        return final_hidden_states.type(hidden_states.dtype)\n-\n     def forward(self, hidden_states):\n         residuals = hidden_states\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         hidden_states = hidden_states + self.shared_experts(residuals)\n         return hidden_states\n "
        },
        {
            "sha": "d24cd201569b08422f12f203e3b41dfd47310e50",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 99,
            "deletions": 302,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -12,7 +12,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch GPTSANJapanese model.\"\"\"\n \n from typing import Optional, Union\n \n@@ -23,88 +22,11 @@\n from ....cache_utils import Cache\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n-from ....utils import (\n-    DUMMY_INPUTS,\n-    DUMMY_MASK,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_torch_fx_proxy,\n-    logging,\n-)\n+from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring, is_torch_fx_proxy\n from ....utils.deprecation import deprecate_kwarg\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-_CONFIG_FOR_DOC = \"GPTSanJapaneseConfig\"\n-_CHECKPOINT_FOR_DOC = \"Tanrei/GPTSAN-japanese\"\n-\n-####################################################\n-# This dict contains ids and associated url\n-# for the pretrained weights provided with the models\n-####################################################\n-\n-\n-def router_z_loss_func(router_logits: torch.Tensor) -> float:\n-    r\"\"\"\n-    Compute the router z-loss implemented in PyTorch.\n-\n-    The router z-loss was introduced in [Designing Effective Sparse Expert Models](https://huggingface.co/papers/2202.08906).\n-    It encourages router logits to remain small in an effort to improve stability.\n-\n-    Args:\n-        router_logits (`float`):\n-            Input logits of shape [batch_size, sequence_length, num_experts]\n-\n-    Returns:\n-        Scalar router z-loss.\n-    \"\"\"\n-    num_groups, tokens_per_group, _ = router_logits.shape\n-    log_z = torch.logsumexp(router_logits, dim=-1)\n-    z_loss = log_z**2\n-    return torch.sum(z_loss) / (num_groups * tokens_per_group)\n-\n-\n-def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        router_probs (`torch.Tensor`):\n-            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n-        expert_indices (`torch.Tensor`):\n-            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    num_experts = router_probs.shape[-1]\n-\n-    # cast the expert indices to int64, otherwise one-hot encoding will fail\n-    if expert_indices.dtype != torch.int64:\n-        expert_indices = expert_indices.to(torch.int64)\n-\n-    if len(expert_indices.shape) == 2:\n-        expert_indices = expert_indices.unsqueeze(2)\n-\n-    expert_mask = torch.nn.functional.one_hot(expert_indices, num_experts)\n-\n-    # For a given token, determine if it was routed to a given expert.\n-    expert_mask = torch.max(expert_mask, axis=-2).values\n-\n-    # cast to float32 otherwise mean will fail\n-    expert_mask = expert_mask.to(torch.float32)\n-    tokens_per_group_and_expert = torch.mean(expert_mask, axis=-2)\n-\n-    router_prob_per_group_and_expert = torch.mean(router_probs, axis=-2)\n-    return torch.mean(tokens_per_group_and_expert * router_prob_per_group_and_expert) * (num_experts**2)\n-\n-\n class GPTSanJapaneseDenseActDense(nn.Module):\n     \"\"\"\n     FFN Layer for Switch Transformer and Extra layers\n@@ -159,7 +81,7 @@ def __init__(self, config: GPTSanJapaneseConfig):\n         self.ignore_padding_tokens = config.router_ignore_padding_tokens\n         self.dtype = getattr(torch, config.router_dtype)\n \n-    def _compute_router_probabilities(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         r\"\"\"\n         Computes router probabilities from input hidden states.\n \n@@ -179,100 +101,43 @@ def _compute_router_probabilities(self, hidden_states: torch.Tensor) -> tuple[to\n         # We also store the previous dtype to cast back the output to the previous dtype\n         self.input_dtype = hidden_states.dtype\n         hidden_states = hidden_states.to(self.dtype)\n-\n         if self.training and self.jitter_noise > 0:\n             # Multiply the token inputs by the uniform distribution - adding some noise\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n-\n-        # Shape: [num_groups, tokens_per_group, num_experts]\n-        self._cast_classifier()\n         router_logits = self.classifier(hidden_states)\n \n         # Apply Softmax and cast back to the original `dtype`\n-        router_probabilities = nn.functional.softmax(router_logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n-        return router_probabilities, router_logits\n-\n-    def _cast_classifier(self):\n-        r\"\"\"\n-        `bitsandbytes` `Linear8bitLt` layers does not support manual casting Therefore we need to check if they are an\n-        instance of the `Linear8bitLt` class by checking special attributes.\n-        \"\"\"\n-        if not (hasattr(self.classifier, \"SCB\") or hasattr(self.classifier, \"CB\")):\n-            self.classifier = self.classifier.to(self.dtype)\n-\n-    def forward(self, hidden_states: torch.Tensor) -> tuple:\n-        r\"\"\"\n-        Generic forward function for every Router class. Each Router expects to have the same input hidden states\n-        (`hidden_states`) corresponding to the hidden states for each token, the `expert_capacity` corresponding to the\n-        number of tokens the Router will send to each expert, some Routers can send up to few tokens to each expert.\n-\n-        Each Router works as the following: it expects the hidden states for each token, gets the `router_probs` and\n-        `router_logits` from the `router_weights`. This will assign for each token, the raw probability to be assigned\n-        to an expert. Then each Router class will have to define its own `_compute_routing_instructions`.\n-\n-        Args:\n-            hidden_states (`torch.Tensor`) :\n-                [num_groups, tokens_per_group, hidden_dim] inputs to send to experts.\n-        Returns:\n-            tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`] Tuple containing the expert index, the router probs\n-            and the router logits. The router probabilities and logits are required to compute the loss.\n-        \"\"\"\n-        router_probs, router_logits = self._compute_router_probabilities(hidden_states)\n-\n-        expert_index = torch.argmax(router_probs, dim=-1)\n+        router_probs = nn.functional.softmax(router_logits, dim=-1, dtype=self.dtype).to(self.input_dtype)\n+        router_logits, expert_index = torch.max(router_probs, dim=-1, keepdim=True)\n         expert_index = torch.nn.functional.one_hot(expert_index, num_classes=self.num_experts)\n-\n-        # Mask tokens outside expert capacity. Sum over each sequence\n         token_priority = torch.cumsum(expert_index, dim=-2)\n         # mask if the token routed to to the expert will overflow\n         expert_capacity_mask = token_priority <= self.expert_capacity\n         expert_index = expert_index * expert_capacity_mask\n-\n         router_probs = torch.max(router_probs, dim=-1).values.unsqueeze(-1)\n-        return expert_index, router_probs, router_logits\n+        return router_probs, expert_index, router_logits\n \n \n-class GPTSanJapaneseSparseMLP(nn.Module):\n-    r\"\"\"\n-    Implementation of the Switch Transformers Sparse MLP module.\n-    \"\"\"\n-\n-    def __init__(self, config: GPTSanJapaneseConfig, expert_class: nn.Module = GPTSanJapaneseDenseActDense):\n+class GPTSanExperts(nn.ModuleDict):\n+    def __init__(self, config: GPTSanJapaneseConfig):\n         super().__init__()\n-        # Step 1: Get the correct router according to its class\n-        self.router = GPTSanJapaneseTop1Router(config)\n-\n-        # Step 2: Get the experts\n-        self.experts = nn.ModuleDict()\n         for idx in range(config.num_experts):\n-            self.experts[f\"expert_{idx}\"] = expert_class(config)\n-\n-    def forward(self, hidden_states):\n-        r\"\"\"\n-        Hold on, this will be slightly tricky to understand In the correct order, a MoE layer does the following:\n-\n-        1- Gets the `router_mask` from the router. The shape of the mask is `(batch_size, sequence_length, num_expert)`\n-        and corresponds to the argmax of the `router_probs`. The probabilities are needed in the computation of the\n-        hidden states : they are broadcasted to the hidden states values (can be interpreted as a scaling factor).\n-\n-        2- Dispatch the tokens to its associated experts. We do a classic for loop over the experts and assign for each\n-        expert the corresponding hidden states.\n-\n-        \"\"\"\n-        # Step 1: Get the router_mask from the router as well as the probabilities\n-        router_mask, router_probs, router_logits = self.router(hidden_states)\n-        expert_index = torch.argmax(router_mask, dim=-1)\n+            self.add_module(f\"expert_{idx}\", GPTSanJapaneseDenseActDense(config))\n \n-        # The routers introduced might not always map all the tokens, to a router, which means that some hidden states\n-        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the selected ones.\n \n-        next_states = hidden_states.clone()\n-        for idx, expert in enumerate(self.experts.values()):\n-            token_indices = router_mask[:, :, idx].bool()\n-            next_states[token_indices] = expert(hidden_states[token_indices]).to(next_states.dtype)\n-\n-        hidden_states = router_probs * next_states\n-        return hidden_states, (router_logits, expert_index)\n+class GPTSanJapaneseSparseMLP(nn.Module):\n+    def __init__(self, config: GPTSanJapaneseConfig):\n+        super().__init__()\n+        self.router = GPTSanJapaneseTop1Router(config)\n+        self.experts = GPTSanExperts(config)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        _, selected_experts, routing_weights = self.router(hidden_states)\n+        hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n+        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return hidden_states\n \n \n class GPTSanJapaneseLayerSparseFF(nn.Module):\n@@ -680,15 +545,22 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, GPTSanJapaneseModel):\n+            # Mesh TensorFlow embeddings initialization\n+            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n             module.embed_tokens.weight.data.normal_(mean=0.0, std=factor * 1.0)\n             module.position_embeddings.weight.data.normal_(mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"extra_position_embeddings\") and module.extra_position_embeddings is not None:\n                 module.extra_position_embeddings.weight.data.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, (GPTSanJapaneseModel, GPTSanJapaneseForConditionalGeneration)):\n+            # Mesh TensorFlow embeddings initialization\n+            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L1624\n             module.final_logits_bias.data.normal_(mean=0.0, std=factor * 1.0)\n             if hasattr(module, \"lm_head\") and not self.config.tie_word_embeddings:\n                 module.lm_head.weight.data.normal_(mean=0.0, std=factor * 1.0)\n         elif isinstance(module, GPTSanJapaneseDenseActDense):\n+            # Mesh TensorFlow FF initialization\n+            # See https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/transformer/transformer_layers.py#L56\n+            # and https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/layers.py#L89\n             module.wi.weight.data.normal_(mean=0.0, std=factor * ((self.config.d_model) ** -0.5))\n             if hasattr(module.wi, \"bias\") and module.wi.bias is not None:\n                 module.wi.bias.data.zero_()\n@@ -705,6 +577,8 @@ def _init_weights(self, module):\n             module.q_proj.weight.data.normal_(mean=0.0, std=factor * ((d_model * key_value_proj_dim) ** -0.5))\n             module.out_proj.weight.data.normal_(mean=0.0, std=factor * ((n_heads * key_value_proj_dim) ** -0.5))\n         elif isinstance(module, GPTSanJapaneseSparseMLP):\n+            # Mesh TensorFlow attention initialization to avoid scaling before softmax\n+            # See https://github.com/tensorflow/mesh/blob/fa19d69eafc9a482aff0b59ddd96b025c0cb207d/mesh_tensorflow/transformer/attention.py#L136\n             d_model = self.config.d_model\n             key_value_proj_dim = self.config.d_model\n             n_heads = self.config.num_heads\n@@ -741,77 +615,6 @@ def _shift_right(self, input_ids):\n         return shifted_input_ids\n \n \n-GPTSAN_JAPANESE_START_DOCSTRING = r\"\"\"\n-\n-    The [GPTSAN-japanese](https://github.com/tanreinama/GPTSAN) model was proposed in General-purpose Swich transformer\n-    based Japanese language model\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`GPTSanJapaneseConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-GPTSAN_JAPANESE_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. GPTSAN-japanese is a model that generates sentence\n-            continuations or predicts tokens at mask positions. Special tokens required for inputs to the model are\n-            automatically appended.\n-        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            An input that masks the Prefix part in the Prefix-LM input. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **prefix** input,\n-            - 0 for tokens that are **not-prefix** input.\n-        spout (`torch.Tensor` of shape `(batch_size, config.d_spout)`):\n-                This vector is transformed through an 8-layer FFN and can be used instead of `past_key_values`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n-            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n-            input (see `past_key_values`). This is useful if you want more control over how to convert\n-            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        router_logits (`tuple(torch.FloatTensor)`, *optional*, returned when `output_router_logits=True` is passed or when `config.add_router_probs=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_experts)`.\n-            Router logits of the decoder model, useful to compute the auxiliary loss for Mixture of Experts models.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare GPTSAN-japanese Model transformer outputting raw hidden-states without any specific head on top.\",\n-    GPTSAN_JAPANESE_START_DOCSTRING,\n-)\n class GPTSanJapaneseModel(GPTSanJapanesePreTrainedModel):\n     def __init__(self, config: GPTSanJapaneseConfig):\n         super().__init__(config)\n@@ -842,7 +645,7 @@ def __init__(self, config: GPTSanJapaneseConfig):\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n-    @add_start_docstrings_to_model_forward(GPTSAN_JAPANESE_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -971,12 +774,6 @@ def forward(\n         # Merge prefix_lm_mask and attention_mask\n         extended_attention_mask = prefix_lm_mask * attention_mask.unsqueeze(1).unsqueeze(2)\n \n-        # outputs\n-        present_key_value_states = () if self.config.use_cache or use_cache else None\n-        all_hidden_states = () if self.config.output_hidden_states or output_hidden_states else None\n-        all_attentions = () if self.config.output_attentions or output_attentions else None\n-        all_router_probs = () if self.config.output_router_logits or output_router_logits else None\n-\n         for layer, past in enumerate(pasts_or_spout_value):\n             if layer == self.config.num_switch_layers:\n                 if self.config.num_ext_layers > 0:\n@@ -987,68 +784,84 @@ def forward(\n                         hidden_states[i] += torch.gather(\n                             self.extra_position_embeddings.weight, dim=0, index=gather_position[i]\n                         )\n-\n-            output_router_tuple = (\n-                self.config.output_router_logits or output_router_logits\n-            ) and layer < self.config.num_switch_layers\n-            block_output = self.blocks[layer](\n+            hidden_states = self.blocks[layer](\n                 hidden_states=hidden_states,\n                 past_key_values=past,\n                 attention_mask=extended_attention_mask,\n-                use_cache=self.config.use_cache or use_cache,\n-                output_attentions=self.config.output_attentions or output_attentions,\n-                output_router_tuple=output_router_tuple,\n             )\n \n-            outpos = 0\n-            hidden_states = block_output[outpos]\n-            if self.config.output_hidden_states or output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-            if self.config.use_cache or use_cache:\n-                outpos += 1\n-                present = block_output[outpos]\n-                present_key_value_states += (present,)\n-            if self.config.output_attentions or output_attentions:\n-                outpos += 1\n-                attention_probs = block_output[outpos]\n-                all_attentions += (attention_probs,)\n-            if output_router_tuple:\n-                outpos += 1\n-                router_tuple = block_output[outpos]\n-                all_router_probs.append(router_tuple[0])\n-\n         hidden_states = self.last_project(hidden_states)\n         hidden_states = self.act(hidden_states)\n \n-        if self.config.output_hidden_states or output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    present_key_value_states,\n-                    all_hidden_states,\n-                    all_attentions,\n-                    all_router_probs,\n-                ]\n-                if v is not None\n-            )\n+        return MoEModelOutputWithPastAndCrossAttentions(last_hidden_state=hidden_states)\n \n-        return MoEModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            past_key_values=present_key_value_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_attentions,\n-            router_probs=all_router_probs,\n-        )\n+\n+####################################################\n+# This dict contains ids and associated url\n+# for the pretrained weights provided with the models\n+####################################################\n+\n+\n+# TODO import from switch\n+def router_z_loss_func(router_logits: torch.Tensor) -> float:\n+    r\"\"\"\n+    Compute the router z-loss implemented in PyTorch.\n+\n+    The router z-loss was introduced in [Designing Effective Sparse Expert Models](https://huggingface.co/papers/2202.08906).\n+    It encourages router logits to remain small in an effort to improve stability.\n+\n+    Args:\n+        router_logits (`float`):\n+            Input logits of shape [batch_size, sequence_length, num_experts]\n+\n+    Returns:\n+        Scalar router z-loss.\n+    \"\"\"\n+    num_groups, tokens_per_group, _ = router_logits.shape\n+    log_z = torch.logsumexp(router_logits, dim=-1)\n+    z_loss = log_z**2\n+    return torch.sum(z_loss) / (num_groups * tokens_per_group)\n+\n+\n+def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        router_probs (`torch.Tensor`):\n+            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n+        expert_indices (`torch.Tensor`):\n+            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    num_experts = router_probs.shape[-1]\n+\n+    # cast the expert indices to int64, otherwise one-hot encoding will fail\n+    if expert_indices.dtype != torch.int64:\n+        expert_indices = expert_indices.to(torch.int64)\n+\n+    if len(expert_indices.shape) == 2:\n+        expert_indices = expert_indices.unsqueeze(2)\n+\n+    expert_mask = torch.nn.functional.one_hot(expert_indices, num_experts)\n+\n+    # For a given token, determine if it was routed to a given expert.\n+    expert_mask = torch.max(expert_mask, axis=-2).values\n+\n+    # cast to float32 otherwise mean will fail\n+    expert_mask = expert_mask.to(torch.float32)\n+    tokens_per_group_and_expert = torch.mean(expert_mask, axis=-2)\n+\n+    router_prob_per_group_and_expert = torch.mean(router_probs, axis=-2)\n+    return torch.mean(tokens_per_group_and_expert * router_prob_per_group_and_expert) * (num_experts**2)\n \n \n-@add_start_docstrings(\n-    \"The bare GPTSAN-japanese Model with a language modeling head.\",\n-    GPTSAN_JAPANESE_START_DOCSTRING,\n-)\n class GPTSanJapaneseForConditionalGeneration(GPTSanJapanesePreTrainedModel):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n@@ -1060,7 +873,6 @@ def __init__(self, config: GPTSanJapaneseConfig):\n         if not self.config.torchscript:\n             self.lm_head.weight = self.model.embed_tokens.weight\n \n-    @add_start_docstrings_to_model_forward(GPTSAN_JAPANESE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1176,7 +988,7 @@ def forward(\n         router_probs = None\n         aux_loss = None\n         if labels is not None:\n-            # move labels to correct device\n+            # move labels to correct device to enable model parallelism\n             labels = labels.to(lm_logits.device)\n \n             loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n@@ -1190,21 +1002,6 @@ def forward(\n \n             loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    loss,\n-                    lm_logits,\n-                    outputs.past_key_values,\n-                    outputs.hidden_states,\n-                    outputs.router_probs,\n-                    z_loss,\n-                    aux_loss,\n-                ]\n-                if v is not None\n-            )\n-\n         return MoECausalLMOutputWithPast(\n             loss=loss,\n             logits=lm_logits,"
        },
        {
            "sha": "cabe44f391bc69d5256361ed75e2d69c0f64295f",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -130,6 +130,9 @@ class Dots1Config(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_local_experts\": \"n_routed_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "ea6698b470de27f783f0d9b8968557a914d9d555",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 69,
            "deletions": 66,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -246,12 +246,11 @@ def forward(\n \n \n class Dots1MLP(nn.Module):\n-    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+    def __init__(self, config, intermediate_size=None):\n         super().__init__()\n         self.config = config\n-        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n-\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n@@ -262,79 +261,80 @@ def forward(self, x):\n         return down_proj\n \n \n-class Dots1MoE(nn.Module):\n+class Dots1TopkRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.n_routed_experts = config.n_routed_experts\n+\n+        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n+        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n+        return router_logits\n+\n+\n+class Dots1NaiveMoe(nn.ModuleList):\n     \"\"\"\n-    A mixed expert module containing shared experts.\n+    ModuleList of experts.\n     \"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n-        self.config = config\n-        self.experts = nn.ModuleList(\n-            [Dots1MLP(config, intermediate_size=config.moe_intermediate_size) for _ in range(config.n_routed_experts)]\n-        )\n-        self.gate = Dots1TopkRouter(config)\n-        self.shared_experts = Dots1MLP(\n-            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n-        )\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(Dots1MLP(config, intermediate_size=config.moe_intermediate_size))\n \n-    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n-        r\"\"\"\n-        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n-        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n         \"\"\"\n-        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n-        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n-        expert_mask = expert_mask.permute(2, 0, 1)\n-\n-        for expert_idx in range(len(self.experts)):\n-            expert = self.experts[expert_idx]\n-            mask = expert_mask[expert_idx]\n-            token_indices, weight_indices = torch.where(mask)\n-\n-            if token_indices.numel() > 0:\n-                expert_weights = topk_weights[token_indices, weight_indices]\n-                expert_input = hidden_states[token_indices]\n-                expert_output = expert(expert_input)\n-                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n-                final_hidden_states.index_add_(0, token_indices, weighted_output)\n-\n-        # in original deepseek, the output of the experts are gathered once we leave this module\n-        # thus the moe module is itelsf an IsolatedParallel module\n-        # and all expert are \"local\" meaning we shard but we don't gather\n-        return final_hidden_states.type(hidden_states.dtype)\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n \n-    def forward(self, hidden_states):\n-        residuals = hidden_states\n-        orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n-        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n-        hidden_states = hidden_states + self.shared_experts(residuals)\n-        return hidden_states\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n \n-class Dots1TopkRouter(nn.Module):\n+class Dots1MoE(nn.Module):\n+    \"\"\"\n+    A mixed expert module containing shared experts.\n+    \"\"\"\n+\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.top_k = config.num_experts_per_tok\n+        self.experts = Dots1NaiveMoe(config)\n+        self.gate = Dots1TopkRouter(config)\n+        self.shared_experts = Dots1MLP(\n+            config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n+        )\n         self.n_routed_experts = config.n_routed_experts\n-        self.routed_scaling_factor = config.routed_scaling_factor\n         self.n_group = config.n_group\n         self.topk_group = config.topk_group\n         self.norm_topk_prob = config.norm_topk_prob\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.top_k = config.num_experts_per_tok\n \n-        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n-        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n-\n-    @torch.no_grad()\n-    def get_topk_indices(self, scores):\n-        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = router_logits.sigmoid()  # main diff with deepseekv3\n+        router_logits = router_logits + self.gate.e_score_correction_bias\n         group_scores = (\n-            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .topk(2, dim=-1)[0]\n-            .sum(dim=-1)\n+            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n         )\n         group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n         group_mask = torch.zeros_like(group_scores)\n@@ -344,22 +344,25 @@ def get_topk_indices(self, scores):\n             .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n             .reshape(-1, self.n_routed_experts)\n         )\n-        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n+        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n         topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n-        return topk_indices\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.view(-1, self.config.hidden_size)\n-        router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n-        scores = router_logits.sigmoid()\n-        topk_indices = self.get_topk_indices(scores)\n-        topk_weights = scores.gather(1, topk_indices)\n+        topk_weights = router_logits.gather(1, topk_indices)\n         if self.norm_topk_prob:\n             denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n             topk_weights /= denominator\n         topk_weights = topk_weights * self.routed_scaling_factor\n         return topk_indices, topk_weights\n \n+    def forward(self, hidden_states):\n+        residuals = hidden_states\n+        orig_shape = hidden_states.shape\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = hidden_states + self.shared_experts(residuals)\n+        return hidden_states\n+\n \n class Dots1DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Dots1Config, layer_idx: int):"
        },
        {
            "sha": "8f6e574edcb7a29d765b222d2e61abba4d8f16c3",
            "filename": "src/transformers/models/dots1/modular_dots1.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodular_dots1.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -12,6 +12,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import torch\n+\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import logging\n@@ -52,12 +54,33 @@ class Dots1MLP(DeepseekV3MLP):\n     pass\n \n \n-class Dots1MoE(DeepseekV3MoE):\n+class Dots1TopkRouter(DeepseekV3TopkRouter):\n     pass\n \n \n-class Dots1TopkRouter(DeepseekV3TopkRouter):\n-    pass\n+class Dots1MoE(DeepseekV3MoE):\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = router_logits.sigmoid()  # main diff with deepseekv3\n+        router_logits = router_logits + self.gate.e_score_correction_bias\n+        group_scores = (\n+            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+        )\n+        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+        group_mask = torch.zeros_like(group_scores)\n+        group_mask.scatter_(1, group_idx, 1)\n+        score_mask = (\n+            group_mask.unsqueeze(-1)\n+            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .reshape(-1, self.n_routed_experts)\n+        )\n+        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        topk_weights = router_logits.gather(1, topk_indices)\n+        if self.norm_topk_prob:\n+            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n+            topk_weights /= denominator\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n \n \n class Dots1DecoderLayer(DeepseekV3DecoderLayer):"
        },
        {
            "sha": "66d6b1935096c21873132df58e9148c73a3899c1",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 78,
            "deletions": 99,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -29,7 +29,6 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n@@ -67,7 +66,7 @@ def __init__(self, config, intermediate_size=None):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.hidden_size\n-        self.intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n \n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n@@ -288,99 +287,107 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n-    \"\"\"\n-    This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accommodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n-\n-    Ernie 4.5 MoE's original formula is based on case (2) with\n-    (optional) shared experts and a corrections bias during gating.\n-    \"\"\"\n+class Ernie4_5_MoeRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.moe_k\n+        self.num_experts = config.moe_num_experts\n+        self.norm_min = config.moe_norm_min\n+        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, device_type: str\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            router_logits = self.gate(hidden_states.float())\n+            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n+            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n+            routing_weights = routing_weights / torch.clamp(\n+                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n+            )\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return router_logits, selected_experts, routing_weights\n+\n \n+class Ernie4_5_MoeExperts(nn.ModuleList):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n-        self.top_k = config.moe_k\n+        for _ in range(self.num_experts):\n+            self.append(Ernie4_5_MoeMLP(config))\n \n-        # correction bias (yes it seems to be a typo with statics <> statistics)\n-        self.moe_statics = Ernie4_5_MoeStatics(config)\n+    def forward(\n+        self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # gating\n-        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n-        self.experts = nn.ModuleList(\n-            [Ernie4_5_MoeMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n-        )\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * routing_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.num_experts = config.moe_num_experts\n+        self.top_k = config.moe_k\n         self.norm_min = config.moe_norm_min\n \n-        # (optional) shared experts for all forwards\n+        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n+        self.experts = Ernie4_5_MoeExperts(config)\n+\n         self.shared_experts = None\n         if config.moe_num_shared_experts > 0:\n             self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-\n-        # (Optional) shared experts\n-        if self.shared_experts is not None:\n-            shared_output = self.shared_experts(hidden_states)\n-\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n         device_type = (\n             hidden_states.device.type\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            # router_logits: (batch * sequence_length, n_experts)\n-            router_logits = self.gate(hidden_states.float())\n \n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n+            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-            routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return selected_experts, routing_weights\n \n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, _ = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, self.hidden_dim)\n \n-        # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+        if self.shared_experts is not None:\n+            shared_output = self.shared_experts(hidden_states_reshaped)\n \n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+        router_logits = self.gate(hidden_states_reshaped.float())\n+        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states_reshaped, router_logits)\n \n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n-        # Add (optional) shared experts to the result\n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output\n \n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, self.hidden_dim)\n+        return final_hidden_states\n \n \n class Ernie4_5_MoeDecoderLayer(GradientCheckpointingLayer):\n@@ -406,49 +413,25 @@ def __init__(self, config, layer_idx):\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> torch.FloatTensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n         # Self Attention\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n-            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n+            use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n@@ -457,11 +440,7 @@ def forward(\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = self.mlp(hidden_states)\n-        # For the MoE layers, we need to unpack\n-        if isinstance(hidden_states, tuple):\n-            hidden_states, _ = hidden_states\n         hidden_states = residual + hidden_states\n-\n         return hidden_states\n \n \n@@ -478,7 +457,7 @@ class Ernie4_5_MoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Ernie4_5_MoeSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n         \"attentions\": Ernie4_5_MoeAttention,\n     }"
        },
        {
            "sha": "836b0c1cd4422d657734ba54f364c32698e1bf2b",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 71,
            "deletions": 63,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -96,99 +96,107 @@ def forward(self, hidden_states):\n         return hidden_states + self.e_score_correction_bias.squeeze()\n \n \n-class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n-    \"\"\"\n-    This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accommodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n-\n-    Ernie 4.5 MoE's original formula is based on case (2) with\n-    (optional) shared experts and a corrections bias during gating.\n-    \"\"\"\n+class Ernie4_5_MoeRouter(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.moe_k\n+        self.num_experts = config.moe_num_experts\n+        self.norm_min = config.moe_norm_min\n+        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, device_type: str\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            router_logits = self.gate(hidden_states.float())\n+            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n+            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n+            routing_weights = routing_weights / torch.clamp(\n+                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n+            )\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return router_logits, selected_experts, routing_weights\n \n+\n+class Ernie4_5_MoeExperts(nn.ModuleList):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.moe_num_experts\n-        self.top_k = config.moe_k\n+        for _ in range(self.num_experts):\n+            self.append(Ernie4_5_MoeMLP(config))\n \n-        # correction bias (yes it seems to be a typo with statics <> statistics)\n-        self.moe_statics = Ernie4_5_MoeStatics(config)\n+    def forward(\n+        self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # gating\n-        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n-        self.experts = nn.ModuleList(\n-            [Ernie4_5_MoeMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n-        )\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * routing_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class Ernie4_5_MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.num_experts = config.moe_num_experts\n+        self.top_k = config.moe_k\n         self.norm_min = config.moe_norm_min\n \n-        # (optional) shared experts for all forwards\n+        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n+        self.moe_statics = Ernie4_5_MoeStatics(config)\n+        self.experts = Ernie4_5_MoeExperts(config)\n+\n         self.shared_experts = None\n         if config.moe_num_shared_experts > 0:\n             self.shared_experts = Ernie4_5_MoeMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n \n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-\n-        # (Optional) shared experts\n-        if self.shared_experts is not None:\n-            shared_output = self.shared_experts(hidden_states)\n-\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n         device_type = (\n             hidden_states.device.type\n             if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n             else \"cpu\"\n         )\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            # router_logits: (batch * sequence_length, n_experts)\n-            router_logits = self.gate(hidden_states.float())\n \n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_bias = self.moe_statics.e_score_correction_bias.squeeze()\n+            _, selected_experts = torch.topk(routing_weights + routing_bias, self.top_k, dim=-1)\n             routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )\n-            routing_weights = routing_weights.to(hidden_states.dtype)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return selected_experts, routing_weights\n \n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, _ = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, self.hidden_dim)\n \n-        # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+        if self.shared_experts is not None:\n+            shared_output = self.shared_experts(hidden_states_reshaped)\n \n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+        router_logits = self.gate(hidden_states_reshaped.float())\n+        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states_reshaped, router_logits)\n \n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n-        # Add (optional) shared experts to the result\n         if self.shared_experts is not None:\n             final_hidden_states = final_hidden_states + shared_output\n \n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, self.hidden_dim)\n+        return final_hidden_states\n \n \n class Ernie4_5_MoeDecoderLayer(Qwen3MoeDecoderLayer):\n@@ -219,7 +227,7 @@ class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n     # Not supporting multi-token prediction (MTP) atm\n     _keys_to_ignore_on_load_unexpected = [\"mtp\"]\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Ernie4_5_MoeSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Ernie4_5_MoeDecoderLayer,\n         \"attentions\": Ernie4_5_MoeAttention,\n     }"
        },
        {
            "sha": "8808c7f8743900050e4a3bd435559b5ffc98fe73",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -166,16 +166,24 @@ def update(\n \n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n-        for layer_idx in range(len(self.key_cache)):\n-            device = self.key_cache[layer_idx].device\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.value_cache[layer_idx].device\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n-\n-            device = self.conv_states[layer_idx].device\n-            self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.ssm_states[layer_idx].device\n-            self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+        if self.get_seq_length() > 0:\n+            for layer_idx in range(len(self.key_cache)):\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+                device = self.conv_states[layer_idx].device\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.ssm_states[layer_idx].device\n+                self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        kv_length = self.get_seq_length(layer_idx) + query_length\n+        return kv_length, kv_offset\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\""
        },
        {
            "sha": "1777b8f3123771a4a61a23aa5c03f3bcf44f658d",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 67,
            "deletions": 93,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -22,7 +22,6 @@\n from typing import Callable, Optional, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -35,7 +34,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_flex_olmo import FlexOlmoConfig\n@@ -263,60 +262,75 @@ def forward(\n         return attn_output, attn_weights\n \n \n+class FlexOlmoExperts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        for _ in range(config.num_experts):\n+            self.append(FlexOlmoMLP(config))\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n class FlexOlmoSparseMoeBlock(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.num_experts = config.num_experts\n         self.top_k = config.num_experts_per_tok\n         self.norm_topk_prob = config.norm_topk_prob\n         self.gate = nn.Linear(config.hidden_size, self.num_experts, bias=False)\n-        self.experts = nn.ModuleList([FlexOlmoMLP(config) for _ in range(self.num_experts)])\n+        self.experts = FlexOlmoExperts(config)\n+\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        if self.norm_topk_prob:\n+            top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n+        top_k_weights = top_k_weights.to(hidden_states.dtype)\n+        return top_k_index, top_k_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n         router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        if self.norm_topk_prob:\n-            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n+        final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n+            batch_size, sequence_length, hidden_dim\n         )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be selected\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        return final_hidden_states\n \n \n class FlexOlmoDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: FlexOlmoConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.self_attn = FlexOlmoAttention(config=config, layer_idx=layer_idx)\n-\n         self.mlp = FlexOlmoSparseMoeBlock(config)\n         self.post_attention_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -332,31 +346,6 @@ def forward(\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs,\n     ) -> torch.FloatTensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n         residual = hidden_states\n \n         # Self Attention\n@@ -374,7 +363,7 @@ def forward(\n \n         # Fully Connected\n         residual = hidden_states\n-        hidden_states, _ = self.mlp(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n         return hidden_states\n@@ -393,7 +382,7 @@ class FlexOlmoPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(FlexOlmoSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": FlexOlmoDecoderLayer,\n         \"attentions\": FlexOlmoAttention,\n     }\n@@ -549,8 +538,8 @@ def load_balancing_loss_func(\n         # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n         router_per_expert_attention_mask = (\n             attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n             .to(compute_device)\n         )\n \n@@ -559,29 +548,29 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n \n \n+@auto_docstring\n class FlexOlmoForCausalLM(FlexOlmoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)\n         self.model = FlexOlmoModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -592,14 +581,11 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n-    ) -> Union[tuple, MoeCausalLMOutputWithPast]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -623,31 +609,25 @@ def forward(\n         'Hey, are you conscious? Can you talk to me?\\nIâ€™m not sure if youâ€™re conscious of this, but Iâ€™m'\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -659,20 +639,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "adaa1e7a159f50131b6604cf83aaf085a29e6801",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -16,13 +16,14 @@\n from typing import Optional\n \n import torch\n+from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.generic import check_model_inputs\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..mixtral.modeling_mixtral import MixtralModel, MixtralPreTrainedModel\n from ..olmo2.modeling_olmo2 import Olmo2Attention, Olmo2RMSNorm, Olmo2RotaryEmbedding\n from ..olmoe.configuration_olmoe import OlmoeConfig\n@@ -260,7 +261,7 @@ def forward(\n \n         # Fully Connected\n         residual = hidden_states\n-        hidden_states, _ = self.mlp(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n         hidden_states = self.post_feedforward_layernorm(hidden_states)\n         hidden_states = residual + hidden_states\n         return hidden_states\n@@ -269,7 +270,11 @@ def forward(\n # FlexOlmo uses Mixtral model as its base instead of OlmoE model since Mixtral is more up-to-date with the rest\n # of the transformers library. For example, it uses the newer mechanisms of recording submodule outputs.\n class FlexOlmoPreTrainedModel(MixtralPreTrainedModel):\n-    pass\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"hidden_states\": FlexOlmoDecoderLayer,\n+        \"attentions\": FlexOlmoAttention,\n+    }\n \n \n # FlexOlmo uses Mixtral model as its base instead of OlmoE model since Mixtral is more up-to-date with the rest"
        },
        {
            "sha": "77b162efa3bd39c398bf0e9f109600d470ad1786",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -166,6 +166,9 @@ class Glm4MoeConfig(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_local_experts\": \"n_routed_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "fea2b97685d2ed46aed38bea199e8fb45c547801",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 69,
            "deletions": 64,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -208,12 +208,11 @@ def forward(\n \n \n class Glm4MoeMLP(nn.Module):\n-    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+    def __init__(self, config, intermediate_size=None):\n         super().__init__()\n         self.config = config\n-        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n-\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n@@ -238,37 +237,10 @@ def __init__(self, config: Glm4MoeConfig):\n         self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n         self.register_buffer(\"e_score_correction_bias\", torch.zeros((self.n_routed_experts), dtype=torch.float32))\n \n-    @torch.no_grad()\n-    def get_topk_indices(self, scores):\n-        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n-        group_scores = (\n-            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .topk(2, dim=-1)[0]\n-            .sum(dim=-1)\n-        )\n-        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n-        group_mask = torch.zeros_like(group_scores)\n-        group_mask.scatter_(1, group_idx, 1)\n-        score_mask = (\n-            group_mask.unsqueeze(-1)\n-            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .reshape(-1, self.n_routed_experts)\n-        )\n-        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n-        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n-        return topk_indices\n-\n     def forward(self, hidden_states):\n         hidden_states = hidden_states.view(-1, self.config.hidden_size)\n         router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n-        scores = router_logits.sigmoid()\n-        topk_indices = self.get_topk_indices(scores)\n-        topk_weights = scores.gather(1, topk_indices)\n-        if self.norm_topk_prob:\n-            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n-            topk_weights /= denominator\n-        topk_weights = topk_weights * self.routed_scaling_factor\n-        return topk_indices, topk_weights\n+        return router_logits\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n@@ -292,6 +264,40 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n+class Glm4MoeNaiveMoe(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(Glm4MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n class Glm4MoeMoE(nn.Module):\n     \"\"\"\n     A mixed expert module containing shared experts.\n@@ -300,49 +306,48 @@ class Glm4MoeMoE(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.experts = nn.ModuleList(\n-            [\n-                Glm4MoeMLP(config, intermediate_size=config.moe_intermediate_size)\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n+        self.experts = Glm4MoeNaiveMoe(config)\n         self.gate = Glm4MoeTopkRouter(config)\n         self.shared_experts = Glm4MoeMLP(\n             config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n         )\n+        self.n_routed_experts = config.n_routed_experts\n+        self.n_group = config.n_group\n+        self.topk_group = config.topk_group\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.top_k = config.num_experts_per_tok\n \n-    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n-        r\"\"\"\n-        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n-        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n-        \"\"\"\n-        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n-        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n-        expert_mask = expert_mask.permute(2, 0, 1)\n-\n-        for expert_idx in range(len(self.experts)):\n-            expert = self.experts[expert_idx]\n-            mask = expert_mask[expert_idx]\n-            token_indices, weight_indices = torch.where(mask)\n-\n-            if token_indices.numel() > 0:\n-                expert_weights = topk_weights[token_indices, weight_indices]\n-                expert_input = hidden_states[token_indices]\n-                expert_output = expert(expert_input)\n-                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n-                final_hidden_states.index_add_(0, token_indices, weighted_output)\n-\n-        # in original deepseek, the output of the experts are gathered once we leave this module\n-        # thus the moe module is itelsf an IsolatedParallel module\n-        # and all expert are \"local\" meaning we shard but we don't gather\n-        return final_hidden_states.type(hidden_states.dtype)\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = router_logits.sigmoid()\n+        router_logits = router_logits + self.gate.e_score_correction_bias\n+        group_scores = (\n+            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+        )\n+        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+        group_mask = torch.zeros_like(group_scores)\n+        group_mask.scatter_(1, group_idx, 1)\n+        score_mask = (\n+            group_mask.unsqueeze(-1)\n+            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .reshape(-1, self.n_routed_experts)\n+        )\n+        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        topk_weights = router_logits.gather(1, topk_indices)\n+        if self.norm_topk_prob:\n+            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n+            topk_weights /= denominator\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n \n     def forward(self, hidden_states):\n         residuals = hidden_states\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         hidden_states = hidden_states + self.shared_experts(residuals)\n         return hidden_states\n "
        },
        {
            "sha": "b671f9b2403b7bddaaf7ab366024e0deec8fb324",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -181,6 +181,9 @@ class Glm4MoeConfig(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_local_experts\": \"n_routed_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "425edd64bbed020c7cefd674a1d4a5cd80369b0a",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -236,6 +236,9 @@ class Glm4vMoeTextConfig(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_local_experts\": \"n_routed_experts\",\n+    }\n     base_config_key = \"text_config\"\n \n     def __init__("
        },
        {
            "sha": "0b65072b240443e68a38114a55e4038583501b4f",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 69,
            "deletions": 64,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -253,37 +253,44 @@ def __init__(self, config: Glm4vMoeTextConfig):\n         self.weight = nn.Parameter(torch.empty((self.n_routed_experts, config.hidden_size)))\n         self.register_buffer(\"e_score_correction_bias\", torch.zeros((self.n_routed_experts), dtype=torch.float32))\n \n-    @torch.no_grad()\n-    def get_topk_indices(self, scores):\n-        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n-        group_scores = (\n-            scores_for_choice.view(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .topk(2, dim=-1)[0]\n-            .sum(dim=-1)\n-        )\n-        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n-        group_mask = torch.zeros_like(group_scores)\n-        group_mask.scatter_(1, group_idx, 1)\n-        score_mask = (\n-            group_mask.unsqueeze(-1)\n-            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n-            .reshape(-1, self.n_routed_experts)\n-        )\n-        scores_for_choice = scores_for_choice.masked_fill(~score_mask.bool(), 0.0)\n-        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n-        return topk_indices\n-\n     def forward(self, hidden_states):\n         hidden_states = hidden_states.view(-1, self.config.hidden_size)\n         router_logits = F.linear(hidden_states.type(torch.float32), self.weight.type(torch.float32))\n-        scores = router_logits.sigmoid()\n-        topk_indices = self.get_topk_indices(scores)\n-        topk_weights = scores.gather(1, topk_indices)\n-        if self.norm_topk_prob:\n-            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n-            topk_weights /= denominator\n-        topk_weights = topk_weights * self.routed_scaling_factor\n-        return topk_indices, topk_weights\n+        return router_logits\n+\n+\n+class Glm4vMoeTextNaiveMoe(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(Glm4vMoeTextMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n \n class Glm4vMoeTextMoE(nn.Module):\n@@ -294,60 +301,58 @@ class Glm4vMoeTextMoE(nn.Module):\n     def __init__(self, config: Glm4vMoeTextConfig):\n         super().__init__()\n         self.config = config\n-        self.experts = nn.ModuleList(\n-            [\n-                Glm4vMoeTextMLP(config, intermediate_size=config.moe_intermediate_size)\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n+        self.experts = Glm4vMoeTextNaiveMoe(config)\n         self.gate = Glm4vMoeTextTopkRouter(config)\n         self.shared_experts = Glm4vMoeTextMLP(\n             config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts\n         )\n+        self.n_routed_experts = config.n_routed_experts\n+        self.n_group = config.n_group\n+        self.topk_group = config.topk_group\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.routed_scaling_factor = config.routed_scaling_factor\n+        self.top_k = config.num_experts_per_tok\n \n-    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n-        r\"\"\"\n-        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n-        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n-        \"\"\"\n-        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n-        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n-        expert_mask = expert_mask.permute(2, 0, 1)\n-\n-        for expert_idx in range(len(self.experts)):\n-            expert = self.experts[expert_idx]\n-            mask = expert_mask[expert_idx]\n-            token_indices, weight_indices = torch.where(mask)\n-\n-            if token_indices.numel() > 0:\n-                expert_weights = topk_weights[token_indices, weight_indices]\n-                expert_input = hidden_states[token_indices]\n-                expert_output = expert(expert_input)\n-                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n-                final_hidden_states.index_add_(0, token_indices, weighted_output)\n-\n-        # in original deepseek, the output of the experts are gathered once we leave this module\n-        # thus the moe module is itelsf an IsolatedParallel module\n-        # and all expert are \"local\" meaning we shard but we don't gather\n-        return final_hidden_states.type(hidden_states.dtype)\n+    def route_tokens_to_experts(self, router_logits):\n+        router_logits = router_logits.sigmoid()\n+        router_logits = router_logits + self.gate.e_score_correction_bias\n+        group_scores = (\n+            router_logits.view(-1, self.n_group, self.n_routed_experts // self.n_group).topk(2, dim=-1)[0].sum(dim=-1)\n+        )\n+        group_idx = torch.topk(group_scores, k=self.topk_group, dim=-1, sorted=False)[1]\n+        group_mask = torch.zeros_like(group_scores)\n+        group_mask.scatter_(1, group_idx, 1)\n+        score_mask = (\n+            group_mask.unsqueeze(-1)\n+            .expand(-1, self.n_group, self.n_routed_experts // self.n_group)\n+            .reshape(-1, self.n_routed_experts)\n+        )\n+        scores_for_choice = router_logits.masked_fill(~score_mask.bool(), 0.0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        topk_weights = router_logits.gather(1, topk_indices)\n+        if self.norm_topk_prob:\n+            denominator = topk_weights.sum(dim=-1, keepdim=True) + 1e-20\n+            topk_weights /= denominator\n+        topk_weights = topk_weights * self.routed_scaling_factor\n+        return topk_indices, topk_weights\n \n     def forward(self, hidden_states):\n         residuals = hidden_states\n         orig_shape = hidden_states.shape\n-        topk_indices, topk_weights = self.gate(hidden_states)\n+        router_logits = self.gate(hidden_states)\n+        topk_indices, topk_weights = self.route_tokens_to_experts(router_logits)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         hidden_states = hidden_states + self.shared_experts(residuals)\n         return hidden_states\n \n \n class Glm4vMoeTextMLP(nn.Module):\n-    def __init__(self, config, hidden_size=None, intermediate_size=None):\n+    def __init__(self, config, intermediate_size=None):\n         super().__init__()\n         self.config = config\n-        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n+        self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n-\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)"
        },
        {
            "sha": "201814df3f4e6bf157fd8cc4e40ac320b9f18424",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -24,6 +24,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n+from ..deepseek_v3.modeling_deepseek_v3 import DeepseekV3NaiveMoe\n from ..glm4.modeling_glm4 import Glm4Attention\n from ..glm4_moe.configuration_glm4_moe import Glm4MoeConfig\n from ..glm4_moe.modeling_glm4_moe import (\n@@ -408,16 +409,15 @@ def __init__(self, config: Glm4vMoeTextConfig):\n         super().__init__(config)\n \n \n+class Glm4vMoeTextNaiveMoe(DeepseekV3NaiveMoe):\n+    pass\n+\n+\n class Glm4vMoeTextMoE(Glm4MoeMoE):\n     def __init__(self, config: Glm4vMoeTextConfig):\n         super().__init__(config)\n         self.config = config\n-        self.experts = nn.ModuleList(\n-            [\n-                Glm4vMoeTextMLP(config, intermediate_size=config.moe_intermediate_size)\n-                for _ in range(config.n_routed_experts)\n-            ]\n-        )\n+        self.experts = Glm4vMoeTextNaiveMoe(config)\n         self.gate = Glm4vMoeTextTopkRouter(config)\n         self.shared_experts = Glm4vMoeTextMLP(\n             config=config, intermediate_size=config.moe_intermediate_size * config.n_shared_experts"
        },
        {
            "sha": "d55aa3f31d4fd7ca06fd09a640f216fb620b16b9",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -166,8 +166,8 @@ def __init__(self, config):\n         self.experts = GptOssExperts(config)\n \n     def forward(self, hidden_states):\n-        router_scores, router_indices = self.router(hidden_states)  # (num_experts, seq_len)\n-        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n+        router_scores, router_indices = self.router(hidden_states)\n+        routed_out = self.experts(hidden_states, router_indices, router_scores)\n         return routed_out, router_scores\n \n "
        },
        {
            "sha": "5e1452438a75f79994d78663b44f1620324c86db",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -165,8 +165,8 @@ def __init__(self, config):\n         self.experts = GptOssExperts(config)\n \n     def forward(self, hidden_states):\n-        router_scores, router_indices = self.router(hidden_states)  # (num_experts, seq_len)\n-        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n+        router_scores, router_indices = self.router(hidden_states)\n+        routed_out = self.experts(hidden_states, router_indices, router_scores)\n         return routed_out, router_scores\n \n "
        },
        {
            "sha": "94893cf4b7b4a86e6104f517c5d12d72d7f42fd4",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -166,8 +166,6 @@ def __init__(\n         self.use_cache = use_cache\n         self.rope_theta = rope_theta\n         self.rope_scaling = rope_scaling\n-        # this model has rope embedding type, hardcoded for BC\n-        self.position_embedding_type = \"rope\"\n \n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout"
        },
        {
            "sha": "8dca068b59153218f378e36a0dcfc47ecc44cdf1",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 223,
            "deletions": 504,
            "changes": 727,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/granitemoe/modular_granitemoe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_granitemoe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -16,119 +22,26 @@\n from typing import Callable, Optional, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n+from torch.nn import functional as F\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_granitemoe import GraniteMoeConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n-def load_balancing_loss_func(\n-    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        gate_logits:\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n-        return 0\n-\n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n-\n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n-\n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n-\n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n-\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n-            .to(compute_device)\n-        )\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n-\n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n-    return overall_loss * num_experts\n-\n-\n-# Copied from transformers.models.granite.modeling_granite.GraniteRMSNorm with Granite->GraniteMoe\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class GraniteMoeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -149,7 +62,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.granite.modeling_granite.GraniteRotaryEmbedding with Granite->GraniteMoe\n class GraniteMoeRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n@@ -186,43 +98,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.granite.modeling_granite.rotate_half with Granite->GraniteMoe\n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-# Copied from transformers.models.granite.modeling_granite.apply_rotary_pos_emb with Granite->GraniteMoe\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n-# Copied from transformers.models.jetmoe.modeling_jetmoe.JetMoeParallelExperts with JetMoe->GraniteMoe\n class GraniteMoeParallelExperts(nn.Module):\n     def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n         \"\"\"\n@@ -268,11 +143,11 @@ def forward(self, inputs, expert_size):\n         return results\n \n \n-# Copied from transformers.models.jetmoe.modeling_jetmoe.JetMoeTopKGating with JetMoe->GraniteMoe\n class GraniteMoeTopKGating(nn.Module):\n     def __init__(self, input_size: int, num_experts: int, top_k: int):\n         \"\"\"\n         Initialize the top-k gating mechanism.\n+\n         Args:\n             input_size (`int`):\n                 Size of the input.\n@@ -342,22 +217,9 @@ def __init__(self, config: GraniteMoeConfig):\n         )\n \n     def forward(self, layer_input):\n-        \"\"\"\n-        Forward pass of the mixture of experts layer.\n-\n-        Args:\n-            layer_input (Tensor):\n-                Input tensor.\n-\n-        Returns:\n-            Tensor:\n-                Output tensor.\n-            Tensor:\n-                Router logits.\n-        \"\"\"\n         bsz, length, emb_size = layer_input.size()\n         layer_input = layer_input.reshape(-1, emb_size)\n-        _, batch_index, batch_gates, expert_size, router_logits = self.router(layer_input)\n+        _, batch_index, batch_gates, expert_size, _ = self.router(layer_input)\n \n         expert_inputs = layer_input[batch_index]\n         hidden_states = self.input_linear(expert_inputs, expert_size)\n@@ -370,10 +232,43 @@ def forward(self, layer_input):\n         zeros = torch.zeros((bsz * length, self.input_size), dtype=expert_outputs.dtype, device=expert_outputs.device)\n         layer_output = zeros.index_add(0, batch_index, expert_outputs)\n         layer_output = layer_output.view(bsz, length, self.input_size)\n-        return layer_output, router_logits\n+        return layer_output\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n \n \n-# Copied from transformers.models.granite.modeling_granite.repeat_kv with Granite->GraniteMoe\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -386,68 +281,77 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-# copied from transformers.models.granite.modeling_granite.GraniteAttention with Granite->GraniteMoe\n-# no longer copied after attention refactors\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class GraniteMoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: GraniteMoeConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.attention_multiplier  # Only diff with llama\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n \n-        self.scaling = config.attention_multiplier\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n-        **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings if position_embeddings is not None else (None, None)\n-        if position_embeddings is not None:\n-            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n@@ -469,126 +373,48 @@ def forward(\n             **kwargs,\n         )\n \n-        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n         return attn_output, attn_weights\n \n \n-def eager_attention_forward(\n-    module: nn.Module,\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n-    scaling: float,\n-    dropout: float = 0.0,\n-    **kwargs,\n-):\n-    key_states = repeat_kv(key, module.num_key_value_groups)\n-    value_states = repeat_kv(value, module.num_key_value_groups)\n-\n-    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n-    if attention_mask is not None:\n-        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-        attn_weights = attn_weights + causal_mask\n-\n-    # upcast attention to fp32\n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n-    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n-    attn_output = torch.matmul(attn_weights, value_states)\n-    attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-    return attn_output, attn_weights\n-\n-\n class GraniteMoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n         self.self_attn = GraniteMoeAttention(config=config, layer_idx=layer_idx)\n-        if config.num_local_experts > 0:\n-            self.block_sparse_moe = GraniteMoeMoE(config)\n+        self.block_sparse_moe = GraniteMoeMoE(config)\n         self.input_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-        self.residual_multiplier = config.residual_multiplier\n+        self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n         **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n             **kwargs,\n         )\n-\n-        hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        # Fully Connected\n+        hidden_states = residual + hidden_states * self.residual_multiplier  # diff\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n-\n-        hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        hidden_states = self.block_sparse_moe(hidden_states)\n+        hidden_states = residual + hidden_states * self.residual_multiplier  # diff\n+        return hidden_states\n \n \n @auto_docstring\n@@ -600,8 +426,14 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n \n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": GraniteMoeDecoderLayer,\n+        \"attentions\": GraniteMoeAttention,\n+    }\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n@@ -621,61 +453,35 @@ def __init__(self, config: GraniteMoeConfig):\n             [GraniteMoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = GraniteMoeRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-\n         self.embedding_multiplier = config.embedding_multiplier\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-\n-        self.position_embedding_type = config.position_embedding_type\n-        self.rotary_emb = GraniteMoeRotaryEmbedding(config) if self.position_embedding_type == \"rope\" else None\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        inputs_embeds = inputs_embeds * self.embedding_multiplier\n-\n-        if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache(config=self.config)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -684,208 +490,143 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(  # ONLY DIFF WITH MIXTRAL: NO SLIDING\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n-\n-        # embed positions\n+        inputs_embeds = inputs_embeds * self.embedding_multiplier\n         hidden_states = inputs_embeds\n \n-        position_embeddings = None\n         # create position embeddings to be shared across the decoder layers\n-        if self.rotary_emb is not None:\n-            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                output_router_logits=output_router_logits,\n-                position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n+        hidden_states = self.norm(hidden_states)\n \n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n \n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n \n-        hidden_states = self.norm(hidden_states)\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n \n-        if not return_dict:\n-            return tuple(\n-                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n-            )\n-        return MoeModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n+@auto_docstring\n class GraniteMoeForCausalLM(GraniteMoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config: GraniteMoeConfig):\n         super().__init__(config)\n         self.model = GraniteMoeModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_local_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+        self.logits_scaling = config.logits_scaling\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n     @auto_docstring\n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -894,11 +635,7 @@ def forward(\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n@@ -925,33 +662,22 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n         # Only compute necessary logits\n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits / self.config.logits_scaling\n@@ -971,20 +697,13 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "a105d77f36c7101041cb3b6f99b95de5d588ae2d",
            "filename": "src/transformers/models/granitemoe/modular_granitemoe.py",
            "status": "added",
            "additions": 326,
            "deletions": 0,
            "changes": 326,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodular_granitemoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,326 @@\n+# coding=utf-8\n+# Copyright 2024 IBM and the HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import can_return_tuple, check_model_inputs\n+from ..granite.modeling_granite import GraniteRMSNorm, GraniteRotaryEmbedding\n+from ..jetmoe.modeling_jetmoe import JetMoeParallelExperts, JetMoeTopKGating\n+from ..llama.modeling_llama import LlamaAttention, LlamaPreTrainedModel\n+from ..mixtral.modeling_mixtral import MixtralDecoderLayer, MixtralForCausalLM, MixtralModel, load_balancing_loss_func\n+from .configuration_granitemoe import GraniteMoeConfig\n+\n+\n+class GraniteMoeRMSNorm(GraniteRMSNorm):\n+    pass\n+\n+\n+class GraniteMoeRotaryEmbedding(GraniteRotaryEmbedding):\n+    pass\n+\n+\n+class GraniteMoeParallelExperts(JetMoeParallelExperts):\n+    pass\n+\n+\n+class GraniteMoeTopKGating(JetMoeTopKGating):\n+    pass\n+\n+\n+class GraniteMoeMoE(nn.Module):\n+    \"\"\"\n+    A Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n+\n+    Args:\n+        config:\n+            Configuration object with model hyperparameters.\n+    \"\"\"\n+\n+    def __init__(self, config: GraniteMoeConfig):\n+        super().__init__()\n+\n+        self.input_size = config.hidden_size\n+        self.hidden_size = config.intermediate_size\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.input_linear = GraniteMoeParallelExperts(config.num_local_experts, self.input_size, self.hidden_size * 2)\n+        self.output_linear = GraniteMoeParallelExperts(config.num_local_experts, self.hidden_size, self.input_size)\n+\n+        self.router = GraniteMoeTopKGating(\n+            input_size=self.input_size,\n+            num_experts=config.num_local_experts,\n+            top_k=config.num_experts_per_tok,\n+        )\n+\n+    def forward(self, layer_input):\n+        bsz, length, emb_size = layer_input.size()\n+        layer_input = layer_input.reshape(-1, emb_size)\n+        _, batch_index, batch_gates, expert_size, _ = self.router(layer_input)\n+\n+        expert_inputs = layer_input[batch_index]\n+        hidden_states = self.input_linear(expert_inputs, expert_size)\n+        chunked_hidden_states = hidden_states.chunk(2, dim=-1)\n+        hidden_states = self.activation(chunked_hidden_states[0]) * chunked_hidden_states[1]\n+        expert_outputs = self.output_linear(hidden_states, expert_size)\n+\n+        expert_outputs = expert_outputs * batch_gates[:, None]\n+\n+        zeros = torch.zeros((bsz * length, self.input_size), dtype=expert_outputs.dtype, device=expert_outputs.device)\n+        layer_output = zeros.index_add(0, batch_index, expert_outputs)\n+        layer_output = layer_output.view(bsz, length, self.input_size)\n+        return layer_output\n+\n+\n+class GraniteMoeAttention(LlamaAttention):\n+    def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n+        super().__init__(self, config, layer_idx)\n+        self.scaling = config.attention_multiplier  # Only diff with llama\n+\n+\n+class GraniteMoeDecoderLayer(MixtralDecoderLayer):\n+    def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.self_attn = GraniteMoeAttention(config=config, layer_idx=layer_idx)\n+        self.block_sparse_moe = GraniteMoeMoE(config)\n+        self.input_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states * self.residual_multiplier  # diff\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n+        hidden_states = residual + hidden_states * self.residual_multiplier  # diff\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class GraniteMoePreTrainedModel(LlamaPreTrainedModel, PreTrainedModel):\n+    config: GraniteMoeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"GraniteMoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+\n+    def _init_weights(self, module):\n+        PreTrainedModel._init_weights(self, module)\n+        if isinstance(module, GraniteMoeParallelExperts):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+\n+\n+@auto_docstring\n+class GraniteMoeModel(MixtralModel):\n+    def __init__(self, config: GraniteMoeConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [GraniteMoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.embedding_multiplier = config.embedding_multiplier\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(  # ONLY DIFF WITH MIXTRAL: NO SLIDING\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+        inputs_embeds = inputs_embeds * self.embedding_multiplier\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class GraniteMoeForCausalLM(MixtralForCausalLM):\n+    def __init__(self, config: GraniteMoeConfig):\n+        super().__init__(config)\n+        self.model = GraniteMoeModel(config)\n+        self.logits_scaling = config.logits_scaling\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_router_logits: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs,\n+    ) -> Union[tuple, MoeCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, GraniteMoeForCausalLM\n+\n+        >>> model = GraniteMoeForCausalLM.from_pretrained(\"ibm/PowerMoE-3b\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"ibm/PowerMoE-3b\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        # Only compute necessary logits\n+        hidden_states = outputs.last_hidden_state\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+        logits = logits / self.config.logits_scaling\n+\n+        loss = None\n+        if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n+            # Flatten the tokens\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+__all__ = [\"GraniteMoeForCausalLM\", \"GraniteMoeModel\", \"GraniteMoePreTrainedModel\"]"
        },
        {
            "sha": "7b8cc9e656889357f98452be48075bc116778e5c",
            "filename": "src/transformers/models/granitemoehybrid/configuration_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fconfiguration_granitemoehybrid.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -16,7 +16,6 @@\n \"\"\"GraniteMoeHybrid model configuration\"\"\"\n \n from ...configuration_utils import PretrainedConfig\n-from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n \n@@ -96,8 +95,6 @@ class GraniteMoeHybridConfig(PretrainedConfig):\n             allow the model to output the auxiliary loss.\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxiliary loss coefficient\n         shared_intermediate_size (`int`, *optional*, defaults to 1024): intermediate size for shared experts.\n-        position_embedding_type (`str`, *optional*): Positional embedding\n-            type to be used; defaults to None. Allowed options: `[None, \"rope\"]`\n         layer_types (`List`, *optional*): list of strings to be used as layer types.\n             Allowed choices: \"mamba\", \"attention\".\n         mamba_n_heads (`int`, *optional*, defaults to 128):\n@@ -166,7 +163,6 @@ def __init__(\n         output_router_logits=False,\n         router_aux_loss_coef=0.001,\n         shared_intermediate_size=1024,\n-        position_embedding_type=None,\n         layer_types=None,\n         mamba_n_heads=128,\n         mamba_n_groups=1,\n@@ -208,7 +204,6 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.shared_intermediate_size = shared_intermediate_size\n-        self.position_embedding_type = position_embedding_type\n \n         mamba_intermediate = mamba_expand * hidden_size\n \n@@ -244,9 +239,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        if self.position_embedding_type == \"rope\":\n-            rope_config_validation(self)\n-\n     # overwrite the function to use in `HybridMambaAttentionDynamicCache`\n     @property\n     def layers_block_type(self):"
        },
        {
            "sha": "60946f5fa8c95e7585745c6925913d7a38b170ec",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 84,
            "deletions": 401,
            "changes": 485,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -22,21 +22,23 @@\n from typing import Any, Callable, Optional, TypedDict, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n+from torch.nn import functional as F\n \n from transformers.activations import ACT2FN\n \n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n \n@@ -53,49 +55,9 @@\n     causal_conv1d_update, causal_conv1d_fn = None, None\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -116,7 +78,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n     value_states = repeat_kv(value, module.num_key_value_groups)\n@@ -126,7 +88,6 @@ def eager_attention_forward(\n         causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n         attn_weights = attn_weights + causal_mask\n \n-    # upcast attention to fp32\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value_states)\n@@ -135,72 +96,50 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n-# copied from transformers.models.granite.modeling_granite.GraniteAttention with Granite->GraniteMoeHybrid\n-# no longer copied after attention refactors\n class GraniteMoeHybridAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.attention_multiplier  # Only diff with llama\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n \n-        self.scaling = config.attention_multiplier\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n-        **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        cos, sin = position_embeddings if position_embeddings is not None else (None, None)\n-        if position_embeddings is not None:\n-            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            cache_kwargs = {\"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n@@ -218,9 +157,8 @@ def forward(\n             **kwargs,\n         )\n \n-        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n         return attn_output, attn_weights\n \n \n@@ -297,16 +235,24 @@ def update(\n \n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n-        for layer_idx in range(len(self.key_cache)):\n-            device = self.key_cache[layer_idx].device\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.value_cache[layer_idx].device\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n-\n-            device = self.conv_states[layer_idx].device\n-            self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.ssm_states[layer_idx].device\n-            self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+        if self.get_seq_length() > 0:\n+            for layer_idx in range(len(self.key_cache)):\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+                device = self.conv_states[layer_idx].device\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.ssm_states[layer_idx].device\n+                self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        kv_length = self.get_seq_length(layer_idx) + query_length\n+        return kv_length, kv_offset\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n@@ -934,6 +880,7 @@ class GraniteFlashAttentionKwargs(TypedDict, total=False):\n     seq_idx: torch.IntTensor\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class GraniteMoeHybridRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -1003,6 +950,7 @@ class GraniteMoeHybridTopKGating(nn.Module):\n     def __init__(self, input_size: int, num_experts: int, top_k: int):\n         \"\"\"\n         Initialize the top-k gating mechanism.\n+\n         Args:\n             input_size (`int`):\n                 Size of the input.\n@@ -1076,22 +1024,9 @@ def __init__(self, config: GraniteMoeHybridConfig):\n         )\n \n     def forward(self, layer_input):\n-        \"\"\"\n-        Forward pass of the mixture of experts layer.\n-\n-        Args:\n-            layer_input (Tensor):\n-                Input tensor.\n-\n-        Returns:\n-            Tensor:\n-                Output tensor.\n-            Tensor:\n-                Router logits.\n-        \"\"\"\n         bsz, length, emb_size = layer_input.size()\n         layer_input = layer_input.reshape(-1, emb_size)\n-        _, batch_index, batch_gates, expert_size, router_logits = self.router(layer_input)\n+        _, batch_index, batch_gates, expert_size, _ = self.router(layer_input)\n \n         expert_inputs = layer_input[batch_index]\n         hidden_states = self.input_linear(expert_inputs, expert_size)\n@@ -1104,7 +1039,7 @@ def forward(self, layer_input):\n         zeros = torch.zeros((bsz * length, self.input_size), dtype=expert_outputs.dtype, device=expert_outputs.device)\n         layer_output = zeros.index_add(0, batch_index, expert_outputs)\n         layer_output = layer_output.view(bsz, length, self.input_size)\n-        return layer_output, router_logits\n+        return layer_output\n \n \n class GraniteMoeHybridDecoderLayer(GradientCheckpointingLayer):\n@@ -1113,12 +1048,11 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.hidden_size = config.hidden_size\n         # Either attention or mamba will be initialized, depending on the layer type.\n         self.self_attn = None\n-        if config.num_local_experts > 0:\n-            self.block_sparse_moe = GraniteMoeHybridMoE(config)\n+        self.block_sparse_moe = GraniteMoeHybridMoE(config)\n         self.input_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-        self.residual_multiplier = config.residual_multiplier\n+        self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n         self.shared_mlp = GraniteMoeHybridMLP(config)\n         self.mamba = None\n \n@@ -1131,44 +1065,16 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         # Accept 0 experts: skip MoE if num_local_experts == 0\n         self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        output_router_logits: Optional[bool] = False,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs.Can be used to provide `GraniteFlashAttentionKwargs` for\n-                padding-free training and/or improve torch.compile performance.\n-        \"\"\"\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n \n@@ -1180,44 +1086,28 @@ def forward(\n                 attention_mask=attention_mask,\n                 **kwargs,\n             )\n-            # No attention weights for state space layers\n-            self_attn_weights = None\n         else:\n-            hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states, _ = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n         hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n \n         if self.has_experts:\n-            moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+            moe_hidden_states = self.block_sparse_moe(hidden_states)\n             hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n         else:\n             hidden_states = self.shared_mlp(hidden_states)\n-            router_logits = None\n \n         hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -1229,8 +1119,14 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n \n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": GraniteMoeHybridDecoderLayer,\n+        \"attentions\": GraniteMoeHybridAttention,\n+    }\n     _is_stateful = True\n \n     def _init_weights(self, module):\n@@ -1293,263 +1189,72 @@ def __init__(self, config: GraniteMoeHybridConfig):\n             [GraniteMoeHybridDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = GraniteMoeHybridRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-\n         self.embedding_multiplier = config.embedding_multiplier\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-\n-        self.position_embedding_type = config.position_embedding_type\n-        self.rotary_emb = GraniteMoeHybridRotaryEmbedding(config) if self.position_embedding_type == \"rope\" else None\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n     @auto_docstring\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n-        ## overwritten because `HybridMambaAttentionDynamicCache` is needed\n-        if use_cache and past_key_values is None:\n-            logger.warning_once(\n-                \"GraniteMoeHybrid requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. \"\n-                \"Because one was not provided, no cache will be returned.\"\n-            )\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n-        if position_ids is None:\n-            position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            self.config,\n+            inputs_embeds,\n+            attention_mask,\n+            cache_position,\n+            past_key_values,\n         )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        position_embeddings = None\n-        # create position embeddings to be shared across the decoder layers\n-        if self.rotary_emb is not None:\n-            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n             layer_mask = mamba_mask if decoder_layer.layer_type == \"mamba\" else causal_mask\n \n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=layer_mask,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                output_router_logits=output_router_logits,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                if layer_outputs[1] is not None:\n-                    # append attentions only of attention layers. Mamba layers return `None` as the attention weights\n-                    all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                if layer_outputs[-1] is not None:\n-                    # append router logits only of expert layers. Regular MLP layers return `None` as the router logits\n-                    all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         if past_key_values and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n     def _update_mamba_mask(self, attention_mask, cache_position):\n         \"\"\"\n         No need for zeroing states when\n@@ -1630,8 +1335,8 @@ def load_balancing_loss_func(\n         # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n         router_per_expert_attention_mask = (\n             attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n             .to(compute_device)\n         )\n \n@@ -1640,31 +1345,31 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n \n \n+@auto_docstring\n class GraniteMoeHybridForCausalLM(GraniteMoeHybridPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config: GraniteMoeHybridConfig):\n         super().__init__(config)\n         self.model = GraniteMoeHybridModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_local_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+        self.logits_scaling = config.logits_scaling\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n     @auto_docstring\n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1673,11 +1378,7 @@ def forward(\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n@@ -1704,33 +1405,22 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n         # Only compute necessary logits\n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits / self.config.logits_scaling\n@@ -1750,20 +1440,13 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "73e796cdc7054825faad7e239f1ed55e940fc411",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 57,
            "deletions": 119,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -13,16 +13,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...cache_utils import Cache\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n from ..bamba.configuration_bamba import BambaConfig\n from ..bamba.modeling_bamba import BambaMixer, BambaRMSNormGated, HybridMambaAttentionDynamicCache\n from ..granitemoeshared.modeling_granitemoeshared import (\n@@ -33,6 +35,7 @@\n     GraniteMoeSharedMLP,\n     GraniteMoeSharedModel,\n     GraniteMoeSharedPreTrainedModel,\n+    eager_attention_forward,\n )\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n \n@@ -44,6 +47,44 @@ class GraniteMoeHybridAttention(GraniteMoeSharedAttention):\n     def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n \n+    def forward(  # FIME: @ARTHUR this forward is also classic: attention nope\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if past_key_values is not None:\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n \n class GraniteMoeHybridMambaLayer(BambaMixer):\n     def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n@@ -77,44 +118,16 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         # Accept 0 experts: skip MoE if num_local_experts == 0\n         self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        output_router_logits: Optional[bool] = False,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs.Can be used to provide `GraniteFlashAttentionKwargs` for\n-                padding-free training and/or improve torch.compile performance.\n-        \"\"\"\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n \n@@ -126,44 +139,28 @@ def forward(\n                 attention_mask=attention_mask,\n                 **kwargs,\n             )\n-            # No attention weights for state space layers\n-            self_attn_weights = None\n         else:\n-            hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states, _ = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n \n         hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n \n         if self.has_experts:\n-            moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+            moe_hidden_states = self.block_sparse_moe(hidden_states)\n             hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n         else:\n             hidden_states = self.shared_mlp(hidden_states)\n-            router_logits = None\n \n         hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class GraniteMoeHybridPreTrainedModel(GraniteMoeSharedPreTrainedModel):\n@@ -187,124 +184,65 @@ def __init__(self, config: GraniteMoeHybridConfig):\n         self.layers = nn.ModuleList(\n             [GraniteMoeHybridDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n+        self.embedding_multiplier = config.embedding_multiplier\n \n-    @can_return_tuple\n     @auto_docstring\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n-        ## overwritten because `HybridMambaAttentionDynamicCache` is needed\n-        if use_cache and past_key_values is None:\n-            logger.warning_once(\n-                \"GraniteMoeHybrid requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. \"\n-                \"Because one was not provided, no cache will be returned.\"\n-            )\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n-        if position_ids is None:\n-            position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            self.config,\n+            inputs_embeds,\n+            attention_mask,\n+            cache_position,\n+            past_key_values,\n         )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n \n         # embed positions\n         hidden_states = inputs_embeds\n-\n-        position_embeddings = None\n-        # create position embeddings to be shared across the decoder layers\n-        if self.rotary_emb is not None:\n-            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n             layer_mask = mamba_mask if decoder_layer.layer_type == \"mamba\" else causal_mask\n \n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=layer_mask,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                output_router_logits=output_router_logits,\n-                position_embeddings=position_embeddings,\n                 **kwargs,\n             )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                if layer_outputs[1] is not None:\n-                    # append attentions only of attention layers. Mamba layers return `None` as the attention weights\n-                    all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                if layer_outputs[-1] is not None:\n-                    # append router logits only of expert layers. Regular MLP layers return `None` as the router logits\n-                    all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         if past_key_values and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n     def _update_mamba_mask(self, attention_mask, cache_position):"
        },
        {
            "sha": "e844e23305ca9c9e259fd87070974f79febac5c2",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 81,
            "deletions": 351,
            "changes": 432,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -22,32 +22,25 @@\n from typing import Callable, Optional, TypedDict, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n+from torch.nn import functional as F\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_granitemoeshared import GraniteMoeSharedConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n class GraniteFlashAttentionKwargs(TypedDict, total=False):\n     \"\"\"\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n@@ -99,6 +92,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class GraniteMoeSharedRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -168,6 +162,7 @@ class GraniteMoeSharedTopKGating(nn.Module):\n     def __init__(self, input_size: int, num_experts: int, top_k: int):\n         \"\"\"\n         Initialize the top-k gating mechanism.\n+\n         Args:\n             input_size (`int`):\n                 Size of the input.\n@@ -241,22 +236,9 @@ def __init__(self, config: GraniteMoeSharedConfig):\n         )\n \n     def forward(self, layer_input):\n-        \"\"\"\n-        Forward pass of the mixture of experts layer.\n-\n-        Args:\n-            layer_input (Tensor):\n-                Input tensor.\n-\n-        Returns:\n-            Tensor:\n-                Output tensor.\n-            Tensor:\n-                Router logits.\n-        \"\"\"\n         bsz, length, emb_size = layer_input.size()\n         layer_input = layer_input.reshape(-1, emb_size)\n-        _, batch_index, batch_gates, expert_size, router_logits = self.router(layer_input)\n+        _, batch_index, batch_gates, expert_size, _ = self.router(layer_input)\n \n         expert_inputs = layer_input[batch_index]\n         hidden_states = self.input_linear(expert_inputs, expert_size)\n@@ -269,7 +251,7 @@ def forward(self, layer_input):\n         zeros = torch.zeros((bsz * length, self.input_size), dtype=expert_outputs.dtype, device=expert_outputs.device)\n         layer_output = zeros.index_add(0, batch_index, expert_outputs)\n         layer_output = layer_output.view(bsz, length, self.input_size)\n-        return layer_output, router_logits\n+        return layer_output\n \n \n def rotate_half(x):\n@@ -326,7 +308,7 @@ def eager_attention_forward(\n     attention_mask: Optional[torch.Tensor],\n     scaling: float,\n     dropout: float = 0.0,\n-    **kwargs,\n+    **kwargs: Unpack[TransformersKwargs],\n ):\n     key_states = repeat_kv(key, module.num_key_value_groups)\n     value_states = repeat_kv(value, module.num_key_value_groups)\n@@ -336,7 +318,6 @@ def eager_attention_forward(\n         causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n         attn_weights = attn_weights + causal_mask\n \n-    # upcast attention to fp32\n     attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n     attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n     attn_output = torch.matmul(attn_weights, value_states)\n@@ -345,68 +326,51 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n-# copied from transformers.models.granite.modeling_granite.GraniteAttention with Granite->GraniteMoeShared\n-# no longer copied after attention refactors\n class GraniteMoeSharedAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: GraniteMoeSharedConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = config.attention_multiplier  # Only diff with llama\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n \n-        self.scaling = config.attention_multiplier\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # None or rope embeddings\n-        **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = position_embeddings if position_embeddings is not None else (None, None)\n-        if position_embeddings is not None:\n-            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n@@ -428,24 +392,21 @@ def forward(\n             **kwargs,\n         )\n \n-        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n         return attn_output, attn_weights\n \n \n class GraniteMoeSharedDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n         self.self_attn = GraniteMoeSharedAttention(config=config, layer_idx=layer_idx)\n-        if config.num_local_experts > 0:\n-            self.block_sparse_moe = GraniteMoeSharedMoE(config)\n+        self.block_sparse_moe = GraniteMoeSharedMoE(config)\n         self.input_layernorm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-        self.residual_multiplier = config.residual_multiplier\n+        self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n         self.shared_mlp = None if config.shared_intermediate_size == 0 else GraniteMoeSharedMLP(config)\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n@@ -458,41 +419,14 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs. Can be used to provide `GraniteFlashAttentionKwargs` for\n-                padding-free training and/or improve torch.compile performance.\n-        \"\"\"\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -506,29 +440,16 @@ def forward(\n \n         hidden_states = residual + hidden_states * self.residual_multiplier\n \n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        moe_hidden_states = self.block_sparse_moe(hidden_states)\n \n         if self.shared_mlp is None:\n             hidden_states = moe_hidden_states\n         else:\n             hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n-\n-        del moe_hidden_states\n-\n         hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -540,8 +461,14 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n \n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": GraniteMoeSharedDecoderLayer,\n+        \"attentions\": GraniteMoeSharedAttention,\n+    }\n \n     def _init_weights(self, module):\n         super()._init_weights(module)\n@@ -597,61 +524,35 @@ def __init__(self, config: GraniteMoeSharedConfig):\n             [GraniteMoeSharedDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n         self.norm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = GraniteMoeSharedRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n-\n         self.embedding_multiplier = config.embedding_multiplier\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-\n-        self.position_embedding_type = config.position_embedding_type\n-        self.rotary_emb = GraniteMoeSharedRotaryEmbedding(config) if self.position_embedding_type == \"rope\" else None\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> Union[tuple, BaseModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        inputs_embeds = inputs_embeds * self.embedding_multiplier\n-\n-        if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache(config=self.config)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -660,188 +561,39 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(  # ONLY DIFF WITH MIXTRAL: NO SLIDING\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n-\n-        # embed positions\n+        inputs_embeds = inputs_embeds * self.embedding_multiplier\n         hidden_states = inputs_embeds\n \n-        position_embeddings = None\n         # create position embeddings to be shared across the decoder layers\n-        if self.rotary_emb is not None:\n-            position_embeddings = self.rotary_emb(hidden_states, position_ids)\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                output_router_logits=output_router_logits,\n-                position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n-            )\n-        return MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n-        )\n-\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n def load_balancing_loss_func(\n     gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n@@ -911,8 +663,8 @@ def load_balancing_loss_func(\n         # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n         router_per_expert_attention_mask = (\n             attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n             .to(compute_device)\n         )\n \n@@ -921,31 +673,31 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n \n \n+@auto_docstring\n class GraniteMoeSharedForCausalLM(GraniteMoeSharedPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config: GraniteMoeSharedConfig):\n         super().__init__(config)\n         self.model = GraniteMoeSharedModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_local_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+        self.logits_scaling = config.logits_scaling\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n     @auto_docstring\n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -954,11 +706,7 @@ def forward(\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n@@ -985,33 +733,22 @@ def forward(\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             **kwargs,\n         )\n \n         # Only compute necessary logits\n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n         logits = logits / self.config.logits_scaling\n@@ -1031,20 +768,13 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "5c3241e71b5dcbd995771f3c84c138d53f67bedf",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 3,
            "deletions": 45,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -22,7 +22,6 @@\n from ...cache_utils import Cache\n from ...processing_utils import Unpack\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..granitemoe.modeling_granitemoe import (\n     GraniteMoeDecoderLayer,\n     GraniteMoeForCausalLM,\n@@ -91,7 +90,6 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.shared_mlp = None if config.shared_intermediate_size == 0 else GraniteMoeSharedMLP(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -101,41 +99,14 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        output_router_logits: Optional[bool] = False,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[GraniteFlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs. Can be used to provide `GraniteFlashAttentionKwargs` for\n-                padding-free training and/or improve torch.compile performance.\n-        \"\"\"\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -149,29 +120,16 @@ def forward(\n \n         hidden_states = residual + hidden_states * self.residual_multiplier\n \n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        moe_hidden_states = self.block_sparse_moe(hidden_states)\n \n         if self.shared_mlp is None:\n             hidden_states = moe_hidden_states\n         else:\n             hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n-\n-        del moe_hidden_states\n-\n         hidden_states = residual + hidden_states * self.residual_multiplier\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class GraniteMoeSharedPreTrainedModel(GraniteMoePreTrainedModel):"
        },
        {
            "sha": "db853145f7ee7fb45606d6676a92c353d5196565",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -107,6 +107,10 @@ class HunYuanMoEV1Config(PretrainedConfig):\n \n     model_type = \"hunyuan_v1_moe\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\n+        \"num_experts_per_tok\": \"moe_topk\",\n+        \"num_local_experts\": \"num_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2a799feece62b796b0b8e87ea900ac10d3036b67",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 46,
            "deletions": 38,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -65,7 +65,7 @@ def extra_repr(self):\n \n \n class HunYuanMoEV1MLP(nn.Module):\n-    def __init__(self, config: HunYuanMoEV1Config, layer_idx=None, is_shared_mlp=False):\n+    def __init__(self, config: HunYuanMoEV1Config):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.hidden_size\n@@ -74,7 +74,6 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx=None, is_shared_mlp=Fal\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n-        self.layer_idx = layer_idx\n \n     def forward(self, x):\n         down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n@@ -245,6 +244,41 @@ def forward(self, hidden_states):\n         return logits\n \n \n+class HunYuanMoEV1Experts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config: HunYuanMoEV1Config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(HunYuanMoEV1MLP(config))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n class HunYuanMoEV1Moe(nn.Module):\n     def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n         super().__init__()\n@@ -253,50 +287,24 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n         self.num_experts = config.num_experts if isinstance(config.num_experts, int) else config.num_experts[layer_idx]\n         self.top_k = config.moe_topk if isinstance(config.moe_topk, int) else config.moe_topk[layer_idx]\n         self.gate = HunYuanMoEV1Gate(config, layer_idx=layer_idx)\n-        # self.wg = nn.Linear(config.hidden_size, config.num_experts, bias=False, dtype=torch.float32)\n-        self.experts = nn.ModuleList(\n-            [HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=False) for _ in range(self.num_experts)]\n-        )\n+        self.experts = HunYuanMoEV1Experts(config)\n+        self.shared_mlp = HunYuanMoEV1MLP(config)\n \n-        self.shared_mlp = HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=True)\n+    def route_tokens_to_experts(self, hidden_states):\n+        routing_weights = F.softmax(hidden_states, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        return selected_experts, routing_weights.to(hidden_states.dtype)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_mlp = self.shared_mlp(hidden_states)\n         router_logits = self.gate(hidden_states)\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights).reshape(\n+            batch_size, sequence_length, hidden_dim\n         )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n         return final_hidden_states + hidden_states_mlp\n \n "
        },
        {
            "sha": "5300cf25322b8c56742039af8ab440d85e64ec7e",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 16,
            "deletions": 38,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -41,6 +41,7 @@\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n+from ..mixtral.modeling_mixtral import MixtralExperts\n from .configuration_hunyuan_v1_moe import HunYuanMoEV1Config\n \n \n@@ -52,9 +53,8 @@ class HunYuanMoEV1RMSNorm(LlamaRMSNorm):\n \n \n class HunYuanMoEV1MLP(LlamaMLP):\n-    def __init__(self, config: HunYuanMoEV1Config, layer_idx=None, is_shared_mlp=False):\n+    def __init__(self, config: HunYuanMoEV1Config):\n         super().__init__(config)\n-        self.layer_idx = layer_idx\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n@@ -129,6 +129,10 @@ def forward(self, hidden_states):\n         return logits\n \n \n+class HunYuanMoEV1Experts(MixtralExperts):\n+    pass\n+\n+\n class HunYuanMoEV1Moe(nn.Module):\n     def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n         super().__init__()\n@@ -137,50 +141,24 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n         self.num_experts = config.num_experts if isinstance(config.num_experts, int) else config.num_experts[layer_idx]\n         self.top_k = config.moe_topk if isinstance(config.moe_topk, int) else config.moe_topk[layer_idx]\n         self.gate = HunYuanMoEV1Gate(config, layer_idx=layer_idx)\n-        # self.wg = nn.Linear(config.hidden_size, config.num_experts, bias=False, dtype=torch.float32)\n-        self.experts = nn.ModuleList(\n-            [HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=False) for _ in range(self.num_experts)]\n-        )\n+        self.experts = HunYuanMoEV1Experts(config)\n+        self.shared_mlp = HunYuanMoEV1MLP(config)\n \n-        self.shared_mlp = HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=True)\n+    def route_tokens_to_experts(self, hidden_states):\n+        routing_weights = F.softmax(hidden_states, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        return selected_experts, routing_weights.to(hidden_states.dtype)\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_mlp = self.shared_mlp(hidden_states)\n         router_logits = self.gate(hidden_states)\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        selected_experts, routing_weights = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights).reshape(\n+            batch_size, sequence_length, hidden_dim\n         )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n         return final_hidden_states + hidden_states_mlp\n \n "
        },
        {
            "sha": "d239476adeb9b33cbb602131ef2ef548692583ee",
            "filename": "src/transformers/models/jamba/configuration_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fconfiguration_jamba.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -66,12 +66,6 @@ class JambaConfig(PretrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n-        num_logits_to_keep (`int` or `None`, *optional*, defaults to 1):\n-            Number of prompt logits to calculate during generation. If `None`, all logits will be calculated. If an\n-            integer value, only last `num_logits_to_keep` logits will be calculated. Default is 1 because only the\n-            logits of the last prompt token are needed for generation. For long sequences, the logits for the entire\n-            sequence may use a lot of memory so, setting `num_logits_to_keep=1` will reduce memory footprint\n-            significantly.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n             Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss. See [here]() for more details\n@@ -83,8 +77,6 @@ class JambaConfig(PretrainedConfig):\n             The id of the \"beginning-of-sequence\" token.\n         eos_token_id (`int`, *optional*, defaults to 2):\n             The id of the \"end-of-sequence\" token.\n-        sliding_window (`int`, *optional*):\n-            Sliding window attention window size. If not specified, will default to `None`.\n         max_position_embeddings (`int`, *optional*, defaults to 262144):\n             This value doesn't have any real effect. The maximum sequence length that this model is intended to be\n             used with. It can be used with longer sequences, but performance may degrade.\n@@ -124,6 +116,9 @@ class JambaConfig(PretrainedConfig):\n \n     model_type = \"jamba\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\n+        \"num_local_experts\": \"num_experts\",\n+    }\n \n     def __init__(\n         self,\n@@ -138,13 +133,11 @@ def __init__(\n         initializer_range=0.02,\n         rms_norm_eps=1e-6,\n         use_cache=True,\n-        num_logits_to_keep=1,\n         output_router_logits=False,\n         router_aux_loss_coef=0.001,\n         pad_token_id=0,\n         bos_token_id=1,\n         eos_token_id=2,\n-        sliding_window=None,\n         max_position_embeddings=262144,\n         attention_dropout=0.0,\n         num_experts_per_tok=2,\n@@ -168,7 +161,6 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n-        self.sliding_window = sliding_window\n         self.max_position_embeddings = max_position_embeddings\n         self.attention_dropout = attention_dropout\n \n@@ -182,7 +174,6 @@ def __init__(\n         self.rms_norm_eps = rms_norm_eps\n \n         self.use_cache = use_cache\n-        self.num_logits_to_keep = num_logits_to_keep\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n "
        },
        {
            "sha": "347beac81614f1faba0598d2401715dead2c374a",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 310,
            "deletions": 699,
            "changes": 1009,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,52 +1,33 @@\n-# coding=utf-8\n-# Copyright 2024 AI21 Labs Ltd. and the HuggingFace Inc. team. All rights reserved.\n-#\n-# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n-# and OPT implementations in this library. It has been modified from its\n-# original forms to accommodate minor architectural differences compared\n-# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"PyTorch Jamba model.\"\"\"\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/jamba/modular_jamba.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_jamba.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# This is an automatically generated file by the modularization script. Please do not make any manual changes\n+# to this file as they will be overwritten.\n \n import math\n-from typing import Any, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import (\n-    GenericForSequenceClassification,\n-    GradientCheckpointingLayer,\n-)\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_jamba import JambaConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n if is_mamba_ssm_available():\n     from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n     from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n@@ -58,104 +39,11 @@\n else:\n     causal_conv1d_update, causal_conv1d_fn = None, None\n \n-is_fast_path_available = all(\n-    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n-)\n-\n \n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func with gate->router\n-def load_balancing_loss_func(\n-    router_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        router_logits:\n-            Logits from the `router`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if router_logits is None or not isinstance(router_logits, tuple):\n-        return 0\n-\n-    if isinstance(router_logits, tuple):\n-        compute_device = router_logits[0].device\n-        concatenated_router_logits = torch.cat(\n-            [layer_router.to(compute_device) for layer_router in router_logits], dim=0\n-        )\n-\n-    routing_weights = torch.nn.functional.softmax(concatenated_router_logits, dim=-1)\n-\n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n-\n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_router_logits.shape[0] // (batch_size * sequence_length)\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n-\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n-            .to(compute_device)\n-        )\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n-\n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n-    return overall_loss * num_experts\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Jamba\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class JambaRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -176,19 +64,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n class HybridMambaAttentionDynamicCache:\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n@@ -250,16 +125,24 @@ def update(\n \n     def reorder_cache(self, beam_idx: torch.LongTensor):\n         \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n-        for layer_idx in range(len(self.key_cache)):\n-            device = self.key_cache[layer_idx].device\n-            self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.value_cache[layer_idx].device\n-            self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n-\n-            device = self.conv_states[layer_idx].device\n-            self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n-            device = self.ssm_states[layer_idx].device\n-            self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+        if self.get_seq_length() > 0:\n+            for layer_idx in range(len(self.key_cache)):\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+                device = self.conv_states[layer_idx].device\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.ssm_states[layer_idx].device\n+                self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        kv_length = self.get_seq_length(layer_idx) + query_length\n+        return kv_length, kv_offset\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n@@ -270,7 +153,44 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         return self.key_cache[layer_idx].shape[-2]\n \n \n-# Adapted from transformers.models.mistral.modeling_mistral.MistralAttention with Mistral->Jamba\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class JambaAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n@@ -282,7 +202,7 @@ def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n         self.config = config\n         self.layer_idx = layer_idx\n         if layer_idx is None:\n-            logger.warning_once(\n+            logger.warning(\n                 f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n                 \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n                 \"when creating this class.\"\n@@ -295,7 +215,7 @@ def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n-\n+        self.scaling = self.head_dim**-0.5\n         if (self.head_dim * self.num_heads) != self.hidden_size:\n             raise ValueError(\n                 f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n@@ -306,255 +226,49 @@ def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_values is not None:\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_values\n-\n-\n-# Adapted from transformers.models.mistral.modeling_mistral.MistralFlashAttention2 with Mistral->Jamba\n-class JambaFlashAttention2(JambaAttention):\n-    \"\"\"\n-    Jamba flash attention module. This module inherits from `JambaAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    def forward(\n+    def forward(  # FIME: this is also the classic attention NOPE\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         if past_key_values is not None:\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self.config, \"sliding_window\", None),\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_values\n-\n-\n-# Adapted from transformers.models.mistral.modeling_mistral.MistralSdpaAttention with Mistral->Jamba\n-class JambaSdpaAttention(JambaAttention):\n-    \"\"\"\n-    Jamba attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `JambaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from JambaAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"JambaModel is using JambaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_values is not None:\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = self.is_causal and causal_mask is None and q_len > 1\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_values\n+        return attn_output, attn_weights\n \n \n-JAMBA_ATTENTION_CLASSES = {\n-    \"eager\": JambaAttention,\n-    \"flash_attention_2\": JambaFlashAttention2,\n-    \"sdpa\": JambaSdpaAttention,\n-}\n+is_fast_path_available = all(\n+    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+)\n \n \n-# Adapted from transformers.models.mamba.modeling_mamba.MambaMixer\n class JambaMambaMixer(nn.Module):\n     \"\"\"\n     Compute âˆ†, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n@@ -571,7 +285,9 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.ssm_state_size = config.mamba_d_state\n         self.conv_kernel_size = config.mamba_d_conv\n         self.intermediate_size = config.mamba_expand * config.hidden_size\n-        self.time_step_rank = config.mamba_dt_rank\n+        self.time_step_rank = (\n+            math.ceil(self.hidden_size / 16) if config.mamba_dt_rank == \"auto\" else config.mamba_dt_rank\n+        )\n         self.use_conv_bias = config.mamba_conv_bias\n         self.use_bias = config.mamba_proj_bias\n         self.conv1d = nn.Conv1d(\n@@ -609,8 +325,8 @@ def __init__(self, config: JambaConfig, layer_idx):\n         self.c_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n \n         if not is_fast_path_available:\n-            logger.warning_once(\n-                \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n+            logger.warning(\n+                \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n                 \" is None. To install follow https://github.com/state-spaces/mamba/#installation and\"\n                 \" https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\"\n             )\n@@ -622,14 +338,16 @@ def cuda_kernels_forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n         batch_size, seq_len, _ = hidden_states.shape\n-        use_precomputed_states = (\n-            cache_params is not None\n-            and cache_params.has_previous_state\n-            and seq_len == 1\n-            and cache_params.conv_states[self.layer_idx].shape[0]\n-            == cache_params.ssm_states[self.layer_idx].shape[0]\n-            == batch_size\n-        )\n+        if cache_params is None:\n+            use_precomputed_states = False\n+        else:\n+            use_precomputed_states = (\n+                cache_params.has_previous_state\n+                and seq_len == 1\n+                and cache_params.conv_states[self.layer_idx].shape[0]\n+                == cache_params.ssm_states[self.layer_idx].shape[0]\n+                == batch_size\n+            )\n         # 1. Gated MLP's linear projection\n         projected_states = self.in_proj(hidden_states).transpose(1, 2)\n \n@@ -643,6 +361,8 @@ def cuda_kernels_forward(\n         # 2. Convolution sequence transformation\n         conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n         if use_precomputed_states:\n+            if causal_conv1d_update is None:\n+                raise ImportError(\"causal_conv1d_update is not available\")\n             hidden_states = causal_conv1d_update(\n                 hidden_states.squeeze(-1),\n                 cache_params.conv_states[self.layer_idx],\n@@ -655,6 +375,8 @@ def cuda_kernels_forward(\n             if cache_params is not None:\n                 conv_states = nn.functional.pad(hidden_states, (self.conv_kernel_size - hidden_states.shape[-1], 0))\n                 cache_params.conv_states[self.layer_idx].copy_(conv_states)\n+            if causal_conv1d_fn is None:\n+                raise ImportError(\"causal_conv1d_fn is not available\")\n             hidden_states = causal_conv1d_fn(hidden_states, conv_weights, self.conv1d.bias, activation=self.activation)\n \n         if attention_mask is not None:\n@@ -688,6 +410,8 @@ def cuda_kernels_forward(\n         # 3.c perform the recurrence y â† SSM(A, B, C)(x)\n         time_proj_bias = time_proj_bias.float() if time_proj_bias is not None else None\n         if use_precomputed_states:\n+            if selective_state_update is None:\n+                raise ImportError(\"selective_state_update is not available\")\n             scan_outputs = selective_state_update(\n                 cache_params.ssm_states[self.layer_idx],\n                 hidden_states[..., 0],\n@@ -701,6 +425,8 @@ def cuda_kernels_forward(\n                 dt_softplus=True,\n             ).unsqueeze(-1)\n         else:\n+            if selective_scan_fn is None:\n+                raise ImportError(\"selective_scan_fn is not available\")\n             scan_outputs, ssm_state = selective_scan_fn(\n                 hidden_states,\n                 discrete_time_step,\n@@ -734,7 +460,7 @@ def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttention\n \n         use_cache = isinstance(cache_params, HybridMambaAttentionDynamicCache)\n         # 2. Convolution sequence transformation\n-        if use_cache and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n+        if use_cache and cache_params is not None and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n             if self.training:\n                 # In training mode, we don't want to perform in-place operations on ssm_state so we can compute the backwards pass\n                 ssm_state = cache_params.ssm_states[self.layer_idx].clone()\n@@ -822,24 +548,57 @@ def forward(\n         return self.slow_forward(hidden_states, cache_params, attention_mask)\n \n \n-# Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Jamba\n class JambaMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.hidden_size\n         self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, x):\n         down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n         return down_proj\n \n \n-# Adapted from transformers.models.mixtral.modeling_mixtral.MixtralSparseMoeBlock with Mistral->Jamba\n+class JambaExperts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config: JambaConfig):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(JambaMLP(config))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n class JambaSparseMoeBlock(nn.Module):\n     \"\"\"\n     This implementation is\n@@ -860,213 +619,95 @@ def __init__(self, config: JambaConfig):\n         self.top_k = config.num_experts_per_tok\n \n         self.router = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n-        self.experts = nn.ModuleList([JambaMLP(config) for _ in range(self.num_experts)])\n+        self.experts = JambaExperts(config)\n \n-    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\" \"\"\"\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        return top_k_index, top_k_weights.to(hidden_states.dtype)\n \n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n         router_logits = self.router(hidden_states)\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n-\n-            if top_x.shape[0] == 0:\n-                continue\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n+        hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n+        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return hidden_states\n \n \n class JambaAttentionDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: JambaConfig, layer_idx: int):\n         super().__init__()\n-        num_experts = config.layers_num_experts[layer_idx]\n-        self.self_attn = JAMBA_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        num_experts = config.layers_num_experts[layer_idx] if config.layers_num_experts else 1\n+        self.self_attn = JambaAttention(config, layer_idx)\n \n         ffn_layer_class = JambaSparseMoeBlock if num_experts > 1 else JambaMLP\n         self.feed_forward = ffn_layer_class(config)\n         self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-        \"\"\"\n-\n-        residual = hidden_states\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n         hidden_states = self.input_layernorm(hidden_states)\n-\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        residual = hidden_states\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n         )\n-\n-        # residual connection after attention\n         hidden_states = residual + hidden_states\n-\n-        # feed-forward (experts/MLP)\n         residual = hidden_states\n         hidden_states = self.pre_ff_layernorm(hidden_states)\n-        ff_outputs = self.feed_forward(hidden_states)\n-        if isinstance(ff_outputs, tuple):\n-            hidden_states, router_logits = ff_outputs\n-        else:\n-            hidden_states, router_logits = ff_outputs, None\n+        hidden_states = self.feed_forward(hidden_states)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class JambaMambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: JambaConfig, layer_idx: int):\n         super().__init__()\n-        num_experts = config.layers_num_experts[layer_idx]\n+        num_experts = config.layers_num_experts[layer_idx] if config.layers_num_experts else 1\n         self.mamba = JambaMambaMixer(config=config, layer_idx=layer_idx)\n-\n         ffn_layer_class = JambaSparseMoeBlock if num_experts > 1 else JambaMLP\n         self.feed_forward = ffn_layer_class(config)\n         self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-        \"\"\"\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n         hidden_states = self.mamba(\n             hidden_states=hidden_states,\n             cache_params=past_key_values,\n             attention_mask=attention_mask,\n         )\n-        self_attn_weights = None\n-\n-        # residual connection after mamba\n         hidden_states = residual + hidden_states\n-\n-        # feed-forward (experts/MLP)\n         residual = hidden_states\n         hidden_states = self.pre_ff_layernorm(hidden_states)\n-        ff_outputs = self.feed_forward(hidden_states)\n-        if isinstance(ff_outputs, tuple):\n-            hidden_states, router_logits = ff_outputs\n-        else:\n-            hidden_states, router_logits = ff_outputs, None\n+        hidden_states = self.feed_forward(hidden_states)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if use_cache:\n-            outputs += (past_key_values,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n-@auto_docstring\n class JambaPreTrainedModel(PreTrainedModel):\n     config: JambaConfig\n     base_model_prefix = \"model\"\n@@ -1077,6 +718,11 @@ class JambaPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True\n+    _can_record_outputs = {\n+        \"hidden_states\": [JambaAttentionDecoderLayer, JambaMambaDecoderLayer],\n+        \"attentions\": JambaAttention,\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"router\"),\n+    }\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1100,16 +746,8 @@ def _init_weights(self, module):\n ALL_DECODER_LAYER_TYPES = {\"attention\": JambaAttentionDecoderLayer, \"mamba\": JambaMambaDecoderLayer}\n \n \n-# Adapted from transformers.models.mistral.modeling_mistral.MistralModel with MISTRAL->JAMBA, Mistral->Jamba\n @auto_docstring\n class JambaModel(JambaPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`JambaDecoderLayer`]\n-\n-    Args:\n-        config: JambaConfig\n-    \"\"\"\n-\n     def __init__(self, config: JambaConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -1129,7 +767,7 @@ def __init__(self, config: JambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -1139,151 +777,165 @@ def forward(\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n-        hidden_states = inputs_embeds\n \n         if use_cache and past_key_values is None:\n-            logger.warning_once(\n-                \"Jamba requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was \"\n-                \"provided, so no cache will be returned.\"\n+            past_key_values = HybridMambaAttentionDynamicCache(\n+                config=self.config,\n+                batch_size=inputs_embeds.shape[0],\n+                dtype=inputs_embeds.dtype,\n+                device=inputs_embeds.device,\n             )\n \n         if cache_position is None:\n-            cache_position = torch.arange(hidden_states.shape[1], device=hidden_states.device)\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.LongTensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(attention_mask, inputs_embeds, cache_position)\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n         mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n-\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n+        hidden_states = inputs_embeds\n         for decoder_layer in self.layers:\n-            # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n             layer_mask = mamba_mask if isinstance(decoder_layer, JambaMambaDecoderLayer) else causal_mask\n \n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n                 attention_mask=layer_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                if layer_outputs[1] is not None:\n-                    # append attentions only of attention layers. Mamba layers return `None` as the attention weights\n-                    all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                if layer_outputs[-1] is not None:\n-                    # append router logits only of expert layers. Regular MLP layers return `None` as the router logits\n-                    all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.final_layernorm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         if past_key_values and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n-        next_cache = None if not use_cache else past_key_values\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n+            past_key_values=past_key_values,\n         )\n \n-    def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        target_length = cache_position[-1] + 1\n-\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            if attention_mask.dim() == 2:\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n-                causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n     def _update_mamba_mask(self, attention_mask, cache_position):\n         \"\"\"\n         No need for zeroing states when\n             1. Cached forward\n             2. Attending to all inputs\n         \"\"\"\n         mamba_mask = attention_mask\n-        if cache_position[0] > 0 or (attention_mask is not None and torch.all(attention_mask == 1)):\n+        if (cache_position is not None and cache_position[0] > 0) or (\n+            attention_mask is not None and torch.all(attention_mask == 1)\n+        ):\n             mamba_mask = None\n         return mamba_mask\n \n \n-# Adapted from transformers.models.mixtral.modeling_mixtral.MixtralForCausalLM with MIXTRAL->JAMBA, Mixtral->Jamba\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n+@auto_docstring\n class JambaForCausalLM(JambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config: JambaConfig):\n         super().__init__(config)\n@@ -1293,6 +945,7 @@ def __init__(self, config: JambaConfig):\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1303,12 +956,10 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n@@ -1325,8 +976,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, JambaForCausalLM\n \n-        >>> model = JambaForCausalLM.from_pretrained(\"ai21labs/Jamba-v0.1\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"ai21labs/Jamba-v0.1\")\n+        >>> model = JambaForCausalLM.from_pretrained(\"mistralai/Jamba-8x7B-v0.1\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Jamba-8x7B-v0.1\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1337,15 +988,10 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -1354,13 +1000,13 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n@@ -1395,68 +1041,33 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         attention_mask=None,\n         inputs_embeds=None,\n-        output_router_logits=False,\n         cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n         **kwargs,\n     ):\n-        # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n-\n-        empty_past_kv = past_key_values is None\n-\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        if not empty_past_kv:\n-            if (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-        else:\n+        # create cache if necessary\n+        if past_key_values is None:\n             past_key_values = HybridMambaAttentionDynamicCache(\n                 self.config, input_ids.shape[0], self.dtype, device=self.device\n             )\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if not empty_past_kv:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and past_key_values.get_seq_length() == 0:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n-            model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n+            model_inputs = {\"input_ids\": input_ids}\n \n         model_inputs.update(\n             {\n-                \"position_ids\": position_ids,\n                 \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n+                \"use_cache\": kwargs.get(\"use_cache\"),\n                 \"attention_mask\": attention_mask,\n-                \"output_router_logits\": output_router_logits,\n-                \"logits_to_keep\": self.config.num_logits_to_keep,\n                 \"cache_position\": cache_position,\n             }\n         )\n-\n-        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n-        for key, value in kwargs.items():\n-            if key not in model_inputs:\n-                model_inputs[key] = value\n-\n         return model_inputs\n \n \n-class JambaForSequenceClassification(GenericForSequenceClassification, JambaPreTrainedModel): ...\n+class JambaForSequenceClassification(GenericForSequenceClassification, JambaPreTrainedModel):\n+    pass\n \n \n __all__ = [\"JambaForCausalLM\", \"JambaForSequenceClassification\", \"JambaModel\", \"JambaPreTrainedModel\"]"
        },
        {
            "sha": "61bbe4dd0bfd6b551227dfaee89007d21f8b8711",
            "filename": "src/transformers/models/jamba/modular_jamba.py",
            "status": "added",
            "additions": 802,
            "deletions": 0,
            "changes": 802,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodular_jamba.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,802 @@\n+# This is an automatically generated file by the modularization script. Please do not make any manual changes\n+# to this file as they will be overwritten.\n+\n+import math\n+from typing import Any, Callable, Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    logging,\n+)\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n+from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm, eager_attention_forward\n+from ..mixtral.modeling_mixtral import MixtralExperts, MixtralForCausalLM\n+from .configuration_jamba import JambaConfig\n+\n+\n+if is_mamba_ssm_available():\n+    from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n+    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n+else:\n+    selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n+\n+if is_causal_conv1d_available():\n+    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+else:\n+    causal_conv1d_update, causal_conv1d_fn = None, None\n+\n+is_fast_path_available = all(\n+    (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class JambaRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class HybridMambaAttentionDynamicCache:\n+    \"\"\"\n+    A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n+    (which has a constant shape regardless of seq_len).\n+\n+    This cache has two sets of lists of tensors: `key_cache` and `value_cache` for attention cache and `conv_states`\n+    and `ssm_states` for mamba cache. Each of these lists has `num_layers` tensors. The expected shape for each tensor\n+    For attention layers, `key_cache` and `value_cache` have a shape of `(batch_size, num_heads, seq_len, head_dim)`,\n+    while `conv_states` and `ssm_states` have a shape of `(batch_size, 0)` (empty tensors).\n+    For mamba layers, `key_cache` and `value_cache` have a shape of `(batch_size, 0)` (empty tensors),\n+    while `conv_states` represents the convolution state and has a shape of `(batch_size, d_inner, d_conv)`,\n+    and `ssm_states` represents the ssm state and has a shape of `(batch_size, d_inner, d_state)`.\n+    \"\"\"\n+\n+    is_compileable = False\n+\n+    def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n+        self.dtype = dtype\n+        self.layers_block_type = config.layers_block_type\n+        self.has_previous_state = False  # only used by mamba\n+        intermediate_size = config.mamba_expand * config.hidden_size\n+        ssm_state_size = config.mamba_d_state\n+        conv_kernel_size = config.mamba_d_conv\n+        self.conv_states = []\n+        self.ssm_states = []\n+        self.transformer_layers = []\n+        for i in range(config.num_hidden_layers):\n+            if self.layers_block_type[i] == \"mamba\":\n+                self.conv_states += [\n+                    torch.zeros(batch_size, intermediate_size, conv_kernel_size, device=device, dtype=dtype)\n+                ]\n+                self.ssm_states += [\n+                    torch.zeros(batch_size, intermediate_size, ssm_state_size, device=device, dtype=dtype)\n+                ]\n+            else:\n+                self.conv_states += [torch.tensor([[]] * batch_size, device=device)]\n+                self.ssm_states += [torch.tensor([[]] * batch_size, device=device)]\n+                self.transformer_layers.append(i)\n+\n+        self.key_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n+        self.value_cache = [torch.tensor([[]] * batch_size, device=device) for _ in range(config.num_hidden_layers)]\n+\n+    def update(\n+        self,\n+        key_states: torch.Tensor,\n+        value_states: torch.Tensor,\n+        layer_idx: int,\n+        cache_kwargs: Optional[dict[str, Any]] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        # Update the cache\n+        if self.key_cache[layer_idx].shape[-1] == 0:\n+            self.key_cache[layer_idx] = key_states\n+            self.value_cache[layer_idx] = value_states\n+        else:\n+            self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=2)\n+            self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=2)\n+\n+        return self.key_cache[layer_idx], self.value_cache[layer_idx]\n+\n+    def reorder_cache(self, beam_idx: torch.LongTensor):\n+        \"\"\"Reorders the cache for beam search, given the selected beam indices.\"\"\"\n+        if self.get_seq_length() > 0:\n+            for layer_idx in range(len(self.key_cache)):\n+                device = self.key_cache[layer_idx].device\n+                self.key_cache[layer_idx] = self.key_cache[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.value_cache[layer_idx].device\n+                self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n+\n+                device = self.conv_states[layer_idx].device\n+                self.conv_states[layer_idx] = self.conv_states[layer_idx].index_select(0, beam_idx.to(device))\n+                device = self.ssm_states[layer_idx].device\n+                self.ssm_states[layer_idx] = self.ssm_states[layer_idx].index_select(0, beam_idx.to(device))\n+\n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n+        kv_offset = 0\n+        query_length = cache_position.shape[0]\n+        kv_length = self.get_seq_length(layer_idx) + query_length\n+        return kv_length, kv_offset\n+\n+    def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        # take any layer that contains cache and not empty tensor\n+        layer_idx = self.transformer_layers[0] if layer_idx not in self.transformer_layers else layer_idx\n+        if len(self.key_cache) <= layer_idx:\n+            return 0\n+        return self.key_cache[layer_idx].shape[-2]\n+\n+\n+class JambaAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n+    and \"Generating Long Sequences with Sparse Transformers\".\n+    \"\"\"\n+\n+    def __init__(self, config: JambaConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.hidden_size // self.num_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.is_causal = True\n+        self.attention_dropout = config.attention_dropout\n+        self.scaling = self.head_dim**-0.5\n+        if (self.head_dim * self.num_heads) != self.hidden_size:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n+            )\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+\n+    def forward(  # FIME: this is also the classic attention NOPE\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        if past_key_values is not None:\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class JambaMambaMixer(nn.Module):\n+    \"\"\"\n+    Compute âˆ†, A, B, C, and D the state space parameters and compute the `contextualized_states`.\n+    A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n+    âˆ†, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n+    and is why Mamba is called **selective** state spaces)\n+    \"\"\"\n+\n+    def __init__(self, config: JambaConfig, layer_idx):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.hidden_size = config.hidden_size\n+        self.ssm_state_size = config.mamba_d_state\n+        self.conv_kernel_size = config.mamba_d_conv\n+        self.intermediate_size = config.mamba_expand * config.hidden_size\n+        self.time_step_rank = (\n+            math.ceil(self.hidden_size / 16) if config.mamba_dt_rank == \"auto\" else config.mamba_dt_rank\n+        )\n+        self.use_conv_bias = config.mamba_conv_bias\n+        self.use_bias = config.mamba_proj_bias\n+        self.conv1d = nn.Conv1d(\n+            in_channels=self.intermediate_size,\n+            out_channels=self.intermediate_size,\n+            bias=self.use_conv_bias,\n+            kernel_size=self.conv_kernel_size,\n+            groups=self.intermediate_size,\n+            padding=self.conv_kernel_size - 1,\n+        )\n+\n+        self.activation = config.hidden_act\n+        self.act = ACT2FN[config.hidden_act]\n+\n+        self.use_fast_kernels = config.use_mamba_kernels\n+\n+        # projection of the input hidden states\n+        self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=self.use_bias)\n+        # selective projection used to make dt, B and C input dependent\n+        self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n+        # time step projection (discretization)\n+        self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n+\n+        # S4D real initialization. These are not discretized!\n+        # The core is to load them, compute the discrete states, then write the updated state. Keeps the memory bounded\n+        A = torch.arange(1, self.ssm_state_size + 1)[None, :]\n+        A = A.expand(self.intermediate_size, -1).contiguous()\n+\n+        self.A_log = nn.Parameter(torch.log(A))\n+        self.D = nn.Parameter(torch.ones(self.intermediate_size))\n+        self.out_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=self.use_bias)\n+\n+        self.dt_layernorm = JambaRMSNorm(self.time_step_rank, eps=config.rms_norm_eps)\n+        self.b_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n+        self.c_layernorm = JambaRMSNorm(self.ssm_state_size, eps=config.rms_norm_eps)\n+\n+        if not is_fast_path_available:\n+            logger.warning(\n+                \"The fast path is not available because on of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n+                \" is None. To install follow https://github.com/state-spaces/mamba/#installation and\"\n+                \" https://github.com/Dao-AILab/causal-conv1d. If you want to use the naive implementation, set `use_mamba_kernels=False` in the model config\"\n+            )\n+\n+    def cuda_kernels_forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        batch_size, seq_len, _ = hidden_states.shape\n+        if cache_params is None:\n+            use_precomputed_states = False\n+        else:\n+            use_precomputed_states = (\n+                cache_params.has_previous_state\n+                and seq_len == 1\n+                and cache_params.conv_states[self.layer_idx].shape[0]\n+                == cache_params.ssm_states[self.layer_idx].shape[0]\n+                == batch_size\n+            )\n+        # 1. Gated MLP's linear projection\n+        projected_states = self.in_proj(hidden_states).transpose(1, 2)\n+\n+        # We can't use `mamba_inner_fn` even if in training and without cache params because we have the\n+        # inner layernorms which isn't supported by this fused kernel\n+        hidden_states, gate = projected_states.chunk(2, dim=1)\n+\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        # 2. Convolution sequence transformation\n+        conv_weights = self.conv1d.weight.view(self.conv1d.weight.size(0), self.conv1d.weight.size(2))\n+        if use_precomputed_states:\n+            if causal_conv1d_update is None:\n+                raise ImportError(\"causal_conv1d_update is not available\")\n+            hidden_states = causal_conv1d_update(\n+                hidden_states.squeeze(-1),\n+                cache_params.conv_states[self.layer_idx],\n+                conv_weights,\n+                self.conv1d.bias,\n+                self.activation,\n+            )\n+            hidden_states = hidden_states.unsqueeze(-1)\n+        else:\n+            if cache_params is not None:\n+                conv_states = nn.functional.pad(hidden_states, (self.conv_kernel_size - hidden_states.shape[-1], 0))\n+                cache_params.conv_states[self.layer_idx].copy_(conv_states)\n+            if causal_conv1d_fn is None:\n+                raise ImportError(\"causal_conv1d_fn is not available\")\n+            hidden_states = causal_conv1d_fn(hidden_states, conv_weights, self.conv1d.bias, activation=self.activation)\n+\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        # 3. State Space Model sequence transformation\n+        # 3.a. input varying initialization of time_step, B and C\n+        ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n+        time_step, B, C = torch.split(\n+            ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n+        )\n+\n+        time_step = self.dt_layernorm(time_step)\n+        B = self.b_layernorm(B)\n+        C = self.c_layernorm(C)\n+\n+        # Here we need to apply dt_proj without the bias, as the bias is added in the selective scan kernel.\n+        # This is a hack to apply dt_proj while still using the forward pass of `torch.nn.Linear`, which is needed\n+        # in order to make quantization work. Quantization code replaces `torch.nn.Linear` layers with quantized\n+        # linear layers, and requires to call the forward pass directly.\n+        # Quantized model can't work with the original code:\n+        # ```discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)```\n+        time_proj_bias = self.dt_proj.bias.data\n+        with torch.no_grad():\n+            self.dt_proj.bias.data = torch.zeros_like(self.dt_proj.bias.data)\n+        discrete_time_step = self.dt_proj(time_step).transpose(1, 2)\n+        with torch.no_grad():\n+            self.dt_proj.bias.data = time_proj_bias\n+\n+        A = -torch.exp(self.A_log.float())\n+        # 3.c perform the recurrence y â† SSM(A, B, C)(x)\n+        time_proj_bias = time_proj_bias.float() if time_proj_bias is not None else None\n+        if use_precomputed_states:\n+            if selective_state_update is None:\n+                raise ImportError(\"selective_state_update is not available\")\n+            scan_outputs = selective_state_update(\n+                cache_params.ssm_states[self.layer_idx],\n+                hidden_states[..., 0],\n+                discrete_time_step[..., 0],\n+                A,\n+                B[:, 0],\n+                C[:, 0],\n+                self.D,\n+                gate[..., 0],\n+                time_proj_bias,\n+                dt_softplus=True,\n+            ).unsqueeze(-1)\n+        else:\n+            if selective_scan_fn is None:\n+                raise ImportError(\"selective_scan_fn is not available\")\n+            scan_outputs, ssm_state = selective_scan_fn(\n+                hidden_states,\n+                discrete_time_step,\n+                A,\n+                B.transpose(1, 2),\n+                C.transpose(1, 2),\n+                self.D.float(),\n+                gate,\n+                time_proj_bias,\n+                delta_softplus=True,\n+                return_last_state=True,\n+            )\n+            if ssm_state is not None and cache_params is not None:\n+                cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n+\n+        # 4. Final linear projection\n+        contextualized_states = self.out_proj(scan_outputs.transpose(1, 2))\n+\n+        return contextualized_states\n+\n+    # fmt: off\n+    def slow_forward(self, input_states, cache_params: Optional[HybridMambaAttentionDynamicCache] = None, attention_mask: Optional[torch.LongTensor] = None):\n+        batch_size, seq_len, _ = input_states.shape\n+        dtype = input_states.dtype\n+        # 1. Gated MLP's linear projection\n+        projected_states = self.in_proj(input_states).transpose(1, 2)                   # [batch, 2 * intermediate_size, seq_len]\n+        hidden_states, gate = projected_states.chunk(2, dim=1)\n+\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        use_cache = isinstance(cache_params, HybridMambaAttentionDynamicCache)\n+        # 2. Convolution sequence transformation\n+        if use_cache and cache_params is not None and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n+            if self.training:\n+                # In training mode, we don't want to perform in-place operations on ssm_state so we can compute the backwards pass\n+                ssm_state = cache_params.ssm_states[self.layer_idx].clone()\n+            else:\n+                ssm_state = cache_params.ssm_states[self.layer_idx]\n+\n+            ssm_state = ssm_state.to(hidden_states.device)\n+\n+            if cache_params.has_previous_state and seq_len == 1 and \\\n+                    cache_params.conv_states[self.layer_idx].shape[0] == batch_size:\n+                conv_state = cache_params.conv_states[self.layer_idx]                   # [batch, intermediate_size, conv_kernel_size]\n+                conv_state = torch.roll(conv_state, shifts=-1, dims=-1)\n+                conv_state[:, :, -1] = hidden_states[:, :, 0]\n+                cache_params.conv_states[self.layer_idx] = conv_state\n+                hidden_states = torch.sum(conv_state * self.conv1d.weight[:, 0, :], dim=-1)\n+                if self.use_conv_bias:\n+                    hidden_states += self.conv1d.bias\n+                hidden_states = self.act(hidden_states).to(dtype).unsqueeze(-1)         # [batch, intermediate_size, 1] : decoding\n+            else:\n+                conv_state = nn.functional.pad(\n+                    hidden_states,\n+                    (self.conv_kernel_size - hidden_states.shape[-1], 0)\n+                )\n+                cache_params.conv_states[self.layer_idx] = conv_state\n+                hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])     # [batch, intermediate_size, seq_len]\n+        else:\n+            ssm_state = torch.zeros(\n+                (batch_size, self.intermediate_size, self.ssm_state_size),\n+                device=hidden_states.device, dtype=dtype\n+            )\n+            hidden_states = self.act(self.conv1d(hidden_states)[..., :seq_len])         # [batch, intermediate_size, seq_len]\n+\n+        if attention_mask is not None:\n+            hidden_states = hidden_states * attention_mask.unsqueeze(1)\n+\n+        # 3. State Space Model sequence transformation\n+        # 3.a. Selection:  [batch, seq_len, self.time_step_rank + self.ssm_state_size * 2]\n+        ssm_parameters = self.x_proj(hidden_states.transpose(1, 2))\n+        time_step, B, C = torch.split(\n+            ssm_parameters, [self.time_step_rank, self.ssm_state_size, self.ssm_state_size], dim=-1\n+        )\n+\n+        time_step = self.dt_layernorm(time_step)\n+        B = self.b_layernorm(B)\n+        C = self.c_layernorm(C)\n+\n+        discrete_time_step = self.dt_proj(time_step)                                    # [batch, seq_len, intermediate_size]\n+        discrete_time_step = nn.functional.softplus(discrete_time_step).transpose(1, 2) # [batch, intermediate_size, seq_len]\n+\n+        # 3.b. Discretization: B and C to [batch, seq_len, intermediate_size, ssm_state_size] (SRAM)\n+        A = -torch.exp(self.A_log.float())                                              # [intermediate_size, ssm_state_size]\n+        discrete_A = torch.exp(A[None, :, None, :] * discrete_time_step[:, :, :, None]) # [batch, intermediate_size, seq_len, ssm_state_size]\n+        discrete_B = discrete_time_step[:, :, :, None] * B[:, None, :, :].float()       # [batch, intermediate_size, seq_len, ssm_state_size]\n+        deltaB_u = discrete_B * hidden_states[:, :, :, None].float()\n+        # 3.c perform the recurrence y â† SSM(A, B, C)(x)\n+        scan_outputs = []\n+        for i in range(seq_len):\n+            ssm_state = discrete_A[:, :, i, :] * ssm_state + deltaB_u[:, :, i, :]      # [batch, intermediate_size, ssm_state]\n+            scan_output = torch.matmul(ssm_state.to(dtype), C[:, i, :].unsqueeze(-1))  # [batch, intermediate_size, 1]\n+            scan_outputs.append(scan_output[:, :, 0])\n+        scan_output = torch.stack(scan_outputs, dim=-1)                                # [batch, intermediate_size, seq_len]\n+        scan_output = scan_output + (hidden_states * self.D[None, :, None])\n+        scan_output = (scan_output * self.act(gate))\n+\n+        if use_cache:\n+            cache_params.ssm_states[self.layer_idx] = ssm_state\n+\n+        # 4. Final linear projection\n+        contextualized_states = self.out_proj(scan_output.transpose(1, 2))  # [batch, seq_len, hidden_size]\n+        return contextualized_states\n+    # fmt: on\n+\n+    def forward(\n+        self,\n+        hidden_states,\n+        cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+    ):\n+        if self.use_fast_kernels:\n+            if not is_fast_path_available or \"cuda\" not in self.x_proj.weight.device.type:\n+                raise ValueError(\n+                    \"Fast Mamba kernels are not available. Make sure to they are installed and that the mamba module is on a CUDA device\"\n+                )\n+            return self.cuda_kernels_forward(hidden_states, cache_params, attention_mask)\n+        return self.slow_forward(hidden_states, cache_params, attention_mask)\n+\n+\n+class JambaMLP(LlamaMLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size)\n+\n+\n+class JambaExperts(MixtralExperts):\n+    pass\n+\n+\n+class JambaSparseMoeBlock(nn.Module):\n+    \"\"\"\n+    This implementation is\n+    strictly equivalent to standard MoE with full capacity (no\n+    dropped tokens). It's faster since it formulates MoE operations\n+    in terms of block-sparse operations to accommodate imbalanced\n+    assignments of tokens to experts, whereas standard MoE either\n+    (1) drop tokens at the cost of reduced performance or (2) set\n+    capacity factor to number of experts and thus waste computation\n+    and memory on padding.\n+    \"\"\"\n+\n+    def __init__(self, config: JambaConfig):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.ffn_dim = config.intermediate_size\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+\n+        self.router = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n+        self.experts = JambaExperts(config)\n+\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        return top_k_index, top_k_weights.to(hidden_states.dtype)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n+        router_logits = self.router(hidden_states)\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n+        hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n+        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return hidden_states\n+\n+\n+class JambaAttentionDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: JambaConfig, layer_idx: int):\n+        super().__init__()\n+        num_experts = config.layers_num_experts[layer_idx] if config.layers_num_experts else 1\n+        self.self_attn = JambaAttention(config, layer_idx)\n+\n+        ffn_layer_class = JambaSparseMoeBlock if num_experts > 1 else JambaMLP\n+        self.feed_forward = ffn_layer_class(config)\n+        self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+        hidden_states = self.input_layernorm(hidden_states)\n+        residual = hidden_states\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+        )\n+        hidden_states = residual + hidden_states\n+        residual = hidden_states\n+        hidden_states = self.pre_ff_layernorm(hidden_states)\n+        hidden_states = self.feed_forward(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+class JambaMambaDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: JambaConfig, layer_idx: int):\n+        super().__init__()\n+        num_experts = config.layers_num_experts[layer_idx] if config.layers_num_experts else 1\n+        self.mamba = JambaMambaMixer(config=config, layer_idx=layer_idx)\n+        ffn_layer_class = JambaSparseMoeBlock if num_experts > 1 else JambaMLP\n+        self.feed_forward = ffn_layer_class(config)\n+        self.input_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.pre_ff_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.FloatTensor, Optional[torch.Tensor], Optional[HybridMambaAttentionDynamicCache]]:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states = self.mamba(\n+            hidden_states=hidden_states,\n+            cache_params=past_key_values,\n+            attention_mask=attention_mask,\n+        )\n+        hidden_states = residual + hidden_states\n+        residual = hidden_states\n+        hidden_states = self.pre_ff_layernorm(hidden_states)\n+        hidden_states = self.feed_forward(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+ALL_DECODER_LAYER_TYPES = {\"attention\": JambaAttentionDecoderLayer, \"mamba\": JambaMambaDecoderLayer}\n+\n+\n+class JambaPreTrainedModel(PreTrainedModel):\n+    config: JambaConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"JambaAttentionDecoderLayer\", \"JambaMambaDecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    # Note: only supports HybridMambaAttentionDynamicCache\n+    _is_stateful = True\n+    _can_record_outputs = {\n+        \"hidden_states\": [JambaAttentionDecoderLayer, JambaMambaDecoderLayer],\n+        \"attentions\": JambaAttention,\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"router\"),\n+    }\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, JambaRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, JambaMambaMixer):\n+            A = torch.arange(1, module.ssm_state_size + 1)[None, :]\n+            A = A.expand(module.intermediate_size, -1).contiguous()\n+            module.A_log.data.copy_(torch.log(A))\n+            module.D.data.fill_(1.0)\n+\n+\n+@auto_docstring\n+class JambaModel(JambaPreTrainedModel):\n+    def __init__(self, config: JambaConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        decoder_layers = []\n+        for i in range(config.num_hidden_layers):\n+            layer_class = ALL_DECODER_LAYER_TYPES[config.layers_block_type[i]]\n+            decoder_layers.append(layer_class(config, layer_idx=i))\n+        self.layers = nn.ModuleList(decoder_layers)\n+\n+        self._attn_implementation = config._attn_implementation\n+        self.final_layernorm = JambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = HybridMambaAttentionDynamicCache(\n+                config=self.config,\n+                batch_size=inputs_embeds.shape[0],\n+                dtype=inputs_embeds.dtype,\n+                device=inputs_embeds.device,\n+            )\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.LongTensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+        mamba_mask = self._update_mamba_mask(attention_mask, cache_position)\n+        hidden_states = inputs_embeds\n+        for decoder_layer in self.layers:\n+            layer_mask = mamba_mask if isinstance(decoder_layer, JambaMambaDecoderLayer) else causal_mask\n+\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=layer_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+        hidden_states = self.final_layernorm(hidden_states)\n+\n+        if past_key_values and not past_key_values.has_previous_state:\n+            past_key_values.has_previous_state = True\n+\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+    def _update_mamba_mask(self, attention_mask, cache_position):\n+        \"\"\"\n+        No need for zeroing states when\n+            1. Cached forward\n+            2. Attending to all inputs\n+        \"\"\"\n+        mamba_mask = attention_mask\n+        if (cache_position is not None and cache_position[0] > 0) or (\n+            attention_mask is not None and torch.all(attention_mask == 1)\n+        ):\n+            mamba_mask = None\n+        return mamba_mask\n+\n+\n+class JambaForCausalLM(MixtralForCausalLM):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: JambaConfig):\n+        super().__init__(config)\n+        self.model = JambaModel(config)\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.num_experts\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        **kwargs,\n+    ):\n+        # create cache if necessary\n+        if past_key_values is None:\n+            past_key_values = HybridMambaAttentionDynamicCache(\n+                self.config, input_ids.shape[0], self.dtype, device=self.device\n+            )\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and past_key_values.get_seq_length() == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+        else:\n+            model_inputs = {\"input_ids\": input_ids}\n+\n+        model_inputs.update(\n+            {\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": kwargs.get(\"use_cache\"),\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+            }\n+        )\n+        return model_inputs\n+\n+\n+class JambaForSequenceClassification(GenericForSequenceClassification, JambaPreTrainedModel):\n+    pass\n+\n+\n+__all__ = [\"JambaForCausalLM\", \"JambaForSequenceClassification\", \"JambaModel\", \"JambaPreTrainedModel\"]"
        },
        {
            "sha": "e577f1ba85c3d0a9af1cfa9bc7bb07a72fd7ddbd",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 256,
            "deletions": 644,
            "changes": 900,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/jetmoe/modular_jetmoe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_jetmoe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 JetMoe AI and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,10 +18,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch JetMoe model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n@@ -24,117 +28,77 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import (\n-    GenericForSequenceClassification,\n-    GradientCheckpointingLayer,\n-)\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_jetmoe import JetMoeConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n-def load_balancing_loss_func(\n-    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        gate_logits:\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n-        return 0\n-\n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class JetMoeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        JetMoeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n \n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+class JetMoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+    def __init__(self, config: JetMoeConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n \n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n-            .to(compute_device)\n-        )\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n \n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n \n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n-    return overall_loss * num_experts\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n class JetMoeParallelExperts(nn.Module):\n@@ -286,7 +250,7 @@ def forward(self, layer_input):\n         layer_output = zeros.index_add(0, batch_index, expert_outputs)\n         layer_output = layer_output.view(bsz, length, self.input_size)\n         layer_output = layer_output + self.bias\n-        return layer_output, router_logits\n+        return layer_output\n \n \n class JetMoeMoA(nn.Module):\n@@ -365,73 +329,13 @@ def forward(self, layer_input):\n         raise NotImplementedError(\"This module doesn't support call and forward.\")\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->JetMoe\n-class JetMoeRMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        JetMoeRMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-# Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with Gemma->JetMoe\n-class JetMoeRotaryEmbedding(nn.Module):\n-    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n-\n-    def __init__(self, config: JetMoeConfig, device=None):\n-        super().__init__()\n-        # BC: \"rope_type\" was originally \"type\"\n-        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n-            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-        else:\n-            self.rope_type = \"default\"\n-        self.max_seq_len_cached = config.max_position_embeddings\n-        self.original_max_seq_len = config.max_position_embeddings\n-\n-        self.config = config\n-        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-\n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n-        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n-        self.original_inv_freq = self.inv_freq\n-\n-    @torch.no_grad()\n-    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n-    def forward(self, x, position_ids):\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos() * self.attention_scaling\n-            sin = emb.sin() * self.attention_scaling\n-\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -459,6 +363,44 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class JetMoeAttention(nn.Module):\n     \"\"\"\n     Multi-headed attention from 'Attention Is All You Need' paper.\n@@ -485,364 +427,130 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n                 \"when creating this class.\"\n             )\n \n-        self.top_k = config.num_experts_per_tok\n+        self.num_key_value_groups = config.num_experts_per_tok\n         self.attention_dropout = config.attention_dropout\n         self.kv_projection_size = config.kv_channels * config.num_key_value_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_heads = config.num_attention_heads\n         self.head_dim = config.kv_channels\n-\n+        self.scaling = self.head_dim**-0.5\n         self.experts = JetMoeMoA(config)\n \n         self.kv_proj = torch.nn.Linear(config.hidden_size, self.kv_projection_size * 2, bias=False)\n \n-        self.rotary_emb = JetMoeRotaryEmbedding(config)\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states, router_logits, topo_info = self.experts.map(hidden_states)\n-        key_states, value_states = self.kv_proj(hidden_states).chunk(2, dim=-1)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads for top-k attention experts\n-        key_states = key_states.repeat(1, self.top_k, 1, 1)\n-        value_states = value_states.repeat(1, self.top_k, 1, 1)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.top_k, self.kv_projection_size)\n-\n-        attn_output = self.experts.reduce(attn_output, topo_info)\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, router_logits\n-\n-\n-class JetMoeSdpaAttention(JetMoeAttention):\n-    \"\"\"\n-    JetMoe attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `JetMoeAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from JetMoeAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]], Optional[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"JetMoeModel is using JetMoeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         query_states, router_logits, topo_info = self.experts.map(hidden_states)\n         key_states, value_states = self.kv_proj(hidden_states).chunk(2, dim=-1)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n \n-        cos, sin = self.rotary_emb(value_states, position_ids)\n+        cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads for top-k attention experts\n-        key_states = key_states.repeat(1, self.top_k, 1, 1)\n-        value_states = value_states.repeat(1, self.top_k, 1, 1)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = causal_mask is None and q_len > 1\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.top_k, self.kv_projection_size)\n-\n-        attn_output = self.experts.reduce(attn_output, topo_info)\n-        attn_output = attn_output.view(bsz, q_len, -1)\n-\n-        return attn_output, None, router_logits\n-\n-\n-class JetMoeFlashAttention2(JetMoeAttention):\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: Optional[torch.FloatTensor],\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = False,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[\n-        tuple[torch.Tensor, tuple[torch.Tensor]],\n-        Optional[tuple[torch.Tensor, tuple[torch.Tensor], tuple[torch.Tensor, ...]]],\n-    ]:\n-        \"\"\"\n-        Forward pass of the JetMoeAttention module.\n-\n-        Args:\n-            hidden_states (Optional[torch.FloatTensor]): Input hidden states.\n-            attention_mask (Optional[torch.FloatTensor]): Attention mask.\n-            layer_past (Optional[tuple[torch.Tensor]]): Past layer state.\n-            use_cache (Optional[bool]): Whether to use cached states.\n-            output_attentions (Optional[bool]): Whether to output attention weights.\n-            cache_position (Optional[torch.LongTensor]): Position of the cache.\n-\n-        Returns:\n-            Union[tuple[torch.Tensor, tuple[torch.Tensor]], Optional[tuple[...]]]: Tuple containing outputs.\n-        \"\"\"\n-        output_attentions = False\n-        bsz, q_len, hidden_size = hidden_states.size()\n-\n-        # calculate query, key, values\n-        query_states, router_logits, topo_info = self.experts.map(hidden_states)\n-        key_states, value_states = self.kv_proj(hidden_states).chunk(2, dim=-1)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = self.rotary_emb(value_states, position_ids)\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads for top-k attention experts\n-        key_states = key_states.repeat(1, self.top_k, 1, 1)\n-        value_states = value_states.repeat(1, self.top_k, 1, 1)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (LlamaRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.kv_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n-        ).to(input_dtype)\n-\n-        # output projection\n-        attn_output = attn_output.reshape(bsz, q_len, self.top_k, self.kv_projection_size)\n+        attn_output = attn_output.view(*input_shape, self.num_key_value_groups, -1)\n         attn_output = self.experts.reduce(attn_output, topo_info)\n-        attn_output = attn_output.view(bsz, q_len, hidden_size)  # re-assemble all head outputs side by side\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n+        attn_output = attn_output.view(*input_shape, -1)\n         return attn_output, attn_weights, router_logits\n \n \n-JETMOE_ATTENTION_CLASSES = {\n-    \"eager\": JetMoeAttention,\n-    \"flash_attention_2\": JetMoeFlashAttention2,\n-    \"sdpa\": JetMoeSdpaAttention,\n-}\n-\n-\n-class JetMoeBlock(GradientCheckpointingLayer):\n+class JetMoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n-        \"\"\"\n-        Initialize the JetMoeBlock module.\n-\n-        Args:\n-            config:\n-                Configuration object with model hyperparameters.\n-        \"\"\"\n         super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = JetMoeAttention(config, layer_idx)\n+        self.mlp = JetMoeMoE(config)\n         self.input_layernorm = JetMoeRMSNorm(config.hidden_size)\n-        self.self_attention = JETMOE_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n         self.post_attention_layernorm = JetMoeRMSNorm(config.hidden_size)\n \n-        self.mlp = JetMoeMoE(config)\n-\n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n-        hidden_states: Optional[torch.FloatTensor],\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n         # Self Attention\n-        attn_output, self_attn_weights, attn_router_logits = self.self_attention(\n-            hidden_states=self.input_layernorm(hidden_states),\n+        hidden_states, _, _ = self.self_attn(\n+            hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n+        hidden_states = residual + hidden_states\n \n-        hidden_states = hidden_states + attn_output\n-        x_mlp, mlp_router_logits = self.mlp(self.post_attention_layernorm(hidden_states))\n-        hidden_states = hidden_states + x_mlp\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += attn_router_logits, mlp_router_logits\n-\n-        return outputs\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n \n \n @auto_docstring\n class JetMoePreTrainedModel(PreTrainedModel):\n     config: JetMoeConfig\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = False\n-    _no_split_modules = [\"JetMoeBlock\"]\n+    _no_split_modules = [\"JetMoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"gate\", index=1),\n+        \"hidden_states\": JetMoeDecoderLayer,\n+        \"attentions\": OutputRecorder(JetMoeAttention, index=1),\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, (nn.Linear,)):\n+        if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n@@ -854,73 +562,51 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, JetMoeParallelExperts):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-        elif isinstance(module, (JetMoeMoA, JetMoeMoE)):\n+        elif isinstance(module, JetMoeMoA | JetMoeMoE):\n             module.bias.data.zero_()\n \n \n @auto_docstring\n class JetMoeModel(JetMoePreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`JetMoeBlock`]\n-\n-    Args:\n-        config:\n-            JetMoeConfig\n-    \"\"\"\n-\n     def __init__(self, config: JetMoeConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        self.layers = nn.ModuleList([JetMoeBlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n-        self._attn_implementation = config._attn_implementation\n+        self.layers = nn.ModuleList(\n+            [JetMoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n         self.norm = JetMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = JetMoeRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n+        self._attn_implementation = config._attn_implementation\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache(config=self.config)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -929,177 +615,119 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n-                position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n \n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n \n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-2], layer_outputs[-1])\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        hidden_states = self.norm(hidden_states)\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n \n-        return MoeModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n class JetMoeForCausalLM(JetMoePreTrainedModel, GenerationMixin):\n@@ -1127,36 +755,20 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        output_router_logits: Optional[bool] = False,\n         **kwargs,\n     ) -> MoeCausalLMOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        \"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs.last_hidden_state"
        },
        {
            "sha": "e10e3139a4976df6fc2f319726c6829e42a1c697",
            "filename": "src/transformers/models/jetmoe/modular_jetmoe.py",
            "status": "added",
            "additions": 605,
            "deletions": 0,
            "changes": 605,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodular_jetmoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,605 @@\n+# coding=utf-8\n+# Copyright 2024 JetMoe AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch JetMoe model.\"\"\"\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+from torch.nn import functional as F\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ..llama.modeling_llama import LlamaDecoderLayer\n+from ..mixtral.modeling_mixtral import (\n+    MixtralModel,\n+    MixtralPreTrainedModel,\n+    MixtralRMSNorm,\n+    MixtralRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+    load_balancing_loss_func,\n+)\n+from .configuration_jetmoe import JetMoeConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class JetMoeRMSNorm(MixtralRMSNorm):\n+    pass\n+\n+\n+class JetMoeRotaryEmbedding(MixtralRotaryEmbedding):\n+    pass\n+\n+\n+class JetMoeParallelExperts(nn.Module):\n+    def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n+        \"\"\"\n+        Initialize the JetMoeParallelExperts module.\n+        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's compatible with\n+        many MoE libraries, such as [Megablock](https://github.com/databricks/megablocks) and\n+        [ScatterMoE](https://github.com/shawntan/scattermoe), as well as the\n+        [MoE kernel](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py)\n+        used in vllm.\n+\n+        Args:\n+            num_experts (int):\n+                Number of experts.\n+            input_size (int):\n+                Size of the input.\n+            output_size (int):\n+                Size of the output.\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.empty(num_experts, output_size, input_size))\n+        self.num_experts = num_experts\n+        self.input_size = input_size\n+        self.output_size = output_size\n+\n+    def forward(self, inputs, expert_size):\n+        \"\"\"\n+        Forward pass of the JetMoeParallelExperts module.\n+\n+        Args:\n+            inputs (Tensor):\n+                Input tensor.\n+            expert_size:\n+                Expert size information.\n+\n+        Returns:\n+            Tensor: Output tensor.\n+        \"\"\"\n+        input_list = inputs.split(expert_size, dim=0)\n+        output_list = []\n+        for i in range(self.num_experts):\n+            output_list.append(F.linear(input_list[i], self.weight[i]))\n+        results = torch.cat(output_list, dim=0)\n+        return results\n+\n+\n+class JetMoeTopKGating(nn.Module):\n+    def __init__(self, input_size: int, num_experts: int, top_k: int):\n+        \"\"\"\n+        Initialize the top-k gating mechanism.\n+\n+        Args:\n+            input_size (`int`):\n+                Size of the input.\n+            num_experts (`int`):\n+                Number of experts.\n+            top_k (`int`):\n+                Number of top experts to select.\n+        \"\"\"\n+        super().__init__()\n+\n+        self.num_experts = num_experts\n+        self.input_size = input_size\n+        self.top_k = top_k\n+\n+        self.layer = nn.Linear(input_size, num_experts, bias=False)\n+\n+    def forward(self, hidden_states):\n+        # compute the top_k routing decision\n+        logits = self.layer(hidden_states).float()  # [batch_size x seq_len, num_experts]\n+        top_k_logits, top_k_indices = logits.topk(self.top_k, dim=1)  # [num_tokens, top_k]\n+        top_k_gates = torch.softmax(top_k_logits, dim=1).type_as(hidden_states)  # [num_tokens, top_k]\n+\n+        # compute number of input given to each expert\n+        zeros = torch.zeros(\n+            [top_k_gates.size(0), self.num_experts], dtype=top_k_gates.dtype, device=top_k_gates.device\n+        )  # [num_tokens, num_experts]\n+        gates = zeros.scatter(1, top_k_indices, 1)  # [num_tokens, num_experts]\n+        expert_size = gates.long().sum(0)  # [num_experts,]\n+        # (This cause torch.compile to fail with `torch._dynamo.exc.Unsupported: Backend compiler failed with a fake tensor exception at`)\n+        # (and `DataDependentOutputException`)\n+        expert_size = expert_size.tolist()\n+\n+        # sort and group input tokens according to expert assignment\n+        top_k_experts = top_k_indices.flatten()  # [num_tokens * top_k]\n+        _, index_sorted_experts = top_k_experts.sort(0)  # [num_tokens * top_k]\n+        batch_index = index_sorted_experts.div(self.top_k, rounding_mode=\"trunc\")  # [num_tokens * top_k]\n+\n+        # gather the gate values for grouped input tokens\n+        top_k_gates = top_k_gates.flatten()  # [num_tokens * top_k]\n+        batch_gates = top_k_gates[index_sorted_experts]  # [num_tokens * top_k]\n+\n+        return index_sorted_experts, batch_index, batch_gates, expert_size, logits\n+\n+\n+class JetMoeMoE(nn.Module):\n+    \"\"\"\n+    A Sparsely gated mixture of experts layer with 1-layer Feed-Forward networks as experts.\n+\n+    Args:\n+        config:\n+            Configuration object with model hyperparameters.\n+    \"\"\"\n+\n+    def __init__(self, config: JetMoeConfig):\n+        super().__init__()\n+\n+        self.input_size = config.hidden_size\n+        self.hidden_size = config.intermediate_size\n+        self.activation = ACT2FN[config.activation_function]\n+        self.bias = torch.nn.Parameter(torch.empty(self.input_size))\n+        self.input_linear = JetMoeParallelExperts(config.num_local_experts, self.input_size, self.hidden_size * 2)\n+        self.output_linear = JetMoeParallelExperts(config.num_local_experts, self.hidden_size, self.input_size)\n+\n+        self.router = JetMoeTopKGating(\n+            input_size=self.input_size,\n+            num_experts=config.num_local_experts,\n+            top_k=config.num_experts_per_tok,\n+        )\n+\n+    def forward(self, layer_input):\n+        \"\"\"\n+        Forward pass of the mixture of experts layer.\n+\n+        Args:\n+            layer_input (Tensor):\n+                Input tensor.\n+\n+        Returns:\n+            Tensor:\n+                Output tensor.\n+            Tensor:\n+                Router logits.\n+        \"\"\"\n+        bsz, length, emb_size = layer_input.size()\n+        layer_input = layer_input.reshape(-1, emb_size)\n+        _, batch_index, batch_gates, expert_size, router_logits = self.router(layer_input)\n+\n+        expert_inputs = layer_input[batch_index]\n+        hidden_states = self.input_linear(expert_inputs, expert_size)\n+        chunked_hidden_states = hidden_states.chunk(2, dim=-1)\n+        hidden_states = self.activation(chunked_hidden_states[0]) * chunked_hidden_states[1]\n+        expert_outputs = self.output_linear(hidden_states, expert_size)\n+\n+        expert_outputs = expert_outputs * batch_gates[:, None]\n+\n+        zeros = torch.zeros((bsz * length, self.input_size), dtype=expert_outputs.dtype, device=expert_outputs.device)\n+        layer_output = zeros.index_add(0, batch_index, expert_outputs)\n+        layer_output = layer_output.view(bsz, length, self.input_size)\n+        layer_output = layer_output + self.bias\n+        return layer_output\n+\n+\n+class JetMoeMoA(nn.Module):\n+    \"\"\"\n+    A Sparsely gated mixture of attention layer with pairs of query- and output-projections as experts.\n+\n+    Args:\n+        config:\n+            Configuration object with model hyperparameters.\n+    \"\"\"\n+\n+    def __init__(self, config: JetMoeConfig):\n+        super().__init__()\n+\n+        self.num_experts = config.num_local_experts\n+        self.input_size = config.hidden_size\n+        self.hidden_size = config.kv_channels * config.num_key_value_heads\n+        self.top_k = config.num_experts_per_tok\n+        self.bias = torch.nn.Parameter(torch.empty(self.input_size))\n+\n+        self.input_linear = JetMoeParallelExperts(self.num_experts, self.input_size, self.hidden_size)\n+        self.output_linear = JetMoeParallelExperts(self.num_experts, self.hidden_size, self.input_size)\n+\n+        self.router = JetMoeTopKGating(\n+            input_size=self.input_size,\n+            num_experts=self.num_experts,\n+            top_k=self.top_k,\n+        )\n+\n+    def map(self, layer_input):\n+        \"\"\"\n+        Map inputs to attention experts according to routing decision and compute query projection inside each experts.\n+        \"\"\"\n+\n+        # Compute gating topology\n+        bsz, length, emb_size = layer_input.size()\n+        layer_input = layer_input.reshape(-1, emb_size)  # [bsz * length, emb_size]\n+        index_sorted_experts, batch_index, batch_gates, expert_size, router_logits = self.router(layer_input)\n+        topo_info = (index_sorted_experts, batch_index, batch_gates, expert_size)\n+\n+        # Group inputs according to topology and compute query projection\n+        expert_inputs = layer_input[batch_index]  # [bsz * length * top_k, emb_size]\n+        expert_outputs = self.input_linear(expert_inputs, expert_size)  # [bsz * length * top_k, hidden_size]\n+\n+        # Ungroup queries back to original order\n+        zeros = torch.zeros(\n+            (bsz * length * self.top_k, self.hidden_size), dtype=expert_outputs.dtype, device=expert_outputs.device\n+        )\n+        layer_output = zeros.index_add(0, index_sorted_experts, expert_outputs)\n+        layer_output = layer_output.view(bsz, length, self.top_k, -1)  # [bsz, length, top_k, hidden_size]\n+        return layer_output, router_logits, topo_info\n+\n+    def reduce(self, layer_input, topo_info):\n+        \"\"\"\n+        Compute output projection inside each attention experts and merge the outputs of different experts.\n+        \"\"\"\n+        bsz, length, k, hidden_size = layer_input.size()\n+        layer_input = layer_input.reshape(-1, hidden_size)  # [bsz * length * k, hidden_size]\n+        index_sorted_experts, batch_index, batch_gates, expert_size = topo_info\n+\n+        # Group inputs according to topology and compute output projection\n+        expert_inputs = layer_input[index_sorted_experts]  # [bsz * length * top_k, hidden_size]\n+        expert_outputs = self.output_linear(expert_inputs, expert_size)  # [bsz * length * top_k, emb_size]\n+\n+        # Apply gates to attention expert outputs\n+        expert_outputs = expert_outputs * batch_gates[:, None]\n+\n+        # Ungroup and merge outputs to original order\n+        zeros = torch.zeros((bsz * length, self.input_size), dtype=expert_outputs.dtype, device=expert_outputs.device)\n+        layer_output = zeros.index_add(0, batch_index, expert_outputs)\n+        layer_output = layer_output.view(bsz, length, self.input_size)\n+        layer_output = layer_output + self.bias\n+        return layer_output\n+\n+    def forward(self, layer_input):\n+        raise NotImplementedError(\"This module doesn't support call and forward.\")\n+\n+\n+class JetMoeAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention from 'Attention Is All You Need' paper.\n+    \"\"\"\n+\n+    def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n+        \"\"\"\n+        Initialize the JetMoeAttention module.\n+\n+        Args:\n+            config:\n+                Configuration object with model hyperparameters.\n+            layer_idx:\n+                Index of the layer in the model.\n+        \"\"\"\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.is_causal = True\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.num_key_value_groups = config.num_experts_per_tok\n+        self.attention_dropout = config.attention_dropout\n+        self.kv_projection_size = config.kv_channels * config.num_key_value_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.kv_channels\n+        self.scaling = self.head_dim**-0.5\n+        self.experts = JetMoeMoA(config)\n+\n+        self.kv_proj = torch.nn.Linear(config.hidden_size, self.kv_projection_size * 2, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states, router_logits, topo_info = self.experts.map(hidden_states)\n+        key_states, value_states = self.kv_proj(hidden_states).chunk(2, dim=-1)\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.view(*input_shape, self.num_key_value_groups, -1)\n+        attn_output = self.experts.reduce(attn_output, topo_info)\n+        attn_output = attn_output.view(*input_shape, -1)\n+        return attn_output, attn_weights, router_logits\n+\n+\n+class JetMoeDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.input_layernorm = JetMoeRMSNorm(config.hidden_size)\n+        self.self_attn = JetMoeAttention(config, layer_idx)\n+        self.post_attention_layernorm = JetMoeRMSNorm(config.hidden_size)\n+        self.mlp = JetMoeMoE(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class JetMoePreTrainedModel(MixtralPreTrainedModel):\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"gate\", index=1),\n+        \"hidden_states\": JetMoeDecoderLayer,\n+        \"attentions\": OutputRecorder(JetMoeAttention, index=1),\n+    }\n+    config: JetMoeConfig\n+    base_model_prefix = \"transformer\"\n+    supports_gradient_checkpointing = False\n+    _no_split_modules = [\"JetMoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights.\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, JetMoeRMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, JetMoeParallelExperts):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+        elif isinstance(module, JetMoeMoA | JetMoeMoE):\n+            module.bias.data.zero_()\n+\n+\n+@auto_docstring\n+class JetMoeModel(MixtralModel):\n+    def __init__(self, config: JetMoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [JetMoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self._attn_implementation = config._attn_implementation\n+        self.norm = JetMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class JetMoeForCausalLM(JetMoePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = JetMoeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.aux_loss_coef = config.aux_loss_coef\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.tie_word_embeddings = config.tie_word_embeddings\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        output_router_logits: Optional[bool] = False,\n+        **kwargs,\n+    ) -> MoeCausalLMOutputWithPast:\n+        outputs: MoeModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+class JetMoeForSequenceClassification(GenericForSequenceClassification, JetMoePreTrainedModel): ...\n+\n+\n+__all__ = [\"JetMoeForCausalLM\", \"JetMoeModel\", \"JetMoePreTrainedModel\", \"JetMoeForSequenceClassification\"]"
        },
        {
            "sha": "e13474815ac76bcce7cb1927777032da4445c5ca",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 37,
            "deletions": 41,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -106,7 +106,6 @@ def __init__(self, config, hidden_size=None, intermediate_size=None):\n         self.config = config\n         self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n         self.intermediate_size = config.ffn_hidden_size if intermediate_size is None else intermediate_size\n-\n         self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n@@ -121,20 +120,14 @@ class LongcatFlashTopkRouter(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n+        self.n_routed_experts = config.n_routed_experts + (config.zero_expert_num or 0)\n+        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n \n         self.top_k = config.moe_topk\n-        self.n_routed_experts = config.n_routed_experts + (config.zero_expert_num or 0)\n         self.routed_scaling_factor = config.routed_scaling_factor\n-        self.register_buffer(\"e_score_correction_bias\", torch.zeros(self.n_routed_experts))\n         self.router_bias = getattr(config, \"router_bias\", False)\n         self.classifier = nn.Linear(config.hidden_size, self.n_routed_experts, bias=self.router_bias)\n \n-    @torch.no_grad()\n-    def get_topk_indices(self, scores):\n-        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n-        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n-        return topk_indices\n-\n     def forward(self, hidden_states):\n         hidden_states = hidden_states.view(-1, self.config.hidden_size)\n         router_logits = F.linear(hidden_states.type(torch.float32), self.classifier.weight.type(torch.float32))\n@@ -144,7 +137,40 @@ def forward(self, hidden_states):\n         topk_weights = topk_weights * self.routed_scaling_factor\n         return topk_indices, topk_weights\n \n+    @torch.no_grad()\n+    def get_topk_indices(self, scores):\n+        scores_for_choice = scores.view(-1, self.n_routed_experts) + self.e_score_correction_bias.unsqueeze(0)\n+        topk_indices = torch.topk(scores_for_choice, k=self.top_k, dim=-1, sorted=False)[1]\n+        return topk_indices\n+\n+\n+class LongcatFlashExperts(nn.ModuleList):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.intermediate_size = config.expert_ffn_hidden_size\n+        self.hidden_size = config.hidden_size\n+        self.num_experts = config.n_routed_experts + config.zero_expert_num\n+        self.zero_expert_num = config.zero_expert_num\n+\n+        self.extend(\n+            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(self.num_experts)]\n+            + [nn.Identity() for _ in range(self.zero_expert_num)]\n+        )\n+\n+    def forward(self, hidden_states, top_k_index, top_k_weights):\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n+\n+# remap config key expert_ffn_hidden_size -> moe_intermediate_size\n class LongcatFlashMoE(nn.Module):\n     \"\"\"\n     A mixed expert module containing zero compute (identity) experts.\n@@ -154,45 +180,15 @@ def __init__(self, config):\n         super().__init__()\n         self.intermediate_size = config.expert_ffn_hidden_size\n         self.config = config\n-\n-        self.experts = nn.ModuleList(\n-            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(config.n_routed_experts)]\n-            + [nn.Identity() for _ in range(config.zero_expert_num)]\n-        )\n+        self.experts = LongcatFlashExperts(config)\n \n         self.router = LongcatFlashTopkRouter(config)\n \n-    def moe(self, hidden_states: torch.Tensor, topk_indices: torch.Tensor, topk_weights: torch.Tensor):\n-        r\"\"\"\n-        CALL FOR CONTRIBUTION! I don't have time to optimise this right now, but expert weights need to be fused\n-        to not have to do a loop here (deepseek has 256 experts soooo yeah).\n-        \"\"\"\n-        final_hidden_states = torch.zeros_like(hidden_states, dtype=topk_weights.dtype)\n-        expert_mask = torch.nn.functional.one_hot(topk_indices, num_classes=len(self.experts))\n-        expert_mask = expert_mask.permute(2, 0, 1)\n-\n-        for expert_idx in range(len(self.experts)):\n-            expert = self.experts[expert_idx]\n-            mask = expert_mask[expert_idx]\n-            token_indices, weight_indices = torch.where(mask)\n-\n-            if token_indices.numel() > 0:\n-                expert_weights = topk_weights[token_indices, weight_indices]\n-                expert_input = hidden_states[token_indices]\n-                expert_output = expert(expert_input)\n-                weighted_output = expert_output * expert_weights.unsqueeze(-1)\n-                final_hidden_states.index_add_(0, token_indices, weighted_output)\n-\n-        # in original deepseek, the output of the experts are gathered once we leave this module\n-        # thus the moe module is itelsf an IsolatedParallel module\n-        # and all expert are \"local\" meaning we shard but we don't gather\n-        return final_hidden_states.type(hidden_states.dtype)\n-\n     def forward(self, hidden_states):\n         orig_shape = hidden_states.shape\n         topk_indices, topk_weights = self.router(hidden_states)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         return hidden_states\n \n "
        },
        {
            "sha": "e4891c79423dca84d7dd0f04b5c1ed04f0967fbd",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 33,
            "deletions": 12,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -32,7 +32,6 @@\n     DeepseekV3ForCausalLM,\n     DeepseekV3MLP,\n     DeepseekV3Model,\n-    DeepseekV3MoE,\n     DeepseekV3PreTrainedModel,\n     DeepseekV3RMSNorm,\n     DeepseekV3RotaryEmbedding,\n@@ -56,7 +55,8 @@ class LongcatFlashRotaryEmbedding(DeepseekV3RotaryEmbedding):\n # TODO remap config key ffn_hidden_size -> intermediate_size\n class LongcatFlashMLP(DeepseekV3MLP):\n     def __init__(self, config, hidden_size=None, intermediate_size=None):\n-        super().__init__()\n+        super().__init__(config)\n+        self.hidden_size = config.hidden_size if hidden_size is None else hidden_size\n         self.intermediate_size = config.ffn_hidden_size if intermediate_size is None else intermediate_size\n \n \n@@ -92,30 +92,51 @@ def forward(self, hidden_states):\n         return topk_indices, topk_weights\n \n \n+class LongcatFlashExperts(nn.ModuleList):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.intermediate_size = config.expert_ffn_hidden_size\n+        self.hidden_size = config.hidden_size\n+        self.num_experts = config.n_routed_experts + config.zero_expert_num\n+        self.zero_expert_num = config.zero_expert_num\n+\n+        self.extend(\n+            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(self.num_experts)]\n+            + [nn.Identity() for _ in range(self.zero_expert_num)]\n+        )\n+\n+    def forward(self, hidden_states, top_k_index, top_k_weights):\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n # remap config key expert_ffn_hidden_size -> moe_intermediate_size\n-class LongcatFlashMoE(DeepseekV3MoE):\n+class LongcatFlashMoE(nn.Module):\n     \"\"\"\n     A mixed expert module containing zero compute (identity) experts.\n     \"\"\"\n \n     def __init__(self, config):\n+        super().__init__()\n         self.intermediate_size = config.expert_ffn_hidden_size\n-        super().__init__(config)\n-        del self.gate\n-        del self.shared_experts\n-\n-        self.experts = nn.ModuleList(\n-            [LongcatFlashMLP(config, intermediate_size=self.intermediate_size) for _ in range(config.n_routed_experts)]\n-            + [nn.Identity() for _ in range(config.zero_expert_num)]\n-        )\n+        self.config = config\n+        self.experts = LongcatFlashExperts(config)\n \n         self.router = LongcatFlashTopkRouter(config)\n \n     def forward(self, hidden_states):\n         orig_shape = hidden_states.shape\n         topk_indices, topk_weights = self.router(hidden_states)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        hidden_states = self.moe(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n+        hidden_states = self.experts(hidden_states, topk_indices, topk_weights).view(*orig_shape)\n         return hidden_states\n \n "
        },
        {
            "sha": "ec703cfb92df640582ab1f516a88066de6dbdc24",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -141,6 +141,9 @@ class MiniMaxConfig(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_experts\": \"num_local_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "f12f3b817e3dd08075d95f47d8930a18da98cb3c",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 49,
            "deletions": 91,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -26,8 +26,6 @@\n import torch.nn.functional as F\n from torch import nn\n \n-from transformers.utils.generic import check_model_inputs\n-\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -46,7 +44,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.generic import OutputRecorder\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_minimax import MiniMaxConfig\n \n \n@@ -400,7 +398,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class MiniMaxBlockSparseTop2MLP(nn.Module):\n+class MiniMaxMLP(nn.Module):\n     def __init__(self, config: MiniMaxConfig):\n         super().__init__()\n         self.ffn_dim = config.intermediate_size\n@@ -418,71 +416,65 @@ def forward(self, hidden_states):\n         return current_hidden_states\n \n \n-class MiniMaxSparseMoeBlock(nn.Module):\n+class MiniMaxExperts(nn.ModuleList):\n     \"\"\"\n-    This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accommodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n+    ModuleList of experts.\n     \"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: MiniMaxConfig):\n         super().__init__()\n-        self.hidden_dim = config.hidden_size\n-        self.ffn_dim = config.intermediate_size\n-        self.num_experts = config.num_local_experts\n         self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(MiniMaxMLP(config))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n-        self.experts = nn.ModuleList([MiniMaxBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n \n-        # Jitter parameters\n+class MiniMaxSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n         self.jitter_noise = config.router_jitter_noise\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = MiniMaxExperts(config)\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n+        return top_k_index, top_k_weights.to(router_logits.dtype)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\" \"\"\"\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         if self.training and self.jitter_noise > 0:\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n         router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(router_logits)\n+        hidden_states = self.experts(hidden_states, top_k_index, top_k_weights.to(hidden_states.dtype))\n+        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return hidden_states\n \n \n class MiniMaxDecoderLayer(GradientCheckpointingLayer):\n@@ -518,58 +510,26 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            attention_mask (`torch.Tensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n         hidden_states = self.input_layernorm(hidden_states)\n         residual = hidden_states\n-\n-        # Self Attention\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n         hidden_states = residual * self.attn_alpha_factor + hidden_states * self.attn_beta_factor\n-\n-        # Fully Connected\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         residual = hidden_states\n-        hidden_states, _ = self.block_sparse_moe(hidden_states)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n         hidden_states = residual * self.mlp_alpha_factor + hidden_states * self.mlp_beta_factor\n \n         return hidden_states\n@@ -588,7 +548,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"block_sparse_moe.gate\", index=0),\n         \"hidden_states\": MiniMaxDecoderLayer,\n         \"attentions\": [MiniMaxAttention, MiniMaxLightningAttention],\n     }\n@@ -649,7 +609,6 @@ def __init__(self, config: MiniMaxConfig):\n         self.post_init()\n \n     @check_model_inputs\n-    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -658,7 +617,6 @@ def forward(\n         past_key_values: Optional[MiniMaxCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:"
        },
        {
            "sha": "3ba68fcf234bb2d682cb56ca7c1a41b42a8c8e63",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 37,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -31,7 +31,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.generic import OutputRecorder\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..mixtral.configuration_mixtral import MixtralConfig\n from ..mixtral.modeling_mixtral import (\n     MixtralAttention,\n@@ -405,66 +405,33 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n             self.attn_alpha_factor = config.full_attn_alpha_factor\n             self.attn_beta_factor = config.full_attn_beta_factor\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            attention_mask (`torch.Tensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n         hidden_states = self.input_layernorm(hidden_states)\n         residual = hidden_states\n-\n-        # Self Attention\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n         hidden_states = residual * self.attn_alpha_factor + hidden_states * self.attn_beta_factor\n-\n-        # Fully Connected\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         residual = hidden_states\n-        hidden_states, _ = self.block_sparse_moe(hidden_states)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n         hidden_states = residual * self.mlp_alpha_factor + hidden_states * self.mlp_beta_factor\n \n         return hidden_states\n@@ -473,13 +440,14 @@ def forward(\n class MiniMaxPreTrainedModel(MixtralPreTrainedModel):\n     _can_compile_fullgraph = False\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(MiniMaxSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"block_sparse_moe.gate\", index=0),\n         \"hidden_states\": MiniMaxDecoderLayer,\n         \"attentions\": [MiniMaxAttention, MiniMaxLightningAttention],\n     }\n \n \n class MiniMaxModel(MixtralModel):\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -488,7 +456,6 @@ def forward(\n         past_key_values: Optional[MiniMaxCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:"
        },
        {
            "sha": "b83da64552c5a30ecaaebbaaf7901625032ab73d",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -124,6 +124,9 @@ class MixtralConfig(PretrainedConfig):\n         \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n         \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n     }\n+    attribute_map = {\n+        \"num_experts\": \"num_local_experts\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "533cab15f64791a0bb36453211acb666f1e6e40f",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 49,
            "deletions": 62,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -27,7 +27,6 @@\n from typing import Callable, Optional, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from transformers.utils.generic import check_model_inputs\n@@ -54,7 +53,7 @@\n from .configuration_mixtral import MixtralConfig\n \n \n-class MixtralBlockSparseTop2MLP(nn.Module):\n+class MixtralMLP(nn.Module):\n     def __init__(self, config: MixtralConfig):\n         super().__init__()\n         self.ffn_dim = config.intermediate_size\n@@ -72,71 +71,65 @@ def forward(self, hidden_states):\n         return current_hidden_states\n \n \n-class MixtralSparseMoeBlock(nn.Module):\n+class MixtralExperts(nn.ModuleList):\n     \"\"\"\n-    This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accommodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n+    ModuleList of experts.\n     \"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: MixtralConfig):\n         super().__init__()\n-        self.hidden_dim = config.hidden_size\n-        self.ffn_dim = config.intermediate_size\n-        self.num_experts = config.num_local_experts\n         self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(MixtralMLP(config))\n \n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n-        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n \n-        # Jitter parameters\n+class MixtralSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n         self.jitter_noise = config.router_jitter_noise\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = MixtralExperts(config)\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n+        return top_k_index, top_k_weights.to(router_logits.dtype)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\" \"\"\"\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         if self.training and self.jitter_noise > 0:\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n         router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(router_logits)\n+        hidden_states = self.experts(hidden_states, top_k_index, top_k_weights.to(hidden_states.dtype))\n+        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return hidden_states\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n@@ -316,12 +309,9 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> torch.FloatTensor:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n@@ -332,13 +322,10 @@ def forward(\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n-\n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, _ = self.block_sparse_moe(hidden_states)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n         hidden_states = residual + hidden_states\n-\n         return hidden_states\n \n \n@@ -391,7 +378,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"block_sparse_moe.gate\", index=0),\n         \"hidden_states\": MixtralDecoderLayer,\n         \"attentions\": MixtralAttention,\n     }"
        },
        {
            "sha": "0762e4964d7514ed0a31c750c828c5c8d484c0a9",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 49,
            "deletions": 62,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -22,7 +22,6 @@\n from typing import Optional, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -133,7 +132,7 @@ def load_balancing_loss_func(\n     return overall_loss * num_experts\n \n \n-class MixtralBlockSparseTop2MLP(nn.Module):\n+class MixtralMLP(nn.Module):\n     def __init__(self, config: MixtralConfig):\n         super().__init__()\n         self.ffn_dim = config.intermediate_size\n@@ -151,71 +150,65 @@ def forward(self, hidden_states):\n         return current_hidden_states\n \n \n-class MixtralSparseMoeBlock(nn.Module):\n+class MixtralExperts(nn.ModuleList):\n     \"\"\"\n-    This implementation is\n-    strictly equivalent to standard MoE with full capacity (no\n-    dropped tokens). It's faster since it formulates MoE operations\n-    in terms of block-sparse operations to accommodate imbalanced\n-    assignments of tokens to experts, whereas standard MoE either\n-    (1) drop tokens at the cost of reduced performance or (2) set\n-    capacity factor to number of experts and thus waste computation\n-    and memory on padding.\n+    ModuleList of experts.\n     \"\"\"\n \n-    def __init__(self, config):\n+    def __init__(self, config: MixtralConfig):\n         super().__init__()\n-        self.hidden_dim = config.hidden_size\n-        self.ffn_dim = config.intermediate_size\n-        self.num_experts = config.num_local_experts\n         self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(MixtralMLP(config))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n-        self.experts = nn.ModuleList([MixtralBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n \n-        # Jitter parameters\n+class MixtralSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n         self.jitter_noise = config.router_jitter_noise\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = MixtralExperts(config)\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n+        return top_k_index, top_k_weights.to(router_logits.dtype)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\" \"\"\"\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         if self.training and self.jitter_noise > 0:\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n+        hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n         router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(router_logits)\n+        hidden_states = self.experts(hidden_states, top_k_index, top_k_weights.to(hidden_states.dtype))\n+        hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return hidden_states\n \n \n class MixtralRMSNorm(MistralRMSNorm):\n@@ -247,12 +240,9 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> torch.FloatTensor:\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n         hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             position_embeddings=position_embeddings,\n@@ -263,13 +253,10 @@ def forward(\n             **kwargs,\n         )\n         hidden_states = residual + hidden_states\n-\n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, _ = self.block_sparse_moe(hidden_states)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n         hidden_states = residual + hidden_states\n-\n         return hidden_states\n \n \n@@ -280,7 +267,7 @@ class MixtralRotaryEmbedding(MistralRotaryEmbedding):\n class MixtralPreTrainedModel(MistralPreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(MixtralSparseMoeBlock, index=1),\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"block_sparse_moe.gate\", index=0),\n         \"hidden_states\": MixtralDecoderLayer,\n         \"attentions\": MixtralAttention,\n     }"
        },
        {
            "sha": "d050ca848888b42baf112512f47ff9ddff5a7f1a",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1038,7 +1038,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Moshi\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1121,7 +1120,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->MoshiDepth\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1303,7 +1301,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Moshi\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1386,7 +1383,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Moshi\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "ab0984eb17e5ac73e5031fa7e27a1332c88772ae",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 210,
            "deletions": 614,
            "changes": 824,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -12,7 +12,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch NLLB-MoE model.\"\"\"\n \n import math\n from typing import Callable, Optional, Union\n@@ -35,91 +34,26 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n+    BaseModelOutputWithPastAndCrossAttentions,\n     MoEModelOutput,\n     MoEModelOutputWithPastAndCrossAttentions,\n     Seq2SeqMoEModelOutput,\n     Seq2SeqMoEOutput,\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, can_return_tuple, check_model_inputs\n from .configuration_nllb_moe import NllbMoeConfig\n \n \n if is_torch_flex_attn_available():\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n-\n logger = logging.get_logger(__name__)\n \n \n-####################################################\n-# This dict contains ids and associated url\n-# for the pretrained weights provided with the models\n-####################################################\n-\n-\n-# Copied from transformers.models.bart.modeling_bart.shift_tokens_right\n-def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n-    \"\"\"\n-    Shift input ids one token to the right.\n-    \"\"\"\n-    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n-    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n-    shifted_input_ids[:, 0] = decoder_start_token_id\n-\n-    if pad_token_id is None:\n-        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n-    # replace possible -100 values in labels by `pad_token_id`\n-    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n-\n-    return shifted_input_ids\n-\n-\n-def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        router_probs (`torch.Tensor`):\n-            Probability assigned to each expert per token. Shape: [batch_size, sequence_length, num_experts].\n-        expert_indices (`torch.Tensor`):\n-            Indices tensor of shape [batch_size, sequence_length] identifying the selected expert for a given token.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if router_probs is None:\n-        return 0\n-\n-    num_experts = router_probs.shape[-1]\n-\n-    # cast the expert indices to int64, otherwise one-hot encoding will fail\n-    if expert_indices.dtype != torch.int64:\n-        expert_indices = expert_indices.to(torch.int64)\n-\n-    if len(expert_indices.shape) == 2:\n-        expert_indices = expert_indices.unsqueeze(2)\n-\n-    expert_mask = torch.nn.functional.one_hot(expert_indices, num_experts)\n-\n-    # For a given token, determine if it was routed to a given expert.\n-    expert_mask = torch.max(expert_mask, axis=-2).values\n-\n-    # cast to float32 otherwise mean will fail\n-    expert_mask = expert_mask.to(torch.float32)\n-    tokens_per_group_and_expert = torch.mean(expert_mask, axis=-2)\n-\n-    router_prob_per_group_and_expert = torch.mean(router_probs, axis=-2)\n-    return torch.mean(tokens_per_group_and_expert * router_prob_per_group_and_expert) * (num_experts**2)\n-\n-\n-# Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100ScaledWordEmbedding with M2M100->NllbMoe\n class NllbMoeScaledWordEmbedding(nn.Embedding):\n     \"\"\"\n     This module overrides nn.Embeddings' forward by multiplying with embeddings scale.\n@@ -381,13 +315,11 @@ def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Long\n                 This is used later for computing router z-loss.\n         \"\"\"\n         self.input_dtype = hidden_states.dtype\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.reshape((batch_size * sequence_length), hidden_dim)\n         hidden_states = hidden_states.to(self.dtype)\n         self._cast_classifier()\n         router_logits = self.classifier(hidden_states)\n         top_1_mask, router_probs = self.route_tokens(router_logits, self.input_dtype, padding_mask)\n-        return top_1_mask, router_probs\n+        return top_1_mask, router_probs, router_logits\n \n \n class NllbMoeDenseActDense(nn.Module):\n@@ -398,7 +330,7 @@ def __init__(self, config: NllbMoeConfig, ffn_dim: int):\n         self.dropout = nn.Dropout(config.activation_dropout)\n         self.act = ACT2FN[config.activation_function]\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states: torch.Tensor):\n         hidden_states = self.fc1(hidden_states)\n         hidden_states = self.act(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n@@ -412,72 +344,52 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-class NllbMoeSparseMLP(nn.Module):\n-    r\"\"\"\n-    Implementation of the NLLB-MoE sparse MLP module.\n-    \"\"\"\n-\n-    def __init__(self, config: NllbMoeConfig, ffn_dim: int, expert_class: nn.Module = NllbMoeDenseActDense):\n+class NllbMoeExperts(nn.ModuleDict):\n+    def __init__(self, config: NllbMoeConfig, ffn_dim: int):\n         super().__init__()\n-        self.router = NllbMoeTop2Router(config)\n-        self.moe_token_dropout = config.moe_token_dropout\n-        self.token_dropout = nn.Dropout(self.moe_token_dropout)\n         self.num_experts = config.num_experts\n-\n-        self.experts = nn.ModuleDict()\n         for idx in range(self.num_experts):\n-            self.experts[f\"expert_{idx}\"] = expert_class(config, ffn_dim)\n+            self[f\"expert_{idx}\"] = NllbMoeDenseActDense(config, ffn_dim)\n+        self.moe_token_dropout = config.moe_token_dropout\n+        self.token_dropout = nn.Dropout(self.moe_token_dropout)\n \n-    def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Tensor] = False):\n-        r\"\"\"\n-        The goal of this forward pass is to have the same number of operation as the equivalent `NllbMoeDenseActDense`\n-        (mlp) layer. This means that all of the hidden states should be processed at most twice ( since we are using a\n-        top_2 gating mechanism). This means that we keep the complexity to O(batch_size x sequence_length x hidden_dim)\n-        instead of O(num_experts x batch_size x sequence_length x hidden_dim).\n+    def forward(self, hidden_states: torch.Tensor, router_mask: torch.Tensor, router_probs: torch.Tensor):\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(router_mask, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        1- Get the `router_probs` from the `router`. The shape of the `router_mask` is `(batch_size X sequence_length,\n-        num_expert)` and corresponds to the boolean version of the `router_probs`. The inputs are masked using the\n-        `router_mask`.\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[f\"expert_{expert_idx[0]}\"](current_state) * router_probs[top_x, idx, None]\n+            if self.moe_token_dropout > 0:\n+                if self.training:\n+                    current_hidden_states = self.token_dropout(current_hidden_states)\n+                else:\n+                    current_hidden_states *= 1 - self.moe_token_dropout\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n-        2- Dispatch the hidden_states to its associated experts. The router probabilities are used to weight the\n-        contribution of each experts when updating the masked hidden states.\n \n-        Args:\n-            hidden_states (`torch.Tensor` of shape `(batch_size, sequence_length, hidden_dim)`):\n-                The hidden states\n-            padding_mask (`torch.Tensor`, *optional*, defaults to `False`):\n-                Attention mask. Can be in the causal form or not.\n+class NllbMoeSparseMLP(nn.Module):\n+    r\"\"\"\n+    Implementation of the NLLB-MoE sparse MLP module.\n+    \"\"\"\n \n-        Returns:\n-            hidden_states (`torch.Tensor` of shape `(batch_size, sequence_length, hidden_dim)`):\n-                Updated hidden states\n-            router_logits (`torch.Tensor` of shape `(batch_size, sequence_length, num_experts)`):\n-                Needed for computing the loss\n+    def __init__(self, config: NllbMoeConfig, ffn_dim: int):\n+        super().__init__()\n+        self.router = NllbMoeTop2Router(config)\n+        self.num_experts = config.num_experts\n+        self.experts = NllbMoeExperts(config, ffn_dim)\n \n-        \"\"\"\n+    def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Tensor] = None):\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        top_1_mask, router_probs, _ = self.router(hidden_states, padding_mask)\n+        hidden_states = self.experts(hidden_states, top_1_mask, router_probs)\n+        return hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n-        top_1_mask, router_probs = self.router(hidden_states, padding_mask)\n-        router_mask = router_probs.bool()\n-        hidden_states = hidden_states.reshape((batch_size * sequence_length), hidden_dim)\n-        masked_hidden_states = torch.einsum(\"bm,be->ebm\", hidden_states, router_mask)\n-        for idx, expert in enumerate(self.experts.values()):\n-            token_indices = router_mask[:, idx]\n-            combining_weights = router_probs[token_indices, idx]\n-            expert_output = expert(masked_hidden_states[idx, token_indices])\n-            if self.moe_token_dropout > 0:\n-                if self.training:\n-                    expert_output = self.token_dropout(expert_output)\n-                else:\n-                    expert_output *= 1 - self.moe_token_dropout\n-            masked_hidden_states[idx, token_indices] = torch.einsum(\"b,be->be\", combining_weights, expert_output)\n-        hidden_states = masked_hidden_states.sum(dim=0).reshape(batch_size, sequence_length, hidden_dim)\n-\n-        top_1_expert_index = torch.argmax(top_1_mask, dim=-1)\n-        return hidden_states, (router_probs, top_1_expert_index)\n \n-\n-# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n def eager_attention_forward(\n     module: nn.Module,\n     query: torch.Tensor,\n@@ -504,7 +416,6 @@ def eager_attention_forward(\n     return attn_output, attn_weights\n \n \n-# Copied from transformers.models.musicgen.modeling_musicgen.MusicgenAttention with Musicgen->NllbMoe,key_value_states->encoder_hidden_states\n class NllbMoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -545,31 +456,19 @@ def __init__(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        key_value_states: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n         cache_position: Optional[torch.Tensor] = None,\n-        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n-        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n         **kwargs: Unpack[FlashAttentionKwargs],\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n-\n-        # if encoder_hidden_states are provided this layer is used as a cross-attention layer\n-        # for the decoder\n-        is_cross_attention = encoder_hidden_states is not None\n-\n-        # determine input shapes\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        is_cross_attention = key_value_states is not None\n         bsz, tgt_len = hidden_states.shape[:-1]\n-        src_len = encoder_hidden_states.shape[1] if is_cross_attention else tgt_len\n-\n+        src_len = key_value_states.shape[1] if is_cross_attention else tgt_len\n         q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n         kv_input_shape = (bsz, src_len, -1, self.head_dim)\n \n-        # get query proj\n         query_states = self.q_proj(hidden_states).view(*q_input_shape).transpose(1, 2)\n-\n         is_updated = False\n         if past_key_values is not None:\n             if isinstance(past_key_values, EncoderDecoderCache):\n@@ -582,7 +481,7 @@ def forward(\n             else:\n                 curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_values is not None and is_updated:\n             # reuse k,v, cross_attentions\n             key_states = curr_past_key_value.layers[self.layer_idx].keys\n@@ -613,18 +512,16 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.dropout,\n             scaling=self.scaling,\n-            output_attentions=output_attentions,\n             **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.out_proj(attn_output)\n-\n         return attn_output, attn_weights\n \n \n class NllbMoeEncoderLayer(GradientCheckpointingLayer):\n-    def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n+    def __init__(self, config: NllbMoeConfig, is_sparse: bool = False, layer_idx: int = 0):\n         super().__init__()\n         self.embed_dim = config.d_model\n         self.is_sparse = is_sparse\n@@ -633,6 +530,7 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n             num_heads=config.encoder_attention_heads,\n             dropout=config.attention_dropout,\n             config=config,\n+            layer_idx=layer_idx,\n         )\n         self.attn_dropout = nn.Dropout(config.dropout)\n         self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n@@ -644,61 +542,28 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False):\n         self.ff_dropout = nn.Dropout(config.activation_dropout)\n \n     def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n-        output_attentions: bool = False,\n-        output_router_logits: bool = False,\n+        self, hidden_states: torch.Tensor, attention_mask: torch.Tensor, **kwargs: Unpack[TransformersKwargs]\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`):\n-                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very\n-                large negative values.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n-            hidden_states=hidden_states,\n-            attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-        )\n+        hidden_states, _ = self.self_attn(hidden_states, attention_mask=attention_mask, **kwargs)\n         hidden_states = self.attn_dropout(hidden_states)\n         hidden_states = residual + hidden_states\n-\n         residual = hidden_states\n \n         hidden_states = self.ff_layer_norm(hidden_states)\n         if self.is_sparse:\n-            hidden_states, router_states = self.ffn(hidden_states, attention_mask)\n+            hidden_states = self.ffn(hidden_states, attention_mask)\n         else:\n-            # router_states set to None to track which layers have None gradients.\n-            hidden_states, router_states = self.ffn(hidden_states), None\n-\n+            hidden_states = self.ffn(hidden_states)\n         hidden_states = self.ff_dropout(hidden_states)\n-\n         hidden_states = residual + hidden_states\n-\n         if hidden_states.dtype == torch.float16 and (\n             torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()\n         ):\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_states,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class NllbMoeDecoderLayer(GradientCheckpointingLayer):\n@@ -735,95 +600,61 @@ def __init__(self, config: NllbMoeConfig, is_sparse: bool = False, layer_idx: Op\n         self.ff_layer_norm = nn.LayerNorm(config.d_model)\n         self.ff_dropout = nn.Dropout(config.activation_dropout)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`):\n-                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very\n-                large negative values.\n-            encoder_hidden_states (`torch.FloatTensor`):\n-                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n-            encoder_attention_mask (`torch.FloatTensor`):\n-                encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\n-                very large negative values.\n-            past_key_values (`Cache`):\n-                cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n         residual = hidden_states\n         hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         hidden_states = self.attn_dropout(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        # Cross-Attention Block\n-        cross_attn_weights = None\n         if encoder_hidden_states is not None:\n             residual = hidden_states\n             hidden_states = self.cross_attention_layer_norm(hidden_states)\n \n-            hidden_states, cross_attn_weights = self.cross_attention(\n+            hidden_states, _ = self.cross_attention(\n                 hidden_states=hidden_states,\n-                encoder_hidden_states=encoder_hidden_states,\n+                key_value_states=encoder_hidden_states,\n                 past_key_values=past_key_values,\n                 attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n             hidden_states = self.attn_dropout(hidden_states)\n             hidden_states = residual + hidden_states\n \n-        # Fully Connected\n         residual = hidden_states\n-\n         hidden_states = self.ff_layer_norm(hidden_states)\n         if self.is_sparse:\n-            hidden_states, router_states = self.ffn(hidden_states, attention_mask)\n+            hidden_states = self.ffn(hidden_states, attention_mask)\n         else:\n-            hidden_states, router_states = self.ffn(hidden_states), None\n+            hidden_states = self.ffn(hidden_states)\n \n         hidden_states = self.ff_dropout(hidden_states)\n-\n         hidden_states = residual + hidden_states\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights, cross_attn_weights)\n-\n-        if output_router_logits:\n-            outputs += (router_states,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -856,16 +687,11 @@ def _init_weights(self, module: nn.Module):\n \n \n class NllbMoeEncoder(NllbMoePreTrainedModel):\n-    \"\"\"\n-    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n-    [`NllbMoeEncoderLayer`].\n-\n-    Args:\n-        config:\n-            NllbMoeConfig\n-        embed_tokens (nn.Embedding):\n-            output embedding\n-    \"\"\"\n+    _can_record_outputs = {\n+        \"hidden_states\": NllbMoeEncoderLayer,\n+        \"router_logits\": OutputRecorder(NllbMoeTop2Router, index=2),\n+        \"attentions\": NllbMoeAttention,\n+    }\n \n     def __init__(self, config: NllbMoeConfig, embed_tokens: Optional[nn.Embedding] = None):\n         super().__init__(config)\n@@ -894,75 +720,21 @@ def __init__(self, config: NllbMoeConfig, embed_tokens: Optional[nn.Embedding] =\n         self.layers = nn.ModuleList()\n         for i in range(config.encoder_layers):\n             is_sparse = (i + 1) % sparse_step == 0 if sparse_step > 0 else False\n-            self.layers.append(NllbMoeEncoderLayer(config, is_sparse))\n+            self.layers.append(NllbMoeEncoderLayer(config, is_sparse, layer_idx=i))\n \n         self.layer_norm = nn.LayerNorm(config.d_model)\n-\n         self.gradient_checkpointing = False\n-        # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -977,51 +749,17 @@ def forward(\n             inputs_embeds,\n         )\n \n-        encoder_states = () if output_hidden_states else None\n-        all_router_probs = () if output_router_logits else None\n-        all_attentions = () if output_attentions else None\n-\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n+        for encoder_layer in self.layers:\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n             if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n-                layer_outputs = (None, None, None)\n+                continue\n             else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                )\n-\n-                hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_probs += (layer_outputs[-1],)\n+                hidden_states = encoder_layer(hidden_states, attention_mask, **kwargs)\n \n         last_hidden_state = self.layer_norm(hidden_states)\n+        return MoEModelOutput(last_hidden_state=last_hidden_state)\n \n-        if output_hidden_states:\n-            encoder_states += (last_hidden_state,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v for v in [last_hidden_state, encoder_states, all_attentions, all_router_probs] if v is not None\n-            )\n-\n-        return MoEModelOutput(\n-            last_hidden_state=last_hidden_state,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-            router_probs=all_router_probs,\n-        )\n-\n-    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n     def _update_full_mask(\n         self,\n         attention_mask: Union[torch.Tensor, None],\n@@ -1054,6 +792,13 @@ class NllbMoeDecoder(NllbMoePreTrainedModel):\n             output embedding\n     \"\"\"\n \n+    _can_record_outputs = {\n+        \"hidden_states\": NllbMoeDecoderLayer,\n+        \"attentions\": OutputRecorder(NllbMoeAttention, layer_name=\"self_attn\", index=1),\n+        \"router_logits\": OutputRecorder(NllbMoeTop2Router, index=2),\n+        \"cross_attentions\": OutputRecorder(NllbMoeAttention, layer_name=\"cross_attention\", index=1),\n+    }\n+\n     def __init__(self, config: NllbMoeConfig, embed_tokens: Optional[nn.Embedding] = None):\n         super().__init__(config)\n         self.dropout = config.dropout\n@@ -1087,6 +832,8 @@ def __init__(self, config: NllbMoeConfig, embed_tokens: Optional[nn.Embedding] =\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @auto_docstring\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1096,96 +843,18 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = True,\n-    ):\n-        r\"\"\"\n-        Args:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n-                provide it.\n-\n-                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-                [`PreTrainedTokenizer.__call__`] for details.\n-\n-                [What are input IDs?](../glossary#input-ids)\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                of the decoder.\n-            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n-                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n-                selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-                It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n-                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n-                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n-                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-\n-        # retrieve input_ids and inputs_embeds\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            input_shape = input_ids.size()\n-            input_ids = input_ids.view(-1, input_shape[-1])\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n-\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n+        input_shape = inputs_embeds.size()[:-1]\n \n         # initialize `past_key_values`\n         if use_cache and past_key_values is None:\n             past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n+\n         if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n@@ -1213,80 +882,35 @@ def forward(\n         positions = positions.to(inputs_embeds.device)\n \n         hidden_states = inputs_embeds + positions\n-\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_probs = () if output_router_logits else None\n-        all_cross_attentions = () if output_attentions else None\n-\n         synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for idx, decoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n             dropout_probability = torch.rand([])\n-\n             skip_the_layer = self.training and dropout_probability < self.layerdrop\n             if not skip_the_layer or synced_gpus:\n-                # under fsdp or deepspeed zero3 all gpus must run in sync\n-                layer_outputs = decoder_layer(\n+                hidden_states = decoder_layer(\n                     hidden_states,\n                     attention_mask,\n                     encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                     encoder_attention_mask=encoder_attention_mask,\n                     past_key_values=past_key_values,\n                     use_cache=use_cache,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n                     cache_position=cache_position,\n+                    **kwargs,\n                 )\n \n-                hidden_states = layer_outputs[0]\n-\n             if skip_the_layer:\n                 continue\n \n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-                all_cross_attentions += (layer_outputs[2],)\n-\n-            if output_router_logits:\n-                all_router_probs += (layer_outputs[-1],)\n-\n-        hidden_states = self.layer_norm(hidden_states)\n+        last_hidden_states = self.layer_norm(hidden_states)\n \n-        # Add last layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attns,\n-                    all_cross_attentions,\n-                    all_router_probs,\n-                ]\n-                if v is not None\n-            )\n         return MoEModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            cross_attentions=all_cross_attentions,\n-            router_probs=all_router_probs,\n+            last_hidden_state=last_hidden_states, past_key_values=past_key_values\n         )\n \n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, None],\n@@ -1324,7 +948,6 @@ def _update_causal_mask(\n \n         return attention_mask\n \n-    # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoder._update_cross_attn_mask\n     def _update_cross_attn_mask(\n         self,\n         encoder_hidden_states: Union[torch.Tensor, None],\n@@ -1392,6 +1015,7 @@ def get_encoder(self):\n         return self.encoder\n \n     @auto_docstring\n+    @can_return_tuple\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1403,87 +1027,30 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.Tensor] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], Seq2SeqMoEModelOutput]:\n-        r\"\"\"\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            NllbMoe uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If\n-            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-        Example:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, NllbMoeModel\n-\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/random-nllb-moe-2-experts\")\n-        >>> model = SwitchTransformersModel.from_pretrained(\"hf-internal-testing/random-nllb-moe-2-experts\")\n-\n-        >>> input_ids = tokenizer(\n-        ...     \"Studies have been shown that owning a dog is good for you\", return_tensors=\"pt\"\n-        ... ).input_ids  # Batch size 1\n-        >>> decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"pt\").input_ids  # Batch size 1\n-\n-        >>> # preprocess: Prepend decoder_input_ids with start token which is pad token for NllbMoeModel\n-        >>> decoder_input_ids = model._shift_right(decoder_input_ids)\n-\n-        >>> # forward pass\n-        >>> outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n-        >>> last_hidden_states = outputs.last_hidden_state\n-        ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 inputs_embeds=inputs_embeds,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                output_router_logits=output_router_logits,\n-                return_dict=return_dict,\n-            )\n-        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n-        elif return_dict and not isinstance(encoder_outputs, MoEModelOutput):\n-            encoder_outputs = MoEModelOutput(\n-                last_hidden_state=encoder_outputs[0],\n-                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n-                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n-                router_probs=encoder_outputs[3] if len(encoder_outputs) > 3 else None,\n+                **kwargs,\n             )\n \n         # decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\n         decoder_outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n-            encoder_hidden_states=encoder_outputs[0],\n+            encoder_hidden_states=encoder_outputs.last_hidden_state,\n             encoder_attention_mask=attention_mask,\n             past_key_values=past_key_values,\n             inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            return decoder_outputs + encoder_outputs\n-\n         return Seq2SeqMoEModelOutput(\n             past_key_values=decoder_outputs.past_key_values,\n             cross_attentions=decoder_outputs.cross_attentions,\n@@ -1493,10 +1060,108 @@ def forward(\n             decoder_hidden_states=decoder_outputs.hidden_states,\n             encoder_attentions=encoder_outputs.attentions,\n             decoder_attentions=decoder_outputs.attentions,\n-            encoder_router_logits=encoder_outputs.router_probs,\n-            decoder_router_logits=decoder_outputs.router_probs,\n+            encoder_router_logits=encoder_outputs.router_logits,\n+            decoder_router_logits=decoder_outputs.router_logits,\n+        )\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n         )\n \n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n+def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n+    \"\"\"\n+    Shift input ids one token to the right.\n+    \"\"\"\n+    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n+    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n+    shifted_input_ids[:, 0] = decoder_start_token_id\n+\n+    if pad_token_id is None:\n+        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n+    # replace possible -100 values in labels by `pad_token_id`\n+    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n+\n+    return shifted_input_ids\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1511,7 +1176,7 @@ def __init__(self, config: NllbMoeConfig):\n         super().__init__(config)\n         self.model = NllbMoeModel(config)\n         self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n-\n+        self.num_experts = config.num_experts\n         self.router_z_loss_coef = config.router_z_loss_coef\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         # Initialize weights and apply final processing\n@@ -1523,6 +1188,7 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1536,50 +1202,10 @@ def forward(\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], Seq2SeqMoEOutput]:\n-        r\"\"\"\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-            NllbMoe uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If\n-            `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Example Translation:\n-\n-        ```python\n-        >>> from transformers import AutoTokenizer, NllbMoeForConditionalGeneration\n-\n-        >>> model = NllbMoeForConditionalGeneration.from_pretrained(\"facebook/nllb-moe-54b\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-moe-54b\")\n-\n-        >>> text_to_translate = \"Life is like a box of chocolates\"\n-        >>> model_inputs = tokenizer(text_to_translate, return_tensors=\"pt\")\n-\n-        >>> # translate to French\n-        >>> gen_tokens = model.generate(**model_inputs, forced_bos_token_id=tokenizer.get_lang_id(\"eng_Latn\"))\n-        >>> print(tokenizer.batch_decode(gen_tokens, skip_special_tokens=True))\n-        ```\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n@@ -1599,11 +1225,9 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             decoder_inputs_embeds=decoder_inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n         lm_logits = self.lm_head(outputs[0])\n \n@@ -1616,36 +1240,21 @@ def forward(\n             # todo check in the config if router loss enables\n \n             if output_router_logits:\n-                encoder_router_logits = outputs[-1]\n-                decoder_router_logits = outputs[3 if output_attentions else 4]\n-\n-                # Compute the router loss (z_loss + auxiliary loss) for each router in the encoder and decoder\n-                encoder_router_logits, encoder_expert_indexes = self._unpack_router_logits(encoder_router_logits)\n-                encoder_aux_loss = load_balancing_loss_func(encoder_router_logits, encoder_expert_indexes)\n-\n-                decoder_router_logits, decoder_expert_indexes = self._unpack_router_logits(decoder_router_logits)\n-                decoder_aux_loss = load_balancing_loss_func(decoder_router_logits, decoder_expert_indexes)\n+                encoder_router_logits = outputs.encoder_router_logits\n+                decoder_router_logits = outputs.decoder_router_logits\n+                encoder_aux_loss = load_balancing_loss_func(\n+                    encoder_router_logits, self.num_experts, top_k=2, attention_mask=attention_mask\n+                )\n+                decoder_aux_loss = load_balancing_loss_func(\n+                    decoder_router_logits, self.num_experts, top_k=2, attention_mask=decoder_attention_mask\n+                )\n \n             loss = loss_fct(lm_logits.view(-1, lm_logits.size(-1)), labels.view(-1))\n \n             if output_router_logits and labels is not None:\n                 aux_loss = self.router_aux_loss_coef * (encoder_aux_loss + decoder_aux_loss)\n                 loss = loss + aux_loss\n \n-        output = (loss,) if loss is not None else ()\n-        if not return_dict:\n-            output += (lm_logits,)\n-            if output_router_logits:  # only return the loss if they are not None\n-                output += (\n-                    encoder_aux_loss,\n-                    decoder_aux_loss,\n-                    *outputs[1:],\n-                )\n-            else:\n-                output += outputs[1:]\n-\n-            return output\n-\n         return Seq2SeqMoEOutput(\n             loss=loss,\n             logits=lm_logits,\n@@ -1662,19 +1271,6 @@ def forward(\n             decoder_router_logits=outputs.decoder_router_logits,\n         )\n \n-    def _unpack_router_logits(self, router_outputs):\n-        total_router_logits = []\n-        total_expert_indexes = []\n-        for router_output in router_outputs:\n-            if router_output is not None:\n-                router_logits, expert_indexes = router_output\n-                total_router_logits.append(router_logits)\n-                total_expert_indexes.append(expert_indexes)\n-\n-        total_router_logits = torch.cat(total_router_logits, dim=1) if len(total_router_logits) > 0 else None\n-        total_expert_indexes = torch.stack(total_expert_indexes, dim=1) if len(total_expert_indexes) > 0 else None\n-        return total_router_logits, total_expert_indexes\n-\n \n __all__ = [\n     \"NllbMoeForConditionalGeneration\","
        },
        {
            "sha": "fac2604bbf7e0cb8f105ae2c8cbdd160992e6b99",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 254,
            "deletions": 687,
            "changes": 941,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/olmoe/modular_olmoe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_olmoe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n # You may obtain a copy of the License at\n@@ -9,123 +15,29 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch OLMoE model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n-import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_olmoe import OlmoeConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n-def load_balancing_loss_func(\n-    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        gate_logits:\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n-        return 0\n-\n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n-\n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n-\n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n-\n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n-\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n-            .to(compute_device)\n-        )\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n-\n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n-    return overall_loss * num_experts\n-\n-\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class OlmoeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-5):\n         \"\"\"\n@@ -146,7 +58,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmoe\n class OlmoeRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n@@ -183,15 +94,29 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n+class OlmoeMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -219,24 +144,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.olmo.modeling_olmo.OlmoMLP with Olmo->Olmoe\n-class OlmoeMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, x):\n-        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-        return down_proj\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -249,73 +156,87 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class OlmoeAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-        self.q_norm = OlmoeRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.k_norm = OlmoeRMSNorm(\n-            (self.hidden_size // self.num_heads) * self.num_key_value_heads, eps=config.rms_norm_eps\n+            (config.hidden_size // config.num_attention_heads) * config.num_key_value_heads, eps=config.rms_norm_eps\n         )\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         query_states = self.q_norm(self.q_proj(hidden_states))\n         key_states = self.k_norm(self.k_proj(hidden_states))\n         value_states = self.v_proj(hidden_states)\n \n-        if self.config.clip_qkv is not None:\n+        if self.config.clip_qkv is not None:  # Diff with llama\n             query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n             key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n             value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n+        query_states = query_states.view(*hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(*hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(*hidden_shape).transpose(1, 2)\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n@@ -324,253 +245,61 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-\n-class OlmoeFlashAttention2(OlmoeAttention):\n-    \"\"\"\n-    OLMoE flash attention module. This module inherits from `OlmoeAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_norm(self.q_proj(hidden_states))\n-        key_states = self.k_norm(self.k_proj(hidden_states))\n-        value_states = self.v_proj(hidden_states)\n-        if self.config.clip_qkv is not None:\n-            query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (OlmoeRMSNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-class OlmoeSdpaAttention(OlmoeAttention):\n+class OlmoeExperts(nn.ModuleList):\n     \"\"\"\n-    OLMoE attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `OlmoeAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n+    ModuleList of experts.\n     \"\"\"\n \n-    # Adapted from OlmoeAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"OlmoeModel is using OlmoeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_norm(self.q_proj(hidden_states))\n-        key_states = self.k_norm(self.k_proj(hidden_states))\n-        value_states = self.v_proj(hidden_states)\n-\n-        if self.config.clip_qkv is not None:\n-            query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-            value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        # if attention_mask is not None and cache_position is not None:\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = causal_mask is None and q_len > 1\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None\n-\n+    def __init__(self, config):\n+        super().__init__()\n+        for _ in range(config.num_experts):\n+            self.append(OlmoeMLP(config))\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n \n-OLMOE_ATTENTION_CLASSES = {\n-    \"eager\": OlmoeAttention,\n-    \"flash_attention_2\": OlmoeFlashAttention2,\n-    \"sdpa\": OlmoeSdpaAttention,\n-}\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n \n class OlmoeSparseMoeBlock(nn.Module):\n@@ -580,54 +309,32 @@ def __init__(self, config):\n         self.top_k = config.num_experts_per_tok\n         self.norm_topk_prob = config.norm_topk_prob\n         self.gate = nn.Linear(config.hidden_size, self.num_experts, bias=False)\n-        self.experts = nn.ModuleList([OlmoeMLP(config) for _ in range(self.num_experts)])\n+        self.experts = OlmoeExperts(config)\n+\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        if self.norm_topk_prob:\n+            top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n+        top_k_weights = top_k_weights.to(hidden_states.dtype)\n+        return top_k_index, top_k_weights\n \n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n         router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n-        if self.norm_topk_prob:\n-            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-\n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n+        final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n+            batch_size, sequence_length, hidden_dim\n         )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be selected\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        return final_hidden_states\n \n \n class OlmoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: OlmoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n-        self.self_attn = OLMOE_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n-\n+        self.self_attn = OlmoeAttention(config=config, layer_idx=layer_idx)\n         self.mlp = OlmoeSparseMoeBlock(config)\n         self.input_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -639,49 +346,19 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*):\n-                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n-                query_sequence_length, key_sequence_length)` if default attention is used.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n@@ -692,18 +369,9 @@ def forward(\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, router_logits = self.mlp(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -715,21 +383,13 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"gate\", index=1),\n+        \"hidden_states\": OlmoeDecoderLayer,\n+        \"attentions\": OlmoeAttention,\n+    }\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n-\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, OlmoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n+    _supports_attention_backend = True\n \n \n @auto_docstring\n@@ -738,7 +398,6 @@ def __init__(self, config: OlmoeConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.vocab_size\n-\n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n         self.layers = nn.ModuleList(\n             [OlmoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n@@ -750,46 +409,28 @@ def __init__(self, config: OlmoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[tuple, MoeModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        if use_cache and past_key_values is None:\n-            past_key_values = DynamicCache(config=self.config)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -798,200 +439,141 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(  # diff with mixtral: no sliding\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n-        # embed positions\n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n+        hidden_states = self.norm(hidden_states)\n \n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n \n-            if output_router_logits and layer_outputs[-1] is not None:\n-                all_router_logits += (layer_outputs[-1],)\n \n-        hidden_states = self.norm(hidden_states)\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n \n-        if not return_dict:\n-            return tuple(\n-                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n-            )\n-        return MoeModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: torch.Tensor,\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            device=device,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        device: torch.device,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n+@auto_docstring\n class OlmoeForCausalLM(OlmoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)\n         self.model = OlmoeModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1002,14 +584,11 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n-    ) -> Union[tuple, MoeCausalLMOutputWithPast]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1033,31 +612,25 @@ def forward(\n         'Hey, are you conscious? Can you talk to me?\\nIâ€™m not sure if youâ€™re conscious of this, but Iâ€™m'\n         ```\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        outputs = self.model(\n+        outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n-        hidden_states = outputs[0]\n+        hidden_states = outputs.last_hidden_state\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1069,20 +642,14 @@ def forward(\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1],\n+                outputs.router_logits,\n                 self.num_experts,\n                 self.num_experts_per_tok,\n                 attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n \n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            if output_router_logits:\n-                output = (aux_loss,) + output\n-            return (loss,) + output if loss is not None else output\n-\n         return MoeCausalLMOutputWithPast(\n             loss=loss,\n             aux_loss=aux_loss,"
        },
        {
            "sha": "0949344b5834f0f260e03627b1b52bcf71499347",
            "filename": "src/transformers/models/olmoe/modular_olmoe.py",
            "status": "added",
            "additions": 293,
            "deletions": 0,
            "changes": 293,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,293 @@\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch OLMoE model.\"\"\"\n+\n+from typing import Callable, Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder\n+from ..gemma.modeling_gemma import GemmaMLP\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaRMSNorm,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from ..mixtral.modeling_mixtral import MixtralExperts, MixtralForCausalLM, MixtralModel\n+from .configuration_olmoe import OlmoeConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class OlmoeRMSNorm(LlamaRMSNorm):\n+    def __init__(self, hidden_size, eps=1e-5):\n+        super().__init__(hidden_size, eps)\n+\n+\n+class OlmoeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class OlmoeMLP(GemmaMLP):\n+    pass\n+\n+\n+class OlmoeAttention(LlamaAttention):\n+    def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.q_norm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.k_norm = OlmoeRMSNorm(\n+            (config.hidden_size // config.num_attention_heads) * config.num_key_value_heads, eps=config.rms_norm_eps\n+        )\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states))\n+        key_states = self.k_norm(self.k_proj(hidden_states))\n+        value_states = self.v_proj(hidden_states)\n+\n+        if self.config.clip_qkv is not None:  # Diff with llama\n+            query_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n+            key_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n+            value_states.clamp_(min=-self.config.clip_qkv, max=self.config.clip_qkv)\n+\n+        query_states = query_states.view(*hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(*hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(*hidden_shape).transpose(1, 2)\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),  # main diff with Llama\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class OlmoeExperts(MixtralExperts, nn.ModuleList):\n+    def __init__(self, config):\n+        nn.ModuleList.__init__(self)\n+        for _ in range(config.num_experts):\n+            self.append(OlmoeMLP(config))\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+\n+class OlmoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+        self.gate = nn.Linear(config.hidden_size, self.num_experts, bias=False)\n+        self.experts = OlmoeExperts(config)\n+\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n+        top_k_weights, top_k_index = torch.topk(routing_weights, self.top_k, dim=-1)\n+        if self.norm_topk_prob:\n+            top_k_weights /= top_k_weights.sum(dim=-1, keepdim=True)\n+        top_k_weights = top_k_weights.to(hidden_states.dtype)\n+        return top_k_index, top_k_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states)\n+        top_k_index, top_k_weights = self.route_tokens_to_experts(hidden_states, router_logits)\n+        final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n+            batch_size, sequence_length, hidden_dim\n+        )\n+        return final_hidden_states\n+\n+\n+class OlmoeDecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: OlmoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = OlmoeAttention(config=config, layer_idx=layer_idx)\n+        self.mlp = OlmoeSparseMoeBlock(config)\n+        self.input_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+\n+@auto_docstring\n+class OlmoePreTrainedModel(PreTrainedModel):\n+    config: OlmoeConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"OlmoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"gate\", index=1),\n+        \"hidden_states\": OlmoeDecoderLayer,\n+        \"attentions\": OlmoeAttention,\n+    }\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+\n+\n+@auto_docstring\n+class OlmoeModel(MixtralModel):\n+    def __init__(self, config: OlmoeConfig):\n+        super().__init__(config)\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [OlmoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = OlmoeRotaryEmbedding(config=config)\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(  # diff with mixtral: no sliding\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class OlmoeForCausalLM(MixtralForCausalLM, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = OlmoeModel(config)\n+        self.num_experts = config.num_experts\n+\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, OlmoeForCausalLM\n+\n+        >>> model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        'Hey, are you conscious? Can you talk to me?\\nIâ€™m not sure if youâ€™re conscious of this, but Iâ€™m'\n+        ```\n+        \"\"\"\n+        return super().forward(**super_kwargs)\n+\n+\n+__all__ = [\"OlmoeForCausalLM\", \"OlmoeModel\", \"OlmoePreTrainedModel\"]"
        },
        {
            "sha": "74f0e064c782d64c6c2213c3ba0424ee34ab0397",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 307,
            "deletions": 740,
            "changes": 1047,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/phimoe/modular_phimoe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_phimoe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Microsoft and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -13,135 +19,28 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\"\"\"PyTorch Phimoe model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n-from ...modeling_flash_attention_utils import is_flash_attn_available\n-from ...modeling_layers import (\n-    GenericForSequenceClassification,\n-    GradientCheckpointingLayer,\n-)\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_phimoe import PhimoeConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-# This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph.\n-# It means that the function will not be traced through and simply appear as a node in the graph.\n-_prepare_4d_causal_attention_mask = torch.fx.wrap(_prepare_4d_causal_attention_mask)\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.qwen2_moe.modeling_qwen2_moe.load_balancing_loss_func\n-def load_balancing_loss_func(\n-    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        gate_logits:\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n-        return 0\n-\n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n-\n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n-\n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n-\n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n-\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n-            .to(compute_device)\n-        )\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n-\n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n-    return overall_loss * num_experts\n-\n-\n class PhimoeRotaryEmbedding(nn.Module):\n     def __init__(\n         self,\n@@ -158,8 +57,9 @@ def __init__(\n             self.rope_type = \"default\"\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-    def forward(self, x, seq_len=None):\n+    def forward(self, x, position_ids):\n         mscale = None\n+        seq_len = torch.max(position_ids) + 1\n         if self.config.rope_scaling and seq_len:\n             mscale = (\n                 self.long_mscale\n@@ -168,32 +68,35 @@ def forward(self, x, seq_len=None):\n             )\n         inv_freq, attention_scaling = self.rope_init_fn(self.config, x.device, seq_len)\n         mscale = attention_scaling if mscale is None else mscale\n-        t = torch.arange(seq_len, device=x.device, dtype=torch.float32)\n-        freqs = torch.outer(t, inv_freq)\n+        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n \n-        emb = torch.cat((freqs, freqs), dim=-1)\n-        return (emb.cos() * mscale).to(x.dtype), (emb.sin() * mscale).to(x.dtype)\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * mscale\n+            sin = emb.sin() * mscale\n+        return cos.to(x.dtype), sin.to(x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n     Args:\n         q (`torch.Tensor`): The query tensor.\n         k (`torch.Tensor`): The key tensor.\n         cos (`torch.Tensor`): The cosine part of the rotary embedding.\n         sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`):\n-            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n-            used to pass offsetted position ids when working with a KV-cache.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n         unsqueeze_dim (`int`, *optional*, defaults to 1):\n             The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n             sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n@@ -204,14 +107,13 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n     Returns:\n         `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n     \"\"\"\n-    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n-    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n     q_embed = (q * cos) + (rotate_half(q) * sin)\n     k_embed = (k * cos) + (rotate_half(k) * sin)\n     return q_embed, k_embed\n \n \n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -224,305 +126,104 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class PhimoeAttention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: PhimoeConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: PhimoeConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.is_causal = True\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=self.config.attention_bias)\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n         self.k_proj = nn.Linear(\n-            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.config.attention_bias\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n         self.v_proj = nn.Linear(\n-            self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.config.attention_bias\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=self.config.attention_bias)\n-\n-    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n-        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_values is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights\n-\n-\n-class PhimoeFlashAttention2(PhimoeAttention):\n-    \"\"\"\n-    Phimoe flash attention module. This module inherits from `PhimoeAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=getattr(self.config, \"sliding_window\", None),\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-class PhimoeSdpaAttention(PhimoeAttention):\n-    \"\"\"\n-    Phimoe attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `PhimoeAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from PhimoeAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"PhimoeModel is using PhimoeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_values is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = causal_mask is None and q_len > 1\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None\n-\n-\n-PHIMOE_ATTENTION_CLASSES = {\n-    \"eager\": PhimoeAttention,\n-    \"flash_attention_2\": PhimoeFlashAttention2,\n-    \"sdpa\": PhimoeSdpaAttention,\n-}\n-\n-\n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralBlockSparseTop2MLP with Mixtral->Phimoe\n-class PhimoeBlockSparseTop2MLP(nn.Module):\n+class PhimoeMLP(nn.Module):\n     def __init__(self, config: PhimoeConfig):\n         super().__init__()\n         self.ffn_dim = config.intermediate_size\n@@ -540,7 +241,7 @@ def forward(self, hidden_states):\n         return current_hidden_states\n \n \n-class MultiplierProcessor(torch.autograd.Function):\n+class PhimoeMultiplier(torch.autograd.Function):\n     @staticmethod\n     def forward(\n         ctx,\n@@ -602,6 +303,58 @@ def backward(\n         )\n \n \n+class PhimoeExperts(nn.ModuleList):\n+    \"\"\"\n+    ModuleList of experts.\n+    \"\"\"\n+\n+    def __init__(self, config: PhimoeConfig):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(PhimoeMLP(config))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n+\n+\n+class PhimoeRouter(nn.Linear):\n+    def __init__(self, config: PhimoeConfig):\n+        super().__init__(config.hidden_size, config.num_local_experts, bias=False)\n+        self.top_k = config.num_experts_per_tok\n+        self.hidden_dim = config.hidden_size\n+        self.router_jitter_noise = config.router_jitter_noise\n+        self.input_jitter_noise = config.router_jitter_noise\n+\n+    def forward(self, hidden_states):\n+        if self.training and self.input_jitter_noise > 0:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(\n+                1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise\n+            )\n+        router_logits = super().forward(hidden_states)\n+        return router_logits\n+\n+\n def sparsemixer(scores, jitter_eps, training, top_k=2):\n     \"\"\"\n     Sparse mixer function to select top-k experts and compute multipliers.\n@@ -620,11 +373,6 @@ def sparsemixer(scores, jitter_eps, training, top_k=2):\n     Returns:\n         tuple[torch.Tensor, torch.Tensor]: Multiplier and selected experts tensors.\n     \"\"\"\n-    if top_k != 2:\n-        raise ValueError(\"top_k must be equal to 2\")\n-\n-    # first expert\n-\n     with torch.no_grad():\n         # Compute mask for sparsity\n         mask_logits_threshold, max_ind = scores.max(dim=-1, keepdim=True)\n@@ -659,7 +407,7 @@ def sparsemixer(scores, jitter_eps, training, top_k=2):\n         # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5\n         mask_for_one = torch.add(0.3333, mask_for_one, alpha=0.6667).type_as(masked_gates)\n \n-        multiplier = MultiplierProcessor.apply(\n+        multiplier = PhimoeMultiplier.apply(\n             scores,\n             multiplier_o,\n             selected_experts,\n@@ -711,7 +459,7 @@ def sparsemixer(scores, jitter_eps, training, top_k=2):\n         # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5\n         mask_for_one_top2 = torch.add(0.3333, mask_for_one_top2, alpha=0.6667).type_as(masked_gates_top2)\n \n-        multiplier_top2 = MultiplierProcessor.apply(\n+        multiplier_top2 = PhimoeMultiplier.apply(\n             scores,\n             multiplier_top2_o,\n             selected_experts_top2,\n@@ -748,141 +496,94 @@ def __init__(self, config):\n         self.ffn_dim = config.intermediate_size\n         self.num_experts = config.num_local_experts\n         self.top_k = config.num_experts_per_tok\n-        # gating\n-        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)\n-\n-        self.experts = nn.ModuleList([PhimoeBlockSparseTop2MLP(config) for _ in range(self.num_experts)])\n-\n-        # Jitter parameters\n         self.router_jitter_noise = config.router_jitter_noise\n+        self.gate = PhimoeRouter(config)\n+        self.experts = PhimoeExperts(config)\n         self.input_jitter_noise = config.input_jitter_noise\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\" \"\"\"\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        if self.training and self.input_jitter_noise > 0:\n-            hidden_states *= torch.empty_like(hidden_states).uniform_(\n-                1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise\n-            )\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        router_logits = self.gate(hidden_states)\n-\n+    def route_tokens_to_experts(self, router_logits):\n         routing_weights, selected_experts = sparsemixer(\n             router_logits,\n             jitter_eps=self.router_jitter_noise,\n             training=self.training,\n         )\n+        return routing_weights, selected_experts\n \n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        if self.training and self.input_jitter_noise > 0:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(\n+                1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise\n+            )\n \n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.reshape(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states)\n+        routing_weights, selected_experts = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n-        # Loop over all available experts in the model and perform the computation on each expert\n-        for expert_idx in range(self.num_experts):\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n \n-            if top_x.shape[0] == 0:\n-                continue\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class PhimoeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        PhimoeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n class PhimoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: PhimoeConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n \n-        self.self_attn = PHIMOE_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.self_attn = PhimoeAttention(config, layer_idx)\n \n         self.block_sparse_moe = PhimoeSparseMoeBlock(config)\n-        self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True)\n-        self.post_attention_layernorm = nn.LayerNorm(\n-            config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True\n-        )\n+        self.input_layernorm = PhimoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = PhimoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss, and\n-                should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n-        # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n-            position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n-\n-        # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+        hidden_states = self.block_sparse_moe(hidden_states)\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -894,32 +595,18 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-\n+    _supports_flex_attn = True\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n-\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"hidden_states\": PhimoeDecoderLayer,\n+        \"attentions\": PhimoeAttention,\n+    }\n \n \n @auto_docstring\n class PhimoeModel(PhimoePreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`PhimoeDecoderLayer`]\n-    Args:\n-        config: PhimoeConfig\n-    \"\"\"\n-\n     def __init__(self, config: PhimoeConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -929,15 +616,14 @@ def __init__(self, config: PhimoeConfig):\n         self.layers = nn.ModuleList(\n             [PhimoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self._attn_implementation = config._attn_implementation\n         self.norm = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True)\n         self.rotary_emb = PhimoeRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -947,31 +633,11 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n@@ -987,211 +653,128 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds\n \n-        position_embeddings = self.rotary_emb(hidden_states, seq_len=cache_position[-1] + 1)\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-            layer_outputs = decoder_layer(\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n                 hidden_states,\n+                position_embeddings=position_embeddings,\n                 attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n-\n-            if output_router_logits:\n-                all_router_logits += (layer_outputs[-1],)\n-\n         hidden_states = self.norm(hidden_states)\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n-        return MoeModelOutputWithPast(\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Phimoe. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # StaticCache\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n \n-        return causal_mask\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: PhimoeConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`PhimoeConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n-                if not is_static_sliding_cache or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n+@auto_docstring\n class PhimoeForCausalLM(PhimoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -1201,6 +784,7 @@ def __init__(self, config):\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_local_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1215,12 +799,10 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1229,35 +811,26 @@ def forward(\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n+\n         ```python\n         >>> from transformers import AutoTokenizer, PhimoeForCausalLM\n-        >>> model = PhimoeForCausalLM.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")\n+\n+        >>> model = PhimoeForCausalLM.from_pretrained(\"mistralai/Phimoe-8x7B-v0.1\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Phimoe-8x7B-v0.1\")\n+\n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n         >>> # Generate\n         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        if (\n-            use_cache\n-            and self.config.rope_scaling\n-            and cache_position is not None\n-            and cache_position[0] == self.config.original_max_position_embeddings\n-        ):\n-            logger.warning(\n-                f\"If you are not using the generate method, you may encounter nonsensical outputs after the {self.config.original_max_position_embeddings}th token, as the KV cache needs to be recomputed.\"\n-            )\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n \n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids=input_ids,\n@@ -1266,10 +839,9 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs.last_hidden_state\n@@ -1346,9 +918,4 @@ def prepare_inputs_for_generation(\n class PhimoeForSequenceClassification(GenericForSequenceClassification, PhimoePreTrainedModel): ...\n \n \n-__all__ = [\n-    \"PhimoePreTrainedModel\",\n-    \"PhimoeModel\",\n-    \"PhimoeForCausalLM\",\n-    \"PhimoeForSequenceClassification\",\n-]\n+__all__ = [\"PhimoePreTrainedModel\", \"PhimoeModel\", \"PhimoeForCausalLM\", \"PhimoeForSequenceClassification\"]"
        },
        {
            "sha": "7df807ccafac9e4db8a69e6dac46b23a15c6e611",
            "filename": "src/transformers/models/phimoe/modular_phimoe.py",
            "status": "added",
            "additions": 416,
            "deletions": 0,
            "changes": 416,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodular_phimoe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,416 @@\n+# coding=utf-8\n+# Copyright 2024 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"PyTorch Phimoe model.\"\"\"\n+\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+)\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...utils.generic import OutputRecorder\n+from ..llama.modeling_llama import LlamaAttention\n+from ..mixtral.modeling_mixtral import (\n+    MixtralDecoderLayer,\n+    MixtralExperts,\n+    MixtralForCausalLM,\n+    MixtralMLP,\n+    MixtralModel,\n+    MixtralPreTrainedModel,\n+)\n+from .configuration_phimoe import PhimoeConfig\n+\n+\n+class PhimoeRotaryEmbedding(nn.Module):\n+    def __init__(\n+        self,\n+        config: Optional[PhimoeConfig] = None,\n+    ):\n+        super().__init__()\n+\n+        self.config = config\n+        if config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+            self.short_mscale = config.rope_scaling.get(\"short_mscale\")\n+            self.long_mscale = config.rope_scaling.get(\"long_mscale\")\n+        else:\n+            self.rope_type = \"default\"\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+    def forward(self, x, position_ids):\n+        mscale = None\n+        seq_len = torch.max(position_ids) + 1\n+        if self.config.rope_scaling and seq_len:\n+            mscale = (\n+                self.long_mscale\n+                if seq_len > self.config.rope_scaling[\"original_max_position_embeddings\"]\n+                else self.short_mscale\n+            )\n+        inv_freq, attention_scaling = self.rope_init_fn(self.config, x.device, seq_len)\n+        mscale = attention_scaling if mscale is None else mscale\n+        inv_freq_expanded = inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * mscale\n+            sin = emb.sin() * mscale\n+        return cos.to(x.dtype), sin.to(x.dtype)\n+\n+\n+class PhimoeAttention(LlamaAttention):\n+    pass\n+\n+\n+class PhimoeMLP(MixtralMLP):\n+    pass\n+\n+\n+class PhimoeMultiplier(torch.autograd.Function):\n+    @staticmethod\n+    def forward(\n+        ctx,\n+        scores: torch.Tensor,\n+        multiplier: torch.Tensor,\n+        selected_experts: torch.Tensor,\n+        masked_gates: torch.Tensor,\n+        mask_for_one: torch.Tensor,\n+    ):\n+        \"\"\"\n+        Forward pass for the custom autograd function.\n+\n+        Args:\n+            ctx: Context object to save information for backward computation.\n+            scores (torch.Tensor): Input scores tensor.\n+            multiplier (torch.Tensor): Multiplier tensor.\n+            selected_experts (torch.Tensor): Tensor of selected experts.\n+            masked_gates (torch.Tensor): Masked gates tensor.\n+            mask_for_one (torch.Tensor): Mask for one tensor.\n+\n+        Returns:\n+            torch.Tensor: Result of the forward pass.\n+        \"\"\"\n+        ctx.save_for_backward(multiplier, selected_experts, masked_gates)\n+        return multiplier * mask_for_one\n+\n+    @staticmethod\n+    def backward(\n+        ctx,\n+        grad_at_output: torch.Tensor,\n+    ):\n+        \"\"\"\n+        Backward pass for the custom autograd function.\n+\n+        Args:\n+            ctx: Context object with saved tensors from the forward pass.\n+            grad_at_output (torch.Tensor): Gradient at the output.\n+\n+        Returns:\n+            tuple[torch.Tensor, None, None, None, None]: Gradients for the inputs.\n+        \"\"\"\n+        multiplier, selected_experts, masked_gates = ctx.saved_tensors\n+\n+        grad_at_output = grad_at_output * multiplier\n+\n+        grad_at_scores_expanded = masked_gates * grad_at_output.mul(-1)\n+        grad_at_scores_expanded.scatter_add_(\n+            dim=-1,\n+            index=selected_experts,\n+            src=grad_at_output,\n+        )\n+\n+        return (\n+            grad_at_scores_expanded,\n+            None,\n+            None,\n+            None,\n+            None,\n+        )\n+\n+\n+def sparsemixer(scores, jitter_eps, training, top_k=2):\n+    \"\"\"\n+    Sparse mixer function to select top-k experts and compute multipliers.\n+    Based on the paper: https://huggingface.co/papers/2409.12136\n+    We first replace the TopK(Â·) function as random sampling of discrete variables\n+    in model training. Then, following Liu et al. (2023a) and Liu et al. (2023b), we apply Heun's\n+    third order method to approximate the expert routing gradient and construct a modified\n+    back-propagation to give a mathematically sound gradient estimation for expert routing.\n+\n+    Args:\n+        scores (torch.Tensor): Input scores tensor.\n+        jitter_eps (float): Jitter epsilon for numerical stability.\n+        training (bool): Flag indicating if the model is in training mode.\n+        top_k (int): Number of top experts to select.\n+\n+    Returns:\n+        tuple[torch.Tensor, torch.Tensor]: Multiplier and selected experts tensors.\n+    \"\"\"\n+    with torch.no_grad():\n+        # Compute mask for sparsity\n+        mask_logits_threshold, max_ind = scores.max(dim=-1, keepdim=True)\n+        factor = scores.abs().clamp(min=mask_logits_threshold)\n+        mask_logits_threshold = ((mask_logits_threshold - scores) / factor) > (2 * jitter_eps)\n+\n+    # Apply mask\n+    masked_gates = scores.masked_fill(mask_logits_threshold, float(\"-inf\"))\n+    if training:\n+        selected_experts = (\n+            (\n+                masked_gates\n+                - torch.empty_like(masked_gates, memory_format=torch.legacy_contiguous_format).exponential_().log()\n+            )\n+            .max(dim=-1)[1]\n+            .unsqueeze(-1)\n+        )  # Gumbel sampling, more robust than the multinomial method\n+    else:\n+        selected_experts = max_ind\n+\n+    # Compute scores for gradients\n+    masked_gates = torch.softmax(masked_gates, dim=-1)\n+    multiplier_o = masked_gates.gather(dim=-1, index=selected_experts)\n+\n+    if training:\n+        # Compute midpoint mask\n+        max_scores, max_ind = masked_gates.max(dim=-1, keepdim=True)\n+        mask_for_one = torch.logical_or(\n+            selected_experts == max_ind,\n+            torch.rand_like(max_scores) > 0.75,  # Heun's third-order method\n+        )\n+        # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5\n+        mask_for_one = torch.add(0.3333, mask_for_one, alpha=0.6667).type_as(masked_gates)\n+\n+        multiplier = PhimoeMultiplier.apply(\n+            scores,\n+            multiplier_o,\n+            selected_experts,\n+            masked_gates,\n+            mask_for_one,\n+        )\n+    else:\n+        multiplier = multiplier_o\n+\n+    # Masked out first expert\n+    masked_scores = torch.scatter(\n+        scores,\n+        -1,\n+        selected_experts,\n+        float(\"-inf\"),\n+    )\n+    with torch.no_grad():\n+        # Compute mask for sparsity\n+        mask_logits_threshold, max_ind = masked_scores.max(dim=-1, keepdim=True)\n+        factor = scores.abs().clamp(min=mask_logits_threshold)\n+        mask_logits_threshold = ((mask_logits_threshold - scores) / factor) > (2 * jitter_eps)\n+\n+    # Apply mask\n+    masked_gates_top2 = masked_scores.masked_fill(mask_logits_threshold, float(\"-inf\"))\n+    if training:\n+        selected_experts_top2 = (\n+            (\n+                masked_gates_top2\n+                - torch.empty_like(masked_gates_top2, memory_format=torch.legacy_contiguous_format)\n+                .exponential_()\n+                .log()\n+            )\n+            .max(dim=-1)[1]\n+            .unsqueeze(-1)\n+        )  # Gumbel sampling, more robust than the multinomial method\n+    else:\n+        selected_experts_top2 = max_ind\n+    # Compute scores for gradients\n+    masked_gates_top2 = torch.softmax(masked_gates_top2, dim=-1)\n+    multiplier_top2_o = masked_gates_top2.gather(dim=-1, index=selected_experts_top2)\n+\n+    if training:\n+        # Compute midpoint mask\n+        max_scores, max_ind = masked_gates_top2.max(dim=-1, keepdim=True)\n+        mask_for_one_top2 = torch.logical_or(\n+            selected_experts_top2 == max_ind,\n+            torch.rand_like(max_scores).uniform_() > 0.75,  # Heun's third-order method\n+        )\n+        # 1 -> 1.0 & 0 -> 1./3: lambda x: (x + 0.5) / 1.5\n+        mask_for_one_top2 = torch.add(0.3333, mask_for_one_top2, alpha=0.6667).type_as(masked_gates_top2)\n+\n+        multiplier_top2 = PhimoeMultiplier.apply(\n+            scores,\n+            multiplier_top2_o,\n+            selected_experts_top2,\n+            masked_gates_top2,\n+            mask_for_one_top2,\n+        )\n+    else:\n+        multiplier_top2 = multiplier_top2_o\n+\n+    multiplier = torch.concat((multiplier, multiplier_top2), dim=-1)\n+    selected_experts = torch.concat((selected_experts, selected_experts_top2), dim=-1)\n+\n+    return (\n+        multiplier,\n+        selected_experts,\n+    )\n+\n+\n+class PhimoeExperts(MixtralExperts, nn.ModuleList):\n+    def __init__(self, config: PhimoeConfig):\n+        nn.ModuleList.__init__(self)\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_local_experts\n+        for _ in range(self.num_experts):\n+            self.append(PhimoeMLP(config))\n+\n+\n+class PhimoeRouter(nn.Linear):\n+    def __init__(self, config: PhimoeConfig):\n+        super().__init__(config.hidden_size, config.num_local_experts, bias=False)\n+        self.top_k = config.num_experts_per_tok\n+        self.hidden_dim = config.hidden_size\n+        self.router_jitter_noise = config.router_jitter_noise\n+        self.input_jitter_noise = config.router_jitter_noise\n+\n+    def forward(self, hidden_states):\n+        if self.training and self.input_jitter_noise > 0:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(\n+                1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise\n+            )\n+        router_logits = super().forward(hidden_states)\n+        return router_logits\n+\n+\n+class PhimoeSparseMoeBlock(nn.Module):\n+    \"\"\"\n+    This implementation is\n+    strictly equivalent to standard MoE with full capacity (no\n+    dropped tokens). It's faster since it formulates MoE operations\n+    in terms of block-sparse operations to accommodate imbalanced\n+    assignments of tokens to experts, whereas standard MoE either\n+    (1) drop tokens at the cost of reduced performance or (2) set\n+    capacity factor to number of experts and thus waste computation\n+    and memory on padding.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_dim = config.hidden_size\n+        self.ffn_dim = config.intermediate_size\n+        self.num_experts = config.num_local_experts\n+        self.top_k = config.num_experts_per_tok\n+        self.router_jitter_noise = config.router_jitter_noise\n+        self.gate = PhimoeRouter(config)\n+        self.experts = PhimoeExperts(config)\n+        self.input_jitter_noise = config.input_jitter_noise\n+\n+    def route_tokens_to_experts(self, router_logits):\n+        routing_weights, selected_experts = sparsemixer(\n+            router_logits,\n+            jitter_eps=self.router_jitter_noise,\n+            training=self.training,\n+        )\n+        return routing_weights, selected_experts\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        if self.training and self.input_jitter_noise > 0:\n+            hidden_states *= torch.empty_like(hidden_states).uniform_(\n+                1.0 - self.input_jitter_noise, 1.0 + self.input_jitter_noise\n+            )\n+\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.reshape(-1, hidden_dim)\n+        router_logits = self.gate(hidden_states)\n+        routing_weights, selected_experts = self.route_tokens_to_experts(router_logits)\n+        final_hidden_states = self.experts(hidden_states, selected_experts, routing_weights)\n+        return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+\n+\n+class PhimoeDecoderLayer(MixtralDecoderLayer):\n+    pass\n+\n+\n+class PhimoePreTrainedModel(MixtralPreTrainedModel):\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"hidden_states\": PhimoeDecoderLayer,\n+        \"attentions\": PhimoeAttention,\n+    }\n+\n+\n+class PhimoeModel(MixtralModel):\n+    def __init__(self, config: PhimoeConfig):\n+        super().__init__(config)\n+        self.norm = nn.LayerNorm(config.hidden_size, eps=config.rms_norm_eps, elementwise_affine=True)\n+\n+\n+class PhimoeForCausalLM(MixtralForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=self.config.lm_head_bias)\n+\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3ForCausalLM.prepare_inputs_for_generation\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- this model may need to switch between short and long rope, invalidating the cache in the\n+        # process\n+\n+        # When the first time input length reached long and short factor switching point, enforce re-compute cache\n+        # It will cause downside of slower at this single token position, however, better than current failure.\n+        if (\n+            past_key_values\n+            and self.config.rope_scaling\n+            and input_ids.shape[1] >= self.config.original_max_position_embeddings + 1\n+        ):\n+            past_length = cache_position[0]\n+            if past_length <= self.config.original_max_position_embeddings:\n+                past_key_values = None\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return model_inputs\n+\n+\n+class PhimoeForSequenceClassification(GenericForSequenceClassification, PhimoePreTrainedModel): ...\n+\n+\n+__all__ = [\n+    \"PhimoePreTrainedModel\",\n+    \"PhimoeModel\",\n+    \"PhimoeForCausalLM\",\n+    \"PhimoeForSequenceClassification\",\n+]"
        },
        {
            "sha": "fb4405bca2230b897a3614a03516299f903acdd3",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 14,
            "deletions": 3,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Qwen2MoE model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n@@ -135,6 +135,8 @@ class Qwen2MoeConfig(PretrainedConfig):\n             If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n         qkv_bias (`bool`, *optional*, defaults to `True`):\n             Whether to add a bias to the queries, keys and values.\n+        layer_types (`dict[int, str]`, *optional*): a dictionarry that explicitly maps layer index with\n+            the attention type. The attention type is one of `sliding_attention`, `full_attention`.\n     ```python\n     >>> from transformers import Qwen2MoeModel, Qwen2MoeConfig\n \n@@ -197,16 +199,18 @@ def __init__(\n         router_aux_loss_coef=0.001,\n         mlp_only_layers=None,\n         qkv_bias=True,\n+        layer_types=None,\n         **kwargs,\n     ):\n+        self.layer_types = layer_types\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window if use_sliding_window else None\n+        self.sliding_window = sliding_window if use_sliding_window else 0\n         self.max_window_layers = max_window_layers\n \n         self.num_key_value_heads = num_key_value_heads\n@@ -234,7 +238,14 @@ def __init__(\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n         self.qkv_bias = qkv_bias\n-\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if bool((i + 1) % 2) and i < self.max_window_layers and use_sliding_window\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,"
        },
        {
            "sha": "58d316e5a5875a9b33388143202bc0d293ad7ff7",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 258,
            "deletions": 736,
            "changes": 994,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/qwen2_moe/modular_qwen2_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_qwen2_moe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -17,135 +23,35 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch Qwen2MoE model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n import torch.nn.functional as F\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_layers import (\n     GenericForQuestionAnswering,\n     GenericForSequenceClassification,\n     GenericForTokenClassification,\n     GradientCheckpointingLayer,\n )\n-from ...modeling_outputs import (\n-    MoeCausalLMOutputWithPast,\n-    MoeModelOutputWithPast,\n-)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n-from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-def load_balancing_loss_func(\n-    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n-    num_experts: Optional[int] = None,\n-    top_k=2,\n-    attention_mask: Optional[torch.Tensor] = None,\n-) -> Union[torch.Tensor, int]:\n-    r\"\"\"\n-    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n-\n-    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n-    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n-    experts is too unbalanced.\n-\n-    Args:\n-        gate_logits:\n-            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n-            shape [batch_size X sequence_length, num_experts].\n-        num_experts:\n-            Number of experts\n-        top_k:\n-            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n-            parameter.\n-        attention_mask (`torch.Tensor`, *optional*):\n-            The attention_mask used in forward function\n-            shape [batch_size X sequence_length] if not None.\n-\n-    Returns:\n-        The auxiliary loss.\n-    \"\"\"\n-    if gate_logits is None or not isinstance(gate_logits, tuple):\n-        return 0\n-\n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n-\n-    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n-\n-    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n-    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n-\n-    if attention_mask is None:\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n-    else:\n-        batch_size, sequence_length = attention_mask.shape\n-        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n-        expert_attention_mask = (\n-            attention_mask[None, :, :, None, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n-            .reshape(-1, top_k, num_experts)\n-            .to(compute_device)\n-        )\n-\n-        # Compute the percentage of tokens routed to each experts\n-        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n-            expert_attention_mask, dim=0\n-        )\n-\n-        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n-        router_per_expert_attention_mask = (\n-            attention_mask[None, :, :, None]\n-            .expand((num_hidden_layers, batch_size, sequence_length, routing_weights.shape[1]))\n-            .reshape(-1, routing_weights.shape[1])\n-            .to(compute_device)\n-        )\n-\n-        # Compute the average probability of routing to these experts\n-        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n-            router_per_expert_attention_mask, dim=0\n-        )\n-\n-    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n-    rank = routing_weights.shape[1] * int(device_index)\n-    overall_loss = torch.sum(\n-        tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n-    )\n-    return overall_loss * num_experts\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Qwen2Moe\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen2MoeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -166,7 +72,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2Moe\n class Qwen2MoeRotaryEmbedding(nn.Module):\n     inv_freq: torch.Tensor  # fix linting for `register_buffer`\n \n@@ -203,15 +108,29 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n+class Qwen2MoeMLP(nn.Module):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -239,23 +158,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-# Modified from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Qwen2Moe\n-class Qwen2MoeMLP(nn.Module):\n-    def __init__(self, config, intermediate_size=None):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, x):\n-        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -268,403 +170,179 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-# copied from transformers.models.qwen2.modeling_qwen2.Qwen2Attention with Qwen2->Qwen2Moe\n-# no longer copied after attention refactors\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Qwen2MoeAttention(nn.Module):\n-    \"\"\"\n-    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n-    and \"Generating Long Sequences with Sparse Transformers\".\n-    \"\"\"\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will \"\n-                \"to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.is_causal = True\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=self.config.qkv_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.config.qkv_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.config.qkv_bias)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-\n-        self.rotary_emb = Qwen2MoeRotaryEmbedding(config=self.config)\n-\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_values is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights\n-\n-\n-# NO LONGER EXIST Copied from transformers.models.qwen2.modeling_qwen2.Qwen2FlashAttention2 with Qwen2->Qwen2Moe\n-# TODO cyril: modular\n-class Qwen2MoeFlashAttention2(Qwen2MoeAttention):\n-    \"\"\"\n-    Qwen2Moe flash attention module, following Qwen2Moe attention module. This module inherits from `Qwen2MoeAttention`\n-    as the weights of the module stays untouched. The only required change would be on the forward pass\n-    where it needs to correctly call the public API of flash attention and deal with padding tokens\n-    in case the input contains any of them. Additionally, for sliding window attention, we apply SWA only to the bottom\n-    config.max_window_layers layers.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.qkv_bias)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        if self.config.layer_types[layer_idx] == \"sliding_attention\":\n+            self.sliding_window = config.sliding_window\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_values is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = (\n-                    torch.get_autocast_dtype(device_type)\n-                    if hasattr(torch, \"get_autocast_dtype\")\n-                    else torch.get_autocast_gpu_dtype()\n-                )\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n-            else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        if (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            sliding_window = self.config.sliding_window\n-        else:\n-            sliding_window = None\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=dropout_rate,\n-            sliding_window=sliding_window,\n-            is_causal=self.is_causal,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n-# NO LONGER EXIST Copied from transformers.models.qwen2.modeling_qwen2.Qwen2SdpaAttention with Qwen2->Qwen2Moe\n-# TODO cyril: modular\n-class Qwen2MoeSdpaAttention(Qwen2MoeAttention):\n+class Qwen2MoeExperts(nn.ModuleList):\n     \"\"\"\n-    Qwen2Moe attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Qwen2MoeAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n+    ModuleList of experts.\n     \"\"\"\n \n-    # Adapted from Qwen2MoeAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Qwen2MoeModel is using Qwen2MoeSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, -1, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_values is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = bool(causal_mask is None and q_len > 1)\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        for _ in range(config.num_experts):\n+            self.append(Qwen2MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n \n+    def forward(\n+        self, hidden_states: torch.Tensor, top_k_index: torch.Tensor, top_k_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch_size * sequence_length, hidden_dim)\n+            selected_experts: (batch_size * sequence_length, top_k)\n+            routing_weights: (batch_size * sequence_length, top_k)\n+        Returns:\n+            (batch_size * sequence_length, hidden_dim)\n+        \"\"\"\n+        final_hidden_states = torch.zeros_like(hidden_states)\n+        expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts).permute(2, 1, 0)\n \n-QWEN2MOE_ATTENTION_CLASSES = {\n-    \"eager\": Qwen2MoeAttention,\n-    \"flash_attention_2\": Qwen2MoeFlashAttention2,\n-    \"sdpa\": Qwen2MoeSdpaAttention,\n-}\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_states.shape[-1])\n+            current_hidden_states = self[expert_idx](current_state) * top_k_weights[top_x, idx, None]\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        return final_hidden_states\n \n \n class Qwen2MoeSparseMoeBlock(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.num_experts = config.num_experts\n-        self.top_k = config.num_experts_per_tok\n-        self.norm_topk_prob = config.norm_topk_prob\n-\n         # gating\n         self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n-        self.experts = nn.ModuleList(\n-            [Qwen2MoeMLP(config, intermediate_size=config.moe_intermediate_size) for _ in range(self.num_experts)]\n-        )\n+        self.experts = Qwen2MoeExperts(config)\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n \n         self.shared_expert = Qwen2MoeMLP(config, intermediate_size=config.shared_expert_intermediate_size)\n         self.shared_expert_gate = torch.nn.Linear(config.hidden_size, 1, bias=False)\n \n-    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        \"\"\" \"\"\"\n-        batch_size, sequence_length, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, hidden_dim)\n-        # router_logits: (batch * sequence_length, n_experts)\n-        router_logits = self.gate(hidden_states)\n-\n-        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = F.softmax(router_logits, dim=-1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n         if self.norm_topk_prob:\n             routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n-        # we cast back to the input dtype\n         routing_weights = routing_weights.to(hidden_states.dtype)\n+        return selected_experts, routing_weights\n \n-        final_hidden_states = torch.zeros(\n-            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n-        )\n-\n-        # One hot encode the selected experts to create an expert mask\n-        # this will be used to easily index which expert is going to be sollicitated\n-        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n-\n-        # Loop over all available experts in the model and perform the computation on each expert\n-        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n-        for expert_idx in expert_hit:\n-            expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n-\n-            # Index the correct hidden states and compute the expert hidden state for\n-            # the current expert. We need to make sure to multiply the output hidden\n-            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n-            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n-            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n-\n-            # However `index_add_` only support torch tensors for indexing so we'll use\n-            # the `top_x` tensor here.\n-            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n-\n-        shared_expert_output = self.shared_expert(hidden_states)\n-        shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states)) * shared_expert_output\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        shared_expert_output = self.shared_expert(hidden_states_reshaped)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states_reshaped, router_logits)\n+        expert_output = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n-        final_hidden_states = final_hidden_states + shared_expert_output\n+        shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states_reshaped)) * shared_expert_output\n \n-        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n-        return final_hidden_states, router_logits\n+        expert_output += shared_expert_output\n+        expert_output = expert_output.reshape(batch_size, sequence_length, hidden_dim)\n+        return expert_output\n \n \n class Qwen2MoeDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         super().__init__()\n-        self.hidden_size = config.hidden_size\n-\n-        self.self_attn = QWEN2MOE_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n-\n+        self.self_attn = Qwen2MoeAttention(config, layer_idx)\n         if (layer_idx not in config.mlp_only_layers) and (\n             config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n         ):\n             self.mlp = Qwen2MoeSparseMoeBlock(config)\n         else:\n             self.mlp = Qwen2MoeMLP(config, intermediate_size=config.intermediate_size)\n-\n         self.input_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.hidden_size = config.hidden_size\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n@@ -673,76 +351,32 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_router_logits: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, sequence_length)` where padding elements are indicated by 0.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_router_logits (`bool`, *optional*):\n-                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n-                and should not be returned during inference.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Cache`, *optional*): cached past key and value projection states\n-            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n-                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n-                with `head_dim` being the embedding dimension of each attention head.\n-            kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n-        \"\"\"\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         residual = hidden_states\n-\n         hidden_states = self.input_layernorm(hidden_states)\n-\n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-\n         hidden_states = self.mlp(hidden_states)\n-        if isinstance(hidden_states, tuple):\n-            hidden_states, router_logits = hidden_states\n-        else:\n-            router_logits = None\n-\n         hidden_states = residual + hidden_states\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if output_router_logits:\n-            outputs += (router_logits,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -751,22 +385,17 @@ class Qwen2MoePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2MoeDecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = True\n-\n-    def _init_weights(self, module):\n-        std = self.config.initializer_range\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, Qwen2MoeRMSNorm):\n-            module.weight.data.fill_(1.0)\n+    _supports_flex_attn = True\n+    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"hidden_states\": Qwen2MoeDecoderLayer,\n+        \"attentions\": Qwen2MoeAttention,\n+    }\n \n \n @auto_docstring\n@@ -780,15 +409,14 @@ def __init__(self, config: Qwen2MoeConfig):\n         self.layers = nn.ModuleList(\n             [Qwen2MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self._attn_implementation = config._attn_implementation\n         self.norm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2MoeRotaryEmbedding(config=config)\n-\n         self.gradient_checkpointing = False\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -798,250 +426,151 @@ def forward(\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeModelOutputWithPast:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_router_logits = (\n-            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n-        )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         hidden_states = inputs_embeds\n \n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_router_logits = () if output_router_logits else None\n-\n-        for decoder_layer in self.layers:\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            layer_outputs = decoder_layer(\n+        for i, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):\n+            hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[self.config.layer_types[i]],\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                output_router_logits=output_router_logits,\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n+        hidden_states = self.norm(hidden_states)\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n \n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n \n-            if output_router_logits and layer_outputs[-1] is not None:\n-                all_router_logits += (layer_outputs[-1],)\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        hidden_states = self.norm(hidden_states)\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n \n-        return MoeModelOutputWithPast(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            router_logits=all_router_logits,\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n         )\n \n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Qwen2Moe\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2Moe. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # StaticCache\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Qwen2Moe\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen2MoeConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n \n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen2MoeConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                is_static_sliding_cache = isinstance(past_key_values, StaticCache) and all(past_key_values.is_sliding)\n-                if not is_static_sliding_cache or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n+@auto_docstring\n class Qwen2MoeForCausalLM(Qwen2MoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n@@ -1052,10 +581,10 @@ def __init__(self, config):\n         self.model = Qwen2MoeModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n-\n         self.router_aux_loss_coef = config.router_aux_loss_coef\n         self.num_experts = config.num_experts\n         self.num_experts_per_tok = config.num_experts_per_tok\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1070,12 +599,10 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         output_router_logits: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> MoeCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1088,8 +615,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Qwen2MoeForCausalLM\n \n-        >>> model = Qwen2MoeForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n-        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n+        >>> model = Qwen2MoeForCausalLM.from_pretrained(\"mistralai/Qwen2Moe-8x7B-v0.1\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Qwen2Moe-8x7B-v0.1\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n@@ -1100,13 +627,9 @@ def forward(\n         \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_router_logits = (\n             output_router_logits if output_router_logits is not None else self.config.output_router_logits\n         )\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs: MoeModelOutputWithPast = self.model(\n@@ -1116,10 +639,9 @@ def forward(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             output_router_logits=output_router_logits,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs.last_hidden_state"
        },
        {
            "sha": "bdd53167009c3b008bf521eed7608a2852ac6674",
            "filename": "src/transformers/models/qwen2_moe/modular_qwen2_moe.py",
            "status": "added",
            "additions": 260,
            "deletions": 0,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c",
            "patch": "@@ -0,0 +1,260 @@\n+# coding=utf-8\n+# Copyright 2024 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Qwen2MoE model.\"\"\"\n+\n+from typing import Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+)\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from ..gemma.modeling_gemma import GemmaMLP\n+from ..llama.modeling_llama import LlamaAttention, LlamaDecoderLayer, LlamaRMSNorm, LlamaRotaryEmbedding\n+from ..mixtral.modeling_mixtral import (\n+    MixtralExperts,\n+    MixtralForCausalLM,\n+    MixtralModel,\n+    MixtralPreTrainedModel,\n+)\n+from .configuration_qwen2_moe import Qwen2MoeConfig\n+\n+\n+class Qwen2MoeRMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Qwen2MoeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class Qwen2MoeMLP(GemmaMLP):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+\n+class Qwen2MoeAttention(LlamaAttention):\n+    def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        if self.config.layer_types[layer_idx] == \"sliding_attention\":\n+            self.sliding_window = config.sliding_window\n+\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.qkv_bias)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.qkv_bias)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+\n+\n+class Qwen2MoeExperts(MixtralExperts, nn.Module):\n+    def __init__(self, config):\n+        nn.ModuleList.__init__(self)\n+        self.num_experts = config.num_experts\n+        for _ in range(config.num_experts):\n+            self.append(Qwen2MoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+\n+class Qwen2MoeSparseMoeBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        # gating\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+        self.experts = Qwen2MoeExperts(config)\n+        self.num_experts_per_tok = config.num_experts_per_tok\n+        self.norm_topk_prob = config.norm_topk_prob\n+\n+        self.shared_expert = Qwen2MoeMLP(config, intermediate_size=config.shared_expert_intermediate_size)\n+        self.shared_expert_gate = torch.nn.Linear(config.hidden_size, 1, bias=False)\n+\n+    def route_tokens_to_experts(self, hidden_states, router_logits):\n+        routing_weights = F.softmax(router_logits, dim=-1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.num_experts_per_tok, dim=-1)\n+        if self.norm_topk_prob:\n+            routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        return selected_experts, routing_weights\n+\n+    def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n+        shared_expert_output = self.shared_expert(hidden_states_reshaped)\n+        router_logits = self.gate(hidden_states_reshaped)\n+        selected_experts, routing_weights = self.route_tokens_to_experts(hidden_states_reshaped, router_logits)\n+        expert_output = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n+\n+        shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states_reshaped)) * shared_expert_output\n+\n+        expert_output += shared_expert_output\n+        expert_output = expert_output.reshape(batch_size, sequence_length, hidden_dim)\n+        return expert_output\n+\n+\n+class Qwen2MoeDecoderLayer(LlamaDecoderLayer, nn.Module):\n+    def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n+        nn.Module.__init__()\n+        self.self_attn = Qwen2MoeAttention(config, layer_idx)\n+        if (layer_idx not in config.mlp_only_layers) and (\n+            config.num_experts > 0 and (layer_idx + 1) % config.decoder_sparse_step == 0\n+        ):\n+            self.mlp = Qwen2MoeSparseMoeBlock(config)\n+        else:\n+            self.mlp = Qwen2MoeMLP(config, intermediate_size=config.intermediate_size)\n+        self.input_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.hidden_size = config.hidden_size\n+\n+\n+@auto_docstring\n+class Qwen2MoePreTrainedModel(MixtralPreTrainedModel):\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n+        \"hidden_states\": Qwen2MoeDecoderLayer,\n+        \"attentions\": Qwen2MoeAttention,\n+    }\n+\n+\n+@auto_docstring\n+class Qwen2MoeModel(MixtralModel):\n+    def __init__(self, config: Qwen2MoeConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [Qwen2MoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Qwen2MoeRotaryEmbedding(config=config)\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for i, decoder_layer in enumerate(self.layers[: self.config.num_hidden_layers]):\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[self.config.layer_types[i]],\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+class Qwen2MoeForCausalLM(MixtralForCausalLM, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_experts = config.num_experts\n+        self.model = Qwen2MoeModel(config)\n+\n+\n+class Qwen2MoeForSequenceClassification(GenericForSequenceClassification, Qwen2MoePreTrainedModel): ...\n+\n+\n+class Qwen2MoeForTokenClassification(GenericForTokenClassification, Qwen2MoePreTrainedModel): ...\n+\n+\n+class Qwen2MoeForQuestionAnswering(GenericForQuestionAnswering, Qwen2MoePreTrainedModel): ...\n+\n+\n+__all__ = [\n+    \"Qwen2MoeForCausalLM\",\n+    \"Qwen2MoeForQuestionAnswering\",\n+    \"Qwen2MoeModel\",\n+    \"Qwen2MoePreTrainedModel\",\n+    \"Qwen2MoeForSequenceClassification\",\n+    \"Qwen2MoeForTokenClassification\",\n+]"
        },
        {
            "sha": "d9568865be7ea9e08ee549be403cb2fa17577338",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "e121feb52c087f938e8ddb5f228eee787ebf430a",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 41,
            "deletions": 114,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "f10d24c96c46da4a8694b7f9469634d3ee26c1d1",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 52,
            "deletions": 70,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "973b1624470b1eaf1b1244f5825b3bccacba882a",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 116,
            "deletions": 171,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "2dc964c28f5f3851fe210eb02708473cfb34068a",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "e6550eee3590af7bcbd6ec5969b40e018d2d96bd",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 41,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "ca59fbfd83da1ad9f2dab7a03b6cad05c0903e72",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "44bfff010b31420b78c56dbf9591d3ac1646ab34",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "27146297884cc9f2c2853e44add2707d5f3ceb80",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "935152b4ff4916d4a1d7bb659079f09ebfdba832",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 155,
            "deletions": 550,
            "changes": 705,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "cf4eaf0cedfffbef53fd5f7bc17ebc343f8d8cd9",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "added",
            "additions": 994,
            "deletions": 0,
            "changes": 994,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "9fcdbe79dc9b1ab8f04593d2f2f4d96ddee8858b",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 24,
            "deletions": 11,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "5730bdf58648a4e224718f13e99ab68b438274a4",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 17,
            "deletions": 13,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "dcef3c5237d72fd5bb1caaaa2d210a8c8e7a08a8",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "538e07c34655ac965120c4aeef3819d24ab08238",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "e00528c74cf0d8005c473915d70574292a6879df",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "d439695dc8f342aa15917be14cb13b88611703b4",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "86238c053a3507e4096871c05b2628bc2fed2f27",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 15,
            "deletions": 5,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "81a93dd89c29eed721596aec72df034a3128091d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "29fa59987266866607fc0164a2b215ff58b6b8c4",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "41650aedd0e2aa61507b3f5a3a8d081a502b1532",
            "filename": "utils/check_modular_conversion.py",
            "status": "modified",
            "additions": 19,
            "deletions": 4,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        },
        {
            "sha": "e2a19aa611b96b532e50636ee3e70031605ab8ca",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7938e91faabb051f3a001cd39c173d4697c2d81c/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7938e91faabb051f3a001cd39c173d4697c2d81c/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=7938e91faabb051f3a001cd39c173d4697c2d81c"
        }
    ],
    "stats": {
        "total": 17618,
        "additions": 8433,
        "deletions": 9185
    }
}