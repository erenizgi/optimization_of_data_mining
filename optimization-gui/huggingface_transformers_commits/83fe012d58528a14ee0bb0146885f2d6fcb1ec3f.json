{
    "author": "ArthurZucker",
    "message": "small fix tokenizer regex patch (#42528)\n\n* small fix\n\n* update\n\n* we prob still had 1 issue\n\n* fix\n\n* pop in case",
    "sha": "83fe012d58528a14ee0bb0146885f2d6fcb1ec3f",
    "files": [
        {
            "sha": "988acbf91aaed1c3ef3bc684c55ee82dcfea07bc",
            "filename": "src/transformers/tokenization_utils_tokenizers.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/83fe012d58528a14ee0bb0146885f2d6fcb1ec3f/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/83fe012d58528a14ee0bb0146885f2d6fcb1ec3f/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_tokenizers.py?ref=83fe012d58528a14ee0bb0146885f2d6fcb1ec3f",
            "patch": "@@ -42,7 +42,7 @@\n     TextInput,\n     TruncationStrategy,\n )\n-from .utils import PaddingStrategy, add_end_docstrings, logging\n+from .utils import PaddingStrategy, add_end_docstrings, is_offline_mode, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -219,6 +219,7 @@ def __init__(self, *args, **kwargs):\n \n         # Optionally patches mistral tokenizers with wrong regex\n         if vocab_size > 100000 and getattr(self._tokenizer, \"pre_tokenizer\", None) is not None:\n+            kwargs.pop(\"tokenizer\", None)\n             self._tokenizer = self._patch_mistral_regex(\n                 self._tokenizer,\n                 self.init_kwargs.get(\"name_or_path\", None),\n@@ -1089,7 +1090,12 @@ def is_base_mistral(model_id: str) -> bool:\n                     return True\n             return False\n \n-        if pretrained_model_name_or_path is not None and (is_local or is_base_mistral(pretrained_model_name_or_path)):\n+        if is_offline_mode():\n+            is_local = True\n+\n+        if pretrained_model_name_or_path is not None and (\n+            is_local or (not is_local and is_base_mistral(pretrained_model_name_or_path))\n+        ):\n             _config_file = cached_file(\n                 pretrained_model_name_or_path,\n                 \"config.json\",\n@@ -1126,7 +1132,7 @@ def is_base_mistral(model_id: str) -> bool:\n                         ]\n                     ):\n                         return tokenizer\n-                elif transformers_version and version.parse(transformers_version) >= version.parse(\"5.0.0\"):\n+                elif transformers_version and version.parse(transformers_version) >= version.parse(\"4.57.3\"):\n                     return tokenizer\n \n                 mistral_config_detected = True"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}