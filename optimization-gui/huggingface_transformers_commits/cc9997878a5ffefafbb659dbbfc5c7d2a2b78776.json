{
    "author": "yao-matrix",
    "message": "make model doc device agnostic (#40143)\n\n* make model doc device agnostic\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* Update align.md\n\n* Update aya_vision.md\n\n* Update byt5.md\n\n* refine\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* Update granitevision.md\n\n* Update src/transformers/pytorch_utils.py\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* add doc\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* 3 more\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
    "files": [
        {
            "sha": "ae483638653f20af31eabdebbcc74b4b3e9a3a86",
            "filename": "docs/source/en/internal/file_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -48,3 +48,4 @@ Most of those are only useful if you are studying the general code in the librar\n ## Other Utilities\n \n [[autodoc]] utils._LazyModule\n+[[autodoc]] pytorch_utils.infer_device"
        },
        {
            "sha": "07a1ce8b423c44f26095e44bb09f7de75b89f508",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -66,18 +66,18 @@ from PIL import Image\n from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n \n processor = AutoProcessor.from_pretrained(\"kakaobrain/align-base\")\n-model = AutoModelForZeroShotImageClassification.from_pretrained(\"kakaobrain/align-base\").to(\"cuda\")\n+model = AutoModelForZeroShotImageClassification.from_pretrained(\"kakaobrain/align-base\", device_map=\"auto\")\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = requests.get(url, stream=True)\n inputs = Image.open(image.raw).convert(\"RGB\")\n \n-image_inputs = processor(images=inputs, return_tensors=\"pt\").to(\"cuda\")\n+image_inputs = processor(images=inputs, return_tensors=\"pt\").to(model.device)\n with torch.no_grad():\n     image_embeds = model.get_image_features(**image_inputs)\n \n candidate_labels = [\"a photo of a dog\", \"a photo of a cat\", \"a photo of a person\"]\n-text_inputs = processor(text=candidate_labels, padding=True, return_tensors=\"pt\").to(\"cuda\")\n+text_inputs = processor(text=candidate_labels, padding=True, return_tensors=\"pt\").to(model.device)\n with torch.no_grad():\n     text_embeds = model.get_text_features(**text_inputs)\n "
        },
        {
            "sha": "dfdfea7b4739263ab0b41998944fd62288ec6c11",
            "filename": "docs/source/en/model_doc/aya_vision.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -60,8 +60,8 @@ print(outputs)\n \n ```python\n # pip install 'git+https://github.com/huggingface/transformers.git@v4.49.0-Aya Vision'\n-from transformers import AutoProcessor, AutoModelForImageTextToText\n import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n \n model_id = \"CohereLabs/aya-vision-8b\"\n \n@@ -133,7 +133,7 @@ inputs = processor.apply_chat_template(\n     add_generation_prompt=True,\n     tokenize=True,\n     return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n \n generated = model.generate(**inputs, max_new_tokens=50)\n print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n@@ -148,12 +148,12 @@ print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n - The example below demonstrates inference with multiple images.\n   \n     ```py\n-    from transformers import AutoProcessor, AutoModelForImageTextToText\n     import torch\n+    from transformers import AutoProcessor, AutoModelForImageTextToText\n         \n     processor = AutoProcessor.from_pretrained(\"CohereForAI/aya-vision-8b\")\n     model = AutoModelForImageTextToText.from_pretrained(\n-        \"CohereForAI/aya-vision-8b\", device_map=\"cuda\", torch_dtype=torch.float16\n+        \"CohereForAI/aya-vision-8b\", device_map=\"auto\", torch_dtype=torch.float16\n     )\n     \n     messages = [\n@@ -178,7 +178,7 @@ print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n     \n     inputs = processor.apply_chat_template(\n         messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-    ).to(\"cuda\")\n+    ).to(model.device)\n     \n     gen_tokens = model.generate(\n         **inputs, \n@@ -194,12 +194,12 @@ print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n - The example below demonstrates inference with batched inputs.\n   \n     ```py\n-    from transformers import AutoProcessor, AutoModelForImageTextToText\n     import torch\n+    from transformers import AutoProcessor, AutoModelForImageTextToText\n         \n     processor = AutoProcessor.from_pretrained(model_id)\n     model = AutoModelForImageTextToText.from_pretrained(\n-        \"CohereForAI/aya-vision-8b\", device_map=\"cuda\", torch_dtype=torch.float16\n+        \"CohereForAI/aya-vision-8b\", device_map=\"auto\", torch_dtype=torch.float16\n     )\n     \n     batch_messages = ["
        },
        {
            "sha": "fcfc86eadc02f0ab568646d102a85a08c4f0cfa2",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -43,18 +43,18 @@ Bark can be optimized with just a few extra lines of code, which **significantly\n You can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision.\n \n ```python\n-from transformers import BarkModel\n+from transformers import BarkModel, infer_device\n import torch\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16).to(device)\n ```\n \n #### Using CPU offload\n \n As mentioned above, Bark is made up of 4 sub-models, which are called up sequentially during audio generation. In other words, while one sub-model is in use, the other sub-models are idle.\n \n-If you're using a CUDA device, a simple solution to benefit from an 80% reduction in memory footprint is to offload the submodels from GPU to CPU when they're idle. This operation is called *CPU offloading*. You can use it with one line of code as follows:\n+If you're using a CUDA GPU or Intel XPU, a simple solution to benefit from an 80% reduction in memory footprint is to offload the submodels from device to CPU when they're idle. This operation is called *CPU offloading*. You can use it with one line of code as follows:\n \n ```python\n model.enable_cpu_offload()"
        },
        {
            "sha": "b2fca1939386d96bec28e0517e0368f537d1ea79",
            "filename": "docs/source/en/model_doc/byt5.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbyt5.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -65,7 +65,7 @@ model = AutoModelForSeq2SeqLM.from_pretrained(\n     device_map=\"auto\"\n )\n \n-input_ids = tokenizer(\"summarize: Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy.\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"summarize: Photosynthesis is the process by which plants, algae, and some bacteria convert light energy into chemical energy.\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -102,7 +102,7 @@ model = AutoModelForSeq2SeqLM.from_pretrained(\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/byt5-xl\")\n-input_ids = tokenizer(\"translate English to French: The weather is nice today.\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"translate English to French: The weather is nice today.\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "6b711995e3421ace28cf4ecc41a47af6c611e222",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -78,7 +78,7 @@ from PIL import Image\n import requests\n \n processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n \n # prepare image and text prompt\n url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n@@ -104,7 +104,7 @@ import requests\n \n processor = ChameleonProcessor.from_pretrained(\"facebook/chameleon-7b\")\n \n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n \n # Get three different images\n url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n@@ -124,7 +124,7 @@ prompts = [\n \n # We can simply feed images in the order they have to be used in the text prompt\n # Each \"<image>\" token uses one image leaving the next for the subsequent \"<image>\" tokens\n-inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.bfloat16)\n+inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(device=model.device, dtype=torch.bfloat16)\n \n # Generate\n generate_ids = model.generate(**inputs, max_new_tokens=50)\n@@ -157,7 +157,7 @@ quantization_config = BitsAndBytesConfig(\n     bnb_4bit_compute_dtype=torch.bfloat16,\n )\n \n-model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", quantization_config=quantization_config, device_map=\"cuda\")\n+model = ChameleonForConditionalGeneration.from_pretrained(\"facebook/chameleon-7b\", quantization_config=quantization_config, device_map=\"auto\")\n ```\n \n ### Use Flash-Attention 2 and SDPA to further speed-up generation"
        },
        {
            "sha": "c15a2f6f3a28c8cfb27ce2820d72ea76703dcb37",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -63,7 +63,7 @@ model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", t\n \n # format message with the Command-R chat template\n messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n-input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n output = model.generate(\n     input_ids,\n     max_new_tokens=100,\n@@ -99,7 +99,7 @@ model = AutoModelForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\", t\n \n # format message with the Command-R chat template\n messages = [{\"role\": \"user\", \"content\": \"How do plants make energy?\"}]\n-input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n output = model.generate(\n     input_ids,\n     max_new_tokens=100,"
        },
        {
            "sha": "50d4e96b29ba548e848f4003d14055408af8c2e3",
            "filename": "docs/source/en/model_doc/cohere2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -74,7 +74,7 @@ model = AutoModelForCausalLM.from_pretrained(\n \n # format message with the Command-R chat template\n messages = [{\"role\": \"user\", \"content\": \"Hello, can you please help me book a hotel in Japan?\"}]\n-input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n output = model.generate(\n     input_ids,\n     max_new_tokens=100,\n@@ -116,7 +116,7 @@ model = AutoModelForCausalLM.from_pretrained(\n \n # format message with the Command-R chat template\n messages = [{\"role\": \"user\", \"content\": \"Hello, can you please help me book a hotel in Japan?\"}]\n-input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n output = model.generate(\n     input_ids,\n     max_new_tokens=100,"
        },
        {
            "sha": "28fed293b465a05939bc3f24925dbcdea6a98841",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -119,14 +119,14 @@ print(answer)\n \n     ```py\n     >>> import re\n-    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel, infer_device\n     >>> from datasets import load_dataset\n     >>> import torch\n \n     >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n     >>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n \n-    >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    >>> device = infer_device()\n     >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n     >>> # load document image\n@@ -161,14 +161,14 @@ print(answer)\n \n     ```py\n     >>> import re\n-    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel, infer_device\n     >>> from datasets import load_dataset\n     >>> import torch\n \n     >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n     >>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n \n-    >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+    >>> device = infer_device()\n     >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n     >>> # load document image\n@@ -236,4 +236,4 @@ print(answer)\n ## DonutSwinForImageClassification\n \n [[autodoc]] transformers.DonutSwinForImageClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "3391b9f73f70648bd2c670aa82041cac49b71174",
            "filename": "docs/source/en/model_doc/falcon.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -70,7 +70,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     attn_implementation=\"sdpa\",\n )\n \n-input_ids = tokenizer(\"Write a short poem about coding\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"Write a short poem about coding\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -110,7 +110,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     quantization_config=quantization_config,\n )\n \n-inputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"In quantum physics, entanglement means\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "505c244ccb1fd9c0058e7c27acbc13d449aa865d",
            "filename": "docs/source/en/model_doc/falcon_mamba.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffalcon_mamba.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -67,7 +67,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     device_map=\"auto\"\n )\n \n-input_ids = tokenizer(\"Explain the difference between transformers and SSMs\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"Explain the difference between transformers and SSMs\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids, max_new_tokens=100, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -106,7 +106,7 @@ model = FalconMambaForCausalLM.from_pretrained(\n     quantization_config=quantization_config,\n )\n \n-inputs = tokenizer(\"Explain the concept of state space models in simple terms\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Explain the concept of state space models in simple terms\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "3e949df4ef21bdee2a8f173f44bb43846a3f86bc",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -93,7 +93,7 @@ inputs = processor.apply_chat_template(\n     return_dict=True,\n     return_tensors=\"pt\",\n     add_generation_prompt=True,\n-).to(\"cuda\")\n+).to(model.device)\n \n output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n print(processor.decode(output[0], skip_special_tokens=True))\n@@ -150,7 +150,7 @@ inputs = processor.apply_chat_template(\n     return_dict=True,\n     return_tensors=\"pt\",\n     add_generation_prompt=True,\n-).to(\"cuda\")\n+).to(model.device)\n \n output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n print(processor.decode(output[0], skip_special_tokens=True))\n@@ -207,7 +207,7 @@ visualizer(\"<img>What is shown in this image?\")\n         return_tensors=\"pt\",\n         add_generation_prompt=True,\n     +   do_pan_and_scan=True,\n-        ).to(\"cuda\")\n+        ).to(model.device)\n     ```\n - For Gemma-3 1B checkpoint trained in text-only mode, use [`AutoModelForCausalLM`] instead.\n \n@@ -224,7 +224,7 @@ visualizer(\"<img>What is shown in this image?\")\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )\n-    input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+    input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n     output = model.generate(**input_ids, cache_implementation=\"static\")\n     print(tokenizer.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "3ef861bb10a61a5ab93c65dd54e67afa7a69cf74",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -48,10 +48,10 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n ### Plain text inference\n \n ```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -73,10 +73,10 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n ### Plain text inference batched\n \n ```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -102,10 +102,10 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n GOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an example of how to generate formatted text:\n \n ```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -131,10 +131,10 @@ Here is an example of how to process multiple pages at once:\n \n \n ```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -161,10 +161,9 @@ Here is an example of how to process cropped patches:\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n->>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", torch_dtype=torch.bfloat16, device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -188,10 +187,10 @@ Here is an example of how to process cropped patches:\n GOT supports interactive OCR, where the user can specify the region to be recognized by providing the coordinates or the color of the region's bounding box. Here is an example of how to process a specific region:\n \n ```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -216,11 +215,11 @@ Although this implementation of the model will only output plain text, the outpu\n Here is an example of how to process sheet music:\n \n ```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n >>> import torch\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n >>> import verovio\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+>>> device = infer_device()\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n "
        },
        {
            "sha": "321e0894e5c79b141fe188fa57f8a87cb89681ec",
            "filename": "docs/source/en/model_doc/granite.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -65,7 +65,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     attn_implementation=\"sdpa\"\n )\n \n-inputs = tokenizer(\"Explain quantum computing in simple terms\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Explain quantum computing in simple terms\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n@@ -90,13 +90,13 @@ quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-8b-base\")\n model = AutoModelForCausalLM.from_pretrained(\"ibm-granite/granite-3.3-8b-base\", torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\", quantization_config=quantization_config)\n \n-inputs = tokenizer(\"Explain quantum computing in simple terms\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Explain quantum computing in simple terms\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n \n quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n \n-tokenizer = AutoTokenizer.from_pretrained(\"\"ibm-granite/granite-3.3-2b-base\"\")\n+tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.3-2b-base\")\n model = AutoModelForCausalLM.from_pretrained(\n     \"ibm-granite/granite-3.3-2b-base\",\n     torch_dtype=torch.bfloat16,\n@@ -105,7 +105,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     quantization_config=quantization_config,\n )\n \n-input_ids = tokenizer(\"Explain artificial intelligence to a 10 year old\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"Explain artificial intelligence to a 10 year old\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_length=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "b138c66f79d811d9f07377faf30deafc267aefae",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -31,12 +31,14 @@ Tips:\n \n Sample inference:\n ```python\n-from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n+from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, infer_device\n+\n+device = infer_device()\n \n model_path = \"ibm-granite/granite-vision-3.1-2b-preview\"\n processor = LlavaNextProcessor.from_pretrained(model_path)\n \n-model = LlavaNextForConditionalGeneration.from_pretrained(model_path).to(\"cuda\")\n+model = LlavaNextForConditionalGeneration.from_pretrained(model_path).to(device)\n \n # prepare image and text prompt, using the appropriate prompt template\n url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n@@ -56,7 +58,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n \n \n # autoregressively complete prompt"
        },
        {
            "sha": "2fc54929a4fef53b1621ef1ebf099474714a16be",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -74,10 +74,10 @@ input_text = \"\"\"Plants are among the most remarkable and essential life forms on\n Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # Place global attention on the first token\n-global_attention_mask = torch.zeros_like(input_ids.input_ids).to(\"cuda\")\n+global_attention_mask = torch.zeros_like(input_ids.input_ids).to(model.device)\n global_attention_mask[:, 0] = 1\n \n output = model.generate(**input_ids, global_attention_mask=global_attention_mask, cache_implementation=\"static\")\n@@ -121,10 +121,10 @@ input_text = \"\"\"Plants are among the most remarkable and essential life forms on\n Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n # Place global attention on the first token\n-global_attention_mask = torch.zeros_like(input_ids.input_ids).to(\"cuda\")\n+global_attention_mask = torch.zeros_like(input_ids.input_ids).to(model.device)\n global_attention_mask[:, 0] = 1\n \n output = model.generate(**input_ids, global_attention_mask=global_attention_mask, cache_implementation=\"static\")"
        },
        {
            "sha": "f6ca89fde44a2230e457c31bb26293e483879f0d",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 49,
            "deletions": 47,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -70,30 +70,32 @@ pipeline(text=messages, max_new_tokens=20, return_full_text=False)\n <hfoption id=\"AutoModel\">\n \n ```python\n-import torch  \n-import requests  \n-from PIL import Image  \n-from transformers import AutoProcessor, LlavaNextForConditionalGeneration  \n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, LlavaNextForConditionalGeneration, infer_device\n \n-processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")  \n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16).to(\"cuda\")  \n+device = infer_device()\n \n-url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"  \n-image = Image.open(requests.get(url, stream=True).raw)  \n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n+model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16).to(device)\n \n-conversation = [  \n-    {  \n-        \"role\": \"user\",  \n-        \"content\": [  \n-            {\"type\": \"image\"},  \n-            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},  \n-        ],  \n-    },  \n-]  \n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)  \n-inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda\")  \n-output = model.generate(**inputs, max_new_tokens=100)  \n-print(processor.decode(output[0], skip_special_tokens=True))  \n+url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n+output = model.generate(**inputs, max_new_tokens=100)\n+print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n@@ -105,38 +107,38 @@ Quantization reduces the memory burden of large models by representing the weigh\n The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n \n ```python\n-import torch  \n-import requests  \n-from PIL import Image  \n-from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig  \n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n \n quant_config = BitsAndBytesConfig(  \n     load_in_4bit=True,  \n     bnb_4bit_compute_dtype=torch.float16,  \n     bnb_4bit_quant_type=\"nf4\"  \n-)  \n+)\n \n-processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")  \n-model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quant_config, device_map=\"auto\")  \n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n+model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quant_config, device_map=\"auto\")\n \n-url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_ocr.png\"  \n-image = Image.open(requests.get(url, stream=True).raw)  \n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_ocr.png\"\n+image = Image.open(requests.get(url, stream=True).raw)\n \n-conversation = [  \n-    {  \n-        \"role\": \"user\",  \n-        \"content\": [  \n-            {\"type\": \"image\"},  \n-            {\"type\": \"text\", \"text\": \"What does this chart show?\"},  \n-        ],  \n-    },  \n-]  \n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)  \n-inputs = processor(image, prompt, return_tensors=\"pt\").to(\"cuda\")  \n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What does this chart show?\"},\n+        ],\n+    },\n+]\n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n \n-with torch.inference_mode():  \n-    output = model.generate(**inputs, max_new_tokens=100)  \n-print(processor.decode(output[0], skip_special_tokens=True))  \n+with torch.inference_mode():\n+    output = model.generate(**inputs, max_new_tokens=100)\n+print(processor.decode(output[0], skip_special_tokens=True))\n ```\n \n \n@@ -167,8 +169,8 @@ import requests, torch\n \n processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n model = LlavaNextForConditionalGeneration.from_pretrained(\n-    \"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16\n-).to(\"cuda\")\n+    \"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\"\n+)\n \n # Load multiple images\n url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_next_ocr.png\"\n@@ -181,7 +183,7 @@ conversation = [\n     {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Compare these two images and describe the differences.\"}]}\n ]\n prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-inputs = processor([image1, image2], prompt, return_tensors=\"pt\").to(\"cuda\")\n+inputs = processor([image1, image2], prompt, return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**inputs, max_new_tokens=100)\n print(processor.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "b3e6c1f4c966f421e996d83a0cc7286217f70684",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -67,9 +67,9 @@ outputs[0][\"generated_text\"]\n \n ```py\n import torch\n-from transformers import AutoProcessor, AutoModelForImageTextToText \n+from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device \n \n-torch_device = \"cuda\"\n+torch_device = infer_device()\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(\n@@ -107,10 +107,10 @@ decoded_output\n \n - Mistral 3 supports text-only generation. \n ```py \n-from transformers import AutoProcessor, AutoModelForImageTextToText\n import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n-torch_device = \"cuda\"\n+torch_device = infer_device()\n model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n@@ -144,10 +144,10 @@ print(decoded_output)\n \n - Mistral 3 accepts batched image and text inputs. \n ```py\n-from transformers import AutoProcessor, AutoModelForImageTextToText\n import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n \n-torch_device = \"cuda\"\n+torch_device = infer_device()\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n@@ -186,11 +186,11 @@ messages = [\n \n - Mistral 3 also supported batched image and text inputs with a different number of images for each text. The example below quantizes the model with bitsandbytes. \n \n-```py \n-from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n+```py\n import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, infer_device\n \n-torch_device = \"cuda\"\n+torch_device = infer_device()\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n quantization_config = BitsAndBytesConfig(load_in_4bit=True)"
        },
        {
            "sha": "4ce0f9a23a4c03b6d6d6db1199cad836e16f8ba8",
            "filename": "docs/source/en/model_doc/moonshine.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoonshine.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -67,7 +67,7 @@ model = MoonshineForConditionalGeneration.from_pretrained(\n     torch_dtype=torch.float16,\n     device_map=\"auto\",\n     attn_implementation=\"sdpa\"\n-).to(\"cuda\")\n+)\n \n ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", split=\"validation\")\n audio_sample = ds[0][\"audio\"]\n@@ -77,7 +77,7 @@ input_features = processor(\n     sampling_rate=audio_sample[\"sampling_rate\"],\n     return_tensors=\"pt\"\n )\n-input_features = input_features.to(\"cuda\", dtype=torch.float16)\n+input_features = input_features.to(model.device, dtype=torch.float16)\n \n predicted_ids = model.generate(**input_features, cache_implementation=\"static\")\n transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)"
        },
        {
            "sha": "2bd14229c37c3bb1b4b9199c24949f56768f2156",
            "filename": "docs/source/en/model_doc/sam_hq.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -57,17 +57,17 @@ Below is an example on how to run mask generation given an image and a 2D point:\n import torch\n from PIL import Image\n import requests\n-from transformers import SamHQModel, SamHQProcessor\n+from transformers import infer_device, SamHQModel, SamHQProcessor\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n model = SamHQModel.from_pretrained(\"syscv-community/sam-hq-vit-base\").to(device)\n processor = SamHQProcessor.from_pretrained(\"syscv-community/sam-hq-vit-base\")\n \n img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n input_points = [[[450, 600]]]  # 2D location of a window in the image\n \n-inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(device)\n+inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(model.device)\n with torch.no_grad():\n     outputs = model(**inputs)\n \n@@ -83,9 +83,9 @@ You can also process your own masks alongside the input images in the processor\n import torch\n from PIL import Image\n import requests\n-from transformers import SamHQModel, SamHQProcessor\n+from transformers import infer_device, SamHQModel, SamHQProcessor\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n model = SamHQModel.from_pretrained(\"syscv-community/sam-hq-vit-base\").to(device)\n processor = SamHQProcessor.from_pretrained(\"syscv-community/sam-hq-vit-base\")\n \n@@ -95,7 +95,7 @@ mask_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets\n segmentation_map = Image.open(requests.get(mask_url, stream=True).raw).convert(\"1\")\n input_points = [[[450, 600]]]  # 2D location of a window in the image\n \n-inputs = processor(raw_image, input_points=input_points, segmentation_maps=segmentation_map, return_tensors=\"pt\").to(device)\n+inputs = processor(raw_image, input_points=input_points, segmentation_maps=segmentation_map, return_tensors=\"pt\").to(model.device)\n with torch.no_grad():\n     outputs = model(**inputs)\n \n@@ -141,4 +141,4 @@ A list of official Hugging Face and community (indicated by ) resources to h\n ## SamHQModel\n \n [[autodoc]] SamHQModel\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "9e6c63540ca3ca087bc422b1c35c9504ae26e33a",
            "filename": "docs/source/en/model_doc/stablelm.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -44,8 +44,8 @@ We also provide StableLM Zephyr 3B, an instruction fine-tuned version of the mod\n The following code snippet demonstrates how to use `StableLM 3B 4E1T` for inference:\n \n ```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n->>> device = \"cuda\" # the device to load the model onto\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device, set_seed\n+>>> device = infer_device() # the device to load the model onto\n \n >>> set_seed(0)\n \n@@ -75,8 +75,8 @@ Now, to run the model with Flash Attention 2, refer to the snippet below:\n \n ```python\n >>> import torch\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n->>> device = \"cuda\" # the device to load the model onto\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device, set_seed\n+>>> device = infer_device() # the device to load the model onto\n \n >>> set_seed(0)\n "
        },
        {
            "sha": "22ab81488bf6600cc02243e2aa3bcd823182e7b2",
            "filename": "docs/source/en/model_doc/swin.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -64,12 +64,13 @@ image_processor = AutoImageProcessor.from_pretrained(\n )\n model = AutoModelForImageClassification.from_pretrained(\n     \"microsoft/swin-tiny-patch4-window7-224\",\n-    device_map=\"cuda\"\n+    device_map=\"auto\"\n )\n \n+device = infer_device()\n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = Image.open(requests.get(url, stream=True).raw)\n-inputs = image_processor(image, return_tensors=\"pt\").to(\"cuda\")\n+inputs = image_processor(image, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n   logits = model(**inputs).logits\n@@ -128,4 +129,4 @@ print(f\"The predicted class label is: {predicted_class_label}\")\n     - call\n \n </tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "f3a70b6ec8fcf8630e823d30d80b2f98e3e372b6",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -48,7 +48,7 @@ pipe = pipeline(\n     \"text2text-generation\",\n     model=\"google/t5gemma-2b-2b-prefixlm-it\",\n     torch_dtype=torch.bfloat16,\n-    device=\"cuda\",  # replace with \"mps\" to run on a Mac device\n+    device_map=\"auto\",\n )\n \n messages = [\n@@ -64,8 +64,8 @@ pipe(prompt, max_new_tokens=32)\n \n ```python\n # pip install accelerate\n-from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n import torch\n+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"google/t5gemma-2b-2b-prefixlm-it\")\n model = AutoModelForSeq2SeqLM.from_pretrained(\n@@ -77,7 +77,7 @@ model = AutoModelForSeq2SeqLM.from_pretrained(\n messages = [\n     {\"role\": \"user\", \"content\": \"Tell me an unknown interesting biology fact about the brain.\"},\n ]\n-input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(\"cuda\")\n+input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(model.device)\n \n outputs = model.generate(**input_ids, max_new_tokens=32)\n print(tokenizer.decode(outputs[0]))"
        },
        {
            "sha": "fda22f1c0f56a84a2d035b9422061db1d58c0311",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -46,16 +46,18 @@ The example below demonstrates how to reconstruct the missing pixels with the [`\n import torch\n import requests\n from PIL import Image\n-from transformers import ViTImageProcessor, ViTMAEForPreTraining\n+from transformers import infer_device, ViTImageProcessor, ViTMAEForPreTraining\n+\n+device = infer_device()\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = Image.open(requests.get(url, stream=True).raw)\n \n processor = ViTImageProcessor.from_pretrained(\"facebook/vit-mae-base\")\n inputs = processor(image, return_tensors=\"pt\")\n-inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n+inputs = {k: v.to(device) for k, v in inputs.items()}\n \n-model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\", attn_implementation=\"sdpa\").to(\"cuda\")\n+model = ViTMAEForPreTraining.from_pretrained(\"facebook/vit-mae-base\", attn_implementation=\"sdpa\").to(device)\n with torch.no_grad():\n     outputs = model(**inputs)\n "
        },
        {
            "sha": "bc298ec6ddfe4f591f54fdb8dad7ccc7edbcadda",
            "filename": "docs/source/en/model_doc/vits.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvits.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -64,8 +64,8 @@ from IPython.display import Audio\n from transformers import AutoTokenizer, VitsModel, set_seed\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/mms-tts-eng\")\n-model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\", torch_dtype=torch.float16).to(\"cuda\")\n-inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(\"cuda\")\n+model = VitsModel.from_pretrained(\"facebook/mms-tts-eng\", device_map=\"auto\", torch_dtype=torch.float16)\n+inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\").to(model.device)\n \n set_seed(555)\n "
        },
        {
            "sha": "3bc4821026093a58676fae0bdb6df876a1c4bb0f",
            "filename": "docs/source/en/model_doc/vjepa2.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -76,9 +76,9 @@ import torch\n import numpy as np\n \n from torchcodec.decoders import VideoDecoder\n-from transformers import AutoVideoProcessor, AutoModelForVideoClassification\n+from transformers import AutoVideoProcessor, AutoModelForVideoClassification, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n \n # Load model and video preprocessor\n hf_repo = \"facebook/vjepa2-vitl-fpc16-256-ssv2\""
        },
        {
            "sha": "8652dfbf4fce2271721b477a6460404e7231326f",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -44,10 +44,10 @@ The model supports audio-text instructions, including multi-turn and multi-audio\n \n  audio + text instruction\n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -80,10 +80,10 @@ print(\"=\" * 80)\n \n  multi-audio + text instruction \n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -120,10 +120,10 @@ print(\"=\" * 80)\n \n  multi-turn:\n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -174,10 +174,10 @@ print(\"=\" * 80)\n \n  text only:\n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -209,10 +209,10 @@ print(\"=\" * 80)\n \n  audio only:\n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -244,10 +244,10 @@ print(\"=\" * 80)\n \n  batched inference!\n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device()\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -305,10 +305,10 @@ for decoded_output in decoded_outputs:\n Use the model to transcribe audio (supports English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian)!\n \n ```python\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor\n import torch\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)"
        },
        {
            "sha": "706f1164ad800af1eb33c895b0ecd9dff2d673f8",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -63,14 +63,16 @@ detector(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.pn\n import torch\n from PIL import Image\n import requests\n-from transformers import AutoImageProcessor, AutoModelForObjectDetection\n+from transformers import AutoImageProcessor, AutoModelForObjectDetection, infer_device\n+\n+device = infer_device()\n \n processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-base\")\n-model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\", torch_dtype=torch.float16, attn_implementation=\"sdpa\").to(\"cuda\")\n+model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\", torch_dtype=torch.float16, attn_implementation=\"sdpa\").to(device)\n \n url = \"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"\n image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n-inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n+inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)"
        },
        {
            "sha": "ae542276b44f06f31711beb48175c70d2b3cd071",
            "filename": "docs/source/en/model_doc/zamba2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzamba2.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -46,14 +46,14 @@ pip install transformers>=4.48.0\n ## Inference\n \n ```python\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n \n tokenizer = AutoTokenizer.from_pretrained(\"Zyphra/Zamba2-7B\")\n-model = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba2-7B\", device_map=\"cuda\", torch_dtype=torch.bfloat16)\n+model = AutoModelForCausalLM.from_pretrained(\"Zyphra/Zamba2-7B\", device_map=\"auto\", torch_dtype=torch.bfloat16)\n \n input_text = \"What factors contributed to the fall of the Roman Empire?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n outputs = model.generate(**input_ids, max_new_tokens=100)\n print(tokenizer.decode(outputs[0]))"
        },
        {
            "sha": "81ab60fecd0cb87beaccaa1362f39855c4c4892f",
            "filename": "docs/source/en/tasks/idefics.md",
            "status": "modified",
            "additions": 11,
            "deletions": 13,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fidefics.md?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -84,7 +84,7 @@ manner given existing devices.\n \n ### Quantized model\n \n-If high-memory GPU availability is an issue, you can load the quantized version of the model. To load the model and the \n+If high-memory device availability is an issue, you can load the quantized version of the model. To load the model and the \n processor in 4bit precision, pass a `BitsAndBytesConfig` to the `from_pretrained` method and the model will be compressed \n on the fly while loading.\n \n@@ -131,7 +131,7 @@ As image input to the model, you can use either an image object (`PIL.Image`) or\n ...     \"https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80\",\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n@@ -167,7 +167,7 @@ Textual and image prompts can be passed to the model's processor as a single lis\n ...     \"This is an image of \",\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n@@ -201,7 +201,7 @@ Photo by [Juan Mayobre](https://unsplash.com/@jmayobres).\n ...            \"Describe this image.\\nAssistant:\"\n ...            ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\n@@ -239,7 +239,7 @@ You can steer the model from image captioning to visual question answering by pr\n ...     \"Question: Where are these people and what's the weather like? Answer:\"\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)\n@@ -272,7 +272,7 @@ We can instruct the model to classify the image into one of the categories that\n ...     \"Category: \"\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\n@@ -302,7 +302,7 @@ Photo by [Craig Tidball](https://unsplash.com/@devonshiremedia).\n ...     \"https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80\",\n ...     \"Story: \\n\"]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=200, bad_words_ids=bad_words_ids)\n@@ -356,7 +356,7 @@ for a batch of examples by passing a list of prompts:\n ...     ],\n ... ]\n \n->>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompts, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n@@ -386,11 +386,9 @@ The use and prompting for the conversational use is very similar to using the ba\n ```py\n >>> import torch\n >>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n->>> from accelerate.test_utils.testing import get_backend\n \n->>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n >>> prompts = [\n@@ -410,9 +408,9 @@ The use and prompting for the conversational use is very similar to using the ba\n ... ]\n \n >>> # --batched mode\n->>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(device)\n+>>> inputs = processor(prompts, add_end_of_utterance_token=False, return_tensors=\"pt\").to(model.device)\n >>> # --single sample mode\n->>> # inputs = processor(prompts[0], return_tensors=\"pt\").to(device)\n+>>> # inputs = processor(prompts[0], return_tensors=\"pt\").to(model.device)\n \n >>> # Generation args\n >>> exit_condition = processor.tokenizer(\"<end_of_utterance>\", add_special_tokens=False).input_ids"
        },
        {
            "sha": "2c0e6f45bf1b597fdcc82189a409943df02e7153",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -488,6 +488,7 @@\n         \"Conv1D\",\n         \"apply_chunking_to_forward\",\n         \"prune_layer\",\n+        \"infer_device\",\n     ]\n     _import_structure[\"sagemaker\"] = []\n     _import_structure[\"time_series_utils\"] = []"
        },
        {
            "sha": "06d3c827b792a76a1dea7622baf2adbb97532cf7",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 1,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cc9997878a5ffefafbb659dbbfc5c7d2a2b78776/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=cc9997878a5ffefafbb659dbbfc5c7d2a2b78776",
            "patch": "@@ -21,7 +21,13 @@\n from safetensors.torch import storage_ptr, storage_size\n from torch import nn\n \n-from .utils import is_torch_greater_or_equal, is_torch_xla_available, is_torchdynamo_compiling, logging\n+from .utils import (\n+    is_torch_greater_or_equal,\n+    is_torch_xla_available,\n+    is_torch_xpu_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n \n \n ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n@@ -359,3 +365,16 @@ def wrapper(*args, **kwargs):\n         return wrapper\n \n     return decorator\n+\n+\n+def infer_device():\n+    \"\"\"\n+    Infers available device.\n+    \"\"\"\n+    torch_device = \"cpu\"\n+    if torch.cuda.is_available():\n+        torch_device = \"cuda\"\n+    elif is_torch_xpu_available():\n+        torch_device = \"xpu\"\n+\n+    return torch_device"
        }
    ],
    "stats": {
        "total": 383,
        "additions": 205,
        "deletions": 178
    }
}