{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ Delete generation params from model config (#41695)\n\n* i am so confused, too many circular dependencies. Delete and see what happens\n\n* pop if exists\n\n* fix  a few tests\n\n* fix loading generation params from model config\n\n* oh no, revert this\n\n* replace audios with audio in docs\n\n* fix tests\n\n* fix last test\n\n* i am dumb, typo\n\n* Update src/transformers/generation/configuration_utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update tests/utils/test_modeling_utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
    "files": [
        {
            "sha": "5d6fbda009dc760afc7ff029e9599ac225be3f5d",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 35,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -302,10 +302,9 @@ def __init__(\n         self.sep_token_id = sep_token_id\n         self.decoder_start_token_id = decoder_start_token_id\n \n-        # Retrocompatibility: Parameters for sequence generation. While we will keep the ability to load these\n-        # parameters, saving them will be deprecated. In a distant future, we won't need to load them.\n-        for parameter_name, default_value in self._get_global_generation_defaults().items():\n-            setattr(self, parameter_name, kwargs.pop(parameter_name, default_value))\n+        # Parameters for sequence generation saved in the config are popped instead of loading them.\n+        for parameter_name in self._get_global_generation_defaults().keys():\n+            kwargs.pop(parameter_name, None)\n \n         # Name or path to the pretrained checkpoint\n         self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n@@ -445,14 +444,11 @@ def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool =\n \n         non_default_generation_parameters = self._get_non_default_generation_parameters()\n         if len(non_default_generation_parameters) > 0:\n-            # TODO (joao): this should be an exception if the user has modified the loaded config. See #33886\n-            warnings.warn(\n+            raise ValueError(\n                 \"Some non-default generation parameters are set in the model config. These should go into either a) \"\n                 \"`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file \"\n                 \"(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).\"\n-                \"This warning will become an exception in the future.\"\n                 f\"\\nNon-default generation parameters: {str(non_default_generation_parameters)}\",\n-                UserWarning,\n             )\n \n         os.makedirs(save_directory, exist_ok=True)\n@@ -1101,40 +1097,18 @@ def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n         non_default_generation_parameters = {}\n         decoder_attribute_name = None\n \n-        # Some composite models don't have a default config, use their decoder config as a fallback for default values\n-        # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n-        if not self.has_no_defaults_at_init:\n-            default_config = self.__class__()\n-        else:\n-            decoder_config = self.get_text_config(decoder=True)\n-            if decoder_config is not self:\n-                default_config = decoder_config.__class__()\n-            else:\n-                default_config = None\n-\n         # If it is a composite model, we want to check the subconfig that will be used for generation\n         self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n \n         for parameter_name, default_global_value in self._get_global_generation_defaults().items():\n             if hasattr(self_decoder_config, parameter_name):\n-                is_default_in_config = is_default_generation_value = None\n-                parameter_value = getattr(self_decoder_config, parameter_name)\n-                # Three cases in which is okay for the model config to hold generation config parameters:\n+                parameter_value = getattr(self_decoder_config, parameter_name, None)\n+                # Two cases in which is okay for the model config to hold generation config parameters:\n                 # 1. The parameter is set to `None`, effectively delegating its value to the generation config\n-                if parameter_value is None:\n+                # 2. The parameter is set the global generation defaults\n+                if parameter_value is None or parameter_value == default_global_value:\n                     continue\n-                # 2. If we have a default config, then the instance should hold the same generation defaults\n-                if default_config is not None:\n-                    is_default_in_config = parameter_value == getattr(default_config, parameter_name)\n-                # 3. if we don't have a default config, then the instance should hold the global generation defaults\n-                else:\n-                    is_default_generation_value = parameter_value == default_global_value\n-\n-                is_non_default = (is_default_in_config is False) or (\n-                    is_default_in_config is None and is_default_generation_value is False\n-                )\n-                if is_non_default:\n-                    non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n+                non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n \n         return non_default_generation_parameters\n "
        },
        {
            "sha": "79db89411bc1bfb6c89300f5f40d9fb2dc035753",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 12,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -918,7 +918,9 @@ def from_pretrained(\n         else:\n             logger.info(f\"loading configuration file {configuration_file} from cache at {resolved_config_file}\")\n \n-        if kwargs.get(\"return_unused_kwargs\") is True:\n+        if kwargs.get(\"_from_model_config\", False):\n+            return cls.from_model_config(config_dict)\n+        elif kwargs.get(\"return_unused_kwargs\") is True:\n             config, unused_kwargs = cls.from_dict(config_dict, **kwargs)\n             config._original_object_hash = hash(config)  # Hash to detect whether the instance was modified\n             return config, unused_kwargs\n@@ -1084,19 +1086,19 @@ def to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool =\n             writer.write(self.to_json_string(use_diff=use_diff))\n \n     @classmethod\n-    def from_model_config(cls, model_config: PreTrainedConfig) -> \"GenerationConfig\":\n+    def from_model_config(cls, model_config: PreTrainedConfig | dict) -> \"GenerationConfig\":\n         \"\"\"\n         Instantiates a [`GenerationConfig`] from a [`PreTrainedConfig`]. This function is useful to convert legacy\n         [`PreTrainedConfig`] objects, which may contain generation parameters, into a stand-alone [`GenerationConfig`].\n \n         Args:\n-            model_config (`PreTrainedConfig`):\n+            model_config (`PreTrainedConfig | dict`):\n                 The model config that will be used to instantiate the generation config.\n \n         Returns:\n             [`GenerationConfig`]: The configuration object instantiated from those parameters.\n         \"\"\"\n-        config_dict = model_config.to_dict()\n+        config_dict = model_config.to_dict() if not isinstance(model_config, dict) else model_config\n         config_dict.pop(\"_from_model_config\", None)\n \n         # Removes all `None` from the model config dict -- this lets the generation config defaults to take hold\n@@ -1106,14 +1108,15 @@ def from_model_config(cls, model_config: PreTrainedConfig) -> \"GenerationConfig\"\n \n         # Special case: some models have generation attributes set in the decoder. Use them if still unset in the\n         # generation config (which in turn is defined from the outer attributes of model config).\n-        decoder_config = model_config.get_text_config(decoder=True)\n-        if decoder_config is not model_config:\n-            default_generation_config = GenerationConfig()\n-            decoder_config_dict = decoder_config.to_dict()\n-            for attr in generation_config.to_dict():\n-                is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n-                if attr in decoder_config_dict and is_unset:\n-                    setattr(generation_config, attr, decoder_config_dict[attr])\n+        if not isinstance(model_config, dict):\n+            decoder_config = model_config.get_text_config(decoder=True)\n+            if decoder_config is not model_config:\n+                default_generation_config = GenerationConfig()\n+                decoder_config_dict = decoder_config.to_dict()\n+                for attr in generation_config.to_dict():\n+                    is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n+                    if attr in decoder_config_dict and is_unset:\n+                        setattr(generation_config, attr, decoder_config_dict[attr])\n \n         # If any `output_...` flag is set to `True`, we ensure `return_dict_in_generate` is set to `True`.\n         if generation_config.return_dict_in_generate is False:"
        },
        {
            "sha": "b3685da588be07f6a2493af4633e34ba33192440",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 5,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -410,6 +410,14 @@ def adjust_generation_fn(\n                 logger.info(\n                     \"Generation config file not found, using a generation config created from the model config.\"\n                 )\n+                self.generation_config = GenerationConfig.from_pretrained(\n+                    pretrained_model_name_or_path,\n+                    config_file_name=\"config.json\",\n+                    _from_auto=from_auto_class,\n+                    _from_pipeline=from_pipeline,\n+                    _from_model_config=True,\n+                    **repo_loading_kwargs,\n+                )\n             # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n             if hasattr(self, \"load_custom_generate\") and trust_remote_code:\n                 try:\n@@ -1778,14 +1786,12 @@ def _prepare_generation_config(\n             ):\n                 new_generation_config = GenerationConfig.from_model_config(self.config)\n                 if new_generation_config != self.generation_config:  # 4)\n-                    warnings.warn(\n-                        \"You have modified the pretrained model configuration to control generation. This is a\"\n-                        \" deprecated strategy to control generation and will be removed in v5.\"\n+                    raise ValueError(\n+                        \"You have modified the pretrained model configuration to control generation.\"\n+                        \" This strategy to control generation is not supported anymore. \"\n                         \" Please use and modify the model generation configuration (see\"\n                         \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\",\n-                        UserWarning,\n                     )\n-                    self.generation_config = new_generation_config\n \n             generation_config = self.generation_config\n             using_model_generation_config = True"
        },
        {
            "sha": "28077c38e3dbbf700980bce3405d43604282896a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -1456,6 +1456,8 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n         self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(\n             self.config._attn_implementation, is_init_check=True\n         )\n+        if self.can_generate():\n+            self.generation_config = GenerationConfig.from_model_config(config)\n \n         # for initialization of the loss\n         loss_type = self.__class__.__name__\n@@ -1470,8 +1472,6 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n \n         self.name_or_path = config.name_or_path\n         self.warnings_issued = {}\n-        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n-\n         # Overwrite the class attribute to make it an instance attribute, so models like\n         # `InstructBlipForConditionalGeneration` can dynamically update it without modifying the class attribute\n         # when a different component (e.g. language_model) is used.\n@@ -3174,19 +3174,6 @@ def save_pretrained(\n         # Save the config\n         if is_main_process:\n             if not _hf_peft_config_loaded:\n-                # If the model config has set attributes that should be in the generation config, move them there.\n-                misplaced_generation_parameters = model_to_save.config._get_non_default_generation_parameters()\n-                if self.can_generate() and len(misplaced_generation_parameters) > 0:\n-                    warnings.warn(\n-                        \"Moving the following attributes in the config to the generation config: \"\n-                        f\"{misplaced_generation_parameters}. You are seeing this warning because you've set \"\n-                        \"generation parameters in the model config, as opposed to in the generation config.\",\n-                        UserWarning,\n-                    )\n-                    for param_name, param_value in misplaced_generation_parameters.items():\n-                        setattr(model_to_save.generation_config, param_name, param_value)\n-                        setattr(model_to_save.config, param_name, None)\n-\n                 model_to_save.config.save_pretrained(save_directory)\n             if self.can_generate():\n                 model_to_save.generation_config.save_pretrained(save_directory)"
        },
        {
            "sha": "293929ecfc11500787c1c3d369d6343fdd4483c5",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -166,11 +166,10 @@ def __init__(\n         )\n         self.tie_encoder_decoder = True\n         # ensure backward compatibility for BART CNN models\n-        if self.forced_bos_token_id is None and kwargs.get(\"force_bos_token_to_be_generated\", False):\n+        if kwargs.get(\"force_bos_token_to_be_generated\", False):\n             self.forced_bos_token_id = self.bos_token_id\n             warnings.warn(\n-                f\"Please make sure the config includes `forced_bos_token_id={self.bos_token_id}` in future versions. \"\n-                \"The config can simply be saved and uploaded again to be fixed.\"\n+                f\"Please make sure the generation config includes `forced_bos_token_id={self.bos_token_id}`. \"\n             )\n \n "
        },
        {
            "sha": "99cd2560c21127342e2f21f00d861fc1500d975a",
            "filename": "src/transformers/models/mvp/configuration_mvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Fmvp%2Fconfiguration_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Fmvp%2Fconfiguration_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fconfiguration_mvp.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -172,10 +172,10 @@ def __init__(\n             **kwargs,\n         )\n \n-        if self.forced_bos_token_id is None and kwargs.get(\"force_bos_token_to_be_generated\", False):\n+        if kwargs.get(\"force_bos_token_to_be_generated\", False):\n             self.forced_bos_token_id = self.bos_token_id\n             warnings.warn(\n-                f\"Please make sure the config includes `forced_bos_token_id={self.bos_token_id}` in future versions. \"\n+                f\"Please make sure the generated config includes `forced_bos_token_id={self.bos_token_id}` . \"\n                 \"The config can simply be saved and uploaded again to be fixed.\"\n             )\n "
        },
        {
            "sha": "855175c6e2dd9d5a2105038eac2784b71c8c1626",
            "filename": "src/transformers/models/rag/configuration_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -166,7 +166,7 @@ def __init__(\n \n         self.use_cache = use_cache\n \n-        if self.forced_eos_token_id is None:\n+        if forced_eos_token_id is None:\n             self.forced_eos_token_id = getattr(self.generator, \"forced_eos_token_id\", None)\n \n     @classmethod"
        },
        {
            "sha": "4007d9b0926ce8f275d870fafee91d285f691f60",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -1060,7 +1060,6 @@ def __init__(self, config):\n         self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model)\n         self.embed_patches = UdopPatchEmbeddings(config)\n         self.is_decoder = config.is_decoder\n-        self._max_length = config.max_length\n         self.num_layers = config.num_layers\n \n         self.block = nn.ModuleList("
        },
        {
            "sha": "eba59535b6b023e7df33f0ca125d40b76393f59f",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -975,9 +975,9 @@ def test_xsum_summarization_same_as_fairseq(self):\n         self.assertEqual(EXPECTED_SUMMARY, decoded[0])\n \n     def test_xsum_config_generation_params(self):\n-        config = BartConfig.from_pretrained(\"facebook/bart-large-xsum\")\n+        model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-xsum\")\n         expected_params = {\"num_beams\": 6, \"do_sample\": False, \"early_stopping\": True, \"length_penalty\": 1.0}\n-        config_params = {k: getattr(config, k, \"MISSING\") for k, v in expected_params.items()}\n+        config_params = {k: getattr(model.generation_config, k, \"MISSING\") for k, v in expected_params.items()}\n         self.assertDictEqual(expected_params, config_params)\n \n     @slow"
        },
        {
            "sha": "8fde1f57d405f0ac158f2205b9ac2d75ff7a85b1",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -504,9 +504,9 @@ def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config\n         generated_output = enc_dec_model.generate(\n             input_ids,\n             decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id,\n-            max_length=decoder_config.max_length,\n+            max_length=enc_dec_model.generation_config.max_length,\n         )\n-        self.assertEqual(generated_output.shape, (input_ids.shape[0],) + (decoder_config.max_length,))\n+        self.assertEqual(generated_output.shape, (input_ids.shape[0],) + (enc_dec_model.generation_config.max_length,))\n \n     def create_and_check_encoder_decoder_shared_weights(\n         self,"
        },
        {
            "sha": "34ac5a4b8eca7450d85a05dde8a17e8cfec6e637",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -369,9 +369,9 @@ def check_encoder_decoder_model_generate(\n         generated_output = enc_dec_model.generate(\n             inputs,\n             decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id,\n-            max_length=decoder_config.max_length,\n+            max_length=enc_dec_model.generation_config.max_length,\n         )\n-        self.assertEqual(generated_output.shape, (inputs.shape[0],) + (decoder_config.max_length,))\n+        self.assertEqual(generated_output.shape, (inputs.shape[0],) + (enc_dec_model.generation_config.max_length,))\n \n     def test_encoder_decoder_model(self):\n         input_ids_dict = self.prepare_config_and_inputs()"
        },
        {
            "sha": "06c6aa039d22eb79a1c3616e8e7a94ef403c5f59",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -313,9 +313,9 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             inputs,\n             decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id,\n-            max_length=decoder_config.max_length,\n+            max_length=enc_dec_model.generation_config.max_length,\n         )\n-        self.assertEqual(generated_output.shape, (inputs.shape[0],) + (decoder_config.max_length,))\n+        self.assertEqual(generated_output.shape, (inputs.shape[0],) + (enc_dec_model.generation_config.max_length,))\n \n     def test_encoder_decoder_model(self):\n         input_ids_dict = self.prepare_config_and_inputs()\n@@ -883,10 +883,12 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n             decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n-            max_length=decoder_config.max_length,\n+            max_length=enc_dec_model.generation_config.max_length,\n             **kwargs,\n         )\n-        self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n+        self.assertEqual(\n+            generated_output.shape, (pixel_values.shape[0],) + (enc_dec_model.generation_config.max_length,)\n+        )\n \n     @unittest.skip(reason=\"There are no published pretrained TrOCR checkpoints for now\")\n     def test_real_model_save_load_from_pretrained(self):\n@@ -994,10 +996,12 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n             decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n-            max_length=decoder_config.max_length,\n+            max_length=enc_dec_model.generation_config.max_length,\n             **kwargs,\n         )\n-        self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n+        self.assertEqual(\n+            generated_output.shape, (pixel_values.shape[0],) + (enc_dec_model.generation_config.max_length,)\n+        )\n \n     @unittest.skip(reason=\"VIT2GPT2 also has an integration test for testinf save-load\")\n     def test_real_model_save_load_from_pretrained(self):\n@@ -1105,10 +1109,12 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n             decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n-            max_length=decoder_config.max_length,\n+            max_length=enc_dec_model.generation_config.max_length,\n             **kwargs,\n         )\n-        self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n+        self.assertEqual(\n+            generated_output.shape, (pixel_values.shape[0],) + (enc_dec_model.generation_config.max_length,)\n+        )\n \n     @unittest.skip(reason=\"Donut has an Integration test for that\")\n     def test_real_model_save_load_from_pretrained(self):"
        },
        {
            "sha": "d1718134a9eb5fb3d22944b21ebfd2bc41b063f9",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 31,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -43,29 +43,7 @@\n     \"cross_attention_hidden_size\": 128,\n     \"add_cross_attention\": True,\n     \"tie_encoder_decoder\": True,\n-    \"max_length\": 50,\n-    \"min_length\": 3,\n-    \"do_sample\": True,\n-    \"early_stopping\": True,\n-    \"num_beams\": 3,\n-    \"num_beam_groups\": 3,\n-    \"diversity_penalty\": 0.5,\n-    \"temperature\": 2.0,\n-    \"top_k\": 10,\n-    \"top_p\": 0.7,\n-    \"typical_p\": 0.2,\n-    \"repetition_penalty\": 0.8,\n-    \"length_penalty\": 0.8,\n-    \"no_repeat_ngram_size\": 5,\n-    \"encoder_no_repeat_ngram_size\": 5,\n-    \"bad_words_ids\": [1, 2, 3],\n-    \"num_return_sequences\": 3,\n     \"chunk_size_feed_forward\": 5,\n-    \"output_scores\": True,\n-    \"return_dict_in_generate\": True,\n-    \"forced_bos_token_id\": 2,\n-    \"forced_eos_token_id\": 3,\n-    \"remove_invalid_values\": True,\n     \"architectures\": [\"BertModel\"],\n     \"finetuning_task\": \"translation\",\n     \"id2label\": {0: \"label\"},\n@@ -77,9 +55,6 @@\n     \"eos_token_id\": 8,\n     \"sep_token_id\": 9,\n     \"decoder_start_token_id\": 10,\n-    \"exponential_decay_length_penalty\": (5, 1.01),\n-    \"suppress_tokens\": [0, 1],\n-    \"begin_suppress_tokens\": 2,\n     \"task_specific_params\": {\"translation\": \"some_params\"},\n     \"problem_type\": \"regression\",\n }\n@@ -275,19 +250,19 @@ def test_repo_versioning_before(self):\n         old_configuration = old_transformers.models.auto.AutoConfig.from_pretrained(repo)\n         self.assertEqual(old_configuration.hidden_size, 768)\n \n-    def test_saving_config_with_custom_generation_kwargs_raises_warning(self):\n-        config = BertConfig(min_length=3)  # `min_length = 3` is a non-default generation kwarg\n+    def test_saving_config_with_custom_generation_kwargs_raises_error(self):\n+        config = BertConfig()\n+        config.min_length = 3  # `min_length = 3` is a non-default generation kwarg\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            with self.assertWarns(UserWarning) as cm:\n+            with self.assertRaises(ValueError):\n                 config.save_pretrained(tmp_dir)\n-            self.assertIn(\"min_length\", str(cm.warning))\n \n     def test_get_non_default_generation_parameters(self):\n         config = BertConfig()\n         self.assertFalse(len(config._get_non_default_generation_parameters()) > 0)\n-        config = BertConfig(min_length=3)\n+        config.min_length = 3\n         self.assertTrue(len(config._get_non_default_generation_parameters()) > 0)\n-        config = BertConfig(min_length=0)  # `min_length = 0` is a default generation kwarg\n+        config.min_length = 0  # `min_length = 0` is a default generation kwarg\n         self.assertFalse(len(config._get_non_default_generation_parameters()) > 0)\n \n     def test_loading_config_do_not_raise_future_warnings(self):"
        },
        {
            "sha": "bff95670d65488a19bd1c0d2f015694b36301662",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 39,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=b1bdf9cb390f4d11f3e2eb89bd7c75c0cae8b9a1",
            "patch": "@@ -1608,30 +1608,19 @@ def test_safetensors_torch_from_torch_sharded(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-    def test_modifying_model_config_gets_moved_to_generation_config(self):\n+    def test_saving_model_config_with_generation_params(self):\n         \"\"\"\n-        Calling `model.save_pretrained` should move the changes made to `generate` parameterization in the model config\n-        to the generation config.\n+        Calling `model.save_pretrained` with generation parameters should raise a `ValueError`\n         \"\"\"\n         model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-        # Initially, the repetition penalty has its default value in `model.config`. The `model.generation_config` will\n-        # have the exact same default\n-        self.assertTrue(model.config.repetition_penalty == 1.0)\n         self.assertTrue(model.generation_config.repetition_penalty == 1.0)\n-        # If the user attempts to save a custom generation parameter:\n+        self.assertFalse(hasattr(model.config, \"repetition_penalty\"))\n+\n+        # If the user attempts to save a custom generation parameter, we raise an Error\n         model.config.repetition_penalty = 3.0\n-        with warnings.catch_warnings(record=True) as warning_list:\n+        with self.assertRaises(ValueError):\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.save_pretrained(tmp_dir)\n-                # 1 - That parameter will be removed from `model.config`. We don't want to use `model.config` to store\n-                # generative parameters, and the old default (1.0) would no longer reflect the user's wishes.\n-                self.assertTrue(model.config.repetition_penalty is None)\n-                # 2 - That parameter will be set in `model.generation_config` instead.\n-                self.assertTrue(model.generation_config.repetition_penalty == 3.0)\n-        # 3 - The user will see a warning regarding the custom parameter that has been moved.\n-        self.assertTrue(len(warning_list) == 1)\n-        self.assertTrue(\"Moving the following attributes\" in str(warning_list[0].message))\n-        self.assertTrue(\"repetition_penalty\" in str(warning_list[0].message))\n \n     def test_model_from_pretrained_from_mlx(self):\n         from safetensors import safe_open\n@@ -1729,35 +1718,32 @@ def prepare_inputs_for_generation(self):\n \n     def test_save_and_load_config_with_custom_generation(self):\n         \"\"\"\n-        Regression test for the ability to save and load a config with a custom generation kwarg (i.e. a parameter\n-        that gets moved to the generation config and reset on the model config)\n+        Tests that saving and loading a config with a custom generation kwarg is not possible\n         \"\"\"\n         model = T5ForConditionalGeneration.from_pretrained(TINY_T5)\n \n         # The default for `num_beams` is 1 and `early_stopping` is False\n-        self.assertTrue(model.config.num_beams == 1)\n-        self.assertTrue(model.config.early_stopping is False)\n-\n-        # When we save the model, this custom parameter should be moved to the generation config AND the model\n-        # config should contain `None`\n-        model.config.num_beams = 2\n+        # NOTE: accessible only from generation config, EVEN IF they are saved\n+        # in `config.json` file in the hub\n+        self.assertTrue(model.generation_config.num_beams == 1)\n+        self.assertTrue(model.generation_config.early_stopping is False)\n+        self.assertFalse(hasattr(model.config, \"num_beams\"))\n+        self.assertFalse(hasattr(model.config, \"early_stopping\"))\n+\n+        # Sanity check: We can run `generate` with the model without any warnings\n+        random_ids = torch.randint(0, 100, (1, 5))\n+        with warnings.catch_warnings(record=True) as w:\n+            model.generate(random_ids, max_new_tokens=3)\n+        self.assertTrue(len(w) == 0)\n+\n+        # When we save the model and config has generation-related parameter,\n+        # we will throw an error, nudging user to save attributes in the generation_config\n+        model.config.num_beams = 5\n         model.config.early_stopping = True\n         self.assertTrue(model.generation_config.num_beams == 1)  # unmodified generation config\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir)\n-            new_model = T5ForConditionalGeneration.from_pretrained(tmp_dir)\n-            # moved to generation config\n-            self.assertTrue(new_model.generation_config.num_beams == 2)\n-            self.assertTrue(new_model.generation_config.early_stopping is True)\n-            # reset in the model config\n-            self.assertTrue(new_model.config.num_beams is None)\n-            self.assertTrue(new_model.config.early_stopping is None)\n-\n-            # Sanity check: We can run `generate` with the new model without any warnings\n-            random_ids = torch.randint(0, 100, (1, 5))\n-            with warnings.catch_warnings(record=True) as w:\n-                new_model.generate(random_ids, max_new_tokens=3)\n-            self.assertTrue(len(w) == 0)\n+            with self.assertRaises(ValueError):\n+                model.save_pretrained(tmp_dir)\n \n     def test_load_model_with_state_dict_only(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")"
        }
    ],
    "stats": {
        "total": 251,
        "additions": 93,
        "deletions": 158
    }
}