{
    "author": "mfuntowicz",
    "message": "feat(kernels): add opt-out flag to disable kernels hub usage through the lib (#41990)\n\n* feat(kernels): add opt-out flag to disable kernels hub usage through the lib\n\n* misc(quality): style\n\n* test(kernels): add some opt-out test logic\n\n* chore(quality): style here we go again\n\n* chore(quality): style here we go again ... again\n\n* chore(quality): STYLE",
    "sha": "922e85487b496fb22f44b4a0b697f63726306c1d",
    "files": [
        {
            "sha": "60c8797176e8b82ffab8437e2229fea7c9c25266",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/922e85487b496fb22f44b4a0b697f63726306c1d/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/922e85487b496fb22f44b4a0b697f63726306c1d/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=922e85487b496fb22f44b4a0b697f63726306c1d",
            "patch": "@@ -11,14 +11,15 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import os\n import re\n from collections.abc import Callable\n from functools import partial\n from types import ModuleType\n from typing import Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n-from ..utils import logging\n+from ..utils import ENV_VARS_TRUE_VALUES, logging\n from ..utils.import_utils import is_kernels_available\n from .flash_attention import flash_attention_forward\n \n@@ -33,10 +34,22 @@\n         get_kernel,\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n-        use_kernel_forward_from_hub,\n     )\n \n+    _TRANSFORMERS_USE_HUB_KERNELS = os.environ.get(\"USE_HUB_KERNELS\", \"YES\").upper()\n     _kernels_available = True\n+    _kernels_enabled = _TRANSFORMERS_USE_HUB_KERNELS in ENV_VARS_TRUE_VALUES\n+\n+    def use_kernel_forward_from_hub(layer_name: str):\n+        if _kernels_enabled:\n+            from kernels import use_kernel_forward_from_hub as _kernels_use_kernel_forward_from_hub\n+\n+            return _kernels_use_kernel_forward_from_hub(layer_name)\n+        else:\n+            logger.warning_once(\n+                f\"kernels hub usage is disabled through the environment USE_HUB_KERNELS={_TRANSFORMERS_USE_HUB_KERNELS}\"\n+            )\n+            return lambda cls: cls\n \n     _KERNEL_MAPPING: dict[str, dict[Union[Device, str], LayerRepository]] = {\n         \"MultiScaleDeformableAttention\": {\n@@ -167,6 +180,7 @@ def register_kernel_mapping_transformers(mapping=None):\n \n except ImportError:\n     _kernels_available = False\n+    _kernels_enabled = False\n \n     # Stub to make decorators int transformers work when `kernels`\n     # is not installed."
        },
        {
            "sha": "d050bf6c9d64291505e8f0a3a89e0e8365113fa4",
            "filename": "tests/integrations/test_hub_kernels.py",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/922e85487b496fb22f44b4a0b697f63726306c1d/tests%2Fintegrations%2Ftest_hub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/922e85487b496fb22f44b4a0b697f63726306c1d/tests%2Fintegrations%2Ftest_hub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fintegrations%2Ftest_hub_kernels.py?ref=922e85487b496fb22f44b4a0b697f63726306c1d",
            "patch": "@@ -0,0 +1,122 @@\n+import os\n+import unittest\n+from unittest.mock import patch\n+\n+from transformers.testing_utils import require_kernels\n+\n+\n+@require_kernels\n+class HubKernelsTests(unittest.TestCase):\n+    def test_disable_hub_kernels(self):\n+        \"\"\"\n+        Test that _kernels_enabled is False when USE_HUB_KERNELS when USE_HUB_KERNELS=OFF\n+        \"\"\"\n+        with patch.dict(os.environ, {\"USE_HUB_KERNELS\": \"ON\"}):\n+            # Re-import to ensure the environment variable takes effect\n+            import importlib\n+\n+            from transformers.integrations import hub_kernels\n+\n+            importlib.reload(hub_kernels)\n+\n+            # Verify that kernels are disabled\n+            self.assertFalse(hub_kernels._kernels_enabled)\n+\n+    def test_enable_hub_kernels_default(self):\n+        \"\"\"\n+        Test that _kernels_enabled is True when USE_HUB_KERNELS is not provided (default behavior)\n+        \"\"\"\n+        # Remove USE_HUB_KERNELS from the environment if it exists\n+        env_without_hub_kernels = {k: v for k, v in os.environ.items() if k != \"USE_HUB_KERNELS\"}\n+        with patch.dict(os.environ, env_without_hub_kernels, clear=True):\n+            # Re-import to ensure the environment variable change takes effect\n+            import importlib\n+\n+            from transformers.integrations import hub_kernels\n+\n+            importlib.reload(hub_kernels)\n+\n+            # Verify that kernels are enabled by default\n+            self.assertTrue(hub_kernels._kernels_enabled)\n+\n+    def test_enable_hub_kernels_on(self):\n+        \"\"\"\n+        Test that _kernels_enabled is True when USE_HUB_KERNELS=ON\n+        \"\"\"\n+        with patch.dict(os.environ, {\"USE_HUB_KERNELS\": \"ON\"}):\n+            # Re-import to ensure the environment variable takes effect\n+            import importlib\n+\n+            from transformers.integrations import hub_kernels\n+\n+            importlib.reload(hub_kernels)\n+\n+            # Verify that kernels are enabled\n+            self.assertTrue(hub_kernels._kernels_enabled)\n+\n+    @patch(\"kernels.use_kernel_forward_from_hub\")\n+    def test_use_kernel_forward_from_hub_not_called_when_disabled(self, mocked_use_kernel_forward):\n+        \"\"\"\n+        Test that kernels.use_kernel_forward_from_hub is not called when USE_HUB_KERNELS is disabled\n+        \"\"\"\n+        # Set environment variable to disable hub kernels\n+        with patch.dict(os.environ, {\"USE_HUB_KERNELS\": \"OFF\"}):\n+            # Re-import to ensure the environment variable takes effect\n+            import importlib\n+\n+            from transformers.integrations import hub_kernels\n+\n+            importlib.reload(hub_kernels)\n+\n+            # Call the function with a test layer name\n+            decorator = hub_kernels.use_kernel_forward_from_hub(\"DummyLayer\")\n+\n+            # Verify that the kernels function was never called\n+            mocked_use_kernel_forward.assert_not_called()\n+\n+            # Verify that we get a no-op decorator\n+            class FooClass:\n+                pass\n+\n+            result = decorator(FooClass)\n+            self.assertIs(result, FooClass)\n+\n+    @patch(\"kernels.use_kernel_forward_from_hub\")\n+    def test_use_kernel_forward_from_hub_called_when_enabled_default(self, mocked_use_kernel_forward):\n+        \"\"\"\n+        Test that kernels.use_kernel_forward_from_hub is called when USE_HUB_KERNELS is not set (default)\n+        \"\"\"\n+        # Remove USE_HUB_KERNELS from the environment if it exists\n+        env_without_hub_kernels = {k: v for k, v in os.environ.items() if k != \"USE_HUB_KERNELS\"}\n+        with patch.dict(os.environ, env_without_hub_kernels, clear=True):\n+            # Re-import to ensure the environment variable change takes effect\n+            import importlib\n+\n+            from transformers.integrations import hub_kernels\n+\n+            importlib.reload(hub_kernels)\n+\n+            # Call the function with a test layer name\n+            hub_kernels.use_kernel_forward_from_hub(\"FooLayer\")\n+\n+            # Verify that the kernels function was called once with the correct argument\n+            mocked_use_kernel_forward.assert_called_once_with(\"FooLayer\")\n+\n+    @patch(\"kernels.use_kernel_forward_from_hub\")\n+    def test_use_kernel_forward_from_hub_called_when_enabled_on(self, mocked_use_kernel_forward):\n+        \"\"\"\n+        Test that kernels.use_kernel_forward_from_hub is called when USE_HUB_KERNELS=ON\n+        \"\"\"\n+        with patch.dict(os.environ, {\"USE_HUB_KERNELS\": \"ON\"}):\n+            # Re-import to ensure the environment variable change takes effect\n+            import importlib\n+\n+            from transformers.integrations import hub_kernels\n+\n+            importlib.reload(hub_kernels)\n+\n+            # Call the function with a test layer name\n+            hub_kernels.use_kernel_forward_from_hub(\"FooLayer\")\n+\n+            # Verify that the kernels function was called once with the correct argument\n+            mocked_use_kernel_forward.assert_called_once_with(\"FooLayer\")"
        }
    ],
    "stats": {
        "total": 140,
        "additions": 138,
        "deletions": 2
    }
}