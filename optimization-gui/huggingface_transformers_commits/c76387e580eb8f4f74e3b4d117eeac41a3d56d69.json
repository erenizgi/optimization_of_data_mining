{
    "author": "cyyever",
    "message": "Fix arguments (#40605)\n\n* Fix invalid arguments\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Fix typing\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Add missing self\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Add missing self and other fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n*  More fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n*  More fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
    "files": [
        {
            "sha": "14ab65c1f366a9d2dc8132b8a1397364cbaedd3b",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -637,7 +637,11 @@ def forward(\n         )\n \n     def get_extended_attention_mask(\n-        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n+        self,\n+        attention_mask: Tensor,\n+        input_shape: tuple[int],\n+        device: torch.device = None,\n+        dtype: Optional[torch.dtype] = None,\n     ) -> Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored."
        },
        {
            "sha": "1ccb5ef012207c6f06e6c74ed03358d3f28ea95e",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -269,7 +269,11 @@ def forward(\n         )\n \n     def get_extended_attention_mask(\n-        self, attention_mask: Tensor, input_shape: tuple[int], device: torch.device = None, dtype: torch.float = None\n+        self,\n+        attention_mask: Tensor,\n+        input_shape: tuple[int],\n+        device: torch.device = None,\n+        dtype: Optional[torch.dtype] = None,\n     ) -> Tensor:\n         \"\"\"\n         Makes broadcastable attention and causal masks so that future and masked tokens are ignored."
        },
        {
            "sha": "91e24f4fa8cd445294e83c4b9d12ab22b2e21054",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -1565,13 +1565,12 @@ def prepare_inputs_for_generation(\n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n-            # models currently cannot do asssisted decoding\n+            # models currently cannot do assisted decoding\n             if cache_position[0] == 0 or self.model.rope_deltas is None:\n                 vision_positions, rope_deltas = self.model.get_rope_index(\n                     model_inputs.get(\"input_ids\", None),\n                     image_grid_thw=image_grid_thw,\n                     video_grid_thw=video_grid_thw,\n-                    second_per_grid_ts=second_per_grid_ts,\n                     attention_mask=attention_mask,\n                 )\n                 self.model.rope_deltas = rope_deltas"
        },
        {
            "sha": "ada13b315c2309e3f8e4c55001928660bc3afb54",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -809,13 +809,12 @@ def prepare_inputs_for_generation(\n             # Calculate RoPE index once per generation in the pre-fill stage only.\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n-            # models currently cannot do asssisted decoding\n+            # models currently cannot do assisted decoding\n             if cache_position[0] == 0 or self.model.rope_deltas is None:\n                 vision_positions, rope_deltas = self.model.get_rope_index(\n                     model_inputs.get(\"input_ids\", None),\n                     image_grid_thw=image_grid_thw,\n                     video_grid_thw=video_grid_thw,\n-                    second_per_grid_ts=second_per_grid_ts,\n                     attention_mask=attention_mask,\n                 )\n                 self.model.rope_deltas = rope_deltas"
        },
        {
            "sha": "9d4602e31239925ef31b1c6329f29aba8a3662f2",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -108,13 +108,13 @@ class Sam2ImageProcessorFast(SamImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[Sam2FastImageProcessorKwargs]):\n         BaseImageProcessorFast.__init__(self, **kwargs)\n \n-    def pad_image():\n+    def pad_image(self):\n         raise NotImplementedError(\"No pad_image for SAM 2.\")\n \n-    def _get_preprocess_shape():\n+    def _get_preprocess_shape(self):\n         raise NotImplementedError(\"No _get_preprocess_shape for SAM 2.\")\n \n-    def resize():\n+    def resize(self):\n         raise NotImplementedError(\"No need to override resize for SAM 2.\")\n \n     def _preprocess("
        },
        {
            "sha": "a34b5d61d71a99fec9b9182f3f21c9243eff52d8",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -191,7 +191,7 @@ def torch_multi_head_self_attention(\n         attention_mask: Union[torch.LongTensor, torch.BoolTensor],\n         gated_position_bias: torch.FloatTensor,\n         output_attentions: bool,\n-    ) -> (torch.FloatTensor, torch.FloatTensor):\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n         \"\"\"simple wrapper around torch's multi_head_attention_forward function\"\"\"\n         # self-attention assumes q = k = v\n         query = key = value = hidden_states.transpose(0, 1)"
        },
        {
            "sha": "75e360b6a1d35fef257f515358139af927665975",
            "filename": "src/transformers/models/wavlm/modular_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -122,7 +122,7 @@ def torch_multi_head_self_attention(\n         attention_mask: Union[torch.LongTensor, torch.BoolTensor],\n         gated_position_bias: torch.FloatTensor,\n         output_attentions: bool,\n-    ) -> (torch.FloatTensor, torch.FloatTensor):\n+    ) -> tuple[torch.FloatTensor, torch.FloatTensor]:\n         \"\"\"simple wrapper around torch's multi_head_attention_forward function\"\"\"\n         # self-attention assumes q = k = v\n         query = key = value = hidden_states.transpose(0, 1)"
        },
        {
            "sha": "d1391008227c91048524a87d4fccab2218f7d06a",
            "filename": "src/transformers/models/yolos/modular_yolos.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodular_yolos.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -171,22 +171,22 @@ def post_process_object_detection(\n \n         return results\n \n-    def post_process_segmentation():\n+    def post_process_segmentation(self):\n         raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n \n-    def post_process_instance():\n+    def post_process_instance(self):\n         raise NotImplementedError(\"Instance post-processing is not implemented for Deformable DETR yet.\")\n \n-    def post_process_panoptic():\n+    def post_process_panoptic(self):\n         raise NotImplementedError(\"Panoptic post-processing is not implemented for Deformable DETR yet.\")\n \n-    def post_process_instance_segmentation():\n+    def post_process_instance_segmentation(self):\n         raise NotImplementedError(\"Segmentation post-processing is not implemented for Deformable DETR yet.\")\n \n-    def post_process_semantic_segmentation():\n+    def post_process_semantic_segmentation(self):\n         raise NotImplementedError(\"Semantic segmentation post-processing is not implemented for Deformable DETR yet.\")\n \n-    def post_process_panoptic_segmentation():\n+    def post_process_panoptic_segmentation(self):\n         raise NotImplementedError(\"Panoptic segmentation post-processing is not implemented for Deformable DETR yet.\")\n \n "
        },
        {
            "sha": "25df4dbacb4e3de32fdfac943fa03e910bb372c7",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -294,9 +294,6 @@ def safe_globals():\n if TYPE_CHECKING:\n     import optuna\n \n-    if is_datasets_available():\n-        import datasets\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -418,14 +415,14 @@ class Trainer:\n     def __init__(\n         self,\n         model: Union[PreTrainedModel, nn.Module, None] = None,\n-        args: TrainingArguments = None,\n+        args: Optional[TrainingArguments] = None,\n         data_collator: Optional[DataCollator] = None,\n         train_dataset: Optional[Union[Dataset, IterableDataset, \"datasets.Dataset\"]] = None,\n         eval_dataset: Optional[Union[Dataset, dict[str, Dataset], \"datasets.Dataset\"]] = None,\n         processing_class: Optional[\n             Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]\n         ] = None,\n-        model_init: Optional[Callable[[], PreTrainedModel]] = None,\n+        model_init: Optional[Callable[..., PreTrainedModel]] = None,\n         compute_loss_func: Optional[Callable] = None,\n         compute_metrics: Optional[Callable[[EvalPrediction], dict]] = None,\n         callbacks: Optional[list[TrainerCallback]] = None,"
        },
        {
            "sha": "562a5de65718171b8c1f120b94c617e0750a0714",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c76387e580eb8f4f74e3b4d117eeac41a3d56d69/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=c76387e580eb8f4f74e3b4d117eeac41a3d56d69",
            "patch": "@@ -681,7 +681,7 @@ def get_video_processor_dict(\n         else:\n             video_processor_file = VIDEO_PROCESSOR_NAME\n             try:\n-                # Try to load with a new config name first and if not successfull try with the old file name\n+                # Try to load with a new config name first and if not successful try with the old file name\n                 # NOTE: we will gradually change to saving all processor configs as nested dict in PROCESSOR_NAME\n                 resolved_video_processor_files = cached_files(\n                     pretrained_model_name_or_path,"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 26,
        "deletions": 23
    }
}