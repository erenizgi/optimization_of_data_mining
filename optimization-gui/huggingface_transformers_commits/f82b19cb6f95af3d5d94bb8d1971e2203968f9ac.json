{
    "author": "louie-tsai",
    "message": "add a new flax example for Bert model inference (#34794)\n\n* add a new example for flax inference cases\n\n* Update examples/flax/language-modeling/README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update examples/flax/language-modeling/README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update examples/flax/language-modeling/README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update examples/flax/language-modeling/README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update examples/flax/language-modeling/README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update examples/flax/language-modeling/README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix for \"make fixup\"\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "f82b19cb6f95af3d5d94bb8d1971e2203968f9ac",
    "files": [
        {
            "sha": "9e2dee36213d7e1ad8f5c8ba5575501ed7a2b346",
            "filename": "examples/flax/language-modeling/README.md",
            "status": "modified",
            "additions": 25,
            "deletions": 1,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/f82b19cb6f95af3d5d94bb8d1971e2203968f9ac/examples%2Fflax%2Flanguage-modeling%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f82b19cb6f95af3d5d94bb8d1971e2203968f9ac/examples%2Fflax%2Flanguage-modeling%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2FREADME.md?ref=f82b19cb6f95af3d5d94bb8d1971e2203968f9ac",
            "patch": "@@ -14,7 +14,7 @@ See the License for the specific language governing permissions and\n limitations under the License.\n -->\n \n-# Language model training examples\n+# Language model training and inference examples\n \n The following example showcases how to train a language model from scratch\n using the JAX/Flax backend.\n@@ -542,3 +542,27 @@ python3 -m torch.distributed.launch --nproc_per_node ${NUM_GPUS} run_mlm.py \\\n     --report_to=\"tensorboard\" \\\n     --save_strategy=\"no\"\n ```\n+\n+## Language model inference with bfloat16\n+\n+The following example demonstrates performing inference with a language model using the JAX/Flax backend.\n+\n+The example script run_bert_flax.py uses bert-base-uncased, and the model is loaded into `FlaxBertModel`.\n+The input data are randomly generated tokens, and the model is also jitted with JAX.\n+By default, it uses float32 precision for inference. To enable bfloat16, add the flag shown in the command below.\n+\n+```bash\n+python3 run_bert_flax.py --precision bfloat16\n+> NOTE: For JAX Versions after v0.4.33 or later, users will need to set the below environment variables as a \\\n+> temporary workaround to use Bfloat16 datatype. \\\n+> This restriction is expected to be removed in future version\n+```bash\n+export XLA_FLAGS=--xla_cpu_use_thunk_runtime=false\n+```\n+bfloat16 gives better performance on GPUs and also Intel CPUs (Sapphire Rapids or later) with Advanced Matrix Extension (Intel AMX).  \n+By changing the dtype for `FlaxBertModel `to `jax.numpy.bfloat16`, you get the performance benefits of the underlying hardware.\n+```python\n+import jax\n+model = FlaxBertModel.from_pretrained(\"bert-base-uncased\", config=config, dtype=jax.numpy.bfloat16)\n+```\n+Switching from float32 to bfloat16 can increase the speed of an AWS c7i.4xlarge with Intel Sapphire Rapids by more than 2x."
        },
        {
            "sha": "2e73af4592eb36c99fca5f8f9b45b4a9dc85ebfb",
            "filename": "examples/flax/language-modeling/run_bert_flax.py",
            "status": "added",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/f82b19cb6f95af3d5d94bb8d1971e2203968f9ac/examples%2Fflax%2Flanguage-modeling%2Frun_bert_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f82b19cb6f95af3d5d94bb8d1971e2203968f9ac/examples%2Fflax%2Flanguage-modeling%2Frun_bert_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_bert_flax.py?ref=f82b19cb6f95af3d5d94bb8d1971e2203968f9ac",
            "patch": "@@ -0,0 +1,56 @@\n+#!/usr/bin/env python3\n+import time\n+from argparse import ArgumentParser\n+\n+import jax\n+import numpy as np\n+\n+from transformers import BertConfig, FlaxBertModel\n+\n+\n+parser = ArgumentParser()\n+parser.add_argument(\"--precision\", type=str, choices=[\"float32\", \"bfloat16\"], default=\"float32\")\n+args = parser.parse_args()\n+\n+dtype = jax.numpy.float32\n+if args.precision == \"bfloat16\":\n+    dtype = jax.numpy.bfloat16\n+\n+VOCAB_SIZE = 30522\n+BS = 32\n+SEQ_LEN = 128\n+\n+\n+def get_input_data(batch_size=1, seq_length=384):\n+    shape = (batch_size, seq_length)\n+    input_ids = np.random.randint(1, VOCAB_SIZE, size=shape).astype(np.int32)\n+    token_type_ids = np.ones(shape).astype(np.int32)\n+    attention_mask = np.ones(shape).astype(np.int32)\n+    return {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n+\n+\n+inputs = get_input_data(BS, SEQ_LEN)\n+config = BertConfig.from_pretrained(\"bert-base-uncased\", hidden_act=\"gelu_new\")\n+model = FlaxBertModel.from_pretrained(\"bert-base-uncased\", config=config, dtype=dtype)\n+\n+\n+@jax.jit\n+def func():\n+    outputs = model(**inputs)\n+    return outputs\n+\n+\n+(nwarmup, nbenchmark) = (5, 100)\n+\n+# warmpup\n+for _ in range(nwarmup):\n+    func()\n+\n+# benchmark\n+\n+start = time.time()\n+for _ in range(nbenchmark):\n+    func()\n+end = time.time()\n+print(end - start)\n+print(f\"Throughput: {((nbenchmark * BS)/(end-start)):.3f} examples/sec\")"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 81,
        "deletions": 1
    }
}