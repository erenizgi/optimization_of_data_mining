{
    "author": "LysandreJik",
    "message": "Update llava.md (#34749)\n\nLLava -> Llava",
    "sha": "f5dbfab7f3066604e574d5331e7c105443fef5c6",
    "files": [
        {
            "sha": "7f326bd0c006db1e591e16ed376d86b0088ca9e6",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5dbfab7f3066604e574d5331e7c105443fef5c6/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5dbfab7f3066604e574d5331e7c105443fef5c6/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=f5dbfab7f3066604e574d5331e7c105443fef5c6",
            "patch": "@@ -85,10 +85,10 @@ LLaVa also supports batched inference. Here is how you can do it:\n import requests\n from PIL import Image\n import torch\n-from transformers import AutoProcessor, LLavaForConditionalGeneration\n+from transformers import AutoProcessor, LlavaForConditionalGeneration\n \n # Load the model in half-precision\n-model = LLavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n \n # Get two different images"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}