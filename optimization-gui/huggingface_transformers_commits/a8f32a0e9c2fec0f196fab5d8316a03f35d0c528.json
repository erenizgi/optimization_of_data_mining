{
    "author": "LysandreJik",
    "message": "Revert \"ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ default to `\"auto\"` dtype (#34919)\"\n\nThis reverts commit f5aa90d0fbb7b6f9dd07ac534319d973cd29a123.",
    "sha": "a8f32a0e9c2fec0f196fab5d8316a03f35d0c528",
    "files": [
        {
            "sha": "f6a3b52e99afa9cf495d764b19fdffd109569960",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a8f32a0e9c2fec0f196fab5d8316a03f35d0c528/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a8f32a0e9c2fec0f196fab5d8316a03f35d0c528/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=a8f32a0e9c2fec0f196fab5d8316a03f35d0c528",
            "patch": "@@ -341,9 +341,6 @@ def lazy_load_kernel(kernel_name: str, mapping: dict[str, ModuleType | None] = _\n             mapping[kernel_name] = kernel\n         except FileNotFoundError:\n             mapping[kernel_name] = None\n-        except AssertionError:\n-            # Happens when torch is built without an accelerator backend; fall back to slow path.\n-            mapping[kernel_name] = None\n \n     else:\n         # Try to import is_{kernel_name}_available from ..utils"
        },
        {
            "sha": "7909da6ffedd7a17bd6beb65962112c905453dcd",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 28,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/a8f32a0e9c2fec0f196fab5d8316a03f35d0c528/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a8f32a0e9c2fec0f196fab5d8316a03f35d0c528/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a8f32a0e9c2fec0f196fab5d8316a03f35d0c528",
            "patch": "@@ -268,24 +268,16 @@ def get_torch_context_manager_or_global_device():\n     return device_in_context\n \n \n-def get_state_dict_dtype(state_dict, config_dtype: Optional[torch.dtype] = None):\n+def get_state_dict_dtype(state_dict):\n     \"\"\"\n     Returns the first found floating dtype in `state_dict` if there is one, otherwise returns the first dtype.\n-\n-    If `config_dtype` is provided (for instance when `dtype=\"auto\"` and the config already carries a dtype), it is used.\n     \"\"\"\n-    if config_dtype is not None:\n-        return config_dtype\n-\n-    if len(state_dict) == 0:\n-        return torch.get_default_dtype()\n-\n     for t in state_dict.values():\n         if t.is_floating_point():\n             return t.dtype\n \n     # if no floating dtype was found return whatever the first dtype is\n-    return next(iter(state_dict.values())).dtype\n+    return next(state_dict.values()).dtype\n \n \n str_to_torch_dtype = {\n@@ -730,16 +722,12 @@ def _get_dtype(\n                     if is_sharded and \"dtype\" in sharded_metadata:\n                         dtype = sharded_metadata[\"dtype\"]\n                     elif state_dict is not None:\n-                        dtype = get_state_dict_dtype(state_dict, getattr(config, \"dtype\", None))\n+                        dtype = get_state_dict_dtype(state_dict)\n                     else:\n                         state_dict = load_state_dict(\n                             checkpoint_files[0], map_location=\"meta\", weights_only=weights_only\n                         )\n-                        dtype = get_state_dict_dtype(state_dict, getattr(config, \"dtype\", None))\n-                    config.dtype = dtype\n-                    for sub_config_key in config.sub_configs:\n-                        if (sub_config := getattr(config, sub_config_key)) is not None:\n-                            sub_config.dtype = dtype\n+                        dtype = get_state_dict_dtype(state_dict)\n                     logger.info(\n                         \"Since the `dtype` attribute can't be found in model's config object, \"\n                         \"will use dtype={dtype} as derived from model's weights\"\n@@ -1231,14 +1219,6 @@ def __init__(self, config: PreTrainedConfig, *inputs, **kwargs):\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n         self.config = config\n-        if getattr(self.config, \"dtype\", None) is None:\n-            default_dtype = torch.get_default_dtype()\n-            self.config.dtype = default_dtype\n-            for sub_config_key in self.config.sub_configs:\n-                if (sub_config := getattr(self.config, sub_config_key)) is not None and getattr(\n-                    sub_config, \"dtype\", None\n-                ) is None:\n-                    sub_config.dtype = default_dtype\n \n         # Check the attention implementation is supported, or set it if not yet set (on the internal attr, to avoid\n         # setting it recursively)\n@@ -3809,8 +3789,7 @@ def from_pretrained(\n         output_loading_info = kwargs.pop(\"output_loading_info\", False)\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n-        dtype_kwarg_provided = \"dtype\" in kwargs\n-        dtype = kwargs.pop(\"dtype\", \"auto\")\n+        dtype = kwargs.pop(\"dtype\", None)\n         torch_dtype = kwargs.pop(\"torch_dtype\", None)  # kept for BC\n         device_map = kwargs.pop(\"device_map\", None)\n         max_memory = kwargs.pop(\"max_memory\", None)\n@@ -3841,8 +3820,8 @@ def from_pretrained(\n             _ = kwargs.pop(name, None)\n \n         # For BC on torch_dtype argument\n-        if torch_dtype is not None and (not dtype_kwarg_provided or dtype is None):\n-            dtype = torch_dtype\n+        if torch_dtype is not None:\n+            dtype = dtype if dtype is not None else torch_dtype\n \n         if is_offline_mode() and not local_files_only:\n             local_files_only = True"
        }
    ],
    "stats": {
        "total": 38,
        "additions": 7,
        "deletions": 31
    }
}