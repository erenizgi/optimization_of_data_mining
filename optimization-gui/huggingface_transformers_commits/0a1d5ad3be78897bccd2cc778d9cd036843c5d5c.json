{
    "author": "Aravind-11",
    "message": "ðŸš¨ Remove generic output_attentions warning (#42334)\n\n* Remove generic output_attentions warning\n\nRemoved broad warning for output_attentions in generic.py. Specific backend warnings remain in their respective modules.\n\n* removed comments\n\n* Remove unnecessary blank line in generic.py",
    "sha": "0a1d5ad3be78897bccd2cc778d9cd036843c5d5c",
    "files": [
        {
            "sha": "502b0afaa2e903e5888a2b140973f20448c8d9a7",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a1d5ad3be78897bccd2cc778d9cd036843c5d5c/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a1d5ad3be78897bccd2cc778d9cd036843c5d5c/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=0a1d5ad3be78897bccd2cc778d9cd036843c5d5c",
            "patch": "@@ -874,21 +874,6 @@ def wrapper(self, *args, **kwargs):\n             collected_outputs = defaultdict(tuple)\n             monkey_patched_layers = []\n \n-            # Check attention implementation is properly set for capturing attention outputs\n-            if recordable_keys.get(\"output_attentions\", False):\n-                supported_attn = [\"eager\", \"eager_paged\", \"flex_attention\", \"sdpa\"]\n-                config_attn = getattr(self.config, \"_attn_implementation\", None)\n-                sub_configs = [getattr(self.config, key, None) for key in self.config.sub_configs]\n-                sub_configs_attn = [\n-                    getattr(config, \"_attn_implementation\", None) for config in sub_configs if config is not None\n-                ]\n-                if config_attn not in supported_attn or any(attn not in supported_attn for attn in sub_configs_attn):\n-                    warnings.warn(\n-                        f\"`output_attentions=True` is not supported with `attn_implementation` other than {supported_attn}. \"\n-                        \"Please use `model.set_attn_implementation('eager')` to enable capturing attention outputs.\",\n-                        UserWarning,\n-                    )\n-\n             def make_capture_wrapper(module, orig_forward, key, index):\n                 @wraps(orig_forward)\n                 def wrapped_forward(*args, **kwargs):"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 0,
        "deletions": 15
    }
}