{
    "author": "lkhl",
    "message": "[model] Add VideoLLaMA3 implementation (#40499)\n\n* Add VideoLLaMA3 implementation\n\n* Run style fix\n\n* Switch to modular\n\n* Fix config and smart_resize\n\n* Fix\n\n* Fix\n\n* Fix style\n\n* Fix\n\n* Ruff fix\n\n* Rename\n\n* Rename\n\n* Fix\n\n* Clean\n\n* Fix consistency\n\n* Add doc\n\n* Fix\n\n* Fix\n\n* Fix doc\n\n* Update generated code\n\n* remove test_initialization\n\n* fix tests\n\n* simplify\n\n* tests\n\n* Add VideoLlama3IntegrationTest\n\n* replace asserts\n\n* fix tests\n\n---------\n\nCo-authored-by: steven-ccq <55176896+steven-ccq@users.noreply.github.com>\nCo-authored-by: steven-ccq <1456320989@qq.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
    "files": [
        {
            "sha": "1724ef6a9909fe300e1d0097b96701d184a7eb8a",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -1188,6 +1188,8 @@\n         title: TVP\n       - local: model_doc/udop\n         title: UDOP\n+      - local: model_doc/video_llama_3\n+        title: VideoLlama3\n       - local: model_doc/video_llava\n         title: VideoLlava\n       - local: model_doc/vilt"
        },
        {
            "sha": "2f8e150167429a390389281d6638a19dfcb14d17",
            "filename": "docs/source/en/model_doc/video_llama_3.md",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llama_3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llama_3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llama_3.md?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,229 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on 2025-01-22 and added to Hugging Face Transformers on 2025-10-02.*\n+\n+# VideoLLaMA3\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+</div>\n+\n+## Overview\n+\n+The [VideoLLaMA3](https://huggingface.co/papers/2501.13106) model is a major update to [VideoLLaMA2](https://huggingface.co/papers/2406.07476) from Alibaba DAMO Academy.\n+\n+The abstract from the paper is as following:\n+\n+*In this paper, we propose VideoLLaMA 3, a more advanced multimodal foundation model for image and video understanding. The core design philosophy of VideoLLaMA3 is vision-centric. The meaning of ‚Äúvision-centric‚Äù is two-fold: the vision-centric training paradigm and vision-centric framework design. The key insight of our vision-centric training paradigm is that high-quality image-text data is crucial for both image and video understanding. Instead of preparing massive video-text datasets, we focus on constructing large-scale, high-quality image-text datasets. VideoLLaMA3 has four training stages: 1) Vision Encoder Adaptation, which enables the vision encoder to accept images of variable resolutions\n+as input; 2) Vision-Language Alignment, which jointly tunes the vision encoder, projector, and LLM with large-scale image-text data covering multiple types (including scene images, documents, and charts) as well as text-only data. 3) Multi-task Fine-tuning, which incorporates image-text SFT data for downstream tasks and video-text data to establish a foundation for video understanding. 4) Video-centric Fine-tuning, which further improves the model‚Äôs capability in video understanding. As for the framework design, to better capture fine-grained details in images, the pretrained vision encoder is adapted to encode images of varying sizes into vision tokens with corresponding numbers, rather than a fixed number of tokens. For video inputs, we reduce the number of vision tokens according to their similarity so that the representation of videos will be more precise and compact. Benefiting from vision-centric designs, VideoLLaMA3 achieves compelling performances in both image and video understanding benchmarks.*\n+\n+<img src=\"https://github.com/DAMO-NLP-SG/VideoLLaMA3/raw/refs/heads/main/assets/pipeline.jpg\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> VideoLLaMA3 architecture. Taken from the <a href=\"https://arxiv.org/pdf/2501.13106\">technical report.</a> </small>\n+\n+This model was contributed by [lkhl](https://huggingface.co/lkhl).\n+\n+## Usage example\n+\n+### Single Media inference\n+\n+The model can accept both images and videos as input. Here's an example code for inference.\n+\n+```python\n+import torch\n+from transformers import VideoLlama3ForConditionalGeneration, AutoTokenizer, AutoProcessor\n+\n+# Load the model in half-precision on the available device(s)\n+model = VideoLlama3ForConditionalGeneration.from_pretrained(\"lkhl/VideoLLaMA3-2B-Image-HF\", device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"lkhl/VideoLLaMA3-2B-Image-HF\")\n+\n+\n+conversation = [\n+    {\n+        \"role\":\"user\",\n+        \"content\":[\n+            {\"type\": \"image\", \"image\": \"https://github.com/DAMO-NLP-SG/VideoLLaMA3/raw/refs/heads/main/assets/sora.png\"},\n+            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n+        ]\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n+# Inference: Generation of the output\n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+print(output_text)\n+\n+\n+\n+# Video\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"video\": \"https://github.com/DAMO-NLP-SG/VideoLLaMA3/raw/refs/heads/main/assets/cat_and_chicken.mp4\"},\n+            {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n+\n+# Inference: Generation of the output\n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+print(output_text)\n+```\n+\n+### Batch Mixed Media Inference\n+\n+The model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.\n+\n+```python\n+# Image\n+conversation1 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"image\": \"https://github.com/DAMO-NLP-SG/VideoLLaMA3/raw/refs/heads/main/assets/sora.png\"},\n+            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n+        ]\n+    }\n+]\n+\n+# Video\n+conversation2 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"video\": \"https://github.com/DAMO-NLP-SG/VideoLLaMA3/raw/refs/heads/main/assets/cat_and_chicken.mp4\"},\n+            {\"type\": \"text\", \"text\": \"What happened in the video?\"},\n+        ],\n+    }\n+]\n+\n+# Text\n+conversation3 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": \"What color is a banana?\"\n+    }\n+]\n+\n+\n+conversations = [conversation1, conversation2, conversation3]\n+# Preparation for batch inference\n+inputs = processor.apply_chat_template(\n+    conversations,\n+    fps=1,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    padding=True,\n+    padding_side=\"left\",\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device)\n+\n+\n+# Batch Inference\n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n+output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+print(output_text)\n+```\n+\n+#### Flash-Attention 2 to speed up generation\n+\n+First, make sure to install the latest version of Flash Attention 2:\n+\n+```bash\n+pip install -U flash-attn --no-build-isolation\n+```\n+\n+Also, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n+\n+To load and run a model using Flash Attention-2, simply add `attn_implementation=\"flash_attention_2\"` when loading the model as follows:\n+\n+```python\n+from transformers import VideoLlama3ForConditionalGeneration\n+\n+model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+    \"lkhl/VideoLLaMA3-2B-Image-HF\", \n+    dtype=torch.bfloat16, \n+    attn_implementation=\"flash_attention_2\",\n+)\n+```\n+\n+## VideoLlama3Config\n+\n+[[autodoc]] VideoLlama3Config\n+\n+## VideoLlama3VisionConfig\n+\n+[[autodoc]] VideoLlama3VisionConfig\n+\n+## VideoLlama3ImageProcessor\n+\n+[[autodoc]] VideoLlama3ImageProcessor\n+    - preprocess\n+\n+## VideoLlama3VideoProcessor\n+\n+[[autodoc]] VideoLlama3VideoProcessor\n+    - preprocess\n+\n+## VideoLlama3ImageProcessorFast\n+\n+[[autodoc]] VideoLlama3ImageProcessorFast\n+    - preprocess\n+\n+## VideoLlama3Processor\n+\n+[[autodoc]] VideoLlama3Processor\n+\n+## VideoLlama3Model\n+\n+[[autodoc]] VideoLlama3Model\n+    - forward\n+\n+## VideoLlama3VisionModel\n+\n+[[autodoc]] VideoLlama3VisionModel\n+    - forward\n+\n+## VideoLlama3ForConditionalGeneration\n+\n+[[autodoc]] VideoLlama3ForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "5630063f92ecae0c6278324ca722a1d4b0506c41",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -352,6 +352,7 @@\n     from .univnet import *\n     from .upernet import *\n     from .vaultgemma import *\n+    from .video_llama_3 import *\n     from .video_llava import *\n     from .videomae import *\n     from .vilt import *"
        },
        {
            "sha": "c8142947b7c06bac957aca13d2147a91cb3fb9ab",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -418,6 +418,8 @@\n         (\"upernet\", \"UperNetConfig\"),\n         (\"van\", \"VanConfig\"),\n         (\"vaultgemma\", \"VaultGemmaConfig\"),\n+        (\"video_llama_3\", \"VideoLlama3Config\"),\n+        (\"video_llama_3_vision\", \"VideoLlama3VisionConfig\"),\n         (\"video_llava\", \"VideoLlavaConfig\"),\n         (\"videomae\", \"VideoMAEConfig\"),\n         (\"vilt\", \"ViltConfig\"),\n@@ -878,6 +880,8 @@\n         (\"upernet\", \"UPerNet\"),\n         (\"van\", \"VAN\"),\n         (\"vaultgemma\", \"VaultGemma\"),\n+        (\"video_llama_3\", \"VideoLlama3\"),\n+        (\"video_llama_3_vision\", \"VideoLlama3Vision\"),\n         (\"video_llava\", \"VideoLlava\"),\n         (\"videomae\", \"VideoMAE\"),\n         (\"vilt\", \"ViLT\"),\n@@ -997,6 +1001,7 @@\n         (\"llama4_text\", \"llama4\"),\n         (\"blip_2_qformer\", \"blip_2\"),\n         (\"fastspeech2_conformer_with_hifigan\", \"fastspeech2_conformer\"),\n+        (\"video_llama_3_vision\", \"video_llama_3\"),\n         (\"parakeet_encoder\", \"parakeet\"),\n         (\"parakeet_ctc\", \"parakeet\"),\n     ]"
        },
        {
            "sha": "f864d107914fdd3f3605743cf2ad759ffe95c570",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -186,6 +186,7 @@\n             (\"udop\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),\n             (\"upernet\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),\n             (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n+            (\"video_llama_3\", (\"VideoLlama3ImageProcessor\", \"VideoLlama3ImageProcessorFast\")),\n             (\"videomae\", (\"VideoMAEImageProcessor\", None)),\n             (\"vilt\", (\"ViltImageProcessor\", \"ViltImageProcessorFast\")),\n             (\"vipllava\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "25e6e6aa73b5e352470d795e152c11e996226cda",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -401,6 +401,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"univnet\", \"UnivNetModel\"),\n         (\"van\", \"VanModel\"),\n         (\"vaultgemma\", \"VaultGemmaModel\"),\n+        (\"video_llama_3\", \"VideoLlama3Model\"),\n+        (\"video_llama_3_vision\", \"VideoLlama3VisionModel\"),\n         (\"video_llava\", \"VideoLlavaModel\"),\n         (\"videomae\", \"VideoMAEModel\"),\n         (\"vilt\", \"ViltModel\"),\n@@ -1058,6 +1060,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"shieldgemma2\", \"Gemma3ForConditionalGeneration\"),\n         (\"smolvlm\", \"SmolVLMForConditionalGeneration\"),\n         (\"udop\", \"UdopForConditionalGeneration\"),\n+        (\"video_llama_3\", \"VideoLlama3ForConditionalGeneration\"),\n         (\"vipllava\", \"VipLlavaForConditionalGeneration\"),\n         (\"vision-encoder-decoder\", \"VisionEncoderDecoderModel\"),\n     ]"
        },
        {
            "sha": "1e79dd3180baba7ff9eb380a79604699b8c587f5",
            "filename": "src/transformers/models/video_llama_3/__init__.py",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2F__init__.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,31 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_video_llama_3 import *\n+    from .image_processing_video_llama_3 import *\n+    from .image_processing_video_llama_3_fast import *\n+    from .modeling_video_llama_3 import *\n+    from .processing_video_llama_3 import *\n+    from .video_processing_video_llama_3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "2a0cd1897c04a2e83752a45483f2d8761385dfcf",
            "filename": "src/transformers/models/video_llama_3/configuration_video_llama_3.py",
            "status": "added",
            "additions": 143,
            "deletions": 0,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fconfiguration_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fconfiguration_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fconfiguration_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,143 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/video_llama_3/modular_video_llama_3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_video_llama_3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from ...configuration_utils import PreTrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class VideoLlama3VisionConfig(PreTrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`VideoLlama3VisionModel`]. It is used to instantiate a\n+    VideoLLaMA3 vision encoder model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    VideoLLaMA3-2B [lkhl/VideoLLaMA3-2B-Image-HF](https://huggingface.co/lkhl/VideoLLaMA3-2B-Image-HF).\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    \"\"\"\n+\n+    model_type = \"video_llama_3_vision\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        intermediate_size=3072,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        patch_size=16,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+\n+        self.initializer_range = initializer_range\n+\n+\n+class VideoLlama3Config(PreTrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`VideoLlama3Model`]. It is used to instantiate a\n+    VideoLLaMA3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    VideoLLaMA3-2B [lkhl/VideoLLaMA3-2B-Image-HF](https://huggingface.co/lkhl/VideoLLaMA3-2B-Image-HF).\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `VideoLlama3VisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+    \"\"\"\n+\n+    model_type = \"video_llama_3\"\n+    sub_configs = {\"vision_config\": VideoLlama3VisionConfig, \"text_config\": AutoConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif isinstance(vision_config, PreTrainedConfig):\n+            self.vision_config = vision_config\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+        else:\n+            raise ValueError(\n+                f\"vision_config must be of type `dict` or `PreTrainedConfig`, but got {type(vision_config)}.\"\n+            )\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif isinstance(text_config, PreTrainedConfig):\n+            self.text_config = text_config\n+        elif text_config is None:\n+            self.text_config = CONFIG_MAPPING[\"qwen2\"]()\n+        else:\n+            raise ValueError(f\"text_config must be of type `dict` or `PreTrainedConfig`, but got {type(text_config)}.\")\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"VideoLlama3VisionConfig\", \"VideoLlama3Config\"]"
        },
        {
            "sha": "12f5a4cc7fc0b2124d6c354c80272b417f451752",
            "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3.py",
            "status": "added",
            "additions": 503,
            "deletions": 0,
            "changes": 503,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,503 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/video_llama_3/modular_video_llama_3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_video_llama_3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_processing_utils import BaseImageProcessor\n+from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...processing_utils import ImagesKwargs\n+from ...utils import TensorType, logging\n+from ...video_utils import VideoInput\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VideoLlama3ImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    min_pixels (`int`, *optional*, defaults to `56 * 56`):\n+        The min pixels of the image to resize the image.\n+    max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n+        The max pixels of the image to resize the image.\n+    patch_size (`int`, *optional*, defaults to 14):\n+        The spatial patch size of the vision encoder.\n+    temporal_patch_size (`int`, *optional*, defaults to 2):\n+        The temporal patch size of the vision encoder.\n+    merge_size (`int`, *optional*, defaults to 2):\n+        The merge size of the vision encoder to llm encoder.\n+    \"\"\"\n+\n+    min_pixels: int\n+    max_pixels: int\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+\n+\n+def smart_resize(\n+    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n+):\n+    \"\"\"Rescales the image so that the following conditions are met:\n+\n+    1. Both dimensions (height and width) are divisible by 'factor'.\n+\n+    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n+\n+    3. The aspect ratio of the image is maintained as closely as possible.\n+\n+    \"\"\"\n+    if max(height, width) / min(height, width) > 200:\n+        raise ValueError(\n+            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n+        )\n+    h_bar = round(height / factor) * factor\n+    w_bar = round(width / factor) * factor\n+    if h_bar * w_bar > max_pixels:\n+        beta = math.sqrt((height * width) / max_pixels)\n+        h_bar = max(factor, math.floor(height / beta / factor) * factor)\n+        w_bar = max(factor, math.floor(width / beta / factor) * factor)\n+    elif h_bar * w_bar < min_pixels:\n+        beta = math.sqrt(min_pixels / (height * width))\n+        h_bar = math.ceil(height * beta / factor) * factor\n+        w_bar = math.ceil(width * beta / factor) * factor\n+    return h_bar, w_bar\n+\n+\n+class VideoLlama3ImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a VideoLLaMA3 image processor that dynamically resizes images based on the original images.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions.\n+        size (`dict[str, int]`, *optional*, defaults to `{\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}`):\n+            Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use when resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image.\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n+            The min pixels of the image to resize the image.\n+        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n+            The max pixels of the image to resize the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The spatial patch size of the vision encoder.\n+        temporal_patch_size (`int`, *optional*, defaults to 1):\n+            The temporal patch size of the vision encoder.\n+        merge_size (`int`, *optional*, defaults to 1):\n+            The merge size of the vision encoder to llm encoder.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"image_merge_sizes\"]\n+    valid_kwargs = VideoLlama3ImageProcessorKwargs\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        patch_size: int = 14,\n+        temporal_patch_size: int = 1,\n+        merge_size: int = 1,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+        else:\n+            size = {\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+        self.min_pixels = size[\"shortest_edge\"]\n+        self.max_pixels = size[\"longest_edge\"]\n+        self.size = size\n+\n+        self.do_resize = do_resize\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n+\n+        self.patch_size = patch_size\n+        self.temporal_patch_size = temporal_patch_size\n+        self.merge_size = merge_size\n+        self.do_convert_rgb = do_convert_rgb\n+\n+        if self.temporal_patch_size != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+\n+    def _preprocess(\n+        self,\n+        images: Union[ImageInput, VideoInput],\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n+            vision_info (`list[Dict]`, *optional*):\n+                Optional list of dictionaries containing additional information about vision inputs.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Scale factor to use if rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n+                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n+                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spatial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        images = make_flat_list_of_images(images)\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        height, width = get_image_size(images[0], channel_dim=input_data_format)\n+        resized_height, resized_width = height, width\n+        processed_images = []\n+        for image in images:\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    height,\n+                    width,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=size[\"shortest_edge\"],\n+                    max_pixels=size[\"longest_edge\"],\n+                )\n+                image = resize(\n+                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format\n+                )\n+\n+            if do_rescale:\n+                image = self.rescale(image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            processed_images.append(image)\n+\n+        patches = np.array(processed_images)\n+        if data_format == ChannelDimension.LAST:\n+            patches = patches.transpose(0, 3, 1, 2)\n+        if patches.shape[0] % temporal_patch_size != 0:\n+            repeats = np.repeat(\n+                patches[-1][np.newaxis], temporal_patch_size - (patches.shape[0] % temporal_patch_size), axis=0\n+            )\n+            patches = np.concatenate([patches, repeats], axis=0)\n+        channel = patches.shape[1]\n+        grid_t = patches.shape[0] // temporal_patch_size\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        patches = patches.reshape(\n+            grid_t,\n+            temporal_patch_size,\n+            channel,\n+            grid_h // merge_size,\n+            merge_size,\n+            patch_size,\n+            grid_w // merge_size,\n+            merge_size,\n+            patch_size,\n+        )\n+        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)\n+        flatten_patches = patches.reshape(\n+            grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n+        )\n+\n+        return flatten_patches, (grid_t, grid_h, grid_w)\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        videos: Optional[VideoInput] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            videos (`VideoInput`):\n+                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n+                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            min_pixels (`int`, *optional*, defaults to `self.min_pixels`):\n+                The min pixels of the image to resize the image.\n+            max_pixels (`int`, *optional*, defaults to `self.max_pixels`):\n+                The max pixels of the image to resize the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spatial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        \"\"\"\n+        min_pixels = min_pixels if min_pixels is not None else self.min_pixels\n+        max_pixels = max_pixels if max_pixels is not None else self.max_pixels\n+\n+        if size is not None:\n+            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+            min_pixels = size[\"shortest_edge\"]\n+        elif min_pixels is not None and max_pixels is not None:\n+            # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n+        else:\n+            size = {**self.size}\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n+        merge_size = merge_size if merge_size is not None else self.merge_size\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        if images is not None:\n+            images = self.fetch_images(images)\n+            images = make_flat_list_of_images(images)\n+\n+        if images is not None and not valid_images(images):\n+            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n+\n+        validate_preprocess_arguments(\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        data = {}\n+        if images is not None:\n+            pixel_values, vision_grid_thws = [], []\n+            for image in images:\n+                patches, image_grid_thw = self._preprocess(\n+                    image,\n+                    do_resize=do_resize,\n+                    size=size,\n+                    resample=resample,\n+                    do_rescale=do_rescale,\n+                    rescale_factor=rescale_factor,\n+                    do_normalize=do_normalize,\n+                    image_mean=image_mean,\n+                    image_std=image_std,\n+                    patch_size=patch_size,\n+                    temporal_patch_size=temporal_patch_size,\n+                    merge_size=merge_size,\n+                    data_format=data_format,\n+                    do_convert_rgb=do_convert_rgb,\n+                    input_data_format=input_data_format,\n+                )\n+                pixel_values.extend(patches)\n+                vision_grid_thws.append(image_grid_thw)\n+            data.update(\n+                {\n+                    \"pixel_values\": np.array(pixel_values),\n+                    \"image_grid_thw\": np.array(vision_grid_thws),\n+                    \"image_merge_sizes\": np.array([merge_size] * len(vision_grid_thws)),\n+                }\n+            )\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of image patches per image.\n+        \"\"\"\n+        min_pixels = images_kwargs[\"min_pixels\"] if \"min_pixels\" in images_kwargs else self.size[\"shortest_edge\"]\n+        max_pixels = images_kwargs[\"max_pixels\"] if \"max_pixels\" in images_kwargs else self.size[\"longest_edge\"]\n+        patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n+        merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n+\n+        factor = patch_size * merge_size\n+        resized_height, resized_width = smart_resize(\n+            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels\n+        )\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        return grid_h * grid_w\n+\n+\n+__all__ = [\"VideoLlama3ImageProcessor\"]"
        },
        {
            "sha": "0282a40e4a221a52363c2a75251b040cf230e7aa",
            "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3_fast.py",
            "status": "added",
            "additions": 322,
            "deletions": 0,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,322 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/video_llama_3/modular_video_llama_3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_video_llama_3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring, logging\n+from ...video_utils import VideoInput, make_batched_videos\n+from .image_processing_video_llama_3 import VideoLlama3ImageProcessorKwargs\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def smart_resize(\n+    height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n+):\n+    \"\"\"Rescales the image so that the following conditions are met:\n+\n+    1. Both dimensions (height and width) are divisible by 'factor'.\n+\n+    2. The total number of pixels is within the range ['min_pixels', 'max_pixels'].\n+\n+    3. The aspect ratio of the image is maintained as closely as possible.\n+\n+    \"\"\"\n+    if max(height, width) / min(height, width) > 200:\n+        raise ValueError(\n+            f\"absolute aspect ratio must be smaller than 200, got {max(height, width) / min(height, width)}\"\n+        )\n+    h_bar = round(height / factor) * factor\n+    w_bar = round(width / factor) * factor\n+    if h_bar * w_bar > max_pixels:\n+        beta = math.sqrt((height * width) / max_pixels)\n+        h_bar = max(factor, math.floor(height / beta / factor) * factor)\n+        w_bar = max(factor, math.floor(width / beta / factor) * factor)\n+    elif h_bar * w_bar < min_pixels:\n+        beta = math.sqrt(min_pixels / (height * width))\n+        h_bar = math.ceil(height * beta / factor) * factor\n+        w_bar = math.ceil(width * beta / factor) * factor\n+    return h_bar, w_bar\n+\n+\n+@auto_docstring\n+class VideoLlama3ImageProcessorFast(BaseImageProcessorFast):\n+    do_resize = True\n+    resample = PILImageResampling.BICUBIC\n+    size = {\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}\n+    do_rescale = True\n+    do_normalize = True\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    do_convert_rgb = True\n+    patch_size = 14\n+    temporal_patch_size = 1\n+    merge_size = 1\n+    min_pixels = None\n+    max_pixels = None\n+    valid_kwargs = VideoLlama3ImageProcessorKwargs\n+    model_input_names = [\n+        \"pixel_values\",\n+        \"image_grid_thw\",\n+        \"image_merge_sizes\",\n+        \"pixel_values_videos\",\n+        \"video_grid_thw\",\n+        \"video_merge_sizes\",\n+    ]\n+\n+    def __init__(self, **kwargs: Unpack[VideoLlama3ImageProcessorKwargs]):\n+        size = kwargs.pop(\"size\", None)\n+        min_pixels = kwargs.pop(\"min_pixels\", None)\n+        max_pixels = kwargs.pop(\"max_pixels\", None)\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        size = self.size if size is None else size\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+            size.pop(\"min_pixels\", None)\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+            size.pop(\"max_pixels\", None)\n+        if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        super().__init__(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if min_pixels is not None and max_pixels is not None:\n+            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n+        elif size is not None:\n+            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+            min_pixels = size[\"shortest_edge\"]\n+            max_pixels = size[\"longest_edge\"]\n+        else:\n+            size = {**self.size}\n+\n+        return super()._further_process_kwargs(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        videos: Optional[VideoInput] = None,\n+        **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        return super().preprocess(images, videos, **kwargs)\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        videos: VideoInput,\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess image-like inputs.\n+        To be overridden by subclasses when image-like inputs other than images should be processed.\n+        It can be used for segmentation maps, depth maps, etc.\n+        \"\"\"\n+        # Prepare input images\n+        batch_feature = BatchFeature()\n+        if images is not None:\n+            if kwargs[\"temporal_patch_size\"] != 1:\n+                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+            images = self._prepare_image_like_inputs(\n+                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+            )\n+            batch_feature = self._preprocess(images, **kwargs)\n+            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+                dtype=batch_feature.image_grid_thw.dtype,\n+                device=batch_feature.image_grid_thw.device,\n+            )\n+        if videos is not None:\n+            logger.warning(\n+                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n+                \"This is a deprecated behavior and will be removed in v5.0. \"\n+                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n+            )\n+            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n+            videos = make_batched_videos(videos)\n+            videos = [\n+                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n+                for video in videos\n+            ]\n+            video_outputs = self._preprocess(videos, **kwargs)\n+            batch_feature.update(\n+                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n+            )\n+            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n+                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n+                dtype=video_outputs.image_grid_thw.dtype,\n+                device=video_outputs.image_grid_thw.device,\n+            )\n+        return batch_feature\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        patch_size: int,\n+        temporal_patch_size: int,\n+        merge_size: int,\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ):\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            height, width = stacked_images.shape[-2:]\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    height,\n+                    width,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=size[\"shortest_edge\"],\n+                    max_pixels=size[\"longest_edge\"],\n+                )\n+                stacked_images = self.resize(\n+                    image=stacked_images,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n+                    interpolation=interpolation,\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        processed_grids = {}\n+        for shape, stacked_images in grouped_images.items():\n+            resized_height, resized_width = stacked_images.shape[-2:]\n+            # Fused rescale and normalize\n+            patches = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            if patches.ndim == 4:\n+                # add a temporal dimension if we have images\n+                patches = patches.unsqueeze(1)\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(1, temporal_patch_size - 1, 1, 1, 1)\n+                patches = torch.cat([patches, repeats], dim=1)\n+            batch_size, grid_t, channel = patches.shape[:3]\n+            grid_t = grid_t // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+\n+            patches = patches.view(\n+                batch_size,\n+                grid_t,\n+                temporal_patch_size,\n+                channel,\n+                grid_h // merge_size,\n+                merge_size,\n+                patch_size,\n+                grid_w // merge_size,\n+                merge_size,\n+                patch_size,\n+            )\n+            # Reorder dimensions to group grid and patch information for subsequent flattening.\n+            # (batch, grid_t, grid_h, grid_w, merge_h, merge_w, channel, temp_patch_size, patch_h, patch_w)\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+            flatten_patches = patches.reshape(\n+                batch_size,\n+                grid_t * grid_h * grid_w,\n+                channel * temporal_patch_size * patch_size * patch_size,\n+            )\n+\n+            processed_images_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_grids = reorder_images(processed_grids, grouped_images_index)\n+        pixel_values = torch.cat(processed_images, dim=0)\n+        image_grid_thw = torch.tensor(processed_grids)\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": pixel_values, \"image_grid_thw\": image_grid_thw}, tensor_type=return_tensors\n+        )\n+\n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of image patches for a given image size.\n+\n+        Note: Do not remove this method! It is used by vLLM to infer the number of patches and placeholders\n+        without an image input.\n+\n+        Args:\n+            height (`int`):\n+                Height of the input image.\n+            width (`int`):\n+                Width of the input image.\n+            images_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the image processor.\n+        Returns:\n+            `int`: Number of image patches per image.\n+        \"\"\"\n+        min_pixels = images_kwargs[\"min_pixels\"] if \"min_pixels\" in images_kwargs else self.size[\"shortest_edge\"]\n+        max_pixels = images_kwargs[\"max_pixels\"] if \"max_pixels\" in images_kwargs else self.size[\"longest_edge\"]\n+        patch_size = images_kwargs.get(\"patch_size\", self.patch_size)\n+        merge_size = images_kwargs.get(\"merge_size\", self.merge_size)\n+\n+        factor = patch_size * merge_size\n+        resized_height, resized_width = smart_resize(\n+            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels\n+        )\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        return grid_h * grid_w\n+\n+\n+__all__ = [\"VideoLlama3ImageProcessorFast\"]"
        },
        {
            "sha": "e626d6da96b03cbe2af5aa000a96f18fa6d56f76",
            "filename": "src/transformers/models/video_llama_3/modeling_video_llama_3.py",
            "status": "added",
            "additions": 1117,
            "deletions": 0,
            "changes": 1117,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,1117 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/video_llama_3/modular_video_llama_3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_video_llama_3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from collections.abc import Callable\n+from dataclasses import dataclass\n+from typing import Any, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import LayerNorm\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, ModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from ..auto.modeling_auto import AutoModel\n+from .configuration_video_llama_3 import VideoLlama3Config, VideoLlama3VisionConfig\n+\n+\n+class VideoLlama3VisionRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, dim: int, theta: float = 10000.0) -> None:\n+        super().__init__()\n+        inv_freq = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float) / dim))\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    def forward(self, grid_thw, merge_sizes) -> tuple[torch.Tensor, torch.Tensor]:\n+        pos_ids = []\n+        for (t, h, w), merge_size in zip(grid_thw, merge_sizes):\n+            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n+            hpos_ids = hpos_ids.reshape(\n+                h // merge_size,\n+                merge_size,\n+                w // merge_size,\n+                merge_size,\n+            )\n+            hpos_ids = hpos_ids.permute(0, 2, 1, 3)\n+            hpos_ids = hpos_ids.flatten()\n+\n+            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n+            wpos_ids = wpos_ids.reshape(\n+                h // merge_size,\n+                merge_size,\n+                w // merge_size,\n+                merge_size,\n+            )\n+            wpos_ids = wpos_ids.permute(0, 2, 1, 3)\n+            wpos_ids = wpos_ids.flatten()\n+            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n+\n+        pos_ids = torch.cat(pos_ids, dim=0)\n+        max_grid_thw = grid_thw[:, 1:].max()\n+\n+        seq = torch.arange(max_grid_thw, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        rotary_pos_emb_full = torch.outer(seq, self.inv_freq)\n+        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+\n+        return (emb.cos(), emb.sin())\n+\n+\n+class VideoLlama3VisionEmbeddings(nn.Module):\n+    def __init__(self, config: VideoLlama3VisionConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.patch_size = config.patch_size\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            padding=\"valid\",\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = hidden_states.view(-1, self.config.num_channels, self.patch_size, self.patch_size)\n+        patch_embeds = self.patch_embedding(hidden_states)\n+        embeddings = patch_embeds.view(-1, self.embed_dim)\n+        return embeddings\n+\n+\n+class VideoLlama3VisionMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def apply_rotary_pos_emb_vision(\n+    q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n+) -> tuple[torch.Tensor, torch.Tensor]:\n+    orig_q_dtype = q.dtype\n+    orig_k_dtype = k.dtype\n+    q, k = q.float(), k.float()\n+    cos, sin = cos.unsqueeze(-2).float(), sin.unsqueeze(-2).float()\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    q_embed = q_embed.to(orig_q_dtype)\n+    k_embed = k_embed.to(orig_k_dtype)\n+    return q_embed, k_embed\n+\n+\n+class VideoLlama3VisionAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.is_causal = False\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.num_key_value_groups = 1\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input to the layer of shape `(seq_len, embed_dim)`.\n+            cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+                The cumulative sequence lengths of each image or video feature.\n+            position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+                The cosine and sine position embeddings for vision attention.\n+        \"\"\"\n+        seq_length = hidden_states.shape[0]\n+        query_states = self.q_proj(hidden_states).view(seq_length, self.num_heads, self.head_dim)\n+        key_states = self.k_proj(hidden_states).view(seq_length, self.num_heads, self.head_dim)\n+        value_states = self.v_proj(hidden_states).view(seq_length, self.num_heads, self.head_dim)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n+\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()\n+            attn_output, attn_weights = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs, attn_weights = [], []\n+            for q, k, v in zip(*splits):\n+                attn_output, attn_weight = attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )\n+                attn_outputs.append(attn_output)\n+                attn_weights.append(attn_weight)\n+\n+            attn_output = torch.cat(attn_outputs, dim=1)\n+\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class VideoLlama3VisionEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: VideoLlama3VisionConfig):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.self_attn = VideoLlama3VisionAttention(config=config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = VideoLlama3VisionMLP(config=config)\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input to the layer of shape `(seq_len, embed_dim)`.\n+            cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+                The cumulative sequence lengths of each image or video feature.\n+            position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+                The cosine and sine position embeddings for vision attention.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states,\n+            cu_seqlens=cu_seqlens,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+class VideoLlama3VisionEncoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`VideoLlama3VisionEncoderLayer`].\n+\n+    Args:\n+        config: VideoLlama3VisionConfig\n+    \"\"\"\n+\n+    def __init__(self, config: VideoLlama3VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([VideoLlama3VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutput]:\n+        r\"\"\"\n+        cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+            The cumulative sequence lengths of each image or video feature.\n+        position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+            The cosine and sine position embeddings for vision attention.\n+        \"\"\"\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+@auto_docstring\n+class VideoLlama3PreTrainedModel(PreTrainedModel):\n+    config: VideoLlama3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"VideoLlama3VisionEncoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+\n+\n+class VideoLlama3VisionModel(VideoLlama3PreTrainedModel):\n+    config: VideoLlama3VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": VideoLlama3VisionEncoderLayer,\n+        \"attentions\": VideoLlama3VisionAttention,\n+    }\n+\n+    def __init__(self, config: VideoLlama3VisionConfig):\n+        super().__init__(config)\n+        head_dim = config.hidden_size // config.num_attention_heads\n+\n+        self.rotary_pos_emb = VideoLlama3VisionRotaryEmbedding(head_dim // 2)\n+        self.embeddings = VideoLlama3VisionEmbeddings(config)\n+        self.encoder = VideoLlama3VisionEncoder(config)\n+        self.post_layernorm = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> VideoLlama3VisionEmbeddings:\n+        return self.embeddings.patch_embedding\n+\n+    def pixel_unshuffle(\n+        self,\n+        hidden_states: torch.Tensor,\n+        grid_thw: torch.Tensor,\n+        merge_sizes: torch.Tensor,\n+    ):\n+        hidden_states_chunks = hidden_states.split(grid_thw.prod(dim=1).tolist(), dim=0)\n+        outputs = []\n+\n+        for hidden_states, (t, h, w), merge_size in zip(hidden_states_chunks, grid_thw, merge_sizes):\n+            c = hidden_states.shape[-1]\n+            hidden_states = hidden_states.view(t, h // merge_size, w // merge_size, merge_size, merge_size, c).permute(\n+                0, 1, 3, 2, 4, 5\n+            )\n+            hidden_states = hidden_states.reshape(t, h, w, c).permute(0, 3, 1, 2)\n+            hidden_states = torch.nn.functional.interpolate(\n+                hidden_states, size=(h // merge_size, w // merge_size), mode=\"bilinear\"\n+            )\n+            hidden_states = hidden_states.permute(0, 2, 3, 1).view(-1, c)\n+            outputs.append(hidden_states)\n+\n+        return torch.cat(outputs, dim=0)\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        grid_thw: torch.Tensor,\n+        merge_sizes: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutput]:\n+        r\"\"\"\n+        grid_thw (`torch.LongTensor` of shape `(num_images_or_videos, 3)`):\n+            The temporal, height and width dimensions of feature shape for each image. Each row contains [t, h, w] values.\n+        merge_sizes (`torch.Tensor` of shape `(num_images_or_videos,)`):\n+            The spatial downsampling ratio of each image or video feature.\n+        \"\"\"\n+        position_embeddings = self.rotary_pos_emb(grid_thw, merge_sizes)\n+\n+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n+            dim=0,\n+            # Select dtype based on the following factors:\n+            #  - FA2 requires that cu_seqlens_q must have dtype int32\n+            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n+            # See https://github.com/huggingface/transformers/pull/34852 for more information\n+            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n+        )\n+        cu_seqlens = torch.nn.functional.pad(cu_seqlens, (1, 0), value=0)\n+\n+        hidden_states = self.embeddings(pixel_values.type(self.dtype))\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            hidden_states,\n+            cu_seqlens=cu_seqlens,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+        last_hidden_state = self.pixel_unshuffle(last_hidden_state, grid_thw, merge_sizes)\n+\n+        return BaseModelOutput(last_hidden_state=last_hidden_state)\n+\n+\n+class VideoLlama3Projector(nn.Module):\n+    def __init__(self, config: VideoLlama3Config) -> None:\n+        super().__init__()\n+        in_hidden_size = config.vision_config.hidden_size\n+        out_hidden_size = config.text_config.hidden_size\n+        self.readout = nn.Sequential(\n+            nn.Linear(in_hidden_size, out_hidden_size),\n+            nn.GELU(),\n+            nn.Linear(out_hidden_size, out_hidden_size),\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.readout(hidden_states)\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for VideoLLaMA3 outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class VideoLlama3ModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_images_features, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_video_features, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring\n+class VideoLlama3Model(VideoLlama3PreTrainedModel):\n+    base_model_prefix = \"\"\n+    _checkpoint_conversion_mapping = {}\n+    # Reference: fix gemma3 grad acc #37208\n+    accepts_loss_kwargs = False\n+    _can_compile_fullgraph = False\n+\n+    def __init__(self, config: VideoLlama3Config):\n+        super().__init__(config)\n+        self.vision_model = AutoModel.from_config(config.vision_config)\n+        self.projector = VideoLlama3Projector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_video_features(\n+        self,\n+        pixel_values_videos: torch.FloatTensor,\n+        video_grid_thw: torch.LongTensor,\n+        video_merge_sizes: torch.LongTensor,\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+            video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+                The spatial downsampling ratio of each video feature.\n+        \"\"\"\n+        return self.get_image_features(pixel_values_videos, video_grid_thw, video_merge_sizes)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_grid_thw: torch.LongTensor,\n+        image_merge_sizes: torch.LongTensor,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+            image_merge_sizes (`torch.Tensor` of shape `(num_images,)`):\n+                The spatial downsampling ratio of each image feature.\n+        \"\"\"\n+        image_embeds = self.vision_model(\n+            pixel_values=pixel_values,\n+            grid_thw=image_grid_thw,\n+            merge_sizes=image_merge_sizes,\n+            return_dict=True,\n+        ).last_hidden_state\n+        image_embeds = self.projector(image_embeds)\n+\n+        split_sizes = image_grid_thw.prod(dim=1) // (image_merge_sizes**2)\n+        image_embeds = torch.split(image_embeds, split_sizes.tolist())\n+\n+        return image_embeds\n+\n+    def get_placeholder_mask(\n+        self,\n+        input_ids: torch.LongTensor,\n+        inputs_embeds: torch.FloatTensor,\n+        image_features: Optional[torch.FloatTensor] = None,\n+        video_features: Optional[torch.FloatTensor] = None,\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+            special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_video_mask = special_video_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            special_video_mask = input_ids == self.config.video_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if image_features is not None and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {image_features.shape[0]}\"\n+            )\n+\n+        n_video_tokens = special_video_mask.sum()\n+        special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+            raise ValueError(\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+            )\n+\n+        return special_image_mask, special_video_mask\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, VideoLlama3ModelOutputWithPast]:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        image_merge_sizes (`torch.Tensor` of shape `(num_images,)`):\n+            The spatial downsampling ratio of each image feature.\n+        video_grid_thw (`torch.Tensor` of shape `(num_videos, 3)`):\n+            The temporal, height and width of feature shape of each video before vision encoder.\n+        video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+            The spatial downsampling ratio of each video feature.\n+        video_compression_mask (`torch.BoolTensor` of shape `(num_video_features,)`, *optional*):\n+            The mask to indicate which video features are kept after token compression.\n+        \"\"\"\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_embeds = None\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw, image_merge_sizes)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        video_embeds = None\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw, video_merge_sizes)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            if video_compression_mask is not None:\n+                video_embeds = video_embeds[video_compression_mask.to(video_embeds.device)]\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+\n+        outputs = self.language_model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return VideoLlama3ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_embeds,\n+            video_hidden_states=video_embeds,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for VideoLLaMA3 causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class VideoLlama3CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_images_features, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_video_features, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class VideoLlama3ForConditionalGeneration(VideoLlama3PreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _can_compile_fullgraph = False\n+\n+    def __init__(self, config: VideoLlama3Config):\n+        super().__init__(config)\n+        self.model = VideoLlama3Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    def get_video_features(\n+        self, pixel_values_videos: torch.FloatTensor, video_grid_thw: Optional[torch.LongTensor] = None\n+    ):\n+        return self.model.get_video_features(pixel_values_videos, video_grid_thw)\n+\n+    def get_image_features(self, pixel_values: torch.FloatTensor, image_grid_thw: Optional[torch.LongTensor] = None):\n+        return self.model.get_image_features(pixel_values, image_grid_thw)\n+\n+    # Make modules available through conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, VideoLlama3CausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        image_merge_sizes (`torch.Tensor` of shape `(num_images,)`):\n+            The spatial downsampling ratio of each image feature.\n+        video_grid_thw (`torch.Tensor` of shape `(num_videos, 3)`):\n+            The temporal, height and width of feature shape of each video before vision encoder.\n+        video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+            The spatial downsampling ratio of each video feature.\n+        video_compression_mask (`torch.BoolTensor` of shape `(num_video_features,)`, *optional*):\n+            The mask to indicate which video features are kept after token compression.\n+        \"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            image_grid_thw=image_grid_thw,\n+            image_merge_sizes=image_merge_sizes,\n+            pixel_values_videos=pixel_values_videos,\n+            video_grid_thw=video_grid_thw,\n+            video_merge_sizes=video_merge_sizes,\n+            video_compression_mask=video_compression_mask,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        logits = self.lm_head(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return VideoLlama3CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            image_grid_thw=image_grid_thw,\n+            image_merge_sizes=image_merge_sizes,\n+            pixel_values_videos=pixel_values_videos,\n+            video_grid_thw=video_grid_thw,\n+            video_merge_sizes=video_merge_sizes,\n+            video_compression_mask=video_compression_mask,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        if model_inputs[\"cache_position\"][0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n+\n+        return model_inputs\n+\n+    def _get_image_nums_and_video_nums(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n+        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary.\n+\n+        Returns:\n+            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)\n+            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+\n+        if inputs_embeds is not None:\n+            image_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            video_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            image_mask = input_ids == image_token_id\n+            video_mask = input_ids == video_token_id\n+\n+        if image_grid_thw is not None:\n+            num_image_features = image_grid_thw.prod(dim=1) // (image_merge_sizes**2)\n+        else:\n+            num_image_features = []\n+\n+        if video_grid_thw is not None:\n+            num_video_features = video_grid_thw.prod(dim=1) // (video_merge_sizes**2)\n+            if video_compression_mask is not None:\n+                num_video_features = video_compression_mask.split(num_video_features.tolist())\n+                num_video_features = [mask.sum() for mask in num_video_features]\n+        else:\n+            num_video_features = []\n+\n+        image_nums, video_nums = [], []\n+        start_image_idx, start_video_idx = 0, 0\n+\n+        for num_image_tokens, num_video_tokens in zip(image_mask.sum(dim=1), video_mask.sum(dim=1)):\n+            cu_num_features = 0\n+            image_idx = start_image_idx\n+            while image_idx < len(num_image_features) and cu_num_features < num_image_tokens:\n+                cu_num_features += num_image_features[image_idx]\n+                image_idx += 1\n+            assert cu_num_features == num_image_tokens, (\n+                \"The number of image tokens does not match the number of image features.\"\n+            )\n+            image_nums.append(image_idx - start_image_idx)\n+            start_image_idx = image_idx\n+\n+            cu_num_features = 0\n+            video_idx = start_video_idx\n+            while video_idx < len(num_video_features) and cu_num_features < num_video_tokens:\n+                cu_num_features += num_video_features[video_idx]\n+                video_idx += 1\n+            assert cu_num_features == num_video_tokens, (\n+                \"The number of video tokens does not match the number of video features.\"\n+            )\n+            video_nums.append(video_idx - start_video_idx)\n+            start_video_idx = video_idx\n+\n+        return image_nums, video_nums\n+\n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n+        # Overwritten -- Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\n+            \"pixel_values\",\n+            \"image_grid_thw\",\n+            \"image_merge_sizes\",\n+            \"pixel_values_videos\",\n+            \"video_grid_thw\",\n+            \"video_merge_sizes\",\n+            \"video_compression_mask\",\n+        ]\n+\n+        def _repeat_interleave_samples(x, lengths, repeat_times):\n+            samples = torch.split(x, lengths)\n+            repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+            result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+            return result\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            video_merge_sizes = model_kwargs.get(\"video_merge_sizes\", None)\n+            video_compression_mask = model_kwargs.get(\"video_compression_mask\", None)\n+\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids,\n+                inputs_embeds=model_kwargs.get(\"inputs_embeds\", None),\n+                image_grid_thw=image_grid_thw,\n+                image_merge_sizes=model_kwargs.get(\"image_merge_sizes\", None),\n+                video_grid_thw=video_grid_thw,\n+                video_merge_sizes=video_merge_sizes,\n+                video_compression_mask=video_compression_mask,\n+            )\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_merge_sizes\":\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_compression_mask\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    merge_sizes = torch.split(video_merge_sizes, list(video_nums))\n+                    lengths = [\n+                        (torch.prod(sample, dim=1) // merge_size**2).sum()\n+                        for sample, merge_size in zip(samples, merge_sizes)\n+                    ]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_merge_sizes\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n+    @property\n+    def vision_model(self):\n+        return self.model.vision_model\n+\n+\n+__all__ = [\n+    \"VideoLlama3VisionModel\",\n+    \"VideoLlama3PreTrainedModel\",\n+    \"VideoLlama3Model\",\n+    \"VideoLlama3ForConditionalGeneration\",\n+]"
        },
        {
            "sha": "9b9ad7de9105df8491adda9257a2aa27d063dd68",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "added",
            "additions": 1690,
            "deletions": 0,
            "changes": 1690,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,1690 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from collections.abc import Callable\n+from dataclasses import dataclass\n+from typing import Any, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch.nn import LayerNorm\n+\n+from ...cache_utils import Cache\n+from ...configuration_utils import PreTrainedConfig\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_size,\n+    make_flat_list_of_images,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...modeling_outputs import BaseModelOutput, ModelOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n+from ...utils.generic import check_model_inputs\n+from ...video_utils import (\n+    VideoInput,\n+    group_videos_by_shape,\n+    make_batched_videos,\n+    reorder_videos,\n+)\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..auto.modeling_auto import AutoModel\n+from ..qwen2_vl.image_processing_qwen2_vl import Qwen2VLImageProcessor, Qwen2VLImageProcessorKwargs, smart_resize\n+from ..qwen2_vl.image_processing_qwen2_vl_fast import (\n+    Qwen2VLImageProcessorFast,\n+)\n+from ..qwen2_vl.modeling_qwen2_vl import (\n+    Qwen2VLForConditionalGeneration,\n+    Qwen2VLModel,\n+    Qwen2VLPreTrainedModel,\n+    TransformersKwargs,\n+    VisionRotaryEmbedding,\n+    apply_rotary_pos_emb_vision,\n+    eager_attention_forward,\n+)\n+from ..qwen2_vl.processing_qwen2_vl import (\n+    Qwen2VLProcessor,\n+    Qwen2VLProcessorKwargs,\n+)\n+from ..qwen2_vl.video_processing_qwen2_vl import (\n+    Qwen2VLVideoProcessor,\n+    Qwen2VLVideoProcessorInitKwargs,\n+)\n+from ..siglip.configuration_siglip import SiglipVisionConfig\n+from ..siglip.modeling_siglip import (\n+    SiglipAttention,\n+    SiglipEncoder,\n+    SiglipEncoderLayer,\n+    SiglipMLP,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VideoLlama3VisionConfig(SiglipVisionConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`VideoLlama3VisionModel`]. It is used to instantiate a\n+    VideoLLaMA3 vision encoder model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    VideoLLaMA3-2B [lkhl/VideoLLaMA3-2B-Image-HF](https://huggingface.co/lkhl/VideoLLaMA3-2B-Image-HF).\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+    \"\"\"\n+\n+    model_type = \"video_llama_3_vision\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        intermediate_size=3072,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        patch_size=16,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_channels=num_channels,\n+            patch_size=patch_size,\n+            hidden_act=hidden_act,\n+            layer_norm_eps=layer_norm_eps,\n+            attention_dropout=attention_dropout,\n+            **kwargs,\n+        )\n+\n+        self.initializer_range = initializer_range\n+        del self.image_size\n+\n+\n+class VideoLlama3Config(PreTrainedConfig):\n+    \"\"\"\n+    This is the configuration class to store the configuration of a [`VideoLlama3Model`]. It is used to instantiate a\n+    VideoLLaMA3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of\n+    VideoLLaMA3-2B [lkhl/VideoLLaMA3-2B-Image-HF](https://huggingface.co/lkhl/VideoLLaMA3-2B-Image-HF).\n+\n+    Args:\n+        text_config (`Union[PreTrainedConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        vision_config (`Union[PreTrainedConfig, dict]`,  *optional*, defaults to `VideoLlama3VisionConfig`):\n+            The config object or dictionary of the vision backbone.\n+        image_token_id (`int`, *optional*, defaults to 151655):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 151656):\n+            The video token index to encode the image prompt.\n+    \"\"\"\n+\n+    model_type = \"video_llama_3\"\n+    sub_configs = {\"vision_config\": VideoLlama3VisionConfig, \"text_config\": AutoConfig}\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        text_config=None,\n+        vision_config=None,\n+        image_token_id=151655,\n+        video_token_id=151656,\n+        **kwargs,\n+    ):\n+        if isinstance(vision_config, dict):\n+            self.vision_config = self.sub_configs[\"vision_config\"](**vision_config)\n+        elif isinstance(vision_config, PreTrainedConfig):\n+            self.vision_config = vision_config\n+        elif vision_config is None:\n+            self.vision_config = self.sub_configs[\"vision_config\"]()\n+        else:\n+            raise ValueError(\n+                f\"vision_config must be of type `dict` or `PreTrainedConfig`, but got {type(vision_config)}.\"\n+            )\n+\n+        if isinstance(text_config, dict):\n+            self.text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif isinstance(text_config, PreTrainedConfig):\n+            self.text_config = text_config\n+        elif text_config is None:\n+            self.text_config = CONFIG_MAPPING[\"qwen2\"]()\n+        else:\n+            raise ValueError(f\"text_config must be of type `dict` or `PreTrainedConfig`, but got {type(text_config)}.\")\n+\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+\n+        super().__init__(**kwargs)\n+\n+\n+class VideoLlama3VisionRotaryEmbedding(VisionRotaryEmbedding):\n+    def forward(self, grid_thw, merge_sizes) -> tuple[torch.Tensor, torch.Tensor]:\n+        pos_ids = []\n+        for (t, h, w), merge_size in zip(grid_thw, merge_sizes):\n+            hpos_ids = torch.arange(h).unsqueeze(1).expand(-1, w)\n+            hpos_ids = hpos_ids.reshape(\n+                h // merge_size,\n+                merge_size,\n+                w // merge_size,\n+                merge_size,\n+            )\n+            hpos_ids = hpos_ids.permute(0, 2, 1, 3)\n+            hpos_ids = hpos_ids.flatten()\n+\n+            wpos_ids = torch.arange(w).unsqueeze(0).expand(h, -1)\n+            wpos_ids = wpos_ids.reshape(\n+                h // merge_size,\n+                merge_size,\n+                w // merge_size,\n+                merge_size,\n+            )\n+            wpos_ids = wpos_ids.permute(0, 2, 1, 3)\n+            wpos_ids = wpos_ids.flatten()\n+            pos_ids.append(torch.stack([hpos_ids, wpos_ids], dim=-1).repeat(t, 1))\n+\n+        pos_ids = torch.cat(pos_ids, dim=0)\n+        max_grid_thw = grid_thw[:, 1:].max()\n+\n+        seq = torch.arange(max_grid_thw, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n+        rotary_pos_emb_full = torch.outer(seq, self.inv_freq)\n+        rotary_pos_emb = rotary_pos_emb_full[pos_ids].flatten(1)\n+        emb = torch.cat((rotary_pos_emb, rotary_pos_emb), dim=-1)\n+\n+        return (emb.cos(), emb.sin())\n+\n+\n+class VideoLlama3VisionEmbeddings(nn.Module):\n+    def __init__(self, config: VideoLlama3VisionConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.patch_size = config.patch_size\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            padding=\"valid\",\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = hidden_states.view(-1, self.config.num_channels, self.patch_size, self.patch_size)\n+        patch_embeds = self.patch_embedding(hidden_states)\n+        embeddings = patch_embeds.view(-1, self.embed_dim)\n+        return embeddings\n+\n+\n+class VideoLlama3VisionMLP(SiglipMLP):\n+    pass\n+\n+\n+class VideoLlama3VisionAttention(SiglipAttention):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_key_value_groups = 1\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        del self.scale\n+        del self.dropout\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input to the layer of shape `(seq_len, embed_dim)`.\n+            cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+                The cumulative sequence lengths of each image or video feature.\n+            position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+                The cosine and sine position embeddings for vision attention.\n+        \"\"\"\n+        seq_length = hidden_states.shape[0]\n+        query_states = self.q_proj(hidden_states).view(seq_length, self.num_heads, self.head_dim)\n+        key_states = self.k_proj(hidden_states).view(seq_length, self.num_heads, self.head_dim)\n+        value_states = self.v_proj(hidden_states).view(seq_length, self.num_heads, self.head_dim)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb_vision(query_states, key_states, cos, sin)\n+\n+        query_states = query_states.transpose(0, 1).unsqueeze(0)\n+        key_states = key_states.transpose(0, 1).unsqueeze(0)\n+        value_states = value_states.transpose(0, 1).unsqueeze(0)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # Flash Attention 2: Use cu_seqlens for variable length attention\n+            max_seqlen = (cu_seqlens[1:] - cu_seqlens[:-1]).max()\n+            attn_output, attn_weights = attention_interface(\n+                self,\n+                query_states,\n+                key_states,\n+                value_states,\n+                attention_mask=None,\n+                scaling=self.scaling,\n+                dropout=0.0 if not self.training else self.attention_dropout,\n+                cu_seq_lens_q=cu_seqlens,\n+                cu_seq_lens_k=cu_seqlens,\n+                max_length_q=max_seqlen,\n+                max_length_k=max_seqlen,\n+                is_causal=False,\n+                **kwargs,\n+            )\n+        else:\n+            # Other implementations: Process each chunk separately\n+            lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n+            splits = [\n+                torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)\n+            ]\n+\n+            attn_outputs, attn_weights = [], []\n+            for q, k, v in zip(*splits):\n+                attn_output, attn_weight = attention_interface(\n+                    self,\n+                    q,\n+                    k,\n+                    v,\n+                    attention_mask=None,\n+                    scaling=self.scaling,\n+                    dropout=0.0 if not self.training else self.attention_dropout,\n+                    is_causal=False,\n+                    **kwargs,\n+                )\n+                attn_outputs.append(attn_output)\n+                attn_weights.append(attn_weight)\n+\n+            attn_output = torch.cat(attn_outputs, dim=1)\n+\n+        attn_output = attn_output.reshape(seq_length, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class VideoLlama3VisionEncoderLayer(SiglipEncoderLayer):\n+    def __init__(self, config: VideoLlama3VisionConfig):\n+        super().__init__(config)\n+        self.self_attn = VideoLlama3VisionAttention(config=config)\n+        self.mlp = VideoLlama3VisionMLP(config=config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.Tensor`):\n+                Input to the layer of shape `(seq_len, embed_dim)`.\n+            cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+                The cumulative sequence lengths of each image or video feature.\n+            position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+                The cosine and sine position embeddings for vision attention.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states,\n+            cu_seqlens=cu_seqlens,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+class VideoLlama3VisionEncoder(SiglipEncoder):\n+    def __init__(self, config: VideoLlama3VisionConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList([VideoLlama3VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        cu_seqlens: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutput]:\n+        r\"\"\"\n+        cu_seqlens (`torch.Tensor` of shape `(num_images_or_videos + 1,)`):\n+            The cumulative sequence lengths of each image or video feature.\n+        position_embeddings (`tuple(torch.Tensor, torch.Tensor)` of shape `(num_patches, head_dim // 2)`):\n+            The cosine and sine position embeddings for vision attention.\n+        \"\"\"\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n+                hidden_states,\n+                cu_seqlens=cu_seqlens,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+class VideoLlama3PreTrainedModel(Qwen2VLPreTrainedModel):\n+    config: VideoLlama3Config\n+    _no_split_modules = [\"VideoLlama3VisionEncoderLayer\"]\n+\n+\n+class VideoLlama3VisionModel(VideoLlama3PreTrainedModel):\n+    config: VideoLlama3VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": VideoLlama3VisionEncoderLayer,\n+        \"attentions\": VideoLlama3VisionAttention,\n+    }\n+\n+    def __init__(self, config: VideoLlama3VisionConfig):\n+        super().__init__(config)\n+        head_dim = config.hidden_size // config.num_attention_heads\n+\n+        self.rotary_pos_emb = VideoLlama3VisionRotaryEmbedding(head_dim // 2)\n+        self.embeddings = VideoLlama3VisionEmbeddings(config)\n+        self.encoder = VideoLlama3VisionEncoder(config)\n+        self.post_layernorm = LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> VideoLlama3VisionEmbeddings:\n+        return self.embeddings.patch_embedding\n+\n+    def pixel_unshuffle(\n+        self,\n+        hidden_states: torch.Tensor,\n+        grid_thw: torch.Tensor,\n+        merge_sizes: torch.Tensor,\n+    ):\n+        hidden_states_chunks = hidden_states.split(grid_thw.prod(dim=1).tolist(), dim=0)\n+        outputs = []\n+\n+        for hidden_states, (t, h, w), merge_size in zip(hidden_states_chunks, grid_thw, merge_sizes):\n+            c = hidden_states.shape[-1]\n+            hidden_states = hidden_states.view(t, h // merge_size, w // merge_size, merge_size, merge_size, c).permute(\n+                0, 1, 3, 2, 4, 5\n+            )\n+            hidden_states = hidden_states.reshape(t, h, w, c).permute(0, 3, 1, 2)\n+            hidden_states = torch.nn.functional.interpolate(\n+                hidden_states, size=(h // merge_size, w // merge_size), mode=\"bilinear\"\n+            )\n+            hidden_states = hidden_states.permute(0, 2, 3, 1).view(-1, c)\n+            outputs.append(hidden_states)\n+\n+        return torch.cat(outputs, dim=0)\n+\n+    @check_model_inputs(tie_last_hidden_states=False)\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        grid_thw: torch.Tensor,\n+        merge_sizes: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, BaseModelOutput]:\n+        r\"\"\"\n+        grid_thw (`torch.LongTensor` of shape `(num_images_or_videos, 3)`):\n+            The temporal, height and width dimensions of feature shape for each image. Each row contains [t, h, w] values.\n+        merge_sizes (`torch.Tensor` of shape `(num_images_or_videos,)`):\n+            The spatial downsampling ratio of each image or video feature.\n+        \"\"\"\n+        position_embeddings = self.rotary_pos_emb(grid_thw, merge_sizes)\n+\n+        cu_seqlens = torch.repeat_interleave(grid_thw[:, 1] * grid_thw[:, 2], grid_thw[:, 0]).cumsum(\n+            dim=0,\n+            # Select dtype based on the following factors:\n+            #  - FA2 requires that cu_seqlens_q must have dtype int32\n+            #  - torch.onnx.export requires that cu_seqlens_q must have same dtype as grid_thw\n+            # See https://github.com/huggingface/transformers/pull/34852 for more information\n+            dtype=grid_thw.dtype if torch.jit.is_tracing() else torch.int32,\n+        )\n+        cu_seqlens = torch.nn.functional.pad(cu_seqlens, (1, 0), value=0)\n+\n+        hidden_states = self.embeddings(pixel_values.type(self.dtype))\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            hidden_states,\n+            cu_seqlens=cu_seqlens,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+        last_hidden_state = self.pixel_unshuffle(last_hidden_state, grid_thw, merge_sizes)\n+\n+        return BaseModelOutput(last_hidden_state=last_hidden_state)\n+\n+\n+class VideoLlama3Projector(nn.Module):\n+    def __init__(self, config: VideoLlama3Config) -> None:\n+        super().__init__()\n+        in_hidden_size = config.vision_config.hidden_size\n+        out_hidden_size = config.text_config.hidden_size\n+        self.readout = nn.Sequential(\n+            nn.Linear(in_hidden_size, out_hidden_size),\n+            nn.GELU(),\n+            nn.Linear(out_hidden_size, out_hidden_size),\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.readout(hidden_states)\n+        return hidden_states\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for VideoLLaMA3 outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class VideoLlama3ModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_images_features, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_video_features, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class VideoLlama3Model(Qwen2VLModel):\n+    _checkpoint_conversion_mapping = {}\n+    _can_compile_fullgraph = False\n+\n+    def __init__(self, config: VideoLlama3Config):\n+        PreTrainedModel.__init__(self, config)\n+        self.vision_model = AutoModel.from_config(config.vision_config)\n+        self.projector = VideoLlama3Projector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+\n+        self.post_init()\n+\n+    def get_rope_index(self):\n+        raise AttributeError(\"Not needed for VideoLLaMA3\")\n+\n+    def get_video_features(\n+        self,\n+        pixel_values_videos: torch.FloatTensor,\n+        video_grid_thw: torch.LongTensor,\n+        video_merge_sizes: torch.LongTensor,\n+    ):\n+        \"\"\"\n+        Encodes videos into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+            video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+                The spatial downsampling ratio of each video feature.\n+        \"\"\"\n+        return self.get_image_features(pixel_values_videos, video_grid_thw, video_merge_sizes)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_grid_thw: torch.LongTensor,\n+        image_merge_sizes: torch.LongTensor,\n+    ):\n+        \"\"\"\n+        Encodes images into continuous embeddings that can be forwarded to the language model.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input images.\n+            image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each image in LLM.\n+            image_merge_sizes (`torch.Tensor` of shape `(num_images,)`):\n+                The spatial downsampling ratio of each image feature.\n+        \"\"\"\n+        image_embeds = self.vision_model(\n+            pixel_values=pixel_values,\n+            grid_thw=image_grid_thw,\n+            merge_sizes=image_merge_sizes,\n+            return_dict=True,\n+        ).last_hidden_state\n+        image_embeds = self.projector(image_embeds)\n+\n+        split_sizes = image_grid_thw.prod(dim=1) // (image_merge_sizes**2)\n+        image_embeds = torch.split(image_embeds, split_sizes.tolist())\n+\n+        return image_embeds\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, VideoLlama3ModelOutputWithPast]:\n+        r\"\"\"\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        image_merge_sizes (`torch.Tensor` of shape `(num_images,)`):\n+            The spatial downsampling ratio of each image feature.\n+        video_grid_thw (`torch.Tensor` of shape `(num_videos, 3)`):\n+            The temporal, height and width of feature shape of each video before vision encoder.\n+        video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+            The spatial downsampling ratio of each video feature.\n+        video_compression_mask (`torch.BoolTensor` of shape `(num_video_features,)`, *optional*):\n+            The mask to indicate which video features are kept after token compression.\n+        \"\"\"\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_embeds = None\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw, image_merge_sizes)\n+            image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            image_mask, _ = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        video_embeds = None\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw, video_merge_sizes)\n+            video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            if video_compression_mask is not None:\n+                video_embeds = video_embeds[video_compression_mask.to(video_embeds.device)]\n+            _, video_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+\n+        outputs = self.language_model(\n+            input_ids=None,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return VideoLlama3ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_embeds,\n+            video_hidden_states=video_embeds,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for VideoLLaMA3 causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class VideoLlama3CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_images_features, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(num_video_features, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class VideoLlama3ForConditionalGeneration(Qwen2VLForConditionalGeneration):\n+    _checkpoint_conversion_mapping = {}\n+    _can_compile_fullgraph = False\n+\n+    def __init__(self, config: VideoLlama3Config):\n+        super().__init__(config)  # just to add type hint on config\n+\n+    def visual(self):\n+        raise AttributeError(\"Not needed for VideoLLaMA3\")\n+\n+    @property\n+    def vision_model(self):\n+        return self.model.vision_model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, VideoLlama3CausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        image_merge_sizes (`torch.Tensor` of shape `(num_images,)`):\n+            The spatial downsampling ratio of each image feature.\n+        video_grid_thw (`torch.Tensor` of shape `(num_videos, 3)`):\n+            The temporal, height and width of feature shape of each video before vision encoder.\n+        video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+            The spatial downsampling ratio of each video feature.\n+        video_compression_mask (`torch.BoolTensor` of shape `(num_video_features,)`, *optional*):\n+            The mask to indicate which video features are kept after token compression.\n+        \"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            image_grid_thw=image_grid_thw,\n+            image_merge_sizes=image_merge_sizes,\n+            pixel_values_videos=pixel_values_videos,\n+            video_grid_thw=video_grid_thw,\n+            video_merge_sizes=video_merge_sizes,\n+            video_compression_mask=video_compression_mask,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        logits = self.lm_head(hidden_states)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return VideoLlama3CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            image_grid_thw=image_grid_thw,\n+            image_merge_sizes=image_merge_sizes,\n+            pixel_values_videos=pixel_values_videos,\n+            video_grid_thw=video_grid_thw,\n+            video_merge_sizes=video_merge_sizes,\n+            video_compression_mask=video_compression_mask,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        if model_inputs[\"cache_position\"][0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n+\n+        return model_inputs\n+\n+    def _get_image_nums_and_video_nums(\n+        self,\n+        input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        image_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        video_merge_sizes: Optional[torch.LongTensor] = None,\n+        video_compression_mask: Optional[torch.BoolTensor] = None,\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n+        These parameters are not passed through the processor to avoid unpredictable impacts from interface modifications.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary.\n+\n+        Returns:\n+            image_nums (`torch.LongTensor` of shape `(batch_size, num_images_sample)`)\n+            video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n+        \"\"\"\n+        image_token_id = self.config.image_token_id\n+        video_token_id = self.config.video_token_id\n+\n+        if inputs_embeds is not None:\n+            image_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            video_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            image_mask = input_ids == image_token_id\n+            video_mask = input_ids == video_token_id\n+\n+        if image_grid_thw is not None:\n+            num_image_features = image_grid_thw.prod(dim=1) // (image_merge_sizes**2)\n+        else:\n+            num_image_features = []\n+\n+        if video_grid_thw is not None:\n+            num_video_features = video_grid_thw.prod(dim=1) // (video_merge_sizes**2)\n+            if video_compression_mask is not None:\n+                num_video_features = video_compression_mask.split(num_video_features.tolist())\n+                num_video_features = [mask.sum() for mask in num_video_features]\n+        else:\n+            num_video_features = []\n+\n+        image_nums, video_nums = [], []\n+        start_image_idx, start_video_idx = 0, 0\n+\n+        for num_image_tokens, num_video_tokens in zip(image_mask.sum(dim=1), video_mask.sum(dim=1)):\n+            cu_num_features = 0\n+            image_idx = start_image_idx\n+            while image_idx < len(num_image_features) and cu_num_features < num_image_tokens:\n+                cu_num_features += num_image_features[image_idx]\n+                image_idx += 1\n+            assert cu_num_features == num_image_tokens, (\n+                \"The number of image tokens does not match the number of image features.\"\n+            )\n+            image_nums.append(image_idx - start_image_idx)\n+            start_image_idx = image_idx\n+\n+            cu_num_features = 0\n+            video_idx = start_video_idx\n+            while video_idx < len(num_video_features) and cu_num_features < num_video_tokens:\n+                cu_num_features += num_video_features[video_idx]\n+                video_idx += 1\n+            assert cu_num_features == num_video_tokens, (\n+                \"The number of video tokens does not match the number of video features.\"\n+            )\n+            video_nums.append(video_idx - start_video_idx)\n+            start_video_idx = video_idx\n+\n+        return image_nums, video_nums\n+\n+    def _expand_inputs_for_generation(\n+        self,\n+        expand_size: int = 1,\n+        is_encoder_decoder: bool = False,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        **model_kwargs,\n+    ) -> tuple[torch.LongTensor, dict[str, Any]]:\n+        # Overwritten -- Support for expanding tensors without a batch size dimension\n+        # e.g., pixel_values, image_grid_thw, pixel_values_videos, video_grid_thw, second_per_grid_t\n+        # pixel_values.shape[0] is sum(seqlen_images for samples)\n+        # image_grid_thw.shape[0] is sum(num_images for samples)\n+\n+        if expand_size == 1:\n+            return input_ids, model_kwargs\n+\n+        visual_keys = [\n+            \"pixel_values\",\n+            \"image_grid_thw\",\n+            \"image_merge_sizes\",\n+            \"pixel_values_videos\",\n+            \"video_grid_thw\",\n+            \"video_merge_sizes\",\n+            \"video_compression_mask\",\n+        ]\n+\n+        def _repeat_interleave_samples(x, lengths, repeat_times):\n+            samples = torch.split(x, lengths)\n+            repeat_args = [repeat_times] + [1] * (x.dim() - 1)\n+            result = torch.cat([sample.repeat(*repeat_args) for sample in samples], dim=0)\n+            return result\n+\n+        def _expand_dict_for_generation_visual(dict_to_expand):\n+            image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n+            video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n+            video_merge_sizes = model_kwargs.get(\"video_merge_sizes\", None)\n+            video_compression_mask = model_kwargs.get(\"video_compression_mask\", None)\n+\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids,\n+                inputs_embeds=model_kwargs.get(\"inputs_embeds\", None),\n+                image_grid_thw=image_grid_thw,\n+                image_merge_sizes=model_kwargs.get(\"image_merge_sizes\", None),\n+                video_grid_thw=video_grid_thw,\n+                video_merge_sizes=video_merge_sizes,\n+                video_compression_mask=video_compression_mask,\n+            )\n+            for key in dict_to_expand:\n+                if key == \"pixel_values\":\n+                    # split images into samples\n+                    samples = torch.split(image_grid_thw, list(image_nums))\n+                    # compute the sequence length of images for each sample\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_grid_thw\":\n+                    # get the num of images for each sample\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"image_merge_sizes\":\n+                    lengths = list(image_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"pixel_values_videos\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    lengths = [torch.prod(sample, dim=1).sum() for sample in samples]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_compression_mask\":\n+                    samples = torch.split(video_grid_thw, list(video_nums))\n+                    merge_sizes = torch.split(video_merge_sizes, list(video_nums))\n+                    lengths = [\n+                        (torch.prod(sample, dim=1) // merge_size**2).sum()\n+                        for sample, merge_size in zip(samples, merge_sizes)\n+                    ]\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_grid_thw\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+                elif key == \"video_merge_sizes\":\n+                    lengths = list(video_nums)\n+                    dict_to_expand[key] = _repeat_interleave_samples(\n+                        dict_to_expand[key], lengths=lengths, repeat_times=expand_size\n+                    )\n+\n+            return dict_to_expand\n+\n+        def _expand_dict_for_generation(dict_to_expand):\n+            for key in dict_to_expand:\n+                if (\n+                    key != \"cache_position\"\n+                    and dict_to_expand[key] is not None\n+                    and isinstance(dict_to_expand[key], torch.Tensor)\n+                    and key not in visual_keys\n+                ):\n+                    dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n+            return dict_to_expand\n+\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+\n+        if input_ids is not None:\n+            input_ids = input_ids.repeat_interleave(expand_size, dim=0)\n+\n+        model_kwargs = _expand_dict_for_generation(model_kwargs)\n+\n+        if is_encoder_decoder:\n+            if model_kwargs.get(\"encoder_outputs\") is None:\n+                raise ValueError(\"If `is_encoder_decoder` is True, make sure that `encoder_outputs` is defined.\")\n+            model_kwargs[\"encoder_outputs\"] = _expand_dict_for_generation(model_kwargs[\"encoder_outputs\"])\n+\n+        return input_ids, model_kwargs\n+\n+\n+class VideoLlama3ProcessorKwargs(Qwen2VLProcessorKwargs):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n+        },\n+        \"videos_kwargs\": {\"return_metadata\": True},\n+    }\n+\n+\n+class VideoLlama3Processor(Qwen2VLProcessor):\n+    r\"\"\"\n+    Constructs a VideoLLaMA3 processor which wraps a VideoLLaMA3 image processor and a Qwen2 tokenizer into a single processor.\n+    [`VideoLlama3Processor`] offers all the functionalities of [`VideoLlama3ImageProcessor`] and [`Qwen2Tokenizer`]. See the\n+    [`~VideoLlama3Processor.__call__`] and [`~VideoLlama3Processor.decode`] for more information.\n+    Args:\n+        image_processor ([`VideoLlama3ImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2Tokenizer`], *optional*):\n+            The tokenizer is a required input.\n+        video_processor ([`VideoLlama3VideoProcessor`], *optional*):\n+            The video processor is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+    \"\"\"\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        videos: VideoInput = None,\n+        **kwargs: Unpack[VideoLlama3ProcessorKwargs],\n+    ) -> BatchFeature:\n+        output_kwargs = self._merge_kwargs(\n+            VideoLlama3ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        image_inputs = videos_inputs = {}\n+        if images is not None:\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_grid_thw = image_inputs[\"image_grid_thw\"]\n+            image_merge_sizes = image_inputs[\"image_merge_sizes\"]\n+        else:\n+            image_grid_thw = image_merge_sizes = []\n+\n+        if videos is not None:\n+            videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            num_video_tokens = [\n+                grid_thw.prod() // merge_size**2\n+                for grid_thw, merge_size in zip(videos_inputs[\"video_grid_thw\"], videos_inputs[\"video_merge_sizes\"])\n+            ]\n+            video_compression_masks = videos_inputs[\"video_compression_mask\"].split(num_video_tokens)\n+            if \"return_metadata\" not in kwargs:\n+                video_metadata = videos_inputs.pop(\"video_metadata\")\n+            else:\n+                video_metadata = videos_inputs[\"video_metadata\"]\n+            timestamps = []\n+            for metadata in video_metadata:\n+                if metadata.fps is None:\n+                    logger.warning_once(\n+                        \"VideoLLaMA4 requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                        \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                        \"Defaulting to `fps=1`. Please provide `video_metadata` for more accurate results.\"\n+                    )\n+                metadata.fps = 1 if metadata.fps is None else metadata.fps\n+                timestamps.append(metadata.timestamps)\n+        else:\n+            video_compression_masks = timestamps = []\n+\n+        if not isinstance(text, list):\n+            text = [text]\n+\n+        text = text.copy()  # below lines change text in-place\n+\n+        if images is not None:\n+            image_index = 0\n+            for i in range(len(text)):\n+                while self.image_token in text[i]:\n+                    num_image_tokens = image_grid_thw[image_index].prod() // (image_merge_sizes[image_index] ** 2)\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n+                    image_index += 1\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n+\n+        if videos is not None:\n+            video_index = 0\n+            for i in range(len(text)):\n+                while self.video_token in text[i]:\n+                    frame_compression_masks = video_compression_masks[video_index].split(\n+                        len(video_compression_masks[video_index]) // len(timestamps[video_index])\n+                    )\n+                    num_frame_tokens = [x.sum() for x in frame_compression_masks]\n+                    frame_prompts = [\n+                        f\"Time {t:.1f}s:\" + \"<|placeholder|>\" * n\n+                        for n, t in zip(num_frame_tokens, timestamps[video_index])\n+                    ]\n+                    text[i] = text[i].replace(self.video_token, \",\".join(frame_prompts), 1)\n+                    video_index += 1\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n+\n+\n+class VideoLlama3ImageProcessorKwargs(Qwen2VLImageProcessorKwargs):\n+    pass\n+\n+\n+class VideoLlama3ImageProcessor(Qwen2VLImageProcessor):\n+    r\"\"\"\n+    Constructs a VideoLLaMA3 image processor that dynamically resizes images based on the original images.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions.\n+        size (`dict[str, int]`, *optional*, defaults to `{\"shortest_edge\": 56 * 56, \"longest_edge\": 28 * 28 * 1280}`):\n+            Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use when resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image.\n+        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n+            The min pixels of the image to resize the image.\n+        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n+            The max pixels of the image to resize the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The spatial patch size of the vision encoder.\n+        temporal_patch_size (`int`, *optional*, defaults to 1):\n+            The temporal patch size of the vision encoder.\n+        merge_size (`int`, *optional*, defaults to 1):\n+            The merge size of the vision encoder to llm encoder.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"image_merge_sizes\"]\n+    valid_kwargs = VideoLlama3ImageProcessorKwargs\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        patch_size: int = 14,\n+        temporal_patch_size: int = 1,\n+        merge_size: int = 1,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_convert_rgb=do_convert_rgb,\n+            min_pixels=min_pixels,\n+            max_pixels=max_pixels,\n+            patch_size=patch_size,\n+            temporal_patch_size=temporal_patch_size,\n+            merge_size=merge_size,\n+            **kwargs,\n+        )\n+\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n+\n+        if self.temporal_patch_size != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        videos: Optional[VideoInput] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            videos (`VideoInput`):\n+                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n+                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            min_pixels (`int`, *optional*, defaults to `self.min_pixels`):\n+                The min pixels of the image to resize the image.\n+            max_pixels (`int`, *optional*, defaults to `self.max_pixels`):\n+                The max pixels of the image to resize the image.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                The spatial patch size of the vision encoder.\n+            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n+                The temporal patch size of the vision encoder.\n+            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n+                The merge size of the vision encoder to llm encoder.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        \"\"\"\n+        min_pixels = min_pixels if min_pixels is not None else self.min_pixels\n+        max_pixels = max_pixels if max_pixels is not None else self.max_pixels\n+\n+        if size is not None:\n+            if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+                raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+            min_pixels = size[\"shortest_edge\"]\n+        elif min_pixels is not None and max_pixels is not None:\n+            # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+            size = {\"shortest_edge\": min_pixels, \"longest_edge\": max_pixels}\n+        else:\n+            size = {**self.size}\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n+        merge_size = merge_size if merge_size is not None else self.merge_size\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        if images is not None:\n+            images = self.fetch_images(images)\n+            images = make_flat_list_of_images(images)\n+\n+        if images is not None and not valid_images(images):\n+            raise ValueError(\"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n+\n+        validate_preprocess_arguments(\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        data = {}\n+        if images is not None:\n+            pixel_values, vision_grid_thws = [], []\n+            for image in images:\n+                patches, image_grid_thw = self._preprocess(\n+                    image,\n+                    do_resize=do_resize,\n+                    size=size,\n+                    resample=resample,\n+                    do_rescale=do_rescale,\n+                    rescale_factor=rescale_factor,\n+                    do_normalize=do_normalize,\n+                    image_mean=image_mean,\n+                    image_std=image_std,\n+                    patch_size=patch_size,\n+                    temporal_patch_size=temporal_patch_size,\n+                    merge_size=merge_size,\n+                    data_format=data_format,\n+                    do_convert_rgb=do_convert_rgb,\n+                    input_data_format=input_data_format,\n+                )\n+                pixel_values.extend(patches)\n+                vision_grid_thws.append(image_grid_thw)\n+            data.update(\n+                {\n+                    \"pixel_values\": np.array(pixel_values),\n+                    \"image_grid_thw\": np.array(vision_grid_thws),\n+                    \"image_merge_sizes\": np.array([merge_size] * len(vision_grid_thws)),\n+                }\n+            )\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+class VideoLlama3ImageProcessorFast(Qwen2VLImageProcessorFast):\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    temporal_patch_size = 1\n+    merge_size = 1\n+    valid_kwargs = VideoLlama3ImageProcessorKwargs\n+    model_input_names = [\n+        \"pixel_values\",\n+        \"image_grid_thw\",\n+        \"image_merge_sizes\",\n+        \"pixel_values_videos\",\n+        \"video_grid_thw\",\n+        \"video_merge_sizes\",\n+    ]\n+\n+    def _preprocess_image_like_inputs(\n+        self,\n+        images: ImageInput,\n+        videos: VideoInput,\n+        do_convert_rgb: bool,\n+        input_data_format: ChannelDimension,\n+        device: Optional[Union[str, \"torch.device\"]] = None,\n+        **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        # Prepare input images\n+        batch_feature = BatchFeature()\n+        if images is not None:\n+            if kwargs[\"temporal_patch_size\"] != 1:\n+                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+            images = self._prepare_image_like_inputs(\n+                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+            )\n+            batch_feature = self._preprocess(images, **kwargs)\n+            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+                dtype=batch_feature.image_grid_thw.dtype,\n+                device=batch_feature.image_grid_thw.device,\n+            )\n+        if videos is not None:\n+            logger.warning(\n+                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n+                \"This is a deprecated behavior and will be removed in v5.0. \"\n+                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n+            )\n+            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n+            videos = make_batched_videos(videos)\n+            videos = [\n+                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n+                for video in videos\n+            ]\n+            video_outputs = self._preprocess(videos, **kwargs)\n+            batch_feature.update(\n+                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n+            )\n+            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n+                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n+                dtype=video_outputs.image_grid_thw.dtype,\n+                device=video_outputs.image_grid_thw.device,\n+            )\n+        return batch_feature\n+\n+\n+class VideoLlama3VideoProcessorInitKwargs(Qwen2VLVideoProcessorInitKwargs):\n+    use_token_compression: Optional[bool]\n+\n+\n+class VideoLlama3VideoProcessor(Qwen2VLVideoProcessor):\n+    use_token_compression = True\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    temporal_patch_size = 1\n+    max_frames = 180\n+    return_metadata = True\n+    valid_kwargs = VideoLlama3VideoProcessorInitKwargs\n+    model_input_names = [\"pixel_values_videos\", \"video_grid_thw\", \"video_merge_sizes\", \"video_compression_mask\"]\n+\n+    def _get_compression_mask(\n+        self,\n+        pixel_values_videos: torch.FloatTensor,\n+        video_grid_thw: torch.LongTensor,\n+        video_merge_sizes: torch.LongTensor,\n+        threshold: Optional[float] = 0.1,\n+        min_tokens: Optional[int] = 1,\n+    ) -> torch.BoolTensor:\n+        \"\"\"\n+        Get the compression mask for video tokens based on pixel differences.\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+            video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+                The spatial downsampling ratio of each video feature.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                The threshold to determine whether a token should be kept based on pixel differences.\n+            min_tokens (`int`, *optional*, defaults to 1):\n+                The minimum number of tokens to keep for each frame.\n+        \"\"\"\n+        videos = pixel_values_videos.split(video_grid_thw.prod(dim=1).tolist(), dim=0)\n+        compression_masks = []\n+\n+        for images, grid_size, merge_size in zip(videos, video_grid_thw, video_merge_sizes):\n+            t, h, w = grid_size\n+            if t == 1:\n+                num_tokens = images.size(0) // (merge_size**2)\n+                compression_masks.append(torch.ones((num_tokens,), dtype=torch.bool, device=images.device))\n+            else:\n+                # NOTE: video token compressor\n+                images = images.view(t, (h // merge_size) * (w // merge_size), -1)\n+\n+                pixel_diff = images[1:] - images[:-1]\n+                pixel_diff = torch.abs(pixel_diff).mean(dim=-1) * 255\n+                pixel_diff = torch.cat([torch.full_like(pixel_diff[0:1], threshold + 1), pixel_diff], dim=0)\n+                mask = pixel_diff > threshold\n+                padding_ids = torch.nonzero(mask.sum(dim=1) < min_tokens)[:, 0]\n+                mask[padding_ids, :min_tokens] = 1\n+                compression_masks.append(mask.flatten())\n+\n+        return torch.cat(compression_masks)\n+\n+    def _preprocess(\n+        self,\n+        videos: list[\"torch.Tensor\"],\n+        do_convert_rgb: bool,\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n+        use_token_compression: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n+        **kwargs,\n+    ):\n+        # Group videos by size for batched resizing\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n+        resized_videos_grouped = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            height, width = get_image_size(stacked_videos[0], channel_dim=ChannelDimension.FIRST)\n+            resized_height, resized_width = height, width\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    height,\n+                    width,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=min_pixels,\n+                    max_pixels=max_pixels // shape[0],\n+                )\n+                stacked_videos = self.resize(\n+                    image=stacked_videos,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n+                    interpolation=interpolation,\n+                )\n+            resized_videos_grouped[shape] = stacked_videos\n+        resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n+\n+        # Group videos by size for further processing\n+        # Needed in case do_resize is False, or resize returns videos with different sizes\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(resized_videos)\n+        processed_videos_grouped = {}\n+        processed_grids = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            resized_height, resized_width = get_image_size(stacked_videos[0], channel_dim=ChannelDimension.FIRST)\n+\n+            # Fused rescale and normalize\n+            stacked_videos = self.rescale_and_normalize(\n+                stacked_videos, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            patches = stacked_videos\n+\n+            # Check that videos have `num_frames` divisible by `temporal_patch_size`\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(1, self.temporal_patch_size - 1, 1, 1, 1)\n+                patches = torch.cat([patches, repeats], dim=1)\n+\n+            batch_size, grid_t, channel = patches.shape[:3]\n+            grid_t = grid_t // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+\n+            patches = patches.view(\n+                batch_size,\n+                grid_t,\n+                temporal_patch_size,\n+                channel,\n+                grid_h // merge_size,\n+                merge_size,\n+                patch_size,\n+                grid_w // merge_size,\n+                merge_size,\n+                patch_size,\n+            )\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+            flatten_patches = patches.reshape(\n+                batch_size,\n+                grid_t * grid_h * grid_w,\n+                channel * temporal_patch_size * patch_size * patch_size,\n+            )\n+\n+            processed_videos_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n+\n+        processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n+        processed_grids = reorder_videos(processed_grids, grouped_videos_index)\n+        pixel_values_videos = torch.cat(processed_videos, dim=0)\n+        video_grid_thw = torch.tensor(processed_grids)\n+        video_merge_sizes = torch.tensor([merge_size] * video_grid_thw.size(0)).to(video_grid_thw)\n+\n+        if use_token_compression:\n+            video_compression_mask = self._get_compression_mask(\n+                pixel_values_videos=pixel_values_videos,\n+                video_grid_thw=video_grid_thw,\n+                video_merge_sizes=video_merge_sizes,\n+            )\n+        else:\n+            num_video_tokens = video_grid_thw.prod(-1).sum() // (merge_size**2)\n+            video_compression_mask = torch.ones(\n+                (num_video_tokens,), dtype=torch.bool, device=pixel_values_videos.device\n+            )\n+\n+        return BatchFeature(\n+            data={\n+                \"pixel_values_videos\": pixel_values_videos,\n+                \"video_grid_thw\": video_grid_thw,\n+                \"video_merge_sizes\": video_merge_sizes,\n+                \"video_compression_mask\": video_compression_mask,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+\n+__all__ = [\n+    \"VideoLlama3VisionConfig\",\n+    \"VideoLlama3Config\",\n+    \"VideoLlama3VisionModel\",\n+    \"VideoLlama3PreTrainedModel\",\n+    \"VideoLlama3Model\",\n+    \"VideoLlama3ForConditionalGeneration\",\n+    \"VideoLlama3Processor\",\n+    \"VideoLlama3ImageProcessor\",\n+    \"VideoLlama3ImageProcessorFast\",\n+    \"VideoLlama3VideoProcessor\",\n+]"
        },
        {
            "sha": "37127d7360536db2857652a8558851a3c98aeeaf",
            "filename": "src/transformers/models/video_llama_3/processing_video_llama_3.py",
            "status": "added",
            "additions": 268,
            "deletions": 0,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,268 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/video_llama_3/modular_video_llama_3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_video_llama_3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ...video_utils import VideoInput\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VideoLlama3ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n+        },\n+        \"videos_kwargs\": {\"return_metadata\": True},\n+    }\n+\n+\n+class VideoLlama3Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a VideoLLaMA3 processor which wraps a VideoLLaMA3 image processor and a Qwen2 tokenizer into a single processor.\n+    [`VideoLlama3Processor`] offers all the functionalities of [`VideoLlama3ImageProcessor`] and [`Qwen2Tokenizer`]. See the\n+    [`~VideoLlama3Processor.__call__`] and [`~VideoLlama3Processor.decode`] for more information.\n+    Args:\n+        image_processor ([`VideoLlama3ImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`Qwen2Tokenizer`], *optional*):\n+            The tokenizer is a required input.\n+        video_processor ([`VideoLlama3VideoProcessor`], *optional*):\n+            The video processor is a required input.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    video_processor_class = \"AutoVideoProcessor\"\n+    tokenizer_class = (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\")\n+\n+    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n+        self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n+        self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n+        self.image_token_id = (\n+            tokenizer.image_token_id\n+            if getattr(tokenizer, \"image_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.image_token)\n+        )\n+        self.video_token_id = (\n+            tokenizer.video_token_id\n+            if getattr(tokenizer, \"video_token_id\", None)\n+            else tokenizer.convert_tokens_to_ids(self.video_token)\n+        )\n+        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        videos: VideoInput = None,\n+        **kwargs: Unpack[VideoLlama3ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n+        VideoLlama3ImageProcessor's [`~VideoLlama3ImageProcessor.__call__`] if `vision_infos` is not `None`.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n+                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_values_videos** -- Pixel values of videos to be fed to a model. Returned when `videos` is not `None`.\n+            - **image_grid_thw** -- List of image 3D grid in LLM. Returned when `images` is not `None`.\n+            - **video_grid_thw** -- List of video 3D grid in LLM. Returned when `videos` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            VideoLlama3ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        image_inputs = videos_inputs = {}\n+        if images is not None:\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+            image_grid_thw = image_inputs[\"image_grid_thw\"]\n+            image_merge_sizes = image_inputs[\"image_merge_sizes\"]\n+        else:\n+            image_grid_thw = image_merge_sizes = []\n+\n+        if videos is not None:\n+            videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            num_video_tokens = [\n+                grid_thw.prod() // merge_size**2\n+                for grid_thw, merge_size in zip(videos_inputs[\"video_grid_thw\"], videos_inputs[\"video_merge_sizes\"])\n+            ]\n+            video_compression_masks = videos_inputs[\"video_compression_mask\"].split(num_video_tokens)\n+            if \"return_metadata\" not in kwargs:\n+                video_metadata = videos_inputs.pop(\"video_metadata\")\n+            else:\n+                video_metadata = videos_inputs[\"video_metadata\"]\n+            timestamps = []\n+            for metadata in video_metadata:\n+                if metadata.fps is None:\n+                    logger.warning_once(\n+                        \"VideoLLaMA4 requires frame timestamps to construct prompts, but the `fps` of the input video could not be inferred. \"\n+                        \"Probably `video_metadata` was missing from inputs and you passed pre-sampled frames. \"\n+                        \"Defaulting to `fps=1`. Please provide `video_metadata` for more accurate results.\"\n+                    )\n+                metadata.fps = 1 if metadata.fps is None else metadata.fps\n+                timestamps.append(metadata.timestamps)\n+        else:\n+            video_compression_masks = timestamps = []\n+\n+        if not isinstance(text, list):\n+            text = [text]\n+\n+        text = text.copy()  # below lines change text in-place\n+\n+        if images is not None:\n+            image_index = 0\n+            for i in range(len(text)):\n+                while self.image_token in text[i]:\n+                    num_image_tokens = image_grid_thw[image_index].prod() // (image_merge_sizes[image_index] ** 2)\n+                    text[i] = text[i].replace(self.image_token, \"<|placeholder|>\" * num_image_tokens, 1)\n+                    image_index += 1\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n+\n+        if videos is not None:\n+            video_index = 0\n+            for i in range(len(text)):\n+                while self.video_token in text[i]:\n+                    frame_compression_masks = video_compression_masks[video_index].split(\n+                        len(video_compression_masks[video_index]) // len(timestamps[video_index])\n+                    )\n+                    num_frame_tokens = [x.sum() for x in frame_compression_masks]\n+                    frame_prompts = [\n+                        f\"Time {t:.1f}s:\" + \"<|placeholder|>\" * n\n+                        for n, t in zip(num_frame_tokens, timestamps[video_index])\n+                    ]\n+                    text[i] = text[i].replace(self.video_token, \",\".join(frame_prompts), 1)\n+                    video_index += 1\n+                text[i] = text[i].replace(\"<|placeholder|>\", self.video_token)\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+        self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n+\n+    def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+            video_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (num_frames, height, width) per each video.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = VideoLlama3ProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        if video_sizes is not None:\n+            videos_kwargs = VideoLlama3ProcessorKwargs._defaults.get(\"videos_kwargs\", {})\n+            videos_kwargs.update(kwargs)\n+            num_video_patches = [\n+                self.video_processor.get_number_of_video_patches(*video_size, videos_kwargs)\n+                for video_size in video_sizes\n+            ]\n+            num_video_tokens = [(num_patches // merge_size**2) for num_patches in num_video_patches]\n+            vision_data[\"num_video_tokens\"] = num_video_tokens\n+\n+        return MultiModalData(**vision_data)\n+\n+    def post_process_image_text_to_text(\n+        self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of the model to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `False`):\n+                Whether or not to clean up the tokenization spaces. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(\n+            generated_outputs,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"VideoLlama3Processor\"]"
        },
        {
            "sha": "981d4f39f6edb27aed42b7ac398779575ae109b2",
            "filename": "src/transformers/models/video_llama_3/video_processing_video_llama_3.py",
            "status": "added",
            "additions": 374,
            "deletions": 0,
            "changes": 374,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fvideo_processing_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,374 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/video_llama_3/modular_video_llama_3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_video_llama_3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_size,\n+)\n+from ...processing_utils import Unpack, VideosKwargs\n+from ...utils import TensorType, add_start_docstrings\n+from ...video_processing_utils import BASE_VIDEO_PROCESSOR_DOCSTRING, BaseVideoProcessor\n+from ...video_utils import VideoMetadata, group_videos_by_shape, reorder_videos\n+from .image_processing_video_llama_3 import smart_resize\n+\n+\n+class VideoLlama3VideoProcessorInitKwargs(VideosKwargs, total=False):\n+    min_pixels: int\n+    max_pixels: int\n+    patch_size: int\n+    temporal_patch_size: int\n+    merge_size: int\n+    min_frames: int\n+    max_frames: int\n+    use_token_compression: Optional[bool]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Qwen2-VL image processor that dynamically resizes videos based on the original videos.\",\n+    BASE_VIDEO_PROCESSOR_DOCSTRING,\n+    \"\"\"\n+        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n+            The min pixels of the image to resize the image.\n+        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n+            The max pixels of the image to resize the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The spacial patch size of the vision encoder.\n+        temporal_patch_size (`int`, *optional*, defaults to 2):\n+            The temporal patch size of the vision encoder.\n+        merge_size (`int`, *optional*, defaults to 2):\n+            The merge size of the vision encoder to llm encoder.\n+        min_frames (`int`, *optional*, defaults to 4):\n+            The minimum number of frames that can be sampled.\n+        max_frames (`int`, *optional*, defaults to 768):\n+            The maximum number of frames that can be sampled.\n+    \"\"\",\n+)\n+class VideoLlama3VideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BICUBIC\n+    size = {\"shortest_edge\": 128 * 28 * 28, \"longest_edge\": 28 * 28 * 768}\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    min_pixels = 128 * 28 * 28\n+    max_pixels = 28 * 28 * 768\n+    patch_size = 14\n+    temporal_patch_size = 1\n+    merge_size = 2\n+    min_frames = 4\n+    max_frames = 180\n+    do_sample_frames = False  # Set to False for BC, recommended to set `True` in new models\n+    valid_kwargs = VideoLlama3VideoProcessorInitKwargs\n+    model_input_names = [\"pixel_values_videos\", \"video_grid_thw\", \"video_merge_sizes\", \"video_compression_mask\"]\n+    use_token_compression = True\n+    return_metadata = True\n+\n+    def __init__(self, **kwargs: Unpack[VideoLlama3VideoProcessorInitKwargs]):\n+        size = kwargs.pop(\"size\", None)\n+        min_pixels = kwargs.pop(\"min_pixels\", None)\n+        max_pixels = kwargs.pop(\"max_pixels\", None)\n+        # backward compatibility: override size with min_pixels and max_pixels if they are provided\n+        size = self.size if size is None else size\n+        if min_pixels is not None:\n+            size[\"shortest_edge\"] = min_pixels\n+            size.pop(\"min_pixels\", None)\n+        if max_pixels is not None:\n+            size[\"longest_edge\"] = max_pixels\n+            size.pop(\"max_pixels\", None)\n+        if \"shortest_edge\" not in size or \"longest_edge\" not in size:\n+            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        super().__init__(size=size, min_pixels=min_pixels, max_pixels=max_pixels, **kwargs)\n+\n+    def sample_frames(\n+        self,\n+        metadata: VideoMetadata,\n+        temporal_patch_size: Optional[int] = None,\n+        min_frames: Optional[int] = None,\n+        max_frames: Optional[int] = None,\n+        num_frames: Optional[int] = None,\n+        fps: Optional[Union[int, float]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Default sampling function which uniformly samples the desired number of frames between 0 and total number of frames.\n+        If `fps` is passed along with metadata, `fps` frames per second are sampled uniformty. Arguments `num_frames`\n+        and `fps` are mutually exclusive.\n+\n+        Args:\n+            metadata (`VideoMetadata`):\n+                Metadata of the video containing information about total duration, fps and total number of frames.\n+            temporal_patch_size (`int`, *optional*):\n+                The temporal patch size of the vision encoder. Number of sampled frames will be rounded to be divisible by frame factor.\n+            min_frames (`int`, *optional*):\n+                The minimum number of frames that can be sampled.\n+            max_frames (`int`, *optional*):\n+                The maximum number of frames that can be sampled.\n+            num_frames (`int`, *optional*):\n+                Maximum number of frames to sample. Defaults to `self.num_frames`.\n+            fps (`int` or `float`, *optional*):\n+                Target frames to sample per second. Defaults to `self.fps`.\n+\n+        Returns:\n+            np.ndarray:\n+                Indices to sample video frames.\n+        \"\"\"\n+        if fps is not None and num_frames is not None:\n+            raise ValueError(\"`num_frames` and `fps` are mutually exclusive arguments, please use only one!\")\n+\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        fps = fps if fps is not None else self.fps\n+        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n+        min_frames = min_frames if min_frames is not None else self.min_frames\n+        max_frames = max_frames if max_frames is not None else self.max_frames\n+        total_num_frames = metadata.total_num_frames\n+\n+        # If num_frames is not given but fps is, calculate num_frames from fps\n+        if num_frames is not None:\n+            num_frames = round(num_frames / temporal_patch_size) * temporal_patch_size\n+        elif fps is not None:\n+            if metadata is None or metadata.fps is None:\n+                raise ValueError(\n+                    \"Asked to sample `fps` frames per second but no video metadata was provided which is required when sampling with `fps`. \"\n+                    \"Please pass in `VideoMetadata` object or use a fixed `num_frames` per input video\"\n+                )\n+            max_frames = math.floor(min(max_frames, total_num_frames) / temporal_patch_size) * temporal_patch_size\n+            num_frames = total_num_frames / metadata.fps * fps\n+            num_frames = min(min(max(num_frames, min_frames), max_frames), total_num_frames)\n+            num_frames = math.floor(num_frames / temporal_patch_size) * temporal_patch_size\n+\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"Video can't be sampled. The inferred `num_frames={num_frames}` exceeds `total_num_frames={total_num_frames}`. \"\n+                \"Decrease `num_frames` or `fps` for sampling.\"\n+            )\n+\n+        if num_frames is not None:\n+            indices = torch.arange(0, total_num_frames, total_num_frames / num_frames).int()\n+        else:\n+            indices = torch.arange(0, total_num_frames).int()\n+\n+        return indices\n+\n+    def _preprocess(\n+        self,\n+        videos: list[\"torch.Tensor\"],\n+        do_convert_rgb: bool,\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n+        use_token_compression: Optional[bool] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        device: Optional[\"torch.Tensor\"] = None,\n+        **kwargs,\n+    ):\n+        # Group videos by size for batched resizing\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(videos)\n+        resized_videos_grouped = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            height, width = get_image_size(stacked_videos[0], channel_dim=ChannelDimension.FIRST)\n+            resized_height, resized_width = height, width\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    height,\n+                    width,\n+                    factor=patch_size * merge_size,\n+                    min_pixels=min_pixels,\n+                    max_pixels=max_pixels // shape[0],\n+                )\n+                stacked_videos = self.resize(\n+                    image=stacked_videos,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n+                    interpolation=interpolation,\n+                )\n+            resized_videos_grouped[shape] = stacked_videos\n+        resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n+\n+        # Group videos by size for further processing\n+        # Needed in case do_resize is False, or resize returns videos with different sizes\n+        grouped_videos, grouped_videos_index = group_videos_by_shape(resized_videos)\n+        processed_videos_grouped = {}\n+        processed_grids = {}\n+        for shape, stacked_videos in grouped_videos.items():\n+            resized_height, resized_width = get_image_size(stacked_videos[0], channel_dim=ChannelDimension.FIRST)\n+\n+            # Fused rescale and normalize\n+            stacked_videos = self.rescale_and_normalize(\n+                stacked_videos, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            patches = stacked_videos\n+\n+            # Check that videos have `num_frames` divisible by `temporal_patch_size`\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(1, self.temporal_patch_size - 1, 1, 1, 1)\n+                patches = torch.cat([patches, repeats], dim=1)\n+\n+            batch_size, grid_t, channel = patches.shape[:3]\n+            grid_t = grid_t // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+\n+            patches = patches.view(\n+                batch_size,\n+                grid_t,\n+                temporal_patch_size,\n+                channel,\n+                grid_h // merge_size,\n+                merge_size,\n+                patch_size,\n+                grid_w // merge_size,\n+                merge_size,\n+                patch_size,\n+            )\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+            flatten_patches = patches.reshape(\n+                batch_size,\n+                grid_t * grid_h * grid_w,\n+                channel * temporal_patch_size * patch_size * patch_size,\n+            )\n+\n+            processed_videos_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n+\n+        processed_videos = reorder_videos(processed_videos_grouped, grouped_videos_index)\n+        processed_grids = reorder_videos(processed_grids, grouped_videos_index)\n+        pixel_values_videos = torch.cat(processed_videos, dim=0)\n+        video_grid_thw = torch.tensor(processed_grids)\n+        video_merge_sizes = torch.tensor([merge_size] * video_grid_thw.size(0)).to(video_grid_thw)\n+\n+        if use_token_compression:\n+            video_compression_mask = self._get_compression_mask(\n+                pixel_values_videos=pixel_values_videos,\n+                video_grid_thw=video_grid_thw,\n+                video_merge_sizes=video_merge_sizes,\n+            )\n+        else:\n+            num_video_tokens = video_grid_thw.prod(-1).sum() // (merge_size**2)\n+            video_compression_mask = torch.ones(\n+                (num_video_tokens,), dtype=torch.bool, device=pixel_values_videos.device\n+            )\n+\n+        return BatchFeature(\n+            data={\n+                \"pixel_values_videos\": pixel_values_videos,\n+                \"video_grid_thw\": video_grid_thw,\n+                \"video_merge_sizes\": video_merge_sizes,\n+                \"video_compression_mask\": video_compression_mask,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+    def get_num_of_video_patches(self, num_frames: int, height: int, width: int, videos_kwargs=None):\n+        \"\"\"\n+        A utility that returns number of video patches a given video size.\n+\n+        Args:\n+            num_frames (`int`):\n+                Number of frames in the input video.\n+            height (`int`):\n+                Height of the input video.\n+            width (`int`):\n+                Width of the input video.\n+            videos_kwargs (`dict`, *optional*)\n+                Any kwargs to override defaults of the video processor.\n+        Returns:\n+            `Tuple(int, int)`: Number of placeholder tokens required and number of patches per image.\n+        \"\"\"\n+        min_pixels = videos_kwargs.get(\"min_pixels\", None) or self.size[\"shortest_edge\"]\n+        max_pixels = videos_kwargs.get(\"max_pixels\", None) or self.size[\"longest_edge\"]\n+        patch_size = videos_kwargs.get(\"patch_size\", None) or self.patch_size\n+        merge_size = videos_kwargs.get(\"merge_size\", None) or self.merge_size\n+        temporal_patch_size = videos_kwargs.get(\"temporal_patch_size\", None) or self.temporal_patch_size\n+\n+        factor = patch_size * merge_size\n+        resized_height, resized_width = smart_resize(\n+            height, width, factor, min_pixels=min_pixels, max_pixels=max_pixels\n+        )\n+        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        grid_t = num_frames // temporal_patch_size\n+        return grid_t * grid_h * grid_w\n+\n+    def _get_compression_mask(\n+        self,\n+        pixel_values_videos: torch.FloatTensor,\n+        video_grid_thw: torch.LongTensor,\n+        video_merge_sizes: torch.LongTensor,\n+        threshold: Optional[float] = 0.1,\n+        min_tokens: Optional[int] = 1,\n+    ) -> torch.BoolTensor:\n+        \"\"\"\n+        Get the compression mask for video tokens based on pixel differences.\n+        Args:\n+            pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+                The tensors corresponding to the input videos.\n+            video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+                The temporal, height and width of feature shape of each video in LLM.\n+            video_merge_sizes (`torch.Tensor` of shape `(num_videos,)`):\n+                The spatial downsampling ratio of each video feature.\n+            threshold (`float`, *optional*, defaults to 0.1):\n+                The threshold to determine whether a token should be kept based on pixel differences.\n+            min_tokens (`int`, *optional*, defaults to 1):\n+                The minimum number of tokens to keep for each frame.\n+        \"\"\"\n+        videos = pixel_values_videos.split(video_grid_thw.prod(dim=1).tolist(), dim=0)\n+        compression_masks = []\n+\n+        for images, grid_size, merge_size in zip(videos, video_grid_thw, video_merge_sizes):\n+            t, h, w = grid_size\n+            if t == 1:\n+                num_tokens = images.size(0) // (merge_size**2)\n+                compression_masks.append(torch.ones((num_tokens,), dtype=torch.bool, device=images.device))\n+            else:\n+                # NOTE: video token compressor\n+                images = images.view(t, (h // merge_size) * (w // merge_size), -1)\n+\n+                pixel_diff = images[1:] - images[:-1]\n+                pixel_diff = torch.abs(pixel_diff).mean(dim=-1) * 255\n+                pixel_diff = torch.cat([torch.full_like(pixel_diff[0:1], threshold + 1), pixel_diff], dim=0)\n+                mask = pixel_diff > threshold\n+                padding_ids = torch.nonzero(mask.sum(dim=1) < min_tokens)[:, 0]\n+                mask[padding_ids, :min_tokens] = 1\n+                compression_masks.append(mask.flatten())\n+\n+        return torch.cat(compression_masks)\n+\n+\n+__all__ = [\"VideoLlama3VideoProcessor\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/video_llama_3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2F__init__.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e"
        },
        {
            "sha": "61fd0c26d224d4ab7c9855d7deb3aaf79bbd6508",
            "filename": "tests/models/video_llama_3/test_image_processing_video_llama_3.py",
            "status": "added",
            "additions": 369,
            "deletions": 0,
            "changes": 369,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_image_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_image_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_image_processing_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,369 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import itertools\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import requests\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.models.video_llama_3.image_processing_video_llama_3 import smart_resize\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs, prepare_video_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import VideoLlama3ImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import VideoLlama3ImageProcessorFast\n+\n+\n+class VideoLlama3ImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        num_frames=10,\n+        min_resolution=56,\n+        max_resolution=1024,\n+        min_pixels=14 * 14 * 16,\n+        max_pixels=14 * 14 * 16384,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_resize=True,\n+        patch_size=14,\n+        merge_size=1,\n+        do_convert_rgb=True,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.num_channels = num_channels\n+        self.num_frames = num_frames\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.min_pixels = min_pixels\n+        self.max_pixels = max_pixels\n+        self.patch_size = patch_size\n+        self.merge_size = merge_size\n+        self.do_resize = do_resize\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"min_pixels\": self.min_pixels,\n+            \"max_pixels\": self.max_pixels,\n+            \"patch_size\": self.patch_size,\n+            \"merge_size\": self.merge_size,\n+        }\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        images = prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+        return [[image] for image in images]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            num_frames=self.num_frames,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class VideoLlama3ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = VideoLlama3ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = VideoLlama3ImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = VideoLlama3ImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"min_pixels\"))\n+            self.assertTrue(hasattr(image_processing, \"max_pixels\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"patch_size\"))\n+            self.assertTrue(hasattr(image_processing, \"merge_size\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.min_pixels, 14 * 14 * 16)\n+            self.assertEqual(image_processor.max_pixels, 14 * 14 * 16384)\n+\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, min_pixels=256 * 256, max_pixels=640 * 640\n+            )\n+            self.assertEqual(image_processor.min_pixels, 256 * 256)\n+            self.assertEqual(image_processor.max_pixels, 640 * 640)\n+\n+    def test_select_best_resolution(self):\n+        # Test with a final resize resolution\n+        best_resolution = smart_resize(561, 278, factor=28)\n+        self.assertEqual(best_resolution, (560, 280))\n+\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image[0], Image.Image)\n+\n+            # Test not batched input\n+            process_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (5329, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (37303, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image[0], np.ndarray)\n+\n+            # Test not batched input\n+            process_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (5329, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (37303, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image[0], torch.Tensor)\n+\n+            # Test not batched input\n+            process_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (5329, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (37303, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+    @unittest.skip(reason=\"VideoLlama3ImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_nested_input(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test batched as a list of images\n+            process_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = process_out.pixel_values\n+            image_grid_thws = process_out.image_grid_thw\n+            expected_output_image_shape = (37303, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched as a nested list of images, where each sublist is one batch\n+            image_inputs_nested = image_inputs[:3] + image_inputs[3:]\n+            process_out = image_processing(image_inputs_nested, return_tensors=\"pt\")\n+            encoded_images_nested = process_out.pixel_values\n+            image_grid_thws_nested = process_out.image_grid_thw\n+            expected_output_image_shape = (37303, 588)\n+            expected_image_grid_thws = torch.Tensor([[1, 73, 73]] * 7)\n+            self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Image processor should return same pixel values, independently of ipnut format\n+            self.assertTrue((encoded_images_nested == encoded_images).all())\n+            self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n+\n+    @unittest.skip(\n+        reason=\"`VideoLlama3ImageProcessor` works only with image inputs and doesn't process videos anymore.\"\n+    )\n+    def test_video_inputs(self):\n+        pass\n+\n+    def test_custom_image_size(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                image_processing.save_pretrained(tmpdirname)\n+                image_processor_loaded = image_processing_class.from_pretrained(\n+                    tmpdirname, max_pixels=56 * 56, min_pixels=28 * 28\n+                )\n+\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            process_out = image_processor_loaded(image_inputs, return_tensors=\"pt\")\n+            expected_output_video_shape = [112, 588]\n+            self.assertListEqual(list(process_out.pixel_values.shape), expected_output_video_shape)\n+\n+    def test_custom_pixels(self):\n+        pixel_choices = frozenset(itertools.product((100, 150, 200, 20000), (100, 150, 200, 20000)))\n+        for image_processing_class in self.image_processor_list:\n+            image_processor_dict = self.image_processor_dict.copy()\n+            for a_pixels, b_pixels in pixel_choices:\n+                image_processor_dict[\"min_pixels\"] = min(a_pixels, b_pixels)\n+                image_processor_dict[\"max_pixels\"] = max(a_pixels, b_pixels)\n+                image_processor = image_processing_class(**image_processor_dict)\n+                image_inputs = self.image_processor_tester.prepare_image_inputs()\n+                # Just checking that it doesn't raise an error\n+                image_processor(image_inputs, return_tensors=\"pt\")\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self.assertEqual(encoding_slow.image_grid_thw.dtype, encoding_fast.image_grid_thw.dtype)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.image_grid_thw.float(), encoding_fast.image_grid_thw.float()\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self.assertEqual(encoding_slow.image_grid_thw.dtype, encoding_fast.image_grid_thw.dtype)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.image_grid_thw.float(), encoding_fast.image_grid_thw.float()\n+        )\n+\n+    def test_get_num_patches_without_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            num_patches = image_processing.get_number_of_image_patches(height=100, width=100, images_kwargs={})\n+            self.assertEqual(num_patches, 49)\n+\n+            num_patches = image_processing.get_number_of_image_patches(height=200, width=50, images_kwargs={})\n+            self.assertEqual(num_patches, 56)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={\"patch_size\": 28}\n+            )\n+            self.assertEqual(num_patches, 16)"
        },
        {
            "sha": "eadd50cedb025527d009dee48bcb8cb602552591",
            "filename": "tests/models/video_llama_3/test_modeling_video_llama_3.py",
            "status": "added",
            "additions": 964,
            "deletions": 0,
            "changes": 964,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,964 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch VideoLLaMA3 model.\"\"\"\n+\n+import copy\n+import gc\n+import inspect\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import requests\n+import torch.nn as nn\n+from parameterized import parameterized\n+from PIL import Image\n+\n+from transformers import (\n+    AutoProcessor,\n+    VideoLlama3Config,\n+    VideoLlama3ForConditionalGeneration,\n+    VideoLlama3Model,\n+    VideoLlama3VisionConfig,\n+    VideoLlama3VisionModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    set_config_for_less_flaky_test,\n+    set_model_for_less_flaky_test,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_torch_bf16_available_on_device,\n+    is_torch_fp16_available_on_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+    sdpa_kernel,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+def _test_encoder_eager_matches_sdpa_inference(\n+    self,\n+    dtype,\n+    output_attentions,\n+    enable_kernels,\n+    atols=None,\n+    rtols=None,\n+):\n+    \"\"\"\n+    This test is written as a regular function to be able to overload it easily with different tolerances.\n+    Otherwise, `paramterezie.expand` prevents it as it removes the original function from the namespace.\n+    \"\"\"\n+    if not self.has_attentions:\n+        self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+    if not self.all_model_classes[0]._supports_sdpa:\n+        self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+    # convert shorthand name to torch.dtype\n+    if dtype == \"fp16\":\n+        dtype = torch.float16\n+    elif dtype == \"bf16\":\n+        dtype = torch.bfloat16\n+    elif dtype == \"fp32\":\n+        dtype = torch.float32\n+\n+    if not is_torch_fp16_available_on_device(torch_device) and dtype == torch.float16:\n+        self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+    if not is_torch_bf16_available_on_device(torch_device) and dtype == torch.bfloat16:\n+        self.skipTest(\n+            f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n+        )\n+\n+    # Dictionary of tolerances for eager <> sdpa tests. Key = (device, sdpa_kernels_enabled, dtype)\n+    if atols is None:\n+        atols = {\n+            (\"cpu\", False, torch.float32): 1e-6,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-6,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-6,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-6,\n+            (\"cuda\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+    if rtols is None:\n+        rtols = {\n+            (\"cpu\", False, torch.float32): 1e-4,\n+            (\"cpu\", False, torch.float16): 5e-3,\n+            (\"cpu\", False, torch.bfloat16): 1e-2,\n+            (\"cpu\", True, torch.float32): 1e-4,\n+            (\"cpu\", True, torch.float16): 5e-3,\n+            (\"cpu\", True, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float32): 1e-4,\n+            (\"cuda\", False, torch.bfloat16): 1e-2,\n+            (\"cuda\", False, torch.float16): 5e-3,\n+            (\"cuda\", True, torch.float32): 1e-4,\n+            (\"cuda\", True, torch.bfloat16): 3e-2,  # (different from others)\n+            (\"cuda\", True, torch.float16): 5e-3,\n+        }\n+\n+    for model_class in self.all_model_classes:\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        set_config_for_less_flaky_test(config)\n+\n+        model = model_class(config)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+            model_from_pretrained_kwargs = {\n+                \"pretrained_model_name_or_path\": tmpdirname,\n+                \"dtype\": dtype,\n+            }\n+\n+            if hasattr(config, \"use_mask_token\") or \"use_mask_token\" in inspect.signature(model.__init__).parameters:\n+                model_from_pretrained_kwargs[\"use_mask_token\"] = True\n+\n+            # TODO: remove this try/except, models should have a shared API\n+            try:\n+                model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"sdpa\")\n+            except ValueError:\n+                model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs)\n+            model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+            model_eager = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"eager\")\n+            model_eager = model_eager.eval().to(torch_device)\n+\n+        set_model_for_less_flaky_test(model_eager)\n+        set_model_for_less_flaky_test(model_sdpa)\n+\n+        # TODO: if we can also check with `batch_size=1` without being flaky?\n+        for batch_size in [7]:\n+            input_data_batch_size = batch_size\n+\n+            processed_inputs = {}\n+            processed_inputs[model.main_input_name] = inputs_dict[model.main_input_name]\n+\n+            for key in getattr(self, \"additional_model_inputs\", []):\n+                # Some models don't have all `additional_model_inputs`, especially when we\n+                # craft cases to test model in different settings\n+                if key in inputs_dict:\n+                    processed_inputs[key] = inputs_dict[key]\n+\n+            for key, value in processed_inputs.items():\n+                if torch.is_floating_point(value):\n+                    value = value.to(dtype)\n+\n+                if key == \"pixel_values\":\n+                    continue\n+\n+                # extend value to have at least `input_data_batch_size` elements\n+                if value.shape[0] < input_data_batch_size:\n+                    size = (input_data_batch_size - value.shape[0], *value.shape[1:])\n+                    if key == \"grid_thw\":\n+                        extension = torch.randint(high=5, size=size, dtype=value.dtype, device=torch_device)\n+                    elif key == \"merge_sizes\":\n+                        extension = torch.ones(size=size, dtype=value.dtype, device=torch_device)\n+                    value = torch.cat((value, extension), dim=0).to(torch_device)\n+\n+                processed_inputs[key] = value[:input_data_batch_size]\n+\n+            pixel_values = processed_inputs[\"pixel_values\"]\n+            target_len = torch.sum(processed_inputs[\"grid_thw\"].prod(dim=1) // (processed_inputs[\"merge_sizes\"] ** 2))\n+            if pixel_values.size(0) < target_len:\n+                size = (input_data_batch_size - value.shape[0], *value.shape[1:])\n+                extension = torch.randn(\n+                    size=(target_len - pixel_values.size(0)), dtype=pixel_values.dtype, device=torch_device\n+                )\n+            elif pixel_values.size(0) > target_len:\n+                pixel_values = pixel_values[:target_len]\n+            processed_inputs[\"pixel_values\"] = pixel_values\n+\n+            processed_inputs.update(\n+                {\n+                    \"output_hidden_states\": True,\n+                    \"output_attentions\": output_attentions,\n+                }\n+            )\n+\n+            # TODO: test gradients as well (& for FA2 as well!)\n+            with torch.no_grad():\n+                with sdpa_kernel(\n+                    enable_flash=enable_kernels,\n+                    enable_math=True,\n+                    enable_mem_efficient=enable_kernels,\n+                ):\n+                    prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+                    prepared_inputs = {\n+                        k: v.to(torch_device) if isinstance(v, torch.Tensor) else v for k, v in prepared_inputs.items()\n+                    }\n+                    outputs_eager = model_eager(**prepared_inputs)\n+                    outputs_sdpa = model_sdpa(**prepared_inputs)\n+\n+            key = \"hidden_states\"\n+\n+            # TODO: rename logits -> hidden_states\n+            logits_eager = outputs_eager[key][-1]\n+            logits_sdpa = outputs_sdpa[key][-1]\n+\n+            if torch_device in [\"cpu\", \"cuda\"]:\n+                atol = atols[torch_device, enable_kernels, dtype]\n+                rtol = rtols[torch_device, enable_kernels, dtype]\n+            elif torch_device == \"hpu\":\n+                atol = atols[\"cuda\", enable_kernels, dtype]\n+                rtol = rtols[\"cuda\", enable_kernels, dtype]\n+            elif torch_device == \"xpu\":\n+                # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                # which is implemented on PyTorch level using aten operators and is\n+                # device agnostic with respect to implementation of each aten operator.\n+                atol = atols[\"cuda\", False, dtype]\n+                rtol = rtols[\"cuda\", False, dtype]\n+            else:\n+                atol = 1e-7\n+                rtol = 1e-4\n+\n+            # Avoid test flakiness with bf16!\n+            # bf16 is not good at precision when the magnitude is larger. We have some models like `SiglipVision` with\n+            # this test passing all the time for fp32/fp16 but flaky with bf16. Furthermore, `llama` and `clip` have\n+            # this test passing all the time for bf16: it turns out their outputs are of smaller size (0.1 and 1.0)\n+            # while `siglip` has outputs with maximal values around 3.0/4.0.\n+            outputs_magnitude = float(\n+                (torch.max(logits_sdpa.abs().amax(), logits_eager.abs().amax())).detach().to(\"cpu\")\n+            )\n+            # The choice of `3e-2` in `outputs_magnitude * 1e-2` might not work if a model has even more larger outputs.\n+            # (we can try to analyze the `rtol` more closely element-wise in the future and adjust the `rtol` instead of `atol`).\n+            computed_atol = outputs_magnitude * 3e-2\n+            if dtype == torch.bfloat16:\n+                atol = max(atol, computed_atol)\n+\n+            results = [\n+                torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+            ]\n+\n+            # If 80% batch elements have matched results, it's fine\n+            if np.mean(results) < 0.8:\n+                mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()\n+                raise ValueError(\n+                    f\"mean relative difference for {key}: {mean_relative_diff:.3e}, torch atol = {atol}, torch rtol = \"\n+                    f\"{rtol}\"\n+                )\n+\n+\n+class VideoLlama3VisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        patch_size=2,\n+        num_channels=3,\n+        image_size=14,\n+        is_training=True,\n+        hidden_size=64,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        attention_dropout=0.1,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+        self.seq_length = (self.image_size // self.patch_size) ** 2\n+\n+    def get_config(self):\n+        return VideoLlama3VisionConfig(\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        patch_size = config.patch_size\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size * (self.image_size**2) // (patch_size**2),\n+                self.num_channels * (patch_size**2),\n+            ]\n+        )\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        num_patches = self.image_size // config.patch_size\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"grid_thw\": torch.tensor([[1, num_patches, num_patches]] * self.batch_size, device=torch_device),\n+            \"merge_sizes\": torch.tensor([1] * self.batch_size, device=torch_device),\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class VideoLlama3VisionModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SIGLIP does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (VideoLlama3VisionModel,) if is_torch_available() else ()\n+    additional_model_inputs = [\"grid_thw\", \"merge_sizes\"]\n+    # fx_compatible = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_cpu_offload = False\n+    test_disk_offload_safetensors = False\n+    test_disk_offload_bin = False\n+\n+    def setUp(self):\n+        self.model_tester = VideoLlama3VisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=VideoLlama3VisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    def test_eager_matches_sdpa_inference(\n+        self, name, dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        if use_attention_mask:\n+            self.skipTest(reason=\"VideoLlama3VisionModel does not use attention mask\")\n+        _test_encoder_eager_matches_sdpa_inference(self, dtype, output_attentions, enable_kernels)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n+        seq_len = getattr(self.model_tester, \"seq_length\", None)\n+        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_len)\n+        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            for k in config.sub_configs:\n+                getattr(config, k).output_attentions = True\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0][0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+            )\n+            out_len = len(outputs)\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            self.assertEqual(out_len + 1, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n+            self.assertListEqual(\n+                list(self_attentions[0][0].shape[-3:]),\n+                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n+            )\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(copy.deepcopy(config))\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            seq_length = torch.sum(inputs_dict[\"grid_thw\"].prod(dim=1) // (inputs_dict[\"merge_sizes\"] ** 2))\n+            self.assertListEqual(\n+                list(hidden_states[0].shape),\n+                [seq_length, self.model_tester.hidden_size],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+            for k in config.sub_configs:\n+                getattr(config, k).output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_retain_grad_hidden_states_attentions(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for k in config.sub_configs:\n+            getattr(config, k).output_hidden_states = True\n+\n+        config.output_hidden_states = True\n+        config.output_attentions = self.has_attentions\n+\n+        for k in config.sub_configs:\n+            getattr(config, k).output_attentions = self.has_attentions\n+\n+        # force eager attention to support output attentions\n+        config._attn_implementation = \"eager\"\n+\n+        # no need to test all models as different heads yield the same functionality\n+        model_class = self.all_model_classes[0]\n+        model = model_class._from_config(config, attn_implementation=\"eager\")\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+        outputs = model(**inputs)\n+\n+        output = outputs[0]\n+\n+        # Encoder-/Decoder-only models\n+        hidden_states = outputs.hidden_states[0]\n+        hidden_states.retain_grad()\n+\n+        if self.has_attentions:\n+            attentions = outputs.attentions[0][0]\n+            attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        self.assertIsNotNone(hidden_states.grad)\n+\n+        if self.has_attentions:\n+            self.assertIsNotNone(attentions.grad)\n+\n+    @unittest.skip(\"Vision model requires additional positional inputs (grid_thw and merge_sizes)\")\n+    def test_flash_attn_2_inference_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\"Vision model requires additional positional inputs (grid_thw and merge_sizes)\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(\"Vision model requires additional positional inputs (grid_thw and merge_sizes)\")\n+    def test_flash_attn_kernels_inference_equivalence(self):\n+        pass\n+\n+\n+class VideoLlama3VisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        seq_length=7,\n+        num_channels=3,\n+        image_size=14,\n+        is_training=True,\n+        text_config={\n+            \"attention_dropout\": 0.0,\n+            \"bos_token_id\": 0,\n+            \"eos_token_id\": 1,\n+            \"pad_token_id\": 2,\n+            \"hidden_act\": \"silu\",\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 37,\n+            \"max_position_embeddings\": 512,\n+            \"max_window_layers\": 3,\n+            \"model_type\": \"qwen2\",\n+            \"num_attention_heads\": 4,\n+            \"num_hidden_layers\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"rms_norm_eps\": 1e-06,\n+            \"rope_scaling\": None,\n+            \"rope_theta\": 1000000.0,\n+            \"sliding_window\": None,\n+            \"tie_word_embeddings\": True,\n+            \"vocab_size\": 99,\n+        },\n+        vision_config={\n+            \"attention_dropout\": 0.0,\n+            \"hidden_act\": \"gelu_pytorch_tanh\",\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 64,\n+            \"layer_norm_eps\": 1e-06,\n+            \"model_type\": \"video_llama_3_vision\",\n+            \"num_attention_heads\": 4,\n+            \"num_channels\": 3,\n+            \"num_hidden_layers\": 2,\n+            \"patch_size\": 14,\n+        },\n+        use_token_compression=True,\n+        image_token_id=3,\n+        video_token_id=4,\n+    ):\n+        self.parent = parent\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.patch_size = vision_config[\"patch_size\"]\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.is_training = is_training\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.use_token_compression = use_token_compression\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.num_image_tokens = 32\n+        self.seq_length = seq_length + self.num_image_tokens\n+\n+    def get_config(self):\n+        return VideoLlama3Config(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            use_token_compression=self.use_token_compression,\n+            image_token_id=self.image_token_id,\n+            video_token_id=self.video_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+        patch_size = config.vision_config.patch_size\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size * (self.image_size**2) // (patch_size**2),\n+                self.num_channels * (patch_size**2),\n+            ]\n+        )\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size)\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n+\n+        input_ids[:, -1] = config.text_config.pad_token_id\n+        attention_mask[:, -1] = 0\n+        input_ids[input_ids == self.video_token_id] = config.text_config.pad_token_id\n+        input_ids[input_ids == self.image_token_id] = config.text_config.pad_token_id\n+        input_ids[:, self.num_image_tokens] = self.image_token_id\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size, device=torch_device),\n+            \"image_merge_sizes\": torch.tensor([1] * self.batch_size, device=torch_device),\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class VideoLlama3ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `VideoLlama3ForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            VideoLlama3Model,\n+            VideoLlama3ForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = {\"image-text-to-text\": VideoLlama3ForConditionalGeneration}\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = VideoLlama3VisionText2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=VideoLlama3Config, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images don't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompt has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            curr_input_dict = copy.deepcopy(input_dict)\n+            _ = model(**curr_input_dict)  # successfull forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            patch_size = config.vision_config.patch_size\n+            one_img_length = (self.model_tester.image_size**2) // (patch_size**2)\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-one_img_length:, ...]\n+            curr_input_dict[\"image_grid_thw\"] = curr_input_dict[\"image_grid_thw\"][-1:, ...]\n+            curr_input_dict[\"image_merge_sizes\"] = curr_input_dict[\"image_merge_sizes\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**curr_input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:one_img_length]\n+            image_grid_thw = curr_input_dict[\"image_grid_thw\"][:1]\n+            image_merge_sizes = curr_input_dict[\"image_merge_sizes\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(\n+                    input_ids=input_ids,\n+                    pixel_values=pixel_values,\n+                    image_grid_thw=image_grid_thw,\n+                    image_merge_sizes=image_merge_sizes,\n+                )\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            image_grid_thw = torch.cat([image_grid_thw, image_grid_thw], dim=0)\n+            image_merge_sizes = torch.cat([image_merge_sizes, image_merge_sizes], dim=0)\n+            _ = model(\n+                input_ids=input_ids,\n+                pixel_values=pixel_values,\n+                image_grid_thw=image_grid_thw,\n+                image_merge_sizes=image_merge_sizes,\n+            )\n+\n+    def attention_mask_padding_matches_padding_free_with_position_ids(\n+        self, attn_implementation: str, fa_kwargs: bool = False\n+    ):\n+        max_new_tokens = 30\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.float16]:\n+                dummy_input = dummy_input.to(torch.bfloat16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        dtype=torch.bfloat16,\n+                        attn_implementation=attn_implementation,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                padfree_positions = torch.cat(\n+                    [torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()]\n+                )\n+                padfree_positions = padfree_positions.long().unsqueeze(0).to(torch_device)\n+                padfree_inputs_dict = {\n+                    \"pixel_values\": inputs_dict[\"pixel_values\"],\n+                    \"image_grid_thw\": inputs_dict[\"image_grid_thw\"],\n+                    \"image_merge_sizes\": inputs_dict[\"image_merge_sizes\"],\n+                    \"input_ids\": inputs_dict[\"input_ids\"][dummy_attention_mask.bool()].unsqueeze(0),\n+                    \"position_ids\": padfree_positions,\n+                }\n+\n+                if fa_kwargs:\n+                    cu_seq_lens = [0] + dummy_attention_mask.sum(1).tolist()\n+                    cu_seq_lens = torch.tensor(cu_seq_lens, device=torch_device)\n+                    max_length = cu_seq_lens.diff().max().item()\n+                    padfree_inputs_dict.update(\n+                        {\n+                            \"cu_seq_lens_q\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"cu_seq_lens_k\": cu_seq_lens.cumsum(-1).to(dtype=torch.int32),\n+                            \"max_length_q\": max_length,\n+                            \"max_length_k\": max_length,\n+                        }\n+                    )\n+\n+                # We need to do simple forward without cache in roder to trigger packed SDPA/FLEX/EAGER path\n+                res_padded = model(**inputs_dict, use_cache=False)\n+                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n+\n+@require_torch\n+@slow\n+class VideoLlama3IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"lkhl/VideoLLaMA3-2B-Image-HF\")\n+        self.messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"Describe the image.\"},\n+                ],\n+            }\n+        ]\n+        url = \"https://github.com/DAMO-NLP-SG/VideoLLaMA3/raw/refs/heads/main/assets/sora.png\"\n+        self.image = Image.open(requests.get(url, stream=True).raw)\n+\n+    def tearDown(self):\n+        gc.collect()\n+        backend_empty_cache(torch_device)\n+\n+    def test_small_model_integration_test(self):\n+        model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\", dtype=torch.bfloat16, device_map=torch_device\n+        )\n+\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text], images=[self.image], return_tensors=\"pt\").to(torch_device)\n+\n+        expected_input_ids = [151644, 872, 198] + [151655] * 10549 + [198, 74785, 279, 2168, 13, 151645, 198, 151644, 77091, 198]  # fmt: skip\n+        self.assertEqual(expected_input_ids, inputs.input_ids[0].tolist())\n+\n+        expected_pixel_slice = torch.tensor(\n+            [\n+                [-0.8588, -0.9216, -0.9608],\n+                [-0.9922, -0.9922, -0.9922],\n+                [-0.9686, -0.9686, -0.9294],\n+                [-0.9294, -0.9765, -0.9765],\n+                [-0.9922, -0.9922, -0.9843],\n+                [-0.6000, -0.4118, -0.3647],\n+            ],\n+            dtype=torch.float32,\n+            device=torch_device,\n+        )\n+        torch.testing.assert_close(expected_pixel_slice, inputs.pixel_values[:6, :3], atol=1e-4, rtol=1e-4)\n+\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n+        EXPECTED_DECODED_TEXT = \"user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress\"\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batch(self):\n+        model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\", dtype=torch.bfloat16, device_map=torch_device\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text, text], images=[self.image, self.image], return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress\",\n+            \"user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress\",\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batch_wo_image(self):\n+        model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\", dtype=torch.bfloat16, device_map=torch_device\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        messages2 = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is relativity?\"}]},\n+        ]\n+        text2 = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(\n+            text=[text, text2], images=[self.image], padding=True, padding_side=\"left\", return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress\",\n+            \"user\\nWhat is relativity?\\nassistant\\nRelativity is a scientific theory that describes the relationship between space and time. It was first proposed by\",\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batch_different_resolutions(self):\n+        model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\", dtype=torch.bfloat16, device_map=torch_device\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        text2 = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        image2 = self.image.resize((224, 224))\n+        inputs = self.processor(\n+            text=[text, text2], images=[self.image, image2], padding=True, padding_side=\"left\", return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n+        DECODED_TEXT = self.processor.batch_decode(output, skip_special_tokens=True)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress\",\n+            \"user\\n\\nDescribe the image.\\nassistant\\nThe image depicts a striking urban scene at night. A person is standing in the center of a wet\",\n+        ]  # fmt: skip\n+\n+        self.assertEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    def test_small_model_integration_test_batch_flashatt2(self):\n+        model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\",\n+            dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+            device_map=torch_device,\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(text=[text, text], images=[self.image, self.image], return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+            'user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n+        model = VideoLlama3ForConditionalGeneration.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\",\n+            dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+            device_map=torch_device,\n+        )\n+        text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n+        messages2 = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is relativity?\"}]},\n+        ]\n+        text2 = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n+        inputs = self.processor(\n+            text=[text, text2], images=[self.image], padding=True, padding_side=\"left\", return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=20, do_sample=False, repetition_penalty=None)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'user\\n\\nDescribe the image.\\nassistant\\nThe image captures a vibrant nighttime scene on a bustling city street. A woman in a striking red dress',\n+            'user\\nWhat is relativity?\\nassistant\\nRelativity is a scientific theory that describes the relationship between space and time. It was first proposed by'\n+        ]  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        },
        {
            "sha": "18d53ac8b4b14d2f8c5162c43cd64ed67cfce688",
            "filename": "tests/models/video_llama_3/test_processing_video_llama_3.py",
            "status": "added",
            "additions": 410,
            "deletions": 0,
            "changes": 410,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_processing_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,410 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import inspect\n+import shutil\n+import tempfile\n+import unittest\n+from typing import Optional\n+\n+import numpy as np\n+import pytest\n+from PIL import Image\n+\n+from transformers import AutoProcessor, Qwen2Tokenizer\n+from transformers.testing_utils import require_av, require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import VideoLlama3Processor\n+\n+    if is_torchvision_available():\n+        from transformers import VideoLlama3ImageProcessor, VideoLlama3VideoProcessor\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+def prepare_image_inputs():\n+    \"\"\"This function prepares a list of PIL images\"\"\"\n+    image_inputs = [np.random.randint(255, size=(3, 15, 50), dtype=np.uint8)]\n+    image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+    return image_inputs\n+\n+\n+@require_vision\n+@require_torch\n+@require_torchvision\n+class VideoLlama3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = VideoLlama3Processor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        processor = VideoLlama3Processor.from_pretrained(\n+            \"lkhl/VideoLLaMA3-2B-Image-HF\", patch_size=4, max_pixels=56 * 56, min_pixels=28 * 28\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token = processor.image_token\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def get_video_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).video_processor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    @require_vision\n+    def prepare_image_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of PIL images for testing\"\"\"\n+        if batch_size is None:\n+            return prepare_image_inputs()[0]\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+        return prepare_image_inputs() * batch_size\n+\n+    # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n+    def test_save_load_pretrained_default(self):\n+        tokenizer = self.get_tokenizer()\n+        image_processor = self.get_image_processor()\n+        video_processor = self.get_video_processor()\n+\n+        processor = VideoLlama3Processor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+        processor.save_pretrained(self.tmpdirname)\n+        processor = VideoLlama3Processor.from_pretrained(self.tmpdirname, use_fast=False)\n+\n+        self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertEqual(processor.image_processor.to_json_string(), image_processor.to_json_string())\n+        self.assertIsInstance(processor.tokenizer, Qwen2Tokenizer)\n+        self.assertIsInstance(processor.image_processor, VideoLlama3ImageProcessor)\n+        self.assertIsInstance(processor.video_processor, VideoLlama3VideoProcessor)\n+\n+    def test_image_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        video_processor = self.get_video_processor()\n+\n+        processor = VideoLlama3Processor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_image_proc = image_processor(image_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, text=\"dummy\", return_tensors=\"pt\")\n+\n+        for key in input_image_proc:\n+            self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n+\n+    def test_processor(self):\n+        image_processor = self.get_image_processor()\n+        tokenizer = self.get_tokenizer()\n+        video_processor = self.get_video_processor()\n+\n+        processor = VideoLlama3Processor(\n+            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n+        )\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input)\n+\n+        self.assertListEqual(\n+            list(inputs.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\", \"image_grid_thw\", \"image_merge_sizes\"]\n+        )\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n+        # test if it raises when no text is passed\n+        with pytest.raises(TypeError):\n+            processor(images=image_input)\n+\n+    @require_torch\n+    @require_av\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n+\n+        batch_messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n+                },\n+            ]\n+        ] * batch_size\n+\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n+\n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n+        )\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n+        )\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n+\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=return_tensors,\n+            max_length=100,\n+        )\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n+\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+        )\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n+\n+        out_dict = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n+        )\n+        input_name = getattr(self, input_name)\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+        if modality == \"video\":\n+            # qwen pixels don't scale with bs same way as other models, calculate expected video token count based on video_grid_thw\n+            expected_video_token_count = 0\n+            for thw in out_dict[\"video_grid_thw\"]:\n+                expected_video_token_count += thw[0] * thw[1] * thw[2]\n+            mm_len = expected_video_token_count\n+        else:\n+            mm_len = batch_size * 192\n+        self.assertEqual(len(out_dict[input_name]), mm_len)\n+\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n+\n+    @require_av\n+    def test_apply_chat_template_video_frame_sampling(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\"},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # Add video URL for return dict and load with `num_frames` arg\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        num_frames = 3\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            num_frames=num_frames,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 180)\n+\n+        # Load with `fps` arg\n+        fps = 1\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            fps=fps,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 80)\n+\n+        # Load with `fps` and `num_frames` args, should raise an error\n+        with self.assertRaises(ValueError):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                fps=fps,\n+                num_frames=num_frames,\n+            )\n+\n+        # Load without any arg should load the whole video\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1200)\n+\n+        # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n+        # because we assume they come from one video\n+        messages[0][0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": [\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+            ],\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 192)\n+\n+        # When the inputs are frame URLs/paths we expect that those are already\n+        # sampled and will raise an error is asked to sample again.\n+        with self.assertRaisesRegex(\n+            ValueError, \"Sampling frames from a list of images is not supported! Set `do_sample_frames=False`\"\n+        ):\n+            out_dict_with_video = processor.apply_chat_template(\n+                messages,\n+                add_generation_prompt=True,\n+                tokenize=True,\n+                return_dict=True,\n+                do_sample_frames=True,\n+            )\n+\n+    def test_kwargs_overrides_custom_image_processor_kwargs(self):\n+        processor = self.get_processor()\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 52)\n+        inputs = processor(text=input_str, images=image_input, max_pixels=56 * 56 * 4, return_tensors=\"pt\")\n+        self.assertEqual(inputs[self.images_input_name].shape[0], 52)\n+\n+    def test_special_mm_token_truncation(self):\n+        \"\"\"Tests that special vision tokens do not get truncated when `truncation=True` is set.\"\"\"\n+\n+        processor = self.get_processor()\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modalities=\"image\")\n+        image_input = self.prepare_image_inputs(batch_size=2)\n+\n+        _ = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            truncation=None,\n+            padding=True,\n+        )\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                truncation=True,\n+                padding=True,\n+                max_length=20,\n+            )"
        },
        {
            "sha": "267d8cdb94dbc544002256f1e46a8a0e9e983142",
            "filename": "tests/models/video_llama_3/test_video_processing_video_llama_3.py",
            "status": "added",
            "additions": 353,
            "deletions": 0,
            "changes": 353,
            "blob_url": "https://github.com/huggingface/transformers/blob/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_video_processing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e/tests%2Fmodels%2Fvideo_llama_3%2Ftest_video_processing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_video_processing_video_llama_3.py?ref=cad74496ca19c463a5fcc0b35ef4a1c9da2b8c4e",
            "patch": "@@ -0,0 +1,353 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers.image_utils import get_image_size\n+    from transformers.models.video_llama_3.video_processing_video_llama_3 import smart_resize\n+\n+    if is_torchvision_available():\n+        from transformers import VideoLlama3VideoProcessor\n+\n+\n+class VideoLlama3VideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+        temporal_patch_size=2,\n+        patch_size=14,\n+        min_pixels=20 * 20,\n+        max_pixels=100 * 100 * 8,\n+        merge_size=2,\n+    ):\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+        self.temporal_patch_size = temporal_patch_size\n+        self.patch_size = patch_size\n+        self.min_pixels = min_pixels\n+        self.max_pixels = max_pixels\n+        self.merge_size = merge_size\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"temporal_patch_size\": self.temporal_patch_size,\n+            \"patch_size\": self.patch_size,\n+            \"min_pixels\": self.min_pixels,\n+            \"max_pixels\": self.max_pixels,\n+            \"merge_size\": self.merge_size,\n+        }\n+\n+    @require_vision\n+    def expected_output_video_shape(self, videos, num_frames=None):\n+        num_frames = num_frames if num_frames is not None else self.num_frames\n+        grid_t = num_frames // self.temporal_patch_size\n+        hidden_dim = self.num_channels * self.temporal_patch_size * self.patch_size * self.patch_size\n+        seq_len = 0\n+        for video in videos:\n+            if isinstance(video[0], Image.Image):\n+                video = np.stack([np.array(frame) for frame in video])\n+            height, width = get_image_size(video)\n+            resized_height, resized_width = smart_resize(\n+                height,\n+                width,\n+                factor=self.patch_size * self.merge_size,\n+                min_pixels=self.min_pixels,\n+                max_pixels=self.max_pixels,\n+            )\n+            grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n+            seq_len += grid_t * grid_h * grid_w\n+        return [seq_len, hidden_dim]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+class VideoLlama3VideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = VideoLlama3VideoProcessor if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = VideoLlama3VideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_properties(self):\n+        video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n+        self.assertTrue(hasattr(video_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(video_processing, \"size\"))\n+        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(video_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(video_processing, \"image_std\"))\n+        self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n+\n+    def test_video_processor_from_dict_with_kwargs(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processor = video_processing_class(**self.video_processor_dict)\n+            self.assertEqual(video_processor.min_pixels, self.video_processor_tester.min_pixels)\n+            self.assertEqual(video_processor.max_pixels, self.video_processor_tester.max_pixels)\n+\n+            video_processor = video_processing_class.from_dict(\n+                self.video_processor_dict, min_pixels=256 * 256, max_pixels=640 * 640\n+            )\n+            self.assertEqual(video_processor.min_pixels, 256 * 256)\n+            self.assertEqual(video_processor.max_pixels, 640 * 640)\n+\n+    def test_call_pil(self):\n+        for video_processing_class in self.video_processor_list:\n+            # Initialize video_processing\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"pil\"\n+            )\n+\n+            # Each video is a list of PIL Images\n+            for video in video_inputs:\n+                self.assertIsInstance(video[0], Image.Image)\n+\n+            # Test not batched input\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\")[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            encoded_videos = video_processing(video_inputs, return_tensors=\"pt\")[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_numpy(self):\n+        for video_processing_class in self.video_processor_list:\n+            # Initialize video_processing\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            # create random numpy tensors\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"np\"\n+            )\n+            for video in video_inputs:\n+                self.assertIsInstance(video, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\")[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            encoded_videos = video_processing(video_inputs, return_tensors=\"pt\")[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_pytorch(self):\n+        for video_processing_class in self.video_processor_list:\n+            # Initialize video_processing\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            # create random PyTorch tensors\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"torch\"\n+            )\n+\n+            for video in video_inputs:\n+                self.assertIsInstance(video, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\")[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            encoded_videos = video_processing(video_inputs, return_tensors=\"pt\")[self.input_name]\n+            self.assertEqual(\n+                list(encoded_videos.shape),\n+                expected_output_video_shape,\n+            )\n+\n+    def test_nested_input(self):\n+        \"\"\"Tests that the processor can work with nested list where each video is a list of arrays\"\"\"\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"np\"\n+            )\n+\n+            # Test not batched input\n+            video_inputs_nested = [list(video) for video in video_inputs]\n+            encoded_videos = video_processing(video_inputs_nested[0], return_tensors=\"pt\")[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            encoded_videos = video_processing(video_inputs_nested, return_tensors=\"pt\")[self.input_name]\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    @unittest.skip(\"Skip for now, the test needs adjustment fo Qwen2VL\")\n+    def test_call_numpy_4_channels(self):\n+        for video_processing_class in self.video_processor_list:\n+            # Test that can process videos which have an arbitrary number of channels\n+            # Initialize video_processing\n+            video_processor = video_processing_class(**self.video_processor_dict)\n+\n+            # create random numpy tensors\n+            self.video_processor_tester.num_channels = 4\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False, return_tensors=\"np\"\n+            )\n+\n+            # Test not batched input\n+            encoded_videos = video_processor(\n+                video_inputs[0],\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+            # Test batched\n+            encoded_videos = video_processor(\n+                video_inputs,\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=0,\n+                image_std=1,\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertEqual(list(encoded_videos.shape), expected_output_video_shape)\n+\n+    def test_call_sample_frames(self):\n+        for video_processing_class in self.video_processor_list:\n+            video_processing = video_processing_class(**self.video_processor_dict)\n+\n+            prev_num_frames = self.video_processor_tester.num_frames\n+            self.video_processor_tester.num_frames = 8\n+            video_inputs = self.video_processor_tester.prepare_video_inputs(\n+                equal_resolution=False,\n+                return_tensors=\"torch\",\n+            )\n+\n+            # Force set sampling to False. No sampling is expected even when `num_frames` exists\n+            video_processing.do_sample_frames = False\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=3)[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape([video_inputs[0]])\n+            expected_output_video_shape_batched = self.video_processor_tester.expected_output_video_shape(video_inputs)\n+            self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n+\n+            # Set sampling to True. Video frames should be sampled with `num_frames` in the output\n+            video_processing.do_sample_frames = True\n+\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=4)[self.input_name]\n+            encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=4)[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(\n+                [video_inputs[0]], num_frames=4\n+            )\n+            expected_output_video_shape_batched = self.video_processor_tester.expected_output_video_shape(\n+                video_inputs, num_frames=4\n+            )\n+            self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n+\n+            metadata = [[{\"duration\": 2.0, \"total_num_frames\": 8, \"fps\": 4}]]\n+            batched_metadata = metadata * len(video_inputs)\n+            encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", fps=3, video_metadata=metadata)[\n+                self.input_name\n+            ]\n+            encoded_videos_batched = video_processing(\n+                video_inputs, return_tensors=\"pt\", fps=3, video_metadata=batched_metadata\n+            )[self.input_name]\n+            expected_output_video_shape = self.video_processor_tester.expected_output_video_shape(\n+                [video_inputs[0]], num_frames=6\n+            )\n+            expected_output_video_shape_batched = self.video_processor_tester.expected_output_video_shape(\n+                video_inputs, num_frames=6\n+            )\n+            self.assertListEqual(list(encoded_videos.shape), expected_output_video_shape)\n+            self.assertListEqual(list(encoded_videos_batched.shape), expected_output_video_shape_batched)\n+\n+            # We should raise error when asked to sample more frames than there are in input video\n+            with self.assertRaises(ValueError):\n+                encoded_videos = video_processing(video_inputs[0], return_tensors=\"pt\", num_frames=10)[self.input_name]\n+                encoded_videos_batched = video_processing(video_inputs, return_tensors=\"pt\", num_frames=10)[\n+                    self.input_name\n+                ]\n+\n+            # Assign back the actual num frames in tester\n+            self.video_processor_tester.num_frames = prev_num_frames"
        }
    ],
    "stats": {
        "total": 6785,
        "additions": 6785,
        "deletions": 0
    }
}