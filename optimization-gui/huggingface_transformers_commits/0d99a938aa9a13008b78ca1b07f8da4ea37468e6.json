{
    "author": "ydshieh",
    "message": "Avoid calling `get_max_length` (#34971)\n\nfix\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "0d99a938aa9a13008b78ca1b07f8da4ea37468e6",
    "files": [
        {
            "sha": "b3dd3446cd848ecbee1b1681f56a6e4dca16d435",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=0d99a938aa9a13008b78ca1b07f8da4ea37468e6",
            "patch": "@@ -911,7 +911,7 @@ def prepare_inputs_for_generation(\n         # This part differs from other models because BLOOM needs a 2D mask to construct alibi tensor\n         # The only difference is the usage of 2D instead of 4D mask, but the shape will be static\n         if isinstance(past_key_values, StaticCache) and attention_mask is not None:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n             batch_size, seq_length = attention_mask.shape\n             diff = target_length - seq_length\n "
        },
        {
            "sha": "9cabd48a51021f86983eb7c38d10610aba7ad8ce",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=0d99a938aa9a13008b78ca1b07f8da4ea37468e6",
            "patch": "@@ -909,7 +909,7 @@ def _update_causal_mask(\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "4871fc3584faee05fc57b94c3e9424b7eaffd229",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=0d99a938aa9a13008b78ca1b07f8da4ea37468e6",
            "patch": "@@ -1142,7 +1142,7 @@ def _update_causal_mask(\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        },
        {
            "sha": "63cfbb6e6a5ff0342c039d30c8632691e4dc312d",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d99a938aa9a13008b78ca1b07f8da4ea37468e6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=0d99a938aa9a13008b78ca1b07f8da4ea37468e6",
            "patch": "@@ -353,7 +353,7 @@ def _update_causal_mask(\n         min_dtype = torch.finfo(dtype).min\n         sequence_length = inputs_embeds.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}