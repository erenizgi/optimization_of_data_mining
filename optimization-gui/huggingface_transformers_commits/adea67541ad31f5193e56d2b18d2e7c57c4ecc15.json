{
    "author": "zucchini-nlp",
    "message": "Phi3: fix attn for sliding window (#33586)\n\n* fix phi3 attn fir sliding window\r\n\r\n* fix tests\r\n\r\n* address most comment\r\n\r\n* style\r\n\r\n* update after rebase\r\n\r\n* add more models\r\n\r\n* fix tests",
    "sha": "adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
    "files": [
        {
            "sha": "514f9de706ec63df0bf36bc76547549ef81491e1",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 36,
            "deletions": 14,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -23,7 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -1027,7 +1027,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.gemma.modeling_gemma.GemmaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1046,21 +1046,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1077,6 +1086,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1088,13 +1099,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Mimi\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1103,20 +1113,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: MimiConfig,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1125,6 +1135,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`MimiConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1134,19 +1148,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n "
        },
        {
            "sha": "b0ffe3e56e5972a61173312f21a6111f9ec3ca27",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 153,
            "deletions": 27,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -856,7 +856,7 @@ def _update_causal_mask(\n         use_cache: bool,\n         output_attentions: bool,\n     ):\n-        if self._attn_implementation == \"flash_attention_2\":\n+        if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and use_cache:\n                 is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n                 if is_padding_right:\n@@ -872,12 +872,11 @@ def _update_causal_mask(\n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n-\n-        # cache_position must be valid here no matter which cache we use\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n         using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and not (using_static_cache or using_sliding_window_cache)\n@@ -906,30 +905,18 @@ def _update_causal_mask(\n                 else past_seen_tokens + sequence_length + 1\n             )\n \n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            causal_mask = attention_mask\n-        else:\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n-            )\n-            exclude_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-            if self.config.sliding_window is not None:\n-                if not using_sliding_window_cache or sequence_length > self.config.sliding_window:\n-                    exclude_mask.bitwise_or_(\n-                        torch.arange(target_length, device=device)\n-                        <= (cache_position.reshape(-1, 1) - self.config.sliding_window)\n-                    )\n-            causal_mask *= exclude_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(input_tensor.shape[0], 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.dim() == 2:\n-                    mask_length = attention_mask.shape[-1]\n-                    padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-                    padding_mask = padding_mask == 0\n-                    causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                        padding_mask, min_dtype\n-                    )\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n+        )\n \n         if (\n             self.config._attn_implementation == \"sdpa\"\n@@ -944,6 +931,73 @@ def _update_causal_mask(\n \n         return causal_mask\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        config: MistralConfig,\n+        past_key_values: Cache,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+            config (`MistralConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+        return causal_mask\n+\n \n class MistralForCausalLM(MistralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n@@ -1074,6 +1128,78 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # `contiguous()` needed for compilation use cases\n+            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "9c7fadbb8f885cf3691e25a90c2c44020747d73e",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 106,
            "deletions": 14,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -29,7 +29,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n@@ -1068,7 +1068,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1087,21 +1087,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1118,6 +1127,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1129,13 +1140,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Mixtral\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1144,20 +1154,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: MixtralConfig,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1166,6 +1176,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`MixtralConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1175,19 +1189,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1344,6 +1366,76 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        output_router_logits=False,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+        else:\n+            model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+                \"output_router_logits\": output_router_logits,\n+            }\n+        )\n+        return model_inputs\n+\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "0380c6cd49d6ea03a5bd4e19cc2202a5eded8fad",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 37,
            "deletions": 14,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1036,7 +1036,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1055,21 +1054,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1086,6 +1094,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1097,13 +1107,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Phi3\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1112,20 +1121,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: Phi3Config,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1134,6 +1143,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`Phi3Config`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1143,19 +1156,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1372,6 +1393,8 @@ def prepare_inputs_for_generation(\n                 device=device,\n                 cache_position=cache_position,\n                 batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n             )\n \n         if num_logits_to_keep is not None:"
        },
        {
            "sha": "d1705f04ddb7bbfef391c45aefa307fe9290946d",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 38,
            "deletions": 14,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n@@ -1203,7 +1203,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1222,21 +1222,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1253,6 +1262,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1264,13 +1275,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Phimoe\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1279,20 +1289,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: PhimoeConfig,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1301,6 +1311,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`PhimoeConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1310,19 +1324,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1561,6 +1583,8 @@ def prepare_inputs_for_generation(\n                 device=device,\n                 cache_position=cache_position,\n                 batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n             )\n \n         if num_logits_to_keep is not None:"
        },
        {
            "sha": "50f273ba766ca946c89917541749255d5de13f1b",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 109,
            "deletions": 14,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -28,7 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -954,7 +954,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -973,21 +973,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1004,6 +1013,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1015,13 +1026,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1030,20 +1040,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: Qwen2Config,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1052,6 +1062,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`Qwen2Config`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1061,19 +1075,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1206,6 +1228,79 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n+    # Copied from transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # `contiguous()` needed for compilation use cases\n+            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "2ab13b7227ada65f1b0485c807471c89ef43af86",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 109,
            "deletions": 14,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -29,7 +29,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1135,7 +1135,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1154,21 +1154,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1185,6 +1194,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1196,13 +1207,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2Moe\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1211,20 +1221,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: Qwen2MoeConfig,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1233,6 +1243,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`Qwen2MoeConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1242,19 +1256,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1410,6 +1432,79 @@ def forward(\n             router_logits=outputs.router_logits,\n         )\n \n+    # Copied from transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # `contiguous()` needed for compilation use cases\n+            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "283e38d3a7d508de04336b065713f4579f9b6205",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 38,
            "deletions": 14,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -30,7 +30,7 @@\n from torch.nn import CrossEntropyLoss, LayerNorm\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -1217,7 +1217,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1236,21 +1236,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1267,6 +1276,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -1278,13 +1289,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2VL\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1293,20 +1303,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: Qwen2VLConfig,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1315,6 +1325,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`Qwen2VLConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1324,19 +1338,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1820,6 +1842,8 @@ def prepare_inputs_for_generation(\n                 device=device,\n                 cache_position=cache_position,\n                 batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n             )\n \n         model_inputs.update("
        },
        {
            "sha": "e0fdbef1a3baf52681a3e7ffda0e8b7ecdbba18c",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 109,
            "deletions": 14,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -28,7 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -928,7 +928,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -947,21 +947,30 @@ def _update_causal_mask(\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n                 past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n                 is_training=self.training,\n             ):\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n+        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -978,6 +987,8 @@ def _update_causal_mask(\n             device=device,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n         )\n \n         if (\n@@ -989,13 +1000,12 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Starcoder2\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1004,20 +1014,20 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         device: torch.device,\n         cache_position: torch.Tensor,\n         batch_size: int,\n+        config: Starcoder2Config,\n+        past_key_values: Cache,\n     ):\n         \"\"\"\n         Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n         `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n             attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n             sequence_length (`int`):\n                 The sequence length being processed.\n             target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n@@ -1026,6 +1036,10 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n+            config (`Starcoder2Config`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n         \"\"\"\n         if attention_mask is not None and attention_mask.dim() == 4:\n             # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n@@ -1035,19 +1049,27 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             causal_mask = torch.full(\n                 (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n             )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            diagonal_attend_mask = torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            if config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=device) <= (\n+                        cache_position.reshape(-1, 1) - config.sliding_window\n+                    )\n+                    diagonal_attend_mask |= sliding_attend_mask\n+            causal_mask *= diagonal_attend_mask\n             causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n             if attention_mask is not None:\n                 causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n                 mask_length = attention_mask.shape[-1]\n                 padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n                 padding_mask = padding_mask == 0\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     padding_mask, min_dtype\n                 )\n-\n         return causal_mask\n \n \n@@ -1182,6 +1204,79 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n+    # Copied from transformers.models.mistral.modeling_mistral.MistralForCausalLM.prepare_inputs_for_generation\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n+        # Exception 1: when passing input_embeds, input_ids may be missing entries\n+        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        if past_key_values is not None:\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n+                input_ids = input_ids[:, cache_position]\n+\n+        if attention_mask is not None and position_ids is None:\n+            # create position_ids on the fly for batch generation\n+            position_ids = attention_mask.long().cumsum(-1) - 1\n+            position_ids.masked_fill_(attention_mask == 0, 1)\n+            if past_key_values:\n+                position_ids = position_ids[:, -input_ids.shape[1] :]\n+\n+                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n+                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n+\n+        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n+        else:\n+            # `contiguous()` needed for compilation use cases\n+            model_inputs = {\"input_ids\": input_ids.contiguous(), \"inputs_embeds\": None}\n+\n+        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+            if model_inputs[\"inputs_embeds\"] is not None:\n+                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n+                device = model_inputs[\"inputs_embeds\"].device\n+            else:\n+                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n+                device = model_inputs[\"input_ids\"].device\n+\n+            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n+                attention_mask,\n+                sequence_length=sequence_length,\n+                target_length=past_key_values.get_max_cache_shape(),\n+                dtype=self.lm_head.weight.dtype,\n+                device=device,\n+                cache_position=cache_position,\n+                batch_size=batch_size,\n+                config=self.config,\n+                past_key_values=past_key_values,\n+            )\n+\n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n+        model_inputs.update(\n+            {\n+                \"position_ids\": position_ids,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"use_cache\": use_cache,\n+                \"attention_mask\": attention_mask,\n+            }\n+        )\n+        return model_inputs\n+\n \n @add_start_docstrings(\n     \"\"\""
        },
        {
            "sha": "2c5557dfd67aae6532e250bc748d2cc933562b47",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -615,3 +615,93 @@ def test_phi3_mini_128k_instruct_with_static_cache(self):\n         ]\n \n         self.assertListEqual(output_text, EXPECTED_OUTPUT)\n+\n+    def test_phi3_mini_4k_sliding_window(self):\n+        \"\"\"\n+        This tests that Phi3 doesn't deteriorate in quality for long context generations. Since Phi3 has\n+        sliding window attention, the test is tailored so that (context + max_new_tokens > sliding_window).\n+        See #33586 for more\n+        \"\"\"\n+        model = Phi3ForCausalLM.from_pretrained(\n+            \"microsoft/Phi-3-mini-4k-instruct\", device_map=torch_device, torch_dtype=torch.bfloat16\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n+\n+        input_text = \"\"\"\n+            <|user|>\n+            Tell me about Paris, France.<|end|>\n+            <|assistant|>\n+            Paris, the capital city of France, is renowned for its rich history, iconic landmarks, and vibrant culture. Known as \"The City of Light,\" Paris is situated in the north-central part of the country along the Seine River.\n+\n+            Here are some key aspects of Paris:\n+\n+            1. Landmarks: Paris is home to numerous famous landmarks, including the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-lyses. The Eiffel Tower, built in 1889, is an iconic symbol of Paris and attracts millions of tourists each year. The Louvre Museum, the world's largest art museum, houses thousands of works of art, including the Mona Lisa and the Venus de Milo.\n+\n+            2. History: Paris has a rich history dating back to the 3rd century BC, when it was founded by a Celtic tribe called the Parisii. Over the centuries, the city has been influenced by various cultures, including the Romans, the Franks, and the Normans. The French Revolution in the late 18th century marked a significant turning point in Paris's history, leading to the establishment of the modern French Republic.\n+\n+            3. Culture: Paris is a global center for art, fashion, gastronomy, and culture. The city is home to numerous museums, including the Centre Pompidou, Muse d'Orsay, and Muse Rodin. Paris is also known for its fashion industry, with many famous designers having their origins in the city. The city's cuisine is also highly regarded, with a focus on fresh ingredients, and a wide variety of dishes, including French classics like coq au vin, boeuf bourguignon, and crpes.\n+\n+            4. Architecture: Parisian architecture is characterized by its diverse styles, ranging from Gothic and Romanesque to Art Nouveau and Art Deco. The city's famous Haussmannian buildings, designed by Baron Haussmann in the mid-19th century, are known for their uniform facades, wrought-iron balconies, and large windows.\n+\n+            5. Transportation: Paris has an extensive public transportation system, including the Paris Mtro, RER (suburban trains), and buses. The city's iconic yellow taxis are also a popular mode of transportation.\n+\n+            6. Language: The official language of Paris is French, and the city's residents are known for their charm and politeness.\n+\n+            7. Festivals and Events: Paris hosts numerous festivals and events throughout the year, including the annual Bastille Day celebrations, the Paris Fashion Week, and the famous annual New Year's Eve fireworks on the Eiffel Tower.\n+\n+            8. Geography: Paris is located in the north-central part of France, with the Seine River running through the city. The city's geography is characterized by rolling hills and picturesque parks, such as the Bois de Boulogne and the Jardin des Tuileries.\n+\n+            9. Population: As of 2021, Paris has an estimated population of around 2.2 million residents, with the metropolitan area housing over 12 million people.\n+\n+            In summary, Paris is a city steeped in history, culture, and art, with a unique blend of architectural styles and a vibrant atmosphere that continues to captivate millions of visitors each year.<|end|>\n+            <|user|>\n+            Please give me a list of 5 architectural landmarks in Paris, France.<|end|>\n+            <|assistant|>\n+            1. Eiffel Tower: Designed by Gustave Eiffel and completed in 1889, the Eiffel Tower is an iconic symbol of Paris and France. Standing at 324 meters tall, it was the tallest man-made structure in the world until the completion of the Chrysler Building in New York in 1930. The Eiffel Tower is made of wrought iron and offers visitors stunning views of the city from its three levels.\n+\n+            2. Notre-Dame Cathedral: Located on the le de la Cit, Notre-Dame Cathedral is a masterpiece of French Gothic architecture. Construction began in the 12th century and continued for over 200 years, with the cathedral's completion in the 14th century. The cathedral is famous for its intricate facade, stained-glass windows, and the iconic gargoyles and chimeras.\n+\n+            3. Louvre Museum: Originally built as a fortress in the 12th century, the Louvre Museum is now the world's largest art museum and a historic monument in Paris. The museum's most famous landmark is the iconic glass pyramid entrance, designed by architect I. M. Pei in the 1980s. The Louvre houses over 380,000 works of art, including the Mona Lisa and the Venus de Milo.\n+\n+            4. Sacr-Cur Basilica: The Sacr-Cur Basilica, also known as the Basilique du Sacr-Cur, is a Roman Catholic church and minor basilica located at the summit of the butte Montmartre, the highest point in Paris. The basilica was designed by Paul Abadie and dedicated in 1914. Its white domes and lavender-colored travertine stone make it a distinctive landmark in the Paris skyline.\n+\n+            5. Arc de Triomphe: The Arc de Triomphe is a monumental structure located at the western end of the Champs-lyses. Commissioned by Napoleon in 1806, the Arc was designed by Jean-Franois-Thrse Chalgrin and completed in 1836. The monument honors those who fought and died for France during the French Revolutionary and Napoleonic Wars. The Arc features sculptural reliefs and inscriptions, and its faade is adorned with the names of 357 generals and 660 soldiers.\n+\n+            These five architectural landmarks showcase the diverse styles and historical periods of Paris, from Gothic to Neoclassical, and from the 19th to the 20th centuries. Each landmark has its unique features and contributes to the city's rich architectural heritage.<|end|>\n+            <|user|>\n+            Please give me a list of 10 famous items displayed in the Louvre Museum. Thanks!<|end|>\n+            <|assistant|>\n+            1. Mona Lisa: The Mona Lisa, painted by Leonardo da Vinci in the early 16th century, is arguably the most famous painting in the world. The portrait is known for its enigmatic smile and masterful use of sfumato, a technique that creates a soft, hazy effect.\n+\n+            2. Venus de Milo: This ancient Greek statue, believed to have been created around 130-100 BC, is a masterpiece of Hellenistic sculpture. The Venus de Milo is renowned for its graceful beauty and the mystery surrounding its missing arms.\n+\n+            3. Winged Victory of Samothrace: This Hellenistic sculpture, dating back to the 2nd century BC, depicts the Greek goddess Nike, the personification of victory. The sculpture is celebrated for its dynamic movement and intricate details.\n+\n+            4. Liberty Leading the People: This iconic painting by Eugne Delacroix, created in 1830, commemorates the July Revolution in France. The artwork depicts a woman personifying Liberty leading a group of revolutionaries over the bodies of the fallen.\n+\n+            5. The Wedding at Cana: A 1516 painting by Veronese, The Wedding at Cana is a large-scale work that depicts the biblical story of Jesus turning water into wine at a wedding feast. The painting is known for its vibrant colors and intricate details.\n+\n+            6. The Raft of the Medusa: This 1819 painting by Thodore Gricault is a powerful depiction of the aftermath of the shipwreck of the French frigate Mduse. The painting is famous for its dramatic composition and emotional intensity.\n+\n+            7. The Coronation of Napoleon: This 1805 painting by Jacques-Louis David portrays the coronation of Napoleon Bonaparte as Emperor of the French. The artwork is a masterpiece of neoclassical style and captures the pomp and ceremony of the event.\n+\n+            8. The Death of Socrates: A 1914 painting by Pablo Picasso, The Death of Socrates depicts the moment when the ancient Greek philosopher Socrates drinks the poison hemlock, as ordered by the Athenian government. The painting is a powerful expression of the existential themes associated with Socrates' death.\n+\n+            9. The Turkish Bath: A 1887 painting by Puvis de Chavannes, The Turkish Bath is a large-scale work that depicts a scene of relaxation and leisity in a traditional Turkish bath. The painting is known for its subdued color palette and serene atmosphere.\n+\n+            10. The Wedding at Cana: A 1901 painting by Giovanni Boldini, The Wedding at Cana is a lively depiction of a festive Italian wedding. The painting is characterized by Boldini's signature flamboyant style and his ability to capture the essence of the subjects with a sense of movement and energy.\n+\n+            These 10 famous items in the Louvre Museum showcase a diverse range of artistic styles and periods, from ancient Greek and Roman sculptures to Renaissance and 19th-century paintings. The collection at the Louvre is one of the world's largest and most important museums, housing over 800,000 works of art, and these famous items are just a small selection of the museum's vast collection.<|end|>\n+            <|user|>\n+            Please give me a list of 10 famous French dishes.<|end|>\n+            <|assistant|>\n+            \"\"\"\n+\n+        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device=torch_device)\n+        outputs = model.generate(**inputs, max_new_tokens=100)\n+        output_text = tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[1] :], skip_special_tokens=True)\n+        EXPECTED_OUTPUT = [\n+            '1. Coq au Vin: Coq au Vin is a classic French dish that translates to \"rooster in wine.\" The dish consists of chicken braised with wine, lardons, mushrooms, and garlic. It is a hearty and flavorful dish that is often served with potatoes or rice.\\n\\n            2. Boeuf Bourguignon: Boeuf Bourguignon is a traditional French beef stew that'\n+        ]\n+\n+        self.assertListEqual(output_text, EXPECTED_OUTPUT)"
        },
        {
            "sha": "c7fe6577985f9635e787ce81f6f5385dc051178c",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -64,7 +64,7 @@ def __init__(\n         num_hidden_layers=5,\n         max_window_layers=3,\n         use_sliding_window=True,\n-        sliding_window=2,\n+        sliding_window=50,\n         num_attention_heads=4,\n         num_key_value_heads=2,\n         intermediate_size=37,"
        },
        {
            "sha": "11fc55f6baff402027ca3d4127b26a96180d8388",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -64,7 +64,7 @@ def __init__(\n         num_hidden_layers=5,\n         max_window_layers=3,\n         use_sliding_window=True,\n-        sliding_window=2,\n+        sliding_window=50,\n         num_attention_heads=4,\n         num_key_value_heads=2,\n         intermediate_size=37,"
        },
        {
            "sha": "0ae641d4e5021dc763c8437d15a6f624089bac7b",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/adea67541ad31f5193e56d2b18d2e7c57c4ecc15/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=adea67541ad31f5193e56d2b18d2e7c57c4ecc15",
            "patch": "@@ -18,7 +18,7 @@\n import os\n import re\n from collections import defaultdict, deque\n-from typing import Dict, List, Set\n+from typing import Dict, List, Optional, Set\n \n import libcst as cst\n from check_copies import run_ruff\n@@ -623,7 +623,7 @@ def get_new_part(class_name, base_class):\n     return snake_case\n \n \n-def find_all_dependencies(function: str, dependency_mapping: dict[str, set]):\n+def find_all_dependencies(function: str, dependency_mapping: Dict[str, set]):\n     \"\"\"Return all the dependencies of the given top-level function. Given the following structure in the `modular_xxx.py` file:\n     ```\n     def foo1():\n@@ -1001,8 +1001,8 @@ def _maybe_add_function_to_body(\n         top_level_function: str,\n         body: dict,\n         function_node: cst.FunctionDef,\n-        matching_callers: set | None = None,\n-        parent: str | None = None,\n+        matching_callers: Optional[set] = None,\n+        parent: Optional[str] = None,\n     ) -> bool:\n         \"\"\"Check if the `top_level_function` should be added to the body (i.e. it is not already present, and `matching_callers`\n         is not empy, or `parent`is provided). If it should be added, do it (in the correct location, just before its caller) and return"
        }
    ],
    "stats": {
        "total": 976,
        "additions": 831,
        "deletions": 145
    }
}