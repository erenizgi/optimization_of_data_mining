{
    "author": "ydshieh",
    "message": "Use `lru_cache` for tokenization tests (#36818)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "1fcaad6df96c0d01ca47a2a48175c932ec354954",
    "files": [
        {
            "sha": "73022d5b02d465506a8ca096623fce03cc40c317",
            "filename": "tests/models/albert/test_tokenization_albert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Falbert%2Ftest_tokenization_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Falbert%2Ftest_tokenization_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_tokenization_albert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -34,12 +34,13 @@ class AlbertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     test_sentencepiece_ignore_case = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = AlbertTokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"this is a test\""
        },
        {
            "sha": "f6b66982cc88bb105ef03ecf8e43efd37ec51c1c",
            "filename": "tests/models/bart/test_tokenization_bart.py",
            "status": "modified",
            "additions": 28,
            "deletions": 17,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_tokenization_bart.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -14,13 +14,14 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import BartTokenizer, BartTokenizerFast, BatchEncoding\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors\n+from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -32,8 +33,10 @@ class TestTokenizationBart(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_filter = filter_roberta_detectors\n     # from_pretrained_kwargs = {'add_prefix_space': True}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n         vocab = [\n             \"l\",\n             \"o\",\n@@ -58,22 +61,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return \"lower newer\", \"lower newer\"\n@@ -154,8 +165,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)"
        },
        {
            "sha": "86663ce60c1ae183886784728028456ba5267523",
            "filename": "tests/models/barthez/test_tokenization_barthez.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbarthez%2Ftest_tokenization_barthez.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbarthez%2Ftest_tokenization_barthez.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbarthez%2Ftest_tokenization_barthez.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -31,13 +31,14 @@ class BarthezTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         tokenizer = BarthezTokenizerFast.from_pretrained(\"moussaKam/mbarthez\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n-        tokenizer.save_pretrained(self.tmpdirname, legacy_format=False)\n-        self.tokenizer = tokenizer\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname, legacy_format=False)\n+        cls.tokenizer = tokenizer\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "6eb05a17acc4761456fb05080668a3d545a684a2",
            "filename": "tests/models/bartpho/test_tokenization_bartpho.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,11 +15,12 @@\n \n import os\n import unittest\n+from functools import lru_cache\n \n from transformers.models.bartpho.tokenization_bartpho import VOCAB_FILES_NAMES, BartphoTokenizer\n from transformers.testing_utils import get_tests_dir\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece_bpe.model\")\n@@ -31,24 +32,29 @@ class BartphoTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = [\"▁This\", \"▁is\", \"▁a\", \"▁t\", \"est\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.monolingual_vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"monolingual_vocab_file\"])\n-        with open(self.monolingual_vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.monolingual_vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"monolingual_vocab_file\"])\n+        with open(cls.monolingual_vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             for token in vocab_tokens:\n                 fp.write(f\"{token} {vocab_tokens[token]}\\n\")\n \n-        tokenizer = BartphoTokenizer(SAMPLE_VOCAB, self.monolingual_vocab_file, **self.special_tokens_map)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer = BartphoTokenizer(SAMPLE_VOCAB, cls.monolingual_vocab_file, **cls.special_tokens_map)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return BartphoTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return BartphoTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"This is a là test\""
        },
        {
            "sha": "c4392b306b4febc32f3a4148337d6990c751fec2",
            "filename": "tests/models/bert/test_tokenization_bert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -41,8 +41,9 @@ class BertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     space_between_special_tokens = True\n     from_pretrained_filter = filter_non_english\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -61,8 +62,8 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):\n@@ -257,7 +258,7 @@ def test_sequence_builders(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 sentence = f\"A, naïve {tokenizer_r.mask_token} AllenNLP sentence.\"\n                 tokens = tokenizer_r.encode_plus(\n@@ -312,8 +313,8 @@ def test_change_tokenize_chinese_chars(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n@@ -326,8 +327,8 @@ def test_change_tokenize_chinese_chars(self):\n                 self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)"
        },
        {
            "sha": "1569932d71529530272a1564bc3c5f8b62dc5384",
            "filename": "tests/models/bert_generation/test_tokenization_bert_generation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbert_generation%2Ftest_tokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbert_generation%2Ftest_tokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_tokenization_bert_generation.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -34,11 +34,12 @@ class BertGenerationTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         tokenizer = BertGenerationTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "73020e70527fd73544d63de123374856d0fe117c",
            "filename": "tests/models/bert_japanese/test_tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_japanese%2Ftest_tokenization_bert_japanese.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,6 +17,7 @@\n import os\n import pickle\n import unittest\n+from functools import lru_cache\n \n from transformers import AutoTokenizer\n from transformers.models.bert.tokenization_bert import BertTokenizer\n@@ -31,7 +32,7 @@\n )\n from transformers.testing_utils import custom_tokenizers, require_jumanpp, require_sudachi_projection\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @custom_tokenizers\n@@ -41,8 +42,9 @@ class BertJapaneseTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     space_between_special_tokens = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -72,8 +74,8 @@ def setUp(self):\n             \"です\",\n         ]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):\n@@ -408,17 +410,21 @@ class BertJapaneseCharacterTokenizationTest(TokenizerTesterMixin, unittest.TestC\n     tokenizer_class = BertJapaneseTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"こ\", \"ん\", \"に\", \"ち\", \"は\", \"ば\", \"世\", \"界\", \"、\", \"。\"]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n-    def get_tokenizer(self, **kwargs):\n-        return BertJapaneseTokenizer.from_pretrained(self.tmpdirname, subword_tokenizer_type=\"character\", **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        return BertJapaneseTokenizer.from_pretrained(cls.tmpdirname, subword_tokenizer_type=\"character\", **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"こんにちは、世界。 \\nこんばんは、世界。\""
        },
        {
            "sha": "d0659bc95afca4520d1f0c5f9f39ea4f090a9f23",
            "filename": "tests/models/bertweet/test_tokenization_bertweet.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,37 +15,43 @@\n \n import os\n import unittest\n+from functools import lru_cache\n \n from transformers.models.bertweet.tokenization_bertweet import VOCAB_FILES_NAMES, BertweetTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class BertweetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"vinai/bertweet-base\"\n     tokenizer_class = BertweetTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\"I\", \"m\", \"V@@\", \"R@@\", \"r\", \"e@@\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"a m</w>\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             for token in vocab_tokens:\n                 fp.write(f\"{token} {vocab_tokens[token]}\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return BertweetTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return BertweetTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"I am VinAI Research\""
        },
        {
            "sha": "f8fa29ba4845407192d039e9a36d7e4cf933cf2d",
            "filename": "tests/models/big_bird/test_tokenization_big_bird.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbig_bird%2Ftest_tokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbig_bird%2Ftest_tokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_tokenization_big_bird.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -36,11 +36,12 @@ class BigBirdTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n-        tokenizer = self.tokenizer_class(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer = cls.tokenizer_class(SAMPLE_VOCAB, keep_accents=True)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "4a9c53a6d0577c4be0fcc0cc2d6d4336d3e13f08",
            "filename": "tests/models/biogpt/test_tokenization_biogpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbiogpt%2Ftest_tokenization_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbiogpt%2Ftest_tokenization_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_tokenization_biogpt.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -30,8 +30,9 @@ class BioGptTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = BioGptTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -60,11 +61,11 @@ def setUp(self):\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"l o 123\", \"lo w 1456\", \"e r</w> 1789\", \"\"]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\") as fp:\n             fp.write(json.dumps(vocab_tokens))\n-        with open(self.merges_file, \"w\") as fp:\n+        with open(cls.merges_file, \"w\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n     def get_input_output_texts(self, tokenizer):"
        },
        {
            "sha": "286052558b9035452a02da45dd80491dfc300be8",
            "filename": "tests/models/blenderbot_small/test_tokenization_blenderbot_small.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fblenderbot_small%2Ftest_tokenization_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fblenderbot_small%2Ftest_tokenization_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_tokenization_blenderbot_small.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -18,39 +18,45 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers.models.blenderbot_small.tokenization_blenderbot_small import (\n     VOCAB_FILES_NAMES,\n     BlenderbotSmallTokenizer,\n )\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class BlenderbotSmallTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"facebook/blenderbot_small-90M\"\n     tokenizer_class = BlenderbotSmallTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = [\"__start__\", \"adapt\", \"act\", \"ap@@\", \"te\", \"__end__\", \"__unk__\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n \n         merges = [\"#version: 0.2\", \"a p\", \"t e</w>\", \"ap t</w>\", \"a d\", \"ad apt</w>\", \"a c\", \"ac t</w>\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"__unk__\", \"bos_token\": \"__start__\", \"eos_token\": \"__end__\"}\n+        cls.special_tokens_map = {\"unk_token\": \"__unk__\", \"bos_token\": \"__start__\", \"eos_token\": \"__end__\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return BlenderbotSmallTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return BlenderbotSmallTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"adapt act apte\""
        },
        {
            "sha": "e8e255f49c1e8cf8550a1709f7d28e00b0d12adb",
            "filename": "tests/models/bloom/test_tokenization_bloom.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,14 +13,16 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import copy\n import unittest\n+from functools import lru_cache\n \n from datasets import load_dataset\n \n from transformers import BloomTokenizerFast\n from transformers.testing_utils import require_jinja, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -34,14 +36,21 @@ class BloomTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_vocab_key = \"tokenizer_file\"\n     special_tokens_map = {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"pad_token\": \"<pad>\"}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/tokenizer\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return BloomTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        _kwargs = copy.deepcopy(cls.special_tokens_map)\n+        _kwargs.update(kwargs)\n+        kwargs = _kwargs\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return BloomTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     @unittest.skip(reason=\"This needs a slow tokenizer. Bloom does not have one!\")\n     def test_encode_decode_with_spaces(self):\n@@ -65,7 +74,7 @@ def test_encodings_from_sample_data(self):\n     def test_padding(self, max_length=6):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n                 # tokenizer_r.pad_token = None # Hotfixing padding = None\n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "5024ff3abe54c40fd4a9a15013f3874e7f79749f",
            "filename": "tests/models/byt5/test_tokenization_byt5.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbyt5%2Ftest_tokenization_byt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fbyt5%2Ftest_tokenization_byt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbyt5%2Ftest_tokenization_byt5.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -19,12 +19,13 @@\n import shutil\n import tempfile\n import unittest\n+from functools import lru_cache\n from typing import Tuple\n \n from transformers import AddedToken, BatchEncoding, ByT5Tokenizer\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n if is_torch_available():\n@@ -39,17 +40,22 @@ class ByT5TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = ByT5Tokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = ByT5Tokenizer()\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @cached_property\n     def t5_base_tokenizer(self):\n         return ByT5Tokenizer.from_pretrained(\"google/byt5-small\")\n \n-    def get_tokenizer(self, **kwargs) -> ByT5Tokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> ByT5Tokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5) -> Tuple[str, list]:\n         # XXX The default common tokenizer tests assume that every ID is decodable on its own."
        },
        {
            "sha": "4e46df0edafa99cdeaaa740662f5970872bee798",
            "filename": "tests/models/camembert/test_tokenization_camembert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcamembert%2Ftest_tokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcamembert%2Ftest_tokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcamembert%2Ftest_tokenization_camembert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,6 +15,7 @@\n \n import tempfile\n import unittest\n+from tempfile import TemporaryDirectory\n \n from transformers import AddedToken, CamembertTokenizer, CamembertTokenizerFast\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n@@ -38,12 +39,13 @@ class CamembertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = CamembertTokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @unittest.skip(\n         \"Token maps are not equal because someone set the probability of ('<unk>NOTUSED', -100), so it's never encoded for fast\"\n@@ -72,8 +74,9 @@ def test_vocab_size(self):\n \n     def test_rust_and_python_bpe_tokenizers(self):\n         tokenizer = CamembertTokenizer(SAMPLE_BPE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n-        rust_tokenizer = CamembertTokenizerFast.from_pretrained(self.tmpdirname)\n+        with TemporaryDirectory() as tmpdirname:\n+            tokenizer.save_pretrained(tmpdirname)\n+            rust_tokenizer = CamembertTokenizerFast.from_pretrained(tmpdirname)\n \n         sequence = \"I was born in 92000, and this is falsé.\"\n \n@@ -147,11 +150,11 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n             self.assertTrue(all(item in tokenizer.added_tokens_decoder.items() for item in expected.items()))\n             return tokenizer\n \n-        new_eos = AddedToken(\"[NEW_EOS]\", rstrip=False, lstrip=True, normalized=False)\n+        new_eos = AddedToken(\"[NEW_EOS]\", rstrip=False, lstrip=True, normalized=False, special=True)\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 # Load a slow tokenizer from the hub, init with the new token for fast to also include it\n-                tokenizer = self.tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n+                tokenizer = self.get_tokenizer(pretrained_name, eos_token=new_eos)\n                 EXPECTED_ADDED_TOKENS_DECODER = tokenizer.added_tokens_decoder\n                 with self.subTest(\"Hub -> Slow: Test loading a slow tokenizer from the hub)\"):\n                     self.assertEqual(tokenizer._special_tokens_map[\"eos_token\"], new_eos)\n@@ -191,9 +194,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n \n                 with self.subTest(\"Hub -> Fast: Test loading a fast tokenizer from the hub)\"):\n                     if self.rust_tokenizer_class is not None:\n-                        tokenizer_fast = self.rust_tokenizer_class.from_pretrained(\n-                            pretrained_name, eos_token=new_eos, from_slow=True\n-                        )\n+                        tokenizer_fast = self.get_rust_tokenizer(pretrained_name, eos_token=new_eos, from_slow=True)\n                         self.assertEqual(tokenizer_fast._special_tokens_map[\"eos_token\"], new_eos)\n                         self.assertIn(new_eos, list(tokenizer_fast.added_tokens_decoder.values()))\n                         # We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright"
        },
        {
            "sha": "e2efc99ca99d9c70c1291a26e19a42b99b66b483",
            "filename": "tests/models/canine/test_tokenization_canine.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_tokenization_canine.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -18,31 +18,37 @@\n import shutil\n import tempfile\n import unittest\n+from functools import lru_cache\n \n from transformers import BatchEncoding, CanineTokenizer\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.tokenization_utils import AddedToken\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class CanineTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"nielsr/canine-s\"\n     tokenizer_class = CanineTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = CanineTokenizer()\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @cached_property\n     def canine_tokenizer(self):\n         return CanineTokenizer.from_pretrained(\"google/canine-s\")\n \n-    def get_tokenizer(self, **kwargs) -> CanineTokenizer:\n-        tokenizer = self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> CanineTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        tokenizer = cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n         tokenizer._unicode_vocab_size = 1024\n         return tokenizer\n "
        },
        {
            "sha": "f0dfec6bd7e7d4e1d3fa09b98b4bafbbe3da92dd",
            "filename": "tests/models/clip/test_tokenization_clip.py",
            "status": "modified",
            "additions": 30,
            "deletions": 20,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_tokenization_clip.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,12 +17,13 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import CLIPTokenizer, CLIPTokenizerFast\n from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_ftfy, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -34,28 +35,37 @@ class CLIPTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_kwargs = {}\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = [\"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"i\", \"d\", \"n\", \"lo\", \"l</w>\", \"w</w>\", \"r</w>\", \"t</w>\", \"low</w>\", \"er</w>\", \"lowest</w>\", \"newer</w>\", \"wider\", \"<unk>\", \"<|startoftext|>\", \"<|endoftext|>\"]  # fmt: skip\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"l o\", \"lo w</w>\", \"e r</w>\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return CLIPTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return CLIPTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return CLIPTokenizer.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return CLIPTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n@@ -77,8 +87,8 @@ def test_full_tokenizer(self):\n     def test_check_encoding_slow_fast(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_s = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_s = self.get_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 text = \"A\\n'll 11p223RF☆ho!!to?'d'd''d of a cat to-$''d.\"\n                 text_tokenized_s = tokenizer_s.tokenize(text)\n@@ -138,7 +148,7 @@ def test_offsets_mapping_with_different_add_prefix_space_argument(self):\n                 text_of_1_token = \"hello\"  # `hello` is a token in the vocabulary of `pretrained_name`\n                 text = f\"{text_of_1_token} {text_of_1_token}\"\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name,\n                     use_fast=True,\n                 )\n@@ -151,7 +161,7 @@ def test_offsets_mapping_with_different_add_prefix_space_argument(self):\n \n                 text = f\" {text}\"\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name,\n                     use_fast=True,\n                 )\n@@ -166,7 +176,7 @@ def test_log_warning(self):\n         # Test related to the breaking change introduced in transformers v4.17.0\n         # We need to check that an error in raised when the user try to load a previous version of the tokenizer.\n         with self.assertRaises(ValueError) as context:\n-            self.rust_tokenizer_class.from_pretrained(\"robot-test/old-clip-tokenizer\")\n+            self.get_rust_tokenizer(\"robot-test/old-clip-tokenizer\")\n \n         self.assertTrue(\n             context.exception.args[0].startswith("
        },
        {
            "sha": "1c526e84d147db0a5090cee0935e09eb049b1533",
            "filename": "tests/models/clvp/test_tokenization_clvp.py",
            "status": "modified",
            "additions": 18,
            "deletions": 12,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_tokenization_clvp.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,11 +17,12 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n from typing import List\n \n from transformers import ClvpTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin, slow\n+from ...test_tokenization_common import TokenizerTesterMixin, slow, use_cache_if_possible\n \n \n class ClvpTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -32,8 +33,9 @@ class ClvpTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_seq2seq = False\n     test_sentencepiece_ignore_case = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -62,19 +64,23 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, \"vocab.json\")\n-        self.merges_file = os.path.join(self.tmpdirname, \"merges.txt\")\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, \"vocab.json\")\n+        cls.merges_file = os.path.join(cls.tmpdirname, \"merges.txt\")\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.get_tokenizer with GPT2->Clvp\n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return ClvpTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return ClvpTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     # Copied from transformers.tests.models.gpt2.test_tokenization_gpt2.GPT2TokenizationTest.get_input_output_texts\n     def get_input_output_texts(self, tokenizer):\n@@ -134,7 +140,7 @@ def test_rust_and_python_full_tokenizers(self):\n     def test_padding(self, max_length=15):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "774c17f51308a2456f37cbbca3a94b8380d7cf7d",
            "filename": "tests/models/code_llama/test_tokenization_code_llama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcode_llama%2Ftest_tokenization_code_llama.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -53,15 +53,16 @@ class CodeLlamaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     from_pretrained_kwargs = {}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = CodeLlamaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n         tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n-    def get_tokenizers(self, **kwargs):\n+    def get_tokenizers(cls, **kwargs):\n         kwargs.update({\"pad_token\": \"<PAD>\"})\n         return super().get_tokenizers(**kwargs)\n \n@@ -151,8 +152,8 @@ def test_save_pretrained(self):\n         ]\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n \n@@ -255,7 +256,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -265,7 +266,7 @@ def test_special_tokens_initialization(self):\n                 self.assertTrue(special_token_id in r_output)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name,\n                         additional_special_tokens=added_tokens,\n                         **kwargs,  # , from_slow=True <- unfortunately too slow to convert"
        },
        {
            "sha": "28d388202b8a428d8dbfaa90efce5e23a971acdc",
            "filename": "tests/models/codegen/test_tokenization_codegen.py",
            "status": "modified",
            "additions": 26,
            "deletions": 16,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_tokenization_codegen.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -18,12 +18,13 @@\n import os\n import re\n import unittest\n+from functools import lru_cache\n \n from transformers import CodeGenTokenizer, CodeGenTokenizerFast\n from transformers.models.codegen.tokenization_codegen import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -35,8 +36,9 @@ class CodeGenTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_kwargs = {\"add_prefix_space\": True}\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -64,22 +66,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return CodeGenTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return CodeGenTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return CodeGenTokenizer.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return CodeGenTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n@@ -136,7 +146,7 @@ def test_pretokenized_inputs(self, *args, **kwargs):\n     def test_padding(self, max_length=15):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "cec1334a33551f4a833f6ab0eb1a889f30a00139",
            "filename": "tests/models/cohere/test_tokenization_cohere.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,12 +13,14 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import copy\n import unittest\n+from functools import lru_cache\n \n from transformers import CohereTokenizerFast\n from transformers.testing_utils import require_jinja, require_tokenizers, require_torch_multi_gpu\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -37,14 +39,21 @@ class CohereTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n         \"pad_token\": \"<PAD>\",\n     }\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = CohereTokenizerFast.from_pretrained(\"hf-internal-testing/tiny-random-CohereForCausalLM\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return CohereTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        _kwargs = copy.deepcopy(cls.special_tokens_map)\n+        _kwargs.update(kwargs)\n+        kwargs = _kwargs\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return CohereTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     # This gives CPU OOM on a single-gpu runner (~60G RAM). On multi-gpu runner, it has ~180G RAM which is enough.\n     @require_torch_multi_gpu\n@@ -80,7 +89,7 @@ def test_encodings_from_sample_data(self):\n     def test_padding(self, max_length=10):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n                 # tokenizer_r.pad_token = None # Hotfixing padding = None\n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "32449763eaedb20c8ec0bd2cc90959f4578ca23d",
            "filename": "tests/models/cpmant/test_tokenization_cpmant.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcpmant%2Ftest_tokenization_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fcpmant%2Ftest_tokenization_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_tokenization_cpmant.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -28,8 +28,9 @@ class CPMAntTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = CpmAntTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"<d>\",\n@@ -49,8 +50,8 @@ def setUp(self):\n             \"n\",\n             \"t\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     @tooslow"
        },
        {
            "sha": "e22ca8abe591cca72141d6d1f16e46df1997de39",
            "filename": "tests/models/ctrl/test_tokenization_ctrl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fctrl%2Ftest_tokenization_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fctrl%2Ftest_tokenization_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_tokenization_ctrl.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -16,10 +16,11 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers.models.ctrl.tokenization_ctrl import VOCAB_FILES_NAMES, CTRLTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class CTRLTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -28,25 +29,30 @@ class CTRLTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\"adapt\", \"re@@\", \"a@@\", \"apt\", \"c@@\", \"t\", \"<unk>\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"a p\", \"ap t</w>\", \"r e\", \"a d\", \"ad apt</w>\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return CTRLTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return CTRLTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"adapt react readapt apt\""
        },
        {
            "sha": "dc3c84c8713f5602badd2b5dc974413cf0261d85",
            "filename": "tests/models/deberta/test_tokenization_deberta.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdeberta%2Ftest_tokenization_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdeberta%2Ftest_tokenization_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_tokenization_deberta.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,12 +17,13 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import DebertaTokenizer, DebertaTokenizerFast\n from transformers.models.deberta.tokenization_deberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class DebertaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n@@ -31,8 +32,9 @@ class DebertaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     rust_tokenizer_class = DebertaTokenizerFast\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -59,18 +61,22 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"[UNK]\"}\n+        cls.special_tokens_map = {\"unk_token\": \"[UNK]\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\""
        },
        {
            "sha": "c2e57a5809008c4a20e392336440ee1f476313fa",
            "filename": "tests/models/deberta_v2/test_tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdeberta_v2%2Ftest_tokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdeberta_v2%2Ftest_tokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_tokenization_deberta_v2.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -33,12 +33,13 @@ class DebertaV2TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     test_sentencepiece_ignore_case = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = DebertaV2Tokenizer(SAMPLE_VOCAB, unk_token=\"<unk>\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"this is a test\""
        },
        {
            "sha": "42f6d6a4ad169b0c539aacbf1ae9222d299474b3",
            "filename": "tests/models/distilbert/test_tokenization_distilbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdistilbert%2Ftest_tokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdistilbert%2Ftest_tokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_tokenization_distilbert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,11 +17,11 @@\n from transformers import DistilBertTokenizer, DistilBertTokenizerFast\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ..bert.test_tokenization_bert import BertTokenizationTest\n+from ..bert import test_tokenization_bert\n \n \n @require_tokenizers\n-class DistilBertTokenizationTest(BertTokenizationTest):\n+class DistilBertTokenizationTest(test_tokenization_bert.BertTokenizationTest):\n     tokenizer_class = DistilBertTokenizer\n     rust_tokenizer_class = DistilBertTokenizerFast\n     test_rust_tokenizer = True"
        },
        {
            "sha": "28c5562ec8dd8305fed31ab5d1654f18fac2b8e9",
            "filename": "tests/models/dpr/test_tokenization_dpr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdpr%2Ftest_tokenization_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fdpr%2Ftest_tokenization_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpr%2Ftest_tokenization_dpr.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -25,27 +25,27 @@\n from transformers.testing_utils import require_tokenizers, slow\n from transformers.tokenization_utils_base import BatchEncoding\n \n-from ..bert.test_tokenization_bert import BertTokenizationTest\n+from ..bert import test_tokenization_bert\n \n \n @require_tokenizers\n-class DPRContextEncoderTokenizationTest(BertTokenizationTest):\n+class DPRContextEncoderTokenizationTest(test_tokenization_bert.BertTokenizationTest):\n     tokenizer_class = DPRContextEncoderTokenizer\n     rust_tokenizer_class = DPRContextEncoderTokenizerFast\n     test_rust_tokenizer = True\n     from_pretrained_id = \"facebook/dpr-ctx_encoder-single-nq-base\"\n \n \n @require_tokenizers\n-class DPRQuestionEncoderTokenizationTest(BertTokenizationTest):\n+class DPRQuestionEncoderTokenizationTest(test_tokenization_bert.BertTokenizationTest):\n     tokenizer_class = DPRQuestionEncoderTokenizer\n     rust_tokenizer_class = DPRQuestionEncoderTokenizerFast\n     test_rust_tokenizer = True\n     from_pretrained_id = \"facebook/dpr-ctx_encoder-single-nq-base\"\n \n \n @require_tokenizers\n-class DPRReaderTokenizationTest(BertTokenizationTest):\n+class DPRReaderTokenizationTest(test_tokenization_bert.BertTokenizationTest):\n     tokenizer_class = DPRReaderTokenizer\n     rust_tokenizer_class = DPRReaderTokenizerFast\n     test_rust_tokenizer = True"
        },
        {
            "sha": "0155c21bf280992170c0ed337cfc2a6a782e0fe0",
            "filename": "tests/models/electra/test_tokenization_electra.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Felectra%2Ftest_tokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Felectra%2Ftest_tokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_tokenization_electra.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -40,8 +40,9 @@ class ElectraTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     space_between_special_tokens = True\n     from_pretrained_filter = filter_non_english\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -60,8 +61,8 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):\n@@ -250,7 +251,7 @@ def test_sequence_builders(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 sentence = f\"A, naïve {tokenizer_r.mask_token} AllenNLP sentence.\"\n                 tokens = tokenizer_r.encode_plus(\n@@ -305,8 +306,8 @@ def test_change_tokenize_chinese_chars(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n@@ -319,8 +320,8 @@ def test_change_tokenize_chinese_chars(self):\n                 self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)"
        },
        {
            "sha": "e0013d8a189f34e366af30fa059b8f7cc2fbbe7d",
            "filename": "tests/models/esm/test_tokenization_esm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fesm%2Ftest_tokenization_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fesm%2Ftest_tokenization_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_tokenization_esm.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,31 +17,40 @@\n import os\n import tempfile\n import unittest\n+from functools import lru_cache\n from typing import List\n \n from transformers.models.esm.tokenization_esm import VOCAB_FILES_NAMES, EsmTokenizer\n from transformers.testing_utils import require_tokenizers\n from transformers.tokenization_utils import PreTrainedTokenizer\n from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n \n+from ...test_tokenization_common import use_cache_if_possible\n+\n \n @require_tokenizers\n class ESMTokenizationTest(unittest.TestCase):\n     tokenizer_class = EsmTokenizer\n \n-    def setUp(self):\n-        super().setUp()\n-        self.tmpdirname = tempfile.mkdtemp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n+        cls.tmpdirname = tempfile.mkdtemp()\n         vocab_tokens: List[str] = [\"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\", \"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\", \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\", \"X\", \"B\", \"U\", \"Z\", \"O\", \".\", \"-\", \"<null_1>\", \"<mask>\"]  # fmt: skip\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n-    def get_tokenizers(self, **kwargs) -> List[PreTrainedTokenizerBase]:\n-        return [self.get_tokenizer(**kwargs)]\n+    def get_tokenizers(cls, **kwargs) -> List[PreTrainedTokenizerBase]:\n+        return [cls.get_tokenizer(**kwargs)]\n \n-    def get_tokenizer(self, **kwargs) -> PreTrainedTokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def test_tokenizer_single_example(self):\n         tokenizer = self.tokenizer_class(self.vocab_file)"
        },
        {
            "sha": "23c8a35dc65d990245199b4c2dc5daa46da7c95f",
            "filename": "tests/models/fastspeech2_conformer/test_tokenization_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_tokenization_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_tokenization_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_tokenization_fastspeech2_conformer.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -28,10 +28,11 @@ class FastSpeech2ConformerTokenizerTest(TokenizerTesterMixin, unittest.TestCase)\n     tokenizer_class = FastSpeech2ConformerTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = FastSpeech2ConformerTokenizer.from_pretrained(\"espnet/fastspeech2_conformer\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"this is a test\""
        },
        {
            "sha": "0fd42da306e76ce9a0728caa538c8c2d3d62fc12",
            "filename": "tests/models/flaubert/test_tokenization_flaubert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fflaubert%2Ftest_tokenization_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fflaubert%2Ftest_tokenization_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflaubert%2Ftest_tokenization_flaubert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -30,20 +30,21 @@ class FlaubertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = FlaubertTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"i\", \"d\", \"n\", \"w</w>\", \"r</w>\", \"t</w>\", \"i</w>\", \"lo\", \"low\", \"ne\", \"new\", \"er</w>\", \"low</w>\", \"lowest</w>\", \"new</w>\", \"newer</w>\", \"wider</w>\", \"<unk>\"]  # fmt: skip\n \n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"n e 300\", \"ne w 301\", \"e r</w> 302\", \"\"]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n     # Copied from transformers.tests.models.xlm.test_tokenization_xlm.XLMTokenizationTest.test_full_tokenizer"
        },
        {
            "sha": "c8156a7b208467932ac2a7fa4432da9175391533",
            "filename": "tests/models/fnet/test_tokenization_fnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_tokenization_fnet.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -36,12 +36,13 @@ class FNetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece_ignore_case = True\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = FNetTokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"this is a test\"\n@@ -147,7 +148,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -175,7 +176,7 @@ def test_special_tokens_initialization_from_slow(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs, from_slow=True\n                 )\n                 special_token_id = tokenizer_r.encode(\"<special>\", add_special_tokens=False)[0]\n@@ -198,8 +199,8 @@ def test_padding(self, max_length=50):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id"
        },
        {
            "sha": "cbc96922554023a64f0c00c9927d955863bdde6a",
            "filename": "tests/models/fsmt/test_tokenization_fsmt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffsmt%2Ftest_tokenization_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffsmt%2Ftest_tokenization_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_tokenization_fsmt.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -34,8 +34,9 @@ class FSMTTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = FSMTTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -64,22 +65,22 @@ def setUp(self):\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"l o 123\", \"lo w 1456\", \"e r</w> 1789\", \"\"]\n \n-        self.langs = [\"en\", \"ru\"]\n+        cls.langs = [\"en\", \"ru\"]\n         config = {\n-            \"langs\": self.langs,\n+            \"langs\": cls.langs,\n             \"src_vocab_size\": 10,\n             \"tgt_vocab_size\": 20,\n         }\n \n-        self.src_vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"src_vocab_file\"])\n-        self.tgt_vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"tgt_vocab_file\"])\n-        config_file = os.path.join(self.tmpdirname, \"tokenizer_config.json\")\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.src_vocab_file, \"w\") as fp:\n+        cls.src_vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"src_vocab_file\"])\n+        cls.tgt_vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"tgt_vocab_file\"])\n+        config_file = os.path.join(cls.tmpdirname, \"tokenizer_config.json\")\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.src_vocab_file, \"w\") as fp:\n             fp.write(json.dumps(vocab_tokens))\n-        with open(self.tgt_vocab_file, \"w\") as fp:\n+        with open(cls.tgt_vocab_file, \"w\") as fp:\n             fp.write(json.dumps(vocab_tokens))\n-        with open(self.merges_file, \"w\") as fp:\n+        with open(cls.merges_file, \"w\") as fp:\n             fp.write(\"\\n\".join(merges))\n         with open(config_file, \"w\") as fp:\n             fp.write(json.dumps(config))"
        },
        {
            "sha": "d5e22a3c6ede791e05a9aae7a786ae41e294a228",
            "filename": "tests/models/funnel/test_tokenization_funnel.py",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffunnel%2Ftest_tokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ffunnel%2Ftest_tokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffunnel%2Ftest_tokenization_funnel.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -16,12 +16,13 @@\n \n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import FunnelTokenizer, FunnelTokenizerFast\n from transformers.models.funnel.tokenization_funnel import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -32,8 +33,9 @@ class FunnelTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     space_between_special_tokens = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"<unk>\",\n@@ -50,15 +52,23 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n-    def get_tokenizer(self, **kwargs):\n-        return FunnelTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return FunnelTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n-    def get_rust_tokenizer(self, **kwargs):\n-        return FunnelTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return FunnelTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"UNwant\\u00e9d,running\""
        },
        {
            "sha": "e48d19a25341284178a22158008067bd9636234a",
            "filename": "tests/models/gemma/test_tokenization_gemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -53,12 +53,13 @@ class GemmaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     from_pretrained_kwargs = {}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         # We have a SentencePiece fixture for testing\n         tokenizer = GemmaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n         tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @require_torch\n     def test_batch_tokenization(self):\n@@ -103,7 +104,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -113,7 +114,7 @@ def test_special_tokens_initialization(self):\n                 self.assertTrue(special_token_id in r_output)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name,\n                         additional_special_tokens=added_tokens,\n                         **kwargs,  # , from_slow=True <- unfortunately too slow to convert"
        },
        {
            "sha": "40e9f2fe48eeb27f72c2657000b80fb6ca07b462",
            "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 16,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,12 +17,13 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -34,8 +35,9 @@ class GPT2TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_kwargs = {\"add_prefix_space\": True}\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -63,22 +65,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return GPT2Tokenizer.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return GPT2TokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return GPT2Tokenizer.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return GPT2TokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n@@ -135,7 +145,7 @@ def test_pretokenized_inputs(self, *args, **kwargs):\n     def test_padding(self, max_length=15):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "6402c579560c93261814d2c23ec152053404c21a",
            "filename": "tests/models/gpt_neox_japanese/test_tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_tokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_tokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_tokenization_gpt_neox_japanese.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,14 +17,15 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers.models.gpt_neox_japanese.tokenization_gpt_neox_japanese import (\n     VOCAB_FILES_NAMES,\n     GPTNeoXJapaneseTokenizer,\n )\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -34,8 +35,9 @@ class GPTNeoXJapaneseTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     from_pretrained_kwargs = {\"do_clean_text\": False, \"add_prefix_space\": False}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"こん\",\n@@ -62,18 +64,22 @@ def setUp(self):\n             \"<|endoftext|>\",\n         ]\n         emoji_tokens = {\"emoji\": {\"\\ud83d\\ude00\": \"<|emoji1|>\"}, \"emoji_inv\": {\"<|emoji1|>\": \"\\ud83d\\ude00\"}}  # 😀\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.emoji_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"emoji_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.emoji_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"emoji_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n-        with open(self.emoji_file, \"w\") as emoji_writer:\n+        with open(cls.emoji_file, \"w\") as emoji_writer:\n             emoji_writer.write(json.dumps(emoji_tokens))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return GPTNeoXJapaneseTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return GPTNeoXJapaneseTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"こんにちは、世界。 \\nこんばんは、㔺界。😀\""
        },
        {
            "sha": "a13be778a6e548652f2bfd5ca96d81d630743116",
            "filename": "tests/models/gpt_sw3/test_tokenization_gpt_sw3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -33,13 +33,14 @@ class GPTSw3TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     test_sentencepiece_ignore_case = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = GPTSw3Tokenizer(SAMPLE_VOCAB, eos_token=\"<unk>\", bos_token=\"<unk>\", pad_token=\"<unk>\")\n \n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"This is a test\""
        },
        {
            "sha": "36849bb69833c33823e4c59b476c5fa94454c921",
            "filename": "tests/models/herbert/test_tokenization_herbert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fherbert%2Ftest_tokenization_herbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fherbert%2Ftest_tokenization_herbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fherbert%2Ftest_tokenization_herbert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -33,12 +33,13 @@ class HerbertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     rust_tokenizer_class = HerbertTokenizerFast\n     test_rust_tokenizer = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Use a simpler test file without japanese/chinese characters\n         with open(f\"{get_tests_dir()}/fixtures/sample_text_no_unicode.txt\", encoding=\"utf-8\") as f_data:\n-            self._data = f_data.read().replace(\"\\n\\n\", \"\\n\").strip()\n+            cls._data = f_data.read().replace(\"\\n\\n\", \"\\n\").strip()\n \n         vocab = [\n             \"<s>\",\n@@ -69,11 +70,11 @@ def setUp(self):\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"l o 123\", \"lo w 1456\", \"e r</w> 1789\", \"\"]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\") as fp:\n             fp.write(json.dumps(vocab_tokens))\n-        with open(self.merges_file, \"w\") as fp:\n+        with open(cls.merges_file, \"w\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n     def get_input_output_texts(self, tokenizer):"
        },
        {
            "sha": "7143d8b0e0099ea082f84941cf6756e4eecbfb5e",
            "filename": "tests/models/layoutlm/test_tokenization_layoutlm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutlm%2Ftest_tokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutlm%2Ftest_tokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_tokenization_layoutlm.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -16,12 +16,13 @@\n \n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import LayoutLMTokenizer, LayoutLMTokenizerFast\n from transformers.models.layoutlm.tokenization_layoutlm import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -32,8 +33,9 @@ class LayoutLMTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     space_between_special_tokens = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -50,12 +52,16 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n-    def get_tokenizer(self, **kwargs):\n-        return LayoutLMTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return LayoutLMTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"UNwant\\u00e9d,running\""
        },
        {
            "sha": "e2271d60c6d8188c38eef336f87f9069ce7e8722",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -102,8 +102,9 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -122,8 +123,8 @@ def setUp(self):\n             \"test\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):\n@@ -267,7 +268,7 @@ def test_sequence_builders(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 words, boxes = self.get_words_and_boxes()\n                 words[1] = tokenizer_r.mask_token\n@@ -605,8 +606,8 @@ def test_padding_to_max_length(self):\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n@@ -1060,7 +1061,7 @@ def test_build_inputs_with_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Input tokens id\n                 words, boxes = self.get_words_and_boxes()\n@@ -1363,7 +1364,7 @@ def test_tokenization_python_rust_equals(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 words, boxes = self.get_words_and_boxes()\n \n@@ -1417,7 +1418,7 @@ def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 words, boxes = self.get_words_and_boxes()\n                 tokens_r = tokenizer_r.encode_plus(\n                     words,\n@@ -1715,7 +1716,7 @@ def test_padding_different_model_input_name(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n "
        },
        {
            "sha": "48e36411dbc8cdd18f59b7d727317604447606b3",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 36,
            "deletions": 21,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -20,6 +20,7 @@\n import shutil\n import tempfile\n import unittest\n+from functools import lru_cache\n from typing import List\n \n from parameterized import parameterized\n@@ -41,7 +42,12 @@\n     slow,\n )\n \n-from ...test_tokenization_common import SMALL_TRAINING_CORPUS, TokenizerTesterMixin, merge_model_tokenizer_mappings\n+from ...test_tokenization_common import (\n+    SMALL_TRAINING_CORPUS,\n+    TokenizerTesterMixin,\n+    merge_model_tokenizer_mappings,\n+    use_cache_if_possible,\n+)\n \n \n logger = logging.get_logger(__name__)\n@@ -91,8 +97,9 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -119,22 +126,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return LayoutLMv3TokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return LayoutLMv3TokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n@@ -485,8 +500,8 @@ def test_padding_to_max_length(self):\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n@@ -940,7 +955,7 @@ def test_build_inputs_with_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Input tokens id\n                 words, boxes = self.get_words_and_boxes()\n@@ -1241,7 +1256,7 @@ def test_tokenization_python_rust_equals(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 words, boxes = self.get_words_and_boxes()\n \n@@ -1295,7 +1310,7 @@ def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 words, boxes = self.get_words_and_boxes()\n                 tokens_r = tokenizer_r.encode_plus(\n                     words,\n@@ -1593,7 +1608,7 @@ def test_padding_different_model_input_name(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n "
        },
        {
            "sha": "36f837a89f8b975c3e22965029042ea656918489",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -96,12 +96,13 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = LayoutXLMTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"UNwant\\u00e9d,running\"\n@@ -157,7 +158,7 @@ def test_split_special_tokens(self):\n             _, _, boxes = self.get_question_words_and_boxes()\n \n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_rust = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_rust = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=[special_token], split_special_tokens=True, **kwargs\n                 )\n                 tokenizer_py = self.tokenizer_class.from_pretrained(\n@@ -206,7 +207,7 @@ def test_sequence_builders(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 words, boxes = self.get_words_and_boxes()\n                 words[1] = tokenizer_r.mask_token\n@@ -536,8 +537,8 @@ def test_padding_to_max_length(self):\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n@@ -990,8 +991,8 @@ def test_build_inputs_with_special_tokens(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Input tokens id\n                 words, boxes = self.get_words_and_boxes()\n@@ -1292,7 +1293,7 @@ def test_tokenization_python_rust_equals(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 words, boxes = self.get_words_and_boxes()\n \n@@ -1346,7 +1347,7 @@ def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 words, boxes = self.get_words_and_boxes()\n                 tokens_r = tokenizer_r.encode_plus(\n                     words,\n@@ -1644,7 +1645,7 @@ def test_padding_different_model_input_name(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n \n@@ -1743,7 +1744,7 @@ def test_save_pretrained(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n "
        },
        {
            "sha": "a50acac048d0020b8aad19f3c57317bab28c6c04",
            "filename": "tests/models/led/test_tokenization_led.py",
            "status": "modified",
            "additions": 28,
            "deletions": 17,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_tokenization_led.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -14,13 +14,14 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import BatchEncoding, LEDTokenizer, LEDTokenizerFast\n from transformers.models.led.tokenization_led import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -30,8 +31,10 @@ class TestTokenizationLED(TokenizerTesterMixin, unittest.TestCase):\n     rust_tokenizer_class = LEDTokenizerFast\n     test_rust_tokenizer = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n         vocab = [\n             \"l\",\n             \"o\",\n@@ -56,22 +59,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return \"lower newer\", \"lower newer\"\n@@ -161,8 +172,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)"
        },
        {
            "sha": "2c0e15bffda82219f31efed1d20ac61fe98bd5ed",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -60,13 +60,14 @@ class LlamaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     from_pretrained_kwargs = {}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = LlamaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n         tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_tokenizers(self, **kwargs):\n         kwargs.update({\"pad_token\": \"<PAD>\"})\n@@ -149,8 +150,8 @@ def test_save_pretrained(self):\n         self.tokenizers_list += (self.rust_tokenizer_class, \"hf-internal-testing/llama-tokenizer\", {})\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n \n@@ -253,7 +254,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -263,7 +264,7 @@ def test_special_tokens_initialization(self):\n                 self.assertTrue(special_token_id in r_output)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name,\n                         additional_special_tokens=added_tokens,\n                         **kwargs,  # , from_slow=True <- unfortunately too slow to convert\n@@ -313,8 +314,8 @@ def test_add_prefix_space(self):\n         EXPECTED_WITH_SPACE = [1, 18637, 920, 526, 366, 2599]\n         EXPECTED_WO_SPACE = [1, 29950, 1032, 920, 526, 366, 2599]\n \n-        slow_ = self.tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=False, legacy=False)\n-        fast_ = self.rust_tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=False, legacy=False)\n+        slow_ = self.get_tokenizer(pretrained_name, add_prefix_space=False, legacy=False)\n+        fast_ = self.get_rust_tokenizer(pretrained_name, add_prefix_space=False, legacy=False)\n         self.assertEqual(slow_.encode(inputs), EXPECTED_WO_SPACE)\n         self.assertEqual(slow_.encode(inputs), fast_.encode(inputs))\n         self.assertEqual(slow_.tokenize(inputs), [\"H\", \"ey\", \"▁how\", \"▁are\", \"▁you\", \"▁doing\"])\n@@ -324,8 +325,8 @@ def test_add_prefix_space(self):\n             fast_.decode(EXPECTED_WO_SPACE, skip_special_tokens=True),\n         )\n \n-        slow_ = self.tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=True, legacy=False)\n-        fast_ = self.rust_tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=True, legacy=False)\n+        slow_ = self.get_tokenizer(pretrained_name, add_prefix_space=True, legacy=False)\n+        fast_ = self.get_rust_tokenizer(pretrained_name, add_prefix_space=True, legacy=False)\n         self.assertEqual(slow_.encode(inputs), EXPECTED_WITH_SPACE)\n         self.assertEqual(slow_.encode(inputs), fast_.encode(inputs))\n         self.assertEqual(slow_.tokenize(inputs), [\"▁Hey\", \"▁how\", \"▁are\", \"▁you\", \"▁doing\"])"
        },
        {
            "sha": "303a9ae2d091e88df9641f316a23e7ab265c6d12",
            "filename": "tests/models/longformer/test_tokenization_longformer.py",
            "status": "modified",
            "additions": 35,
            "deletions": 25,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -18,12 +18,13 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import AddedToken, LongformerTokenizer, LongformerTokenizerFast\n from transformers.models.longformer.tokenization_longformer import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -36,8 +37,9 @@ class LongformerTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     rust_tokenizer_class = LongformerTokenizerFast\n     test_rust_tokenizer = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -64,22 +66,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n@@ -173,8 +183,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n@@ -204,7 +214,7 @@ def test_embeded_special_tokens(self):\n \n     def test_change_add_prefix_space_and_trim_offsets_args(self):\n         for trim_offsets, add_prefix_space in itertools.product([True, False], repeat=2):\n-            tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+            tokenizer_r = self.get_rust_tokenizer(\n                 self.tmpdirname, use_fast=True, add_prefix_space=add_prefix_space, trim_offsets=trim_offsets\n             )\n \n@@ -224,7 +234,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                 text_of_1_token = \"hello\"  # `hello` is a token in the vocabulary of `pretrained_name`\n                 text = f\"{text_of_1_token} {text_of_1_token}\"\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=True, trim_offsets=True\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -234,7 +244,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (len(text_of_1_token) + 1, len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=True\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -244,7 +254,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (len(text_of_1_token) + 1, len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=True, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -254,7 +264,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (len(text_of_1_token), len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -276,7 +286,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                 #     (1 + len(text_of_1_token) + 1, 1 + len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 # )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=True\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -286,7 +296,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (1 + len(text_of_1_token) + 1, 1 + len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=True, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -296,7 +306,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (1 + len(text_of_1_token), 1 + len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)"
        },
        {
            "sha": "b935f3e49c4a17dec6a5a85c1da9b4b847e2a239",
            "filename": "tests/models/luke/test_tokenization_luke.py",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -14,12 +14,13 @@\n # limitations under the License.\n \n import unittest\n+from functools import lru_cache\n from typing import Tuple\n \n from transformers import AddedToken, LukeTokenizer\n from transformers.testing_utils import get_tests_dir, require_torch, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/vocab.json\")\n@@ -33,13 +34,17 @@ class LukeTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     from_pretrained_kwargs = {\"cls_token\": \"<s>\"}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n-        self.special_tokens_map = {\"entity_token_1\": \"<ent>\", \"entity_token_2\": \"<ent2>\"}\n+        cls.special_tokens_map = {\"entity_token_1\": \"<ent>\", \"entity_token_2\": \"<ent2>\"}\n \n-    def get_tokenizer(self, task=None, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, task=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n         tokenizer = LukeTokenizer(\n             vocab_file=SAMPLE_VOCAB,\n             merges_file=SAMPLE_MERGE_FILE,\n@@ -137,8 +142,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(\"{} ({})\".format(tokenizer.__class__.__name__, pretrained_name)):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)"
        },
        {
            "sha": "b634d259ed56a3a21a1c6ed45c631acb42ca5126",
            "filename": "tests/models/lxmert/test_tokenization_lxmert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flxmert%2Ftest_tokenization_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Flxmert%2Ftest_tokenization_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_tokenization_lxmert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -32,8 +32,9 @@ class LxmertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     space_between_special_tokens = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -50,8 +51,8 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):"
        },
        {
            "sha": "0632eaf24878caeffd831bb181b210e05a167b96",
            "filename": "tests/models/m2m_100/test_tokenization_m2m_100.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fm2m_100%2Ftest_tokenization_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fm2m_100%2Ftest_tokenization_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_tokenization_m2m_100.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -14,6 +14,7 @@\n \n import tempfile\n import unittest\n+from functools import lru_cache\n from pathlib import Path\n from shutil import copyfile\n \n@@ -32,7 +33,7 @@\n if is_sentencepiece_available():\n     from transformers.models.m2m_100.tokenization_m2m_100 import VOCAB_FILES_NAMES, save_json\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n if is_sentencepiece_available():\n@@ -54,21 +55,26 @@ class M2M100TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_seq2seq = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = [\"</s>\", \"<unk>\", \"▁This\", \"▁is\", \"▁a\", \"▁t\", \"est\", \"\\u0120\", \"<pad>\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-        save_dir = Path(self.tmpdirname)\n+        save_dir = Path(cls.tmpdirname)\n         save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab_file\"])\n         if not (save_dir / VOCAB_FILES_NAMES[\"spm_file\"]).exists():\n             copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"spm_file\"])\n \n-        tokenizer = M2M100Tokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer = M2M100Tokenizer.from_pretrained(cls.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n-    def get_tokenizer(self, **kwargs):\n-        return M2M100Tokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return M2M100Tokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return ("
        },
        {
            "sha": "03814663604eb70a1f5d7777c252ea5ed2207ecf",
            "filename": "tests/models/marian/test_tokenization_marian.py",
            "status": "modified",
            "additions": 15,
            "deletions": 8,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,6 +15,7 @@\n \n import tempfile\n import unittest\n+from functools import lru_cache\n from pathlib import Path\n from shutil import copyfile\n \n@@ -26,7 +27,7 @@\n if is_sentencepiece_available():\n     from transformers.models.marian.tokenization_marian import VOCAB_FILES_NAMES, save_json\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_SP = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -50,22 +51,28 @@ class MarianTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n         vocab = [\"</s>\", \"<unk>\", \"▁This\", \"▁is\", \"▁a\", \"▁t\", \"est\", \"\\u0120\", \"<pad>\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-        save_dir = Path(self.tmpdirname)\n+        save_dir = Path(cls.tmpdirname)\n         save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab\"])\n         save_json(mock_tokenizer_config, save_dir / VOCAB_FILES_NAMES[\"tokenizer_config_file\"])\n         if not (save_dir / VOCAB_FILES_NAMES[\"source_spm\"]).exists():\n             copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"source_spm\"])\n             copyfile(SAMPLE_SP, save_dir / VOCAB_FILES_NAMES[\"target_spm\"])\n \n-        tokenizer = MarianTokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer = MarianTokenizer.from_pretrained(cls.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n-    def get_tokenizer(self, **kwargs) -> MarianTokenizer:\n-        return MarianTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> MarianTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return MarianTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return ("
        },
        {
            "sha": "c7cb39964facd9182bfabd9b69feeb4500a43631",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -50,26 +50,27 @@ class MarkupLMTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_kwargs = {\"cls_token\": \"<s>\"}\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"i\", \"d\", \"n\", \"\\u0120\", \"\\u0120l\", \"\\u0120n\", \"\\u0120lo\", \"\\u0120low\", \"er\", \"\\u0120lowest\", \"\\u0120newer\", \"\\u0120wider\", \"\\u0120hello\", \"\\u0120world\", \"<unk>\",]  # fmt: skip\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.tags_dict = {\"a\": 0, \"abbr\": 1, \"acronym\": 2, \"address\": 3}\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.tags_dict = {\"a\": 0, \"abbr\": 1, \"acronym\": 2, \"address\": 3}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        self.tokenizer_config_file = os.path.join(self.tmpdirname, \"tokenizer_config.json\")\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        cls.tokenizer_config_file = os.path.join(cls.tmpdirname, \"tokenizer_config.json\")\n \n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n-        with open(self.tokenizer_config_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(json.dumps({\"tags_dict\": self.tags_dict}))\n+        with open(cls.tokenizer_config_file, \"w\", encoding=\"utf-8\") as fp:\n+            fp.write(json.dumps({\"tags_dict\": cls.tags_dict}))\n \n     def get_nodes_and_xpaths(self):\n         nodes = [\"hello\", \"world\"]\n@@ -421,8 +422,8 @@ def test_padding_to_max_length(self):\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n@@ -828,8 +829,8 @@ def test_build_inputs_with_special_tokens(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Input tokens id\n                 nodes, xpaths = self.get_nodes_and_xpaths()\n@@ -1010,7 +1011,7 @@ def test_token_type_ids(self):\n     def test_offsets_mapping(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 text = [\"a\", \"wonderful\", \"test\"]\n                 xpaths = [\"html/body\" for _ in range(len(text))]\n@@ -1125,7 +1126,7 @@ def test_tokenization_python_rust_equals(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 nodes, xpaths = self.get_nodes_and_xpaths()\n \n@@ -1187,7 +1188,7 @@ def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 nodes, xpaths = self.get_nodes_and_xpaths()\n                 tokens_r = tokenizer_r.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n                 tokens_p = tokenizer_p.encode_plus(nodes, xpaths=xpaths, add_special_tokens=True)\n@@ -1490,7 +1491,7 @@ def test_padding_different_model_input_name(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n "
        },
        {
            "sha": "f219965ae52febd1f2236d6a8f0141028aed6074",
            "filename": "tests/models/mbart/test_tokenization_mbart.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmbart%2Ftest_tokenization_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmbart%2Ftest_tokenization_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_tokenization_mbart.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -47,12 +47,13 @@ class MBartTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = MBartTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_full_tokenizer(self):\n         tokenizer = MBartTokenizer(SAMPLE_VOCAB, keep_accents=True)\n@@ -139,8 +140,8 @@ def test_save_pretrained(self):\n         self.tokenizers_list[0] = (self.rust_tokenizer_class, \"hf-internal-testing/tiny-random-mbart\", {})\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n "
        },
        {
            "sha": "bed8b8cb376f5c5083cf768d67f3d6fa1ea47073",
            "filename": "tests/models/mbart50/test_tokenization_mbart50.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmbart50%2Ftest_tokenization_mbart50.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmbart50%2Ftest_tokenization_mbart50.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart50%2Ftest_tokenization_mbart50.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -47,12 +47,13 @@ class MBart50TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = MBart50Tokenizer(SAMPLE_VOCAB, src_lang=\"en_XX\", tgt_lang=\"ro_RO\", keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\"\n@@ -117,8 +118,8 @@ def test_save_pretrained(self):\n         self.tokenizers_list[0] = (self.rust_tokenizer_class, \"hf-internal-testing/tiny-random-mbart50\", {})\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n "
        },
        {
            "sha": "2d021606ffccafed9622aed738adc2b30ae5c5b1",
            "filename": "tests/models/mgp_str/test_tokenization_mgp_str.py",
            "status": "modified",
            "additions": 13,
            "deletions": 7,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmgp_str%2Ftest_tokenization_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmgp_str%2Ftest_tokenization_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_tokenization_mgp_str.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,12 +17,13 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import MgpstrTokenizer\n from transformers.models.mgp_str.tokenization_mgp_str import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -33,18 +34,23 @@ class MgpstrTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_kwargs = {}\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = ['[GO]', '[s]', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']  # fmt: skip\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n-    def get_tokenizer(self, **kwargs):\n-        return MgpstrTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return MgpstrTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"tester\""
        },
        {
            "sha": "a86ea690a468bb2ada1505d816674a19298785ed",
            "filename": "tests/models/mluke/test_tokenization_mluke.py",
            "status": "modified",
            "additions": 13,
            "deletions": 8,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,12 +15,13 @@\n \n \n import unittest\n+from functools import lru_cache\n from typing import Tuple\n \n from transformers.models.mluke.tokenization_mluke import MLukeTokenizer\n from transformers.testing_utils import get_tests_dir, require_torch, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -33,13 +34,17 @@ class MLukeTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     from_pretrained_kwargs = {\"cls_token\": \"<s>\"}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n-        self.special_tokens_map = {\"entity_token_1\": \"<ent>\", \"entity_token_2\": \"<ent2>\"}\n+        cls.special_tokens_map = {\"entity_token_1\": \"<ent>\", \"entity_token_2\": \"<ent2>\"}\n \n-    def get_tokenizer(self, task=None, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, task=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n         kwargs.update({\"task\": task})\n         tokenizer = MLukeTokenizer(vocab_file=SAMPLE_VOCAB, entity_vocab_file=SAMPLE_ENTITY_VOCAB, **kwargs)\n         return tokenizer\n@@ -100,8 +105,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(\"{} ({})\".format(tokenizer.__class__.__name__, pretrained_name)):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)"
        },
        {
            "sha": "e0f4bff647ec17911f60fbb1dc9150459d61f413",
            "filename": "tests/models/mobilebert/test_tokenization_mobilebert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmobilebert%2Ftest_tokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmobilebert%2Ftest_tokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_tokenization_mobilebert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -41,8 +41,9 @@ class MobileBERTTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_filter = filter_non_english\n     pre_trained_model_path = \"google/mobilebert-uncased\"\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -61,13 +62,13 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n-        self.tokenizers_list = [\n-            (tokenizer_def[0], self.pre_trained_model_path, tokenizer_def[2])  # else the 'google/' prefix is stripped\n-            for tokenizer_def in self.tokenizers_list\n+        cls.tokenizers_list = [\n+            (tokenizer_def[0], cls.pre_trained_model_path, tokenizer_def[2])  # else the 'google/' prefix is stripped\n+            for tokenizer_def in cls.tokenizers_list\n         ]\n \n     # Copied from tests.models.bert.test_tokenization_bert.BertTokenizationTest.get_input_output_texts\n@@ -275,7 +276,7 @@ def test_sequence_builders(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 sentence = f\"A, naïve {tokenizer_r.mask_token} AllenNLP sentence.\"\n                 tokens = tokenizer_r.encode_plus(\n@@ -331,8 +332,8 @@ def test_change_tokenize_chinese_chars(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n@@ -345,8 +346,8 @@ def test_change_tokenize_chinese_chars(self):\n                 self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)"
        },
        {
            "sha": "740cfd55d37193f5414b60eba8973a0c088544dc",
            "filename": "tests/models/moshi/test_tokenization_moshi.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_tokenization_moshi.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -51,8 +51,9 @@ class MoshiTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     from_pretrained_kwargs = {}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = PreTrainedTokenizerFast(\n@@ -62,10 +63,11 @@ def setUp(self):\n             eos_token=\"</s>\",\n         )\n         tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n-    def get_rust_tokenizer(self, **kwargs) -> PreTrainedTokenizerFast:\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizerFast:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     @unittest.skip(reason=\"No slow tokenizer\")\n     def test_added_tokens_serialization(self):"
        },
        {
            "sha": "2b934a9ed68c19670fb05ba89c0c6b51d981ce28",
            "filename": "tests/models/mpnet/test_tokenization_mpnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmpnet%2Ftest_tokenization_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmpnet%2Ftest_tokenization_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpnet%2Ftest_tokenization_mpnet.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -32,8 +32,9 @@ class MPNetTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     space_between_special_tokens = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -52,8 +53,8 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):"
        },
        {
            "sha": "af44cc961c5b3e3f2d622dba3df1f25c3db12ed0",
            "filename": "tests/models/mvp/test_tokenization_mvp.py",
            "status": "modified",
            "additions": 28,
            "deletions": 17,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -14,13 +14,14 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import BatchEncoding, MvpTokenizer, MvpTokenizerFast\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, require_torch\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors\n+from ...test_tokenization_common import TokenizerTesterMixin, filter_roberta_detectors, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -32,8 +33,10 @@ class TestTokenizationMvp(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_filter = filter_roberta_detectors\n     # from_pretrained_kwargs = {'add_prefix_space': True}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+\n         vocab = [\n             \"l\",\n             \"o\",\n@@ -58,22 +61,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return \"lower newer\", \"lower newer\"\n@@ -153,8 +164,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)"
        },
        {
            "sha": "aab67978f21c57569c6bce4919f84671289f3a8a",
            "filename": "tests/models/myt5/test_tokenization_myt5.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmyt5%2Ftest_tokenization_myt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fmyt5%2Ftest_tokenization_myt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmyt5%2Ftest_tokenization_myt5.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n from transformers import MyT5Tokenizer\n+from transformers.testing_utils import slow\n from transformers.utils import is_tf_available, is_torch_available\n \n from ...test_tokenization_common import TokenizerTesterMixin\n@@ -86,15 +87,14 @@ def test_unrecognized_byte(self):\n         self.assertEqual(decompose_rewriter.rewrite_bytes(in_hex), out_hex)\n \n \n+# This is way too slow, let's not run it on CircleCI. When trying to use cache, we get OOM and worker(s) crashed.\n+@slow\n class MyT5TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = MyT5Tokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n-\n-    def get_tokenizer(self, **kwargs) -> MyT5Tokenizer:\n-        return self.tokenizer_class.from_pretrained(\"Tomlim/myt5-base\", **kwargs)\n+    def get_tokenizer(cls, **kwargs) -> MyT5Tokenizer:\n+        return cls.tokenizer_class.from_pretrained(\"Tomlim/myt5-base\", **kwargs)\n \n     @unittest.skip(reason=\"inputs cannot be pretokenized as ids depend on whole input string\")\n     def test_pretokenized_inputs(self):"
        },
        {
            "sha": "0e9f37d5945e468b3635fa70deae3f3963af0d97",
            "filename": "tests/models/nllb/test_tokenization_nllb.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fnllb%2Ftest_tokenization_nllb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fnllb%2Ftest_tokenization_nllb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb%2Ftest_tokenization_nllb.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -56,12 +56,13 @@ class NllbTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     from_pretrained_kwargs = {}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = NllbTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_full_tokenizer(self):\n         tokenizer = NllbTokenizer(SAMPLE_VOCAB, keep_accents=True)\n@@ -143,8 +144,8 @@ def test_save_pretrained(self):\n         self.tokenizers_list[0] = (self.rust_tokenizer_class, \"hf-internal-testing/tiny-random-nllb\", {})\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n \n@@ -262,7 +263,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -272,7 +273,7 @@ def test_special_tokens_initialization(self):\n                 self.assertTrue(special_token_id in r_output)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name,\n                         additional_special_tokens=added_tokens,\n                         **kwargs,  # , from_slow=True <- unfortunately too slow to convert"
        },
        {
            "sha": "c5da1f0291b5b27c83c8b41a42798085dc250d8b",
            "filename": "tests/models/nougat/test_tokenization_nougat.py",
            "status": "modified",
            "additions": 18,
            "deletions": 9,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fnougat%2Ftest_tokenization_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fnougat%2Ftest_tokenization_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_tokenization_nougat.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,13 +13,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import copy\n import unittest\n+from functools import lru_cache\n \n from transformers import NougatTokenizerFast\n from transformers.models.nougat.tokenization_nougat_fast import markdown_compatible, normalize_list_like_lines\n from transformers.testing_utils import require_levenshtein, require_nltk, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -33,19 +35,26 @@ class NougatTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_vocab_key = \"tokenizer_file\"\n     special_tokens_map = {\"bos_token\": \"<s>\", \"eos_token\": \"</s>\", \"unk_token\": \"<unk>\", \"pad_token\": \"<pad>\"}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = NougatTokenizerFast.from_pretrained(\"facebook/nougat-base\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return NougatTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        _kwargs = copy.deepcopy(cls.special_tokens_map)\n+        _kwargs.update(kwargs)\n+        kwargs = _kwargs\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return NougatTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def test_padding(self, max_length=6):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n                 # Simple input\n                 sentence1 = \"This is a simple input\"\n                 sentence2 = [\"This is a simple input 1\", \"This is a simple input 2\"]"
        },
        {
            "sha": "e91765f93d48e560f94e5017bad31c525b971c87",
            "filename": "tests/models/openai/test_tokenization_openai.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fopenai%2Ftest_tokenization_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fopenai%2Ftest_tokenization_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_tokenization_openai.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -35,8 +35,9 @@ class OpenAIGPTTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -65,11 +66,11 @@ def setUp(self):\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"l o\", \"lo w\", \"e r</w>\", \"\"]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\") as fp:\n             fp.write(json.dumps(vocab_tokens))\n-        with open(self.merges_file, \"w\") as fp:\n+        with open(cls.merges_file, \"w\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n     def get_input_output_texts(self, tokenizer):\n@@ -90,7 +91,7 @@ def test_full_tokenizer(self):\n     def test_padding(self, max_length=15):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "35292d5f240d42b0004f11c35ef6a75eff9f81b0",
            "filename": "tests/models/pegasus/test_tokenization_pegasus.py",
            "status": "modified",
            "additions": 26,
            "deletions": 15,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fpegasus%2Ftest_tokenization_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fpegasus%2Ftest_tokenization_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_tokenization_pegasus.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,12 +13,13 @@\n # limitations under the License.\n \n import unittest\n+from functools import lru_cache\n \n from transformers import PegasusTokenizer, PegasusTokenizerFast\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, require_torch, slow\n from transformers.utils import cached_property\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece_no_bos.model\")\n@@ -33,19 +34,24 @@ class PegasusTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = PegasusTokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @cached_property\n     def _large_tokenizer(self):\n         return PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n \n-    def get_tokenizer(self, **kwargs) -> PegasusTokenizer:\n-        return PegasusTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PegasusTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return PegasusTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return (\"This is a test\", \"This is a test\")\n@@ -70,8 +76,8 @@ def test_vocab_size(self):\n         self.assertEqual(self.get_tokenizer().vocab_size, 1_103)\n \n     def test_mask_tokens_rust_pegasus(self):\n-        rust_tokenizer = self.rust_tokenizer_class.from_pretrained(self.tmpdirname)\n-        py_tokenizer = self.tokenizer_class.from_pretrained(self.tmpdirname)\n+        rust_tokenizer = self.get_rust_tokenizer(self.tmpdirname)\n+        py_tokenizer = self.get_tokenizer(self.tmpdirname)\n         raw_input_str = (\n             \"Let's see which <unk> is the better <unk_token_11> one <mask_1> It seems like this <mask_2> was important\"\n             \" </s> <pad> <pad> <pad>\"\n@@ -138,26 +144,31 @@ class BigBirdPegasusTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = PegasusTokenizer(SAMPLE_VOCAB, offset=0, mask_token_sent=None, mask_token=\"[MASK]\")\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @cached_property\n     def _large_tokenizer(self):\n         return PegasusTokenizer.from_pretrained(\"google/bigbird-pegasus-large-arxiv\")\n \n-    def get_tokenizer(self, **kwargs) -> PegasusTokenizer:\n-        return PegasusTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PegasusTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return PegasusTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         return (\"This is a test\", \"This is a test\")\n \n     def test_mask_tokens_rust_pegasus(self):\n-        rust_tokenizer = self.rust_tokenizer_class.from_pretrained(self.tmpdirname)\n-        py_tokenizer = self.tokenizer_class.from_pretrained(self.tmpdirname)\n+        rust_tokenizer = self.get_rust_tokenizer(self.tmpdirname)\n+        py_tokenizer = self.get_tokenizer(self.tmpdirname)\n         raw_input_str = (\n             \"Let's see which <unk> is the better <unk_token> one [MASK] It seems like this [MASK] was important </s>\"\n             \" <pad> <pad> <pad>\""
        },
        {
            "sha": "16c279ae18a3e02dc4be39a9a6121ed18ef9f57a",
            "filename": "tests/models/perceiver/test_tokenization_perceiver.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fperceiver%2Ftest_tokenization_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fperceiver%2Ftest_tokenization_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_tokenization_perceiver.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -19,12 +19,13 @@\n import shutil\n import tempfile\n import unittest\n+from functools import lru_cache\n from typing import Tuple\n \n from transformers import AddedToken, BatchEncoding, PerceiverTokenizer\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n if is_torch_available():\n@@ -40,17 +41,22 @@ class PerceiverTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = PerceiverTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = PerceiverTokenizer()\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @cached_property\n     def perceiver_tokenizer(self):\n         return PerceiverTokenizer.from_pretrained(\"deepmind/language-perceiver\")\n \n-    def get_tokenizer(self, **kwargs) -> PerceiverTokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PerceiverTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5) -> Tuple[str, list]:\n         # XXX The default common tokenizer tests assume that every ID is decodable on its own."
        },
        {
            "sha": "323355e3cab345fa8d48eb06f1309bee2c578b4e",
            "filename": "tests/models/phobert/test_tokenization_phobert.py",
            "status": "modified",
            "additions": 17,
            "deletions": 11,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,38 +15,44 @@\n \n import os\n import unittest\n+from functools import lru_cache\n \n from transformers.models.phobert.tokenization_phobert import VOCAB_FILES_NAMES, PhobertTokenizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class PhobertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"vinai/phobert-base\"\n     tokenizer_class = PhobertTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\"T@@\", \"i\", \"I\", \"R@@\", \"r\", \"e@@\"]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"l à</w>\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n \n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             for token in vocab_tokens:\n                 fp.write(f\"{token} {vocab_tokens[token]}\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return PhobertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return PhobertTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"Tôi là VinAI Research\""
        },
        {
            "sha": "1ac7d1a7d17133787277f138b8e9039d3146f063",
            "filename": "tests/models/plbart/test_tokenization_plbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fplbart%2Ftest_tokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fplbart%2Ftest_tokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_tokenization_plbart.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -45,12 +45,13 @@ class PLBartTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     rust_tokenizer_class = None\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = PLBartTokenizer(SAMPLE_VOCAB, language_codes=\"base\", keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_full_base_tokenizer(self):\n         tokenizer = PLBartTokenizer(SAMPLE_VOCAB, language_codes=\"base\", keep_accents=True)"
        },
        {
            "sha": "b271c898da1d363120793eaba34e48361a20da35",
            "filename": "tests/models/prophetnet/test_tokenization_prophetnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fprophetnet%2Ftest_tokenization_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fprophetnet%2Ftest_tokenization_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_tokenization_prophetnet.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -36,8 +36,9 @@ class ProphetNetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = ProphetNetTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -56,8 +57,8 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):"
        },
        {
            "sha": "e37ecac9694adadabb94a8cc9a9ea9dde0eab88f",
            "filename": "tests/models/qwen2/test_tokenization_qwen2.py",
            "status": "modified",
            "additions": 30,
            "deletions": 15,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fqwen2%2Ftest_tokenization_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fqwen2%2Ftest_tokenization_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_tokenization_qwen2.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -14,15 +14,17 @@\n # limitations under the License.\n \n \n+import copy\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import AddedToken, Qwen2Tokenizer, Qwen2TokenizerFast\n from transformers.models.qwen2.tokenization_qwen2 import VOCAB_FILES_NAMES, bytes_to_unicode\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -36,8 +38,9 @@ class Qwen2TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_kwargs = None\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # this make sure the vocabuary is complete at the byte level.\n         vocab = list(bytes_to_unicode().values())\n@@ -81,22 +84,34 @@ def setUp(self):\n             \"# #\",\n         ]\n \n-        self.special_tokens_map = {\"eos_token\": \"<|endoftext|>\"}\n+        cls.special_tokens_map = {\"eos_token\": \"<|endoftext|>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return Qwen2Tokenizer.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return Qwen2TokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        _kwargs = copy.deepcopy(cls.special_tokens_map)\n+        _kwargs.update(kwargs)\n+        kwargs = _kwargs\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return Qwen2Tokenizer.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        _kwargs = copy.deepcopy(cls.special_tokens_map)\n+        _kwargs.update(kwargs)\n+        kwargs = _kwargs\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return Qwen2TokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         # this case should cover"
        },
        {
            "sha": "d5e3901b3fb79278ddb84c54f7b3c427220b00ba",
            "filename": "tests/models/reformer/test_tokenization_reformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Freformer%2Ftest_tokenization_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Freformer%2Ftest_tokenization_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_tokenization_reformer.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -34,11 +34,12 @@ class ReformerTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_seq2seq = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         tokenizer = ReformerTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\"\n@@ -84,7 +85,7 @@ def test_rust_and_python_full_tokenizers(self):\n     def test_padding(self, max_length=15):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 # Simple input\n                 s = \"This is a simple input\""
        },
        {
            "sha": "c2ee3619f8e207ea0157fef62a42dd4c15353e9f",
            "filename": "tests/models/rembert/test_tokenization_rembert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Frembert%2Ftest_tokenization_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Frembert%2Ftest_tokenization_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_tokenization_rembert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -39,11 +39,12 @@ class RemBertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece_ignore_case = True\n     pre_trained_model_path = \"google/rembert\"\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         tokenizer = RemBertTokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     # Copied from ReformerTokenizationTest.get_input_output_texts\n     def get_input_output_texts(self, tokenizer):\n@@ -222,7 +223,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n \n                 with self.subTest(\"Hub -> Fast: Test loading a fast tokenizer from the hub)\"):\n                     if self.rust_tokenizer_class is not None:\n-                        tokenizer_fast = self.rust_tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n+                        tokenizer_fast = self.get_rust_tokenizer(pretrained_name, eos_token=new_eos)\n                         self.assertEqual(tokenizer_fast._special_tokens_map[\"eos_token\"], new_eos)\n                         self.assertIn(new_eos, list(tokenizer_fast.added_tokens_decoder.values()))\n                         # We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright"
        },
        {
            "sha": "e2760f646ccef4574b857e653fbed6c7b0269789",
            "filename": "tests/models/roberta/test_tokenization_roberta.py",
            "status": "modified",
            "additions": 35,
            "deletions": 25,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -18,12 +18,13 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n \n from transformers import AddedToken, RobertaTokenizer, RobertaTokenizerFast\n from transformers.models.roberta.tokenization_roberta import VOCAB_FILES_NAMES\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_tokenizers\n@@ -34,8 +35,9 @@ class RobertaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     from_pretrained_kwargs = {\"cls_token\": \"<s>\"}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -62,22 +64,30 @@ def setUp(self):\n         ]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"#version: 0.2\", \"\\u0120 l\", \"\\u0120l o\", \"\\u0120lo w\", \"e r\", \"\"]\n-        self.special_tokens_map = {\"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(self.merges_file, \"w\", encoding=\"utf-8\") as fp:\n+        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"lower newer\"\n@@ -171,8 +181,8 @@ def test_pretokenized_inputs(self):\n     def test_embeded_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n                 tokens_p = tokenizer_p.encode_plus(sentence, add_special_tokens=True, return_token_type_ids=True)\n@@ -202,7 +212,7 @@ def test_embeded_special_tokens(self):\n \n     def test_change_add_prefix_space_and_trim_offsets_args(self):\n         for trim_offsets, add_prefix_space in itertools.product([True, False], repeat=2):\n-            tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+            tokenizer_r = self.get_rust_tokenizer(\n                 self.tmpdirname, use_fast=True, add_prefix_space=add_prefix_space, trim_offsets=trim_offsets\n             )\n \n@@ -222,7 +232,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                 text_of_1_token = \"hello\"  # `hello` is a token in the vocabulary of `pretrained_name`\n                 text = f\"{text_of_1_token} {text_of_1_token}\"\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=True, trim_offsets=True\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -232,7 +242,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (len(text_of_1_token) + 1, len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=True\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -242,7 +252,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (len(text_of_1_token) + 1, len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=True, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -252,7 +262,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (len(text_of_1_token), len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -274,7 +284,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                 #     (1 + len(text_of_1_token) + 1, 1 + len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 # )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=True\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -284,7 +294,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (1 + len(text_of_1_token) + 1, 1 + len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=True, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)\n@@ -294,7 +304,7 @@ def test_offsets_mapping_with_different_add_prefix_space_and_trim_space_argument\n                     (1 + len(text_of_1_token), 1 + len(text_of_1_token) + 1 + len(text_of_1_token)),\n                 )\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, use_fast=True, add_prefix_space=False, trim_offsets=False\n                 )\n                 encoding = tokenizer_r(text, return_offsets_mapping=True, add_special_tokens=False)"
        },
        {
            "sha": "885975b8df34ca05aa1ea820e4c549559645826c",
            "filename": "tests/models/roc_bert/test_tokenization_roc_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Froc_bert%2Ftest_tokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Froc_bert%2Ftest_tokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_tokenization_roc_bert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -41,23 +41,24 @@ class BertTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     space_between_special_tokens = True\n     from_pretrained_filter = filter_non_english\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\", \"你\", \"好\", \"是\", \"谁\", \"a\", \"b\", \"c\", \"d\"]\n         word_shape = {}\n         word_pronunciation = {}\n         for i, value in enumerate(vocab_tokens):\n             word_shape[value] = i\n             word_pronunciation[value] = i\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.word_shape_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"word_shape_file\"])\n-        self.word_pronunciation_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"word_pronunciation_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.word_shape_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"word_shape_file\"])\n+        cls.word_pronunciation_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"word_pronunciation_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n-        with open(self.word_shape_file, \"w\", encoding=\"utf-8\") as word_shape_writer:\n+        with open(cls.word_shape_file, \"w\", encoding=\"utf-8\") as word_shape_writer:\n             json.dump(word_shape, word_shape_writer, ensure_ascii=False)\n-        with open(self.word_pronunciation_file, \"w\", encoding=\"utf-8\") as word_pronunciation_writer:\n+        with open(cls.word_pronunciation_file, \"w\", encoding=\"utf-8\") as word_pronunciation_writer:\n             json.dump(word_pronunciation, word_pronunciation_writer, ensure_ascii=False)\n \n     def test_full_tokenizer(self):\n@@ -204,7 +205,7 @@ def test_clean_text(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 sentence = f\"A, naïve {tokenizer_r.mask_token} AllenNLP sentence.\"\n                 tokens = tokenizer_r.encode_plus(\n@@ -260,8 +261,8 @@ def test_change_tokenize_chinese_chars(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 kwargs[\"tokenize_chinese_chars\"] = True\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n@@ -274,8 +275,8 @@ def test_change_tokenize_chinese_chars(self):\n                 self.assertListEqual(tokens_without_spe_char_r, list_of_commun_chinese_char)\n \n                 kwargs[\"tokenize_chinese_chars\"] = False\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 ids_without_spe_char_r = tokenizer_r.encode(text_with_chinese_char, add_special_tokens=False)\n                 ids_without_spe_char_p = tokenizer_p.encode(text_with_chinese_char, add_special_tokens=False)"
        },
        {
            "sha": "e6db4f0d09b2a9cbb120678e6b58e3375a6e9d3c",
            "filename": "tests/models/roformer/test_tokenization_roformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 9,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Froformer%2Ftest_tokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Froformer%2Ftest_tokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_tokenization_roformer.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -15,11 +15,12 @@\n \n import tempfile\n import unittest\n+from functools import lru_cache\n \n from transformers import RoFormerTokenizer, RoFormerTokenizerFast\n from transformers.testing_utils import require_rjieba, require_tokenizers\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_rjieba\n@@ -31,14 +32,25 @@ class RoFormerTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     space_between_special_tokens = True\n     test_rust_tokenizer = True\n \n-    def setUp(self):\n-        super().setUp()\n-\n-    def get_tokenizer(self, **kwargs):\n-        return self.tokenizer_class.from_pretrained(\"junnyu/roformer_chinese_base\", **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs):\n-        return self.rust_tokenizer_class.from_pretrained(\"junnyu/roformer_chinese_base\", **kwargs)\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        tokenizer = cls.tokenizer_class.from_pretrained(\"junnyu/roformer_chinese_base\")\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def get_chinese_input_output_texts(self):\n         input_text = \"永和服装饰品有限公司,今天天气非常好\""
        },
        {
            "sha": "f55be02e172b893c5a371653c3697308efa2ec35",
            "filename": "tests/models/seamless_m4t/test_tokenization_seamless_m4t.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_tokenization_seamless_m4t.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -59,12 +59,13 @@ class SeamlessM4TTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     from_pretrained_kwargs = {}\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = SeamlessM4TTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_full_tokenizer(self):\n         tokenizer = SeamlessM4TTokenizer(SAMPLE_VOCAB, keep_accents=True)\n@@ -353,7 +354,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -363,7 +364,7 @@ def test_special_tokens_initialization(self):\n                 self.assertTrue(special_token_id in r_output)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name,\n                         additional_special_tokens=added_tokens,\n                         **kwargs,  # , from_slow=True <- unfortunately too slow to convert"
        },
        {
            "sha": "f4bc56c5e334152d5dfd4a96f6c5b5b3bc8850b5",
            "filename": "tests/models/siglip/test_tokenization_siglip.py",
            "status": "modified",
            "additions": 14,
            "deletions": 10,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_tokenization_siglip.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,12 +17,13 @@\n import os\n import tempfile\n import unittest\n+from functools import lru_cache\n \n from transformers import SPIECE_UNDERLINE, AddedToken, BatchEncoding, SiglipTokenizer\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -44,13 +45,13 @@ class SiglipTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = True\n     test_sentencepiece_ignore_case = True\n \n-    # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.setUp with T5->Siglip\n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = SiglipTokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.test_convert_token_and_id with T5->Siglip\n     def test_convert_token_and_id(self):\n@@ -135,9 +136,12 @@ def test_full_tokenizer(self):\n     def siglip_tokenizer(self):\n         return SiglipTokenizer.from_pretrained(\"google/siglip-base-patch16-224\")\n \n-    # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.get_tokenizer with T5->Siglip\n-    def get_tokenizer(self, **kwargs) -> SiglipTokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> SiglipTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     # Copied from tests.models.t5.test_tokenization_t5.T5TokenizationTest.test_rust_and_python_full_tokenizers with T5->Siglip\n     def test_rust_and_python_full_tokenizers(self):\n@@ -227,10 +231,10 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [f\"<extra_id_{i}>\" for i in range(100)] + [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n-                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_cr = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs, from_slow=True\n                 )\n                 tokenizer_p = self.tokenizer_class.from_pretrained("
        },
        {
            "sha": "3fc2926b62e97d4c47be3e1262e4e881852c3b58",
            "filename": "tests/models/speech_to_text/test_tokenization_speech_to_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fspeech_to_text%2Ftest_tokenization_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fspeech_to_text%2Ftest_tokenization_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_tokenization_speech_to_text.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -42,8 +42,9 @@ class SpeechToTextTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         spm_model = sp.SentencePieceProcessor()\n         spm_model.Load(SAMPLE_VOCAB)\n@@ -52,13 +53,13 @@ def setUp(self):\n         vocab += [spm_model.IdToPiece(id_) for id_ in range(len(spm_model))]\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n \n-        save_dir = Path(self.tmpdirname)\n+        save_dir = Path(cls.tmpdirname)\n         save_json(vocab_tokens, save_dir / VOCAB_FILES_NAMES[\"vocab_file\"])\n         if not (save_dir / VOCAB_FILES_NAMES[\"spm_file\"]).exists():\n             copyfile(SAMPLE_VOCAB, save_dir / VOCAB_FILES_NAMES[\"spm_file\"])\n \n-        tokenizer = Speech2TextTokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer = Speech2TextTokenizer.from_pretrained(cls.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "026fd1d2f48e8560d8d0dd438ec9ef906f060a42",
            "filename": "tests/models/speecht5/test_tokenization_speecht5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fspeecht5%2Ftest_tokenization_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fspeecht5%2Ftest_tokenization_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_tokenization_speecht5.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -35,8 +35,9 @@ class SpeechT5TokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = False\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = SpeechT5Tokenizer(SAMPLE_VOCAB)\n@@ -46,7 +47,7 @@ def setUp(self):\n         tokenizer.add_special_tokens({\"mask_token\": mask_token})\n         tokenizer.add_tokens([\"<ctc_blank>\"])\n \n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"this is a test\""
        },
        {
            "sha": "f654e2e5b73ef5cbbea2852804d52fe1338e6753",
            "filename": "tests/models/splinter/test_tokenization_splinter.py",
            "status": "modified",
            "additions": 20,
            "deletions": 10,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsplinter%2Ftest_tokenization_splinter.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,8 +13,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import unittest\n+from functools import lru_cache\n \n-from tests.test_tokenization_common import TokenizerTesterMixin\n+from tests.test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n from transformers import SplinterTokenizerFast, is_tf_available, is_torch_available\n from transformers.models.splinter import SplinterTokenizer\n from transformers.testing_utils import get_tests_dir, slow\n@@ -40,20 +41,29 @@ class SplinterTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     pre_trained_model_path = \"tau/splinter-base\"\n \n     # Copied from transformers.models.siglip.SiglipTokenizationTest.setUp\n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = SplinterTokenizer(SAMPLE_VOCAB)\n         tokenizer.vocab[\"[UNK]\"] = len(tokenizer.vocab)\n         tokenizer.vocab[\"[QUESTION]\"] = len(tokenizer.vocab)\n         tokenizer.vocab[\".\"] = len(tokenizer.vocab)\n         tokenizer.add_tokens(\"this is a test thou shall not determine rigor truly\".split())\n-        tokenizer.save_pretrained(self.tmpdirname)\n-\n-    def get_tokenizer(self, **kwargs) -> SplinterTokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n-\n-    def get_rust_tokenizer(self, **kwargs) -> SplinterTokenizerFast:\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> SplinterTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> SplinterTokenizerFast:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     # Copied from transformers.models.siglip.SiglipTokenizationTest.test_get_vocab\n     def test_get_vocab(self):"
        },
        {
            "sha": "0a75e768ccd3e2d5d924d3ff30200a269e8249d6",
            "filename": "tests/models/squeezebert/test_tokenization_squeezebert.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fsqueezebert%2Ftest_tokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fsqueezebert%2Ftest_tokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsqueezebert%2Ftest_tokenization_squeezebert.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,22 +13,31 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import lru_cache\n \n from transformers import SqueezeBertTokenizer, SqueezeBertTokenizerFast\n from transformers.testing_utils import require_tokenizers, slow\n \n-from ..bert.test_tokenization_bert import BertTokenizationTest\n+from ...test_tokenization_common import use_cache_if_possible\n+\n+# Avoid import `BertTokenizationTest` directly as it will run as `test_tokenization_squeezebert.py::BertTokenizationTest`\n+# together with `test_tokenization_bert.py::BertTokenizationTest`.\n+from ..bert import test_tokenization_bert\n \n \n @require_tokenizers\n-class SqueezeBertTokenizationTest(BertTokenizationTest):\n+class SqueezeBertTokenizationTest(test_tokenization_bert.BertTokenizationTest):\n     tokenizer_class = SqueezeBertTokenizer\n     rust_tokenizer_class = SqueezeBertTokenizerFast\n     test_rust_tokenizer = True\n     from_pretrained_id = \"squeezebert/squeezebert-uncased\"\n \n-    def get_rust_tokenizer(self, **kwargs):\n-        return SqueezeBertTokenizerFast.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs):\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return SqueezeBertTokenizerFast.from_pretrained(pretrained_name, **kwargs)\n \n     @slow\n     def test_sequence_builders(self):"
        },
        {
            "sha": "aba5dde8cb9537e121b0071de0bb78baa025666e",
            "filename": "tests/models/t5/test_tokenization_t5.py",
            "status": "modified",
            "additions": 24,
            "deletions": 16,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_tokenization_t5.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,12 +17,13 @@\n import re\n import tempfile\n import unittest\n+from functools import lru_cache\n \n from transformers import SPIECE_UNDERLINE, AddedToken, BatchEncoding, T5Tokenizer, T5TokenizerFast\n from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_seqio, require_tokenizers, slow\n from transformers.utils import cached_property, is_tf_available, is_torch_available\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n SAMPLE_VOCAB = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n@@ -44,12 +45,13 @@ class T5TokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = T5Tokenizer(SAMPLE_VOCAB)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\"\n@@ -145,11 +147,19 @@ def t5_base_tokenizer(self):\n     def t5_base_tokenizer_fast(self):\n         return T5TokenizerFast.from_pretrained(\"google-t5/t5-base\")\n \n-    def get_tokenizer(self, **kwargs) -> T5Tokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> T5Tokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n-    def get_rust_tokenizer(self, **kwargs) -> T5TokenizerFast:\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> T5TokenizerFast:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def test_rust_and_python_full_tokenizers(self):\n         if not self.test_rust_tokenizer:\n@@ -275,10 +285,10 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [f\"<extra_id_{i}>\" for i in range(100)] + [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n-                tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_cr = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs, from_slow=True\n                 )\n                 tokenizer_p = self.tokenizer_class.from_pretrained(\n@@ -460,10 +470,8 @@ def test_add_prefix_space(self):\n         EXPECTED_WITH_SPACE = [9459, 149, 33, 25, 692, 1]\n         EXPECTED_WO_SPACE = [3845, 63, 149, 33, 25, 692, 1]\n \n-        slow_ = self.tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=False, legacy=False)\n-        fast_ = self.rust_tokenizer_class.from_pretrained(\n-            pretrained_name, add_prefix_space=False, legacy=False, from_slow=True\n-        )\n+        slow_ = self.get_tokenizer(pretrained_name, add_prefix_space=False, legacy=False)\n+        fast_ = self.get_rust_tokenizer(pretrained_name, add_prefix_space=False, legacy=False, from_slow=True)\n         self.assertEqual(slow_.encode(inputs), EXPECTED_WO_SPACE)\n         self.assertEqual(slow_.encode(inputs), fast_.encode(inputs))\n         self.assertEqual(slow_.tokenize(inputs), [\"He\", \"y\", \"▁how\", \"▁are\", \"▁you\", \"▁doing\"])\n@@ -473,8 +481,8 @@ def test_add_prefix_space(self):\n             fast_.decode(EXPECTED_WO_SPACE, skip_special_tokens=True),\n         )\n \n-        slow_ = self.tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=True, legacy=False)\n-        fast_ = self.rust_tokenizer_class.from_pretrained(pretrained_name, add_prefix_space=True, legacy=False)\n+        slow_ = self.get_tokenizer(pretrained_name, add_prefix_space=True, legacy=False)\n+        fast_ = self.get_rust_tokenizer(pretrained_name, add_prefix_space=True, legacy=False)\n         self.assertEqual(slow_.encode(inputs), EXPECTED_WITH_SPACE)\n         self.assertEqual(slow_.encode(inputs), fast_.encode(inputs))\n         self.assertEqual(slow_.tokenize(inputs), [\"▁Hey\", \"▁how\", \"▁are\", \"▁you\", \"▁doing\"])"
        },
        {
            "sha": "f50e9eb8678f8db0be3a659380c3bac7c18a6fc8",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -112,8 +112,9 @@ def get_clean_sequence(\n \n         return output_txt, output_ids\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab_tokens = [\n             \"[UNK]\",\n@@ -132,8 +133,8 @@ def setUp(self):\n             \"low\",\n             \"lowest\",\n         ]\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n             vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n \n     def get_input_output_texts(self, tokenizer):\n@@ -352,7 +353,7 @@ def test_sequence_builders(self):\n     def test_offsets_with_special_characters(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 sentence = f\"A, naïve {tokenizer_r.mask_token} AllenNLP sentence.\"\n                 tokens = tokenizer_r.encode_plus("
        },
        {
            "sha": "c8490c4cc0ef7a208839e3967a3c3bbcbf5f8c0e",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -93,12 +93,13 @@ def get_question_words_and_boxes_batch(self):\n \n         return questions, words, boxes\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = UdopTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_text = \"UNwant\\u00e9d,running\"\n@@ -456,8 +457,8 @@ def test_padding_to_max_length(self):\n     def test_padding(self, max_length=50):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n@@ -922,8 +923,8 @@ def test_build_inputs_with_special_tokens(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Input tokens id\n                 words, boxes = self.get_words_and_boxes()\n@@ -1109,7 +1110,7 @@ def test_token_type_ids(self):\n     def test_offsets_mapping(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 text = [\"a\", \"wonderful\", \"test\"]\n                 boxes = [[1, 8, 12, 20] for _ in range(len(text))]\n@@ -1239,8 +1240,8 @@ def test_tokenization_python_rust_equals(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 words, boxes = self.get_words_and_boxes()\n \n@@ -1293,8 +1294,8 @@ def test_embeded_special_tokens(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 words, boxes = self.get_words_and_boxes()\n                 tokens_r = tokenizer_r.encode_plus_boxes(\n                     words,\n@@ -1320,7 +1321,7 @@ def test_embeded_special_tokens(self):\n     def test_compare_add_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n \n@@ -1402,7 +1403,7 @@ def test_special_tokens_initialization(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 words = \"Hey this is a <special> token\".split()\n@@ -1416,7 +1417,7 @@ def test_special_tokens_initialization(self):\n                 self.assertTrue(special_token_id in r_output)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name, additional_special_tokens=added_tokens, **kwargs, from_slow=True\n                     )\n                     tokenizer_p = self.tokenizer_class.from_pretrained(\n@@ -1591,8 +1592,8 @@ def test_padding_different_model_input_name(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n "
        },
        {
            "sha": "98b02ca5fd88bd7cc21ba76a540fa4c330d827df",
            "filename": "tests/models/vits/test_tokenization_vits.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fvits%2Ftest_tokenization_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fvits%2Ftest_tokenization_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvits%2Ftest_tokenization_vits.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -19,21 +19,23 @@\n import shutil\n import tempfile\n import unittest\n+from functools import lru_cache\n \n from transformers import VitsTokenizer\n from transformers.models.vits.tokenization_vits import VOCAB_FILES_NAMES\n from transformers.testing_utils import slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n class VitsTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"facebook/mms-tts-eng\"\n     tokenizer_class = VitsTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = (\n             \"k ' z y u d h e s w – 3 c p - 1 j m i X f l o 0 b r a 4 2 n _ x v t q 5 6 g ț ţ < > | <pad> <unk>\".split(\n@@ -44,18 +46,22 @@ def setUp(self):\n         vocab_tokens[\" \"] = vocab_tokens[\"X\"]\n         del vocab_tokens[\"X\"]\n \n-        self.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\"}\n+        cls.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\"}\n \n-        self.tmpdirname = tempfile.mkdtemp()\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n         kwargs[\"phonemize\"] = False\n         kwargs[\"normalize\"] = False\n-        return VitsTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return VitsTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20, min_length=5):\n         txt = \"beyonce lives in los angeles\""
        },
        {
            "sha": "5a3906811fef5754998466f5e5b7b2df5b3a7ee1",
            "filename": "tests/models/wav2vec2/test_tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 20,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -21,6 +21,7 @@\n import shutil\n import tempfile\n import unittest\n+from functools import lru_cache\n \n import numpy as np\n \n@@ -33,7 +34,7 @@\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES, Wav2Vec2CTCTokenizerOutput\n from transformers.testing_utils import require_torch, slow\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n global_rng = random.Random()\n@@ -57,22 +58,27 @@ def floats_list(shape, scale=1.0, rng=None, name=None):\n class Wav2Vec2TokenizerTest(unittest.TestCase):\n     tokenizer_class = Wav2Vec2Tokenizer\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n \n-        self.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n+        cls.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n \n-        self.tmpdirname = tempfile.mkdtemp()\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return Wav2Vec2Tokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return Wav2Vec2Tokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def test_tokenizer_decode(self):\n         # TODO(PVP) - change to facebook\n@@ -237,7 +243,7 @@ def _input_values_are_equal(input_values_1, input_values_2):\n \n     def test_save_pretrained(self):\n         pretrained_name = list(self.tokenizer_class.pretrained_vocab_files_map[\"vocab_file\"].keys())[0]\n-        tokenizer = self.tokenizer_class.from_pretrained(pretrained_name)\n+        tokenizer = self.get_tokenizer(pretrained_name)\n         tmpdirname2 = tempfile.mkdtemp()\n \n         tokenizer_files = tokenizer.save_pretrained(tmpdirname2)\n@@ -373,22 +379,27 @@ class Wav2Vec2CTCTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = Wav2Vec2CTCTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n \n-        self.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n+        cls.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n \n-        self.tmpdirname = tempfile.mkdtemp()\n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return Wav2Vec2CTCTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return Wav2Vec2CTCTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def test_tokenizer_add_token_chars(self):\n         tokenizer = self.tokenizer_class.from_pretrained(\"facebook/wav2vec2-base-960h\")"
        },
        {
            "sha": "f9d547acddfde9659bb16e3913c829b41baffe99",
            "filename": "tests/models/wav2vec2_phoneme/test_tokenization_wav2vec2_phoneme.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_phoneme%2Ftest_tokenization_wav2vec2_phoneme.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -17,14 +17,15 @@\n import json\n import os\n import unittest\n+from functools import lru_cache\n from typing import Tuple\n \n from transformers import Wav2Vec2PhonemeCTCTokenizer\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES\n from transformers.models.wav2vec2_phoneme.tokenization_wav2vec2_phoneme import Wav2Vec2PhonemeCTCTokenizerOutput\n from transformers.testing_utils import require_phonemizer\n \n-from ...test_tokenization_common import TokenizerTesterMixin\n+from ...test_tokenization_common import TokenizerTesterMixin, use_cache_if_possible\n \n \n @require_phonemizer\n@@ -33,8 +34,9 @@ class Wav2Vec2PhonemeCTCTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = Wav2Vec2PhonemeCTCTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         vocab = (\n             \"<s> <pad> </s> <unk> n s t ə l a i k d m ɛ ɾ e ɪ p o ɐ z ð f j v b ɹ ʁ ʊ iː r w ʌ u ɡ æ aɪ ʃ h ɔ ɑː \"\n@@ -53,10 +55,10 @@ def setUp(self):\n         ).split(\" \")\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n \n-        self.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n+        cls.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(vocab_tokens) + \"\\n\")\n \n     # overwrite since phonemes require specific creation\n@@ -84,9 +86,13 @@ def get_clean_sequence(self, tokenizer, with_prefix_space=False, max_length=20,\n         output_ids = tokenizer.encode(output_txt, add_special_tokens=False)\n         return output_txt, output_ids\n \n-    def get_tokenizer(self, **kwargs):\n-        kwargs.update(self.special_tokens_map)\n-        return Wav2Vec2PhonemeCTCTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n+        kwargs.update(cls.special_tokens_map)\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return Wav2Vec2PhonemeCTCTokenizer.from_pretrained(pretrained_name, **kwargs)\n \n     def test_tokenizer_add_new_tokens(self):\n         tokenizer = self.tokenizer_class.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")"
        },
        {
            "sha": "61a34c165d8a96b19f8071e3079a221c7e0f2996",
            "filename": "tests/models/whisper/test_tokenization_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -40,12 +40,13 @@ class WhisperTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     test_sentencepiece = False\n     test_seq2seq = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n         tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\")\n         tokenizer.pad_token_id = 50256\n         tokenizer.pad_token = \"<|endoftext|>\"\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "08fa3ebf1a387ef84f1cc36bf189de7fa5dbc11a",
            "filename": "tests/models/xglm/test_tokenization_xglm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -37,12 +37,13 @@ class XGLMTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = XGLMTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "2292b18b8bd4386eeabc7a85a1f5c3dd20412e6b",
            "filename": "tests/models/xlm/test_tokenization_xlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxlm%2Ftest_tokenization_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxlm%2Ftest_tokenization_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_tokenization_xlm.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -29,8 +29,9 @@ class XLMTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     tokenizer_class = XLMTokenizer\n     test_rust_tokenizer = False\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # Adapted from Sennrich et al. 2015 and https://github.com/rsennrich/subword-nmt\n         vocab = [\n@@ -59,11 +60,11 @@ def setUp(self):\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n         merges = [\"l o 123\", \"lo w 1456\", \"e r</w> 1789\", \"\"]\n \n-        self.vocab_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        self.merges_file = os.path.join(self.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(self.vocab_file, \"w\") as fp:\n+        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n+        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n+        with open(cls.vocab_file, \"w\") as fp:\n             fp.write(json.dumps(vocab_tokens))\n-        with open(self.merges_file, \"w\") as fp:\n+        with open(cls.merges_file, \"w\") as fp:\n             fp.write(\"\\n\".join(merges))\n \n     def get_input_output_texts(self, tokenizer):"
        },
        {
            "sha": "ba5a834bfe0cd104e74ffa0c29a26e507b61a14a",
            "filename": "tests/models/xlm_roberta/test_tokenization_xlm_roberta.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -37,12 +37,13 @@ class XLMRobertaTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = XLMRobertaTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\"\n@@ -148,8 +149,8 @@ def test_save_pretrained(self):\n         self.tokenizers_list[0] = (self.rust_tokenizer_class, \"hf-internal-testing/tiny-xlm-roberta\", {})\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n "
        },
        {
            "sha": "307499b605d582c0882b3246b10e27ea6466bcb6",
            "filename": "tests/models/xlnet/test_tokenization_xlnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxlnet%2Ftest_tokenization_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Fmodels%2Fxlnet%2Ftest_tokenization_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_tokenization_xlnet.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -33,12 +33,13 @@ class XLNetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     test_sentencepiece = True\n \n-    def setUp(self):\n-        super().setUp()\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n \n         # We have a SentencePiece fixture for testing\n         tokenizer = XLNetTokenizer(SAMPLE_VOCAB, keep_accents=True)\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     def test_convert_token_and_id(self):\n         \"\"\"Test ``_convert_token_to_id`` and ``_convert_id_to_token``.\"\"\""
        },
        {
            "sha": "6bc870614dbeb2ca34490fd13197749540b21113",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 117,
            "deletions": 90,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -13,6 +13,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import copy\n+import functools\n import inspect\n import itertools\n import json\n@@ -24,6 +26,7 @@\n import traceback\n import unittest\n from collections import OrderedDict\n+from functools import lru_cache\n from itertools import takewhile\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Dict, List, Tuple, Union\n@@ -69,6 +72,38 @@\n     from transformers import PretrainedConfig, PreTrainedModel, TFPreTrainedModel\n \n \n+def use_cache_if_possible(func):\n+    @functools.wraps(func)\n+    def wrapper(*args, **kwargs):\n+        use_cache = kwargs.pop(\"use_cache\", True)\n+\n+        underline_func = func\n+        if \"functools\" in str(func):\n+            underline_func = func.__wrapped__\n+\n+        if not use_cache:\n+            return underline_func(*args, **kwargs)\n+        if any(not arg.__hash__ for arg in args):\n+            return underline_func(*args, **kwargs)\n+        elif any(not kwarg.__hash__ for kwarg in kwargs.values()):\n+            return underline_func(*args, **kwargs)\n+\n+        cached = func(*args, **kwargs)\n+        copied = copy.deepcopy(cached)\n+\n+        if hasattr(copied, \"_tokenizer\") and \"tests.models.clip.test_tokenization_clip.CLIPTokenizationTest\" in str(\n+            args[0]\n+        ):\n+            copied._tokenizer = cached._tokenizer\n+\n+        if hasattr(copied, \"sp_model\"):\n+            copied.sp_model = cached.sp_model\n+\n+        return copied\n+\n+    return wrapper\n+\n+\n logger = logging.get_logger(__name__)\n \n NON_ENGLISH_TAGS = [\"chinese\", \"dutch\", \"french\", \"finnish\", \"german\", \"multilingual\"]\n@@ -198,32 +233,34 @@ class TokenizerTesterMixin:\n     # test_sentencepiece must also be set to True\n     test_sentencepiece_ignore_case = False\n \n-    def setUp(self) -> None:\n+    @classmethod\n+    def setUpClass(cls) -> None:\n         # Tokenizer.filter makes it possible to filter which Tokenizer to case based on all the\n         # information available in Tokenizer (name, rust class, python class, vocab key name)\n-        self.from_pretrained_id = (\n-            [self.from_pretrained_id] if isinstance(self.from_pretrained_id, str) else self.from_pretrained_id\n+        cls.from_pretrained_id = (\n+            [cls.from_pretrained_id] if isinstance(cls.from_pretrained_id, str) else cls.from_pretrained_id\n         )\n \n-        self.tokenizers_list = []\n-        if self.test_rust_tokenizer:\n-            self.tokenizers_list = [\n+        cls.tokenizers_list = []\n+        if cls.test_rust_tokenizer:\n+            cls.tokenizers_list = [\n                 (\n-                    self.rust_tokenizer_class,\n+                    cls.rust_tokenizer_class,\n                     pretrained_id,\n-                    self.from_pretrained_kwargs if self.from_pretrained_kwargs is not None else {},\n+                    cls.from_pretrained_kwargs if cls.from_pretrained_kwargs is not None else {},\n                 )\n-                for pretrained_id in self.from_pretrained_id\n+                for pretrained_id in cls.from_pretrained_id\n             ]\n         else:\n-            self.tokenizers_list = []\n+            cls.tokenizers_list = []\n         with open(f\"{get_tests_dir()}/fixtures/sample_text.txt\", encoding=\"utf-8\") as f_data:\n-            self._data = f_data.read().replace(\"\\n\\n\", \"\\n\").strip()\n+            cls._data = f_data.read().replace(\"\\n\\n\", \"\\n\").strip()\n \n-        self.tmpdirname = tempfile.mkdtemp()\n+        cls.tmpdirname = tempfile.mkdtemp()\n \n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname)\n \n     def get_input_output_texts(self, tokenizer):\n         input_txt = self.get_clean_sequence(tokenizer)[0]\n@@ -267,11 +304,19 @@ def get_tokenizers(self, fast=True, **kwargs) -> List[PreTrainedTokenizerBase]:\n         else:\n             raise ValueError(\"This tokenizer class has no tokenizer to be tested.\")\n \n-    def get_tokenizer(self, **kwargs) -> PreTrainedTokenizer:\n-        return self.tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizer:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n-    def get_rust_tokenizer(self, **kwargs) -> PreTrainedTokenizerFast:\n-        return self.rust_tokenizer_class.from_pretrained(self.tmpdirname, **kwargs)\n+    @classmethod\n+    @use_cache_if_possible\n+    @lru_cache(maxsize=64)\n+    def get_rust_tokenizer(cls, pretrained_name=None, **kwargs) -> PreTrainedTokenizerFast:\n+        pretrained_name = pretrained_name or cls.tmpdirname\n+        return cls.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n \n     def tokenizer_integration_test_util(\n         self,\n@@ -1263,7 +1308,7 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                 if not self.test_rust_tokenizer:\n                     self.skipTest(reason=\"No fast tokenizer defined\")\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name)\n                 self._check_no_pad_token_padding(tokenizer_r, conversations)\n \n                 tokenizer_r.padding_side = \"right\"\n@@ -1446,7 +1491,7 @@ def test_chat_template_return_assistant_tokens_mask_truncated(self):\n                 if not self.test_rust_tokenizer:\n                     self.skipTest(reason=\"No fast tokenizer defined\")\n \n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name)\n \n                 # Find where to truncate, as the amount of tokens is different for different tokenizers and I want the\n                 # truncation to happen in the middle of the assistant content.\n@@ -2050,11 +2095,9 @@ def test_encode_decode_fast_slow_all_tokens(self):\n         if self.rust_tokenizer_class is not None:\n             pretrained_name = self.from_pretrained_id\n \n-            slow_tokenizer = self.tokenizer_class.from_pretrained(pretrained_name, legacy=False)\n+            slow_tokenizer = self.get_tokenizer(pretrained_name, legacy=False)\n             with self.subTest(f\"{pretrained_name}\"):\n-                rust_tokenizer = self.rust_tokenizer_class.from_pretrained(\n-                    pretrained_name, from_slow=True, legacy=False\n-                )\n+                rust_tokenizer = self.get_rust_tokenizer(pretrained_name, from_slow=True, legacy=False)\n                 input_full_vocab_ids = list(\n                     range(len(slow_tokenizer))\n                 )  # TODO let's maybe shuffle this! And run it 4 times. This way we cover more cmbinations\n@@ -2200,14 +2243,10 @@ def test_padding_side_in_kwargs(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 if self.test_rust_tokenizer:\n-                    tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n-                        pretrained_name, padding_side=\"left\", **kwargs\n-                    )\n+                    tokenizer_r = self.get_rust_tokenizer(pretrained_name, padding_side=\"left\", **kwargs)\n                     self.assertEqual(tokenizer_r.padding_side, \"left\")\n \n-                    tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n-                        pretrained_name, padding_side=\"right\", **kwargs\n-                    )\n+                    tokenizer_r = self.get_rust_tokenizer(pretrained_name, padding_side=\"right\", **kwargs)\n                     self.assertEqual(tokenizer_r.padding_side, \"right\")\n \n                     self.assertRaises(\n@@ -2219,10 +2258,10 @@ def test_padding_side_in_kwargs(self):\n                     )\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, padding_side=\"left\", **kwargs)\n+                    tokenizer_p = self.get_tokenizer(pretrained_name, padding_side=\"left\", **kwargs)\n                     self.assertEqual(tokenizer_p.padding_side, \"left\")\n \n-                    tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, padding_side=\"right\", **kwargs)\n+                    tokenizer_p = self.get_tokenizer(pretrained_name, padding_side=\"right\", **kwargs)\n                     self.assertEqual(tokenizer_p.padding_side, \"right\")\n \n                     self.assertRaises(\n@@ -2237,14 +2276,10 @@ def test_truncation_side_in_kwargs(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 if self.test_rust_tokenizer:\n-                    tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n-                        pretrained_name, truncation_side=\"left\", **kwargs\n-                    )\n+                    tokenizer_r = self.get_rust_tokenizer(pretrained_name, truncation_side=\"left\", **kwargs)\n                     self.assertEqual(tokenizer_r.truncation_side, \"left\")\n \n-                    tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n-                        pretrained_name, truncation_side=\"right\", **kwargs\n-                    )\n+                    tokenizer_r = self.get_rust_tokenizer(pretrained_name, truncation_side=\"right\", **kwargs)\n                     self.assertEqual(tokenizer_r.truncation_side, \"right\")\n \n                     self.assertRaises(\n@@ -2256,14 +2291,10 @@ def test_truncation_side_in_kwargs(self):\n                     )\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_p = self.tokenizer_class.from_pretrained(\n-                        pretrained_name, truncation_side=\"left\", **kwargs\n-                    )\n+                    tokenizer_p = self.get_tokenizer(pretrained_name, truncation_side=\"left\", **kwargs)\n                     self.assertEqual(tokenizer_p.truncation_side, \"left\")\n \n-                    tokenizer_p = self.tokenizer_class.from_pretrained(\n-                        pretrained_name, truncation_side=\"right\", **kwargs\n-                    )\n+                    tokenizer_p = self.get_tokenizer(pretrained_name, truncation_side=\"right\", **kwargs)\n                     self.assertEqual(tokenizer_p.truncation_side, \"right\")\n \n                     self.assertRaises(\n@@ -3194,18 +3225,18 @@ def test_prepare_seq2seq_batch(self):\n     def test_is_fast(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n                 # Check is_fast is set correctly\n                 self.assertTrue(tokenizer_r.is_fast)\n \n                 if self.test_slow_tokenizer:\n-                    tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                    tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                     self.assertFalse(tokenizer_p.is_fast)\n \n     def test_fast_only_inputs(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 # Ensure None raise an error\n                 self.assertRaises(TypeError, tokenizer_r.tokenize, None)\n@@ -3216,7 +3247,7 @@ def test_fast_only_inputs(self):\n     def test_alignement_methods(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 words = [\"Wonderful\", \"no\", \"inspiration\", \"example\", \"with\", \"subtoken\"]\n                 text = \" \".join(words)\n@@ -3446,8 +3477,8 @@ def test_tokenization_python_rust_equals(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Ensure basic input match\n                 input_p = tokenizer_p.encode_plus(self._data)\n@@ -3487,8 +3518,8 @@ def test_num_special_tokens_to_add_equal(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Check we have the same number of added_tokens for both pair and non-pair inputs.\n                 self.assertEqual(\n@@ -3505,8 +3536,8 @@ def test_max_length_equal(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Check we have the correct max_length for both pair and non-pair inputs.\n                 self.assertEqual(tokenizer_r.max_len_single_sentence, tokenizer_p.max_len_single_sentence)\n@@ -3520,8 +3551,8 @@ def test_special_tokens_map_equal(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 # sometimes the tokenizer saved online is not the same\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 # Assert the set of special tokens match.\n                 self.assertSequenceEqual(\n@@ -3532,7 +3563,7 @@ def test_special_tokens_map_equal(self):\n     def test_add_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 vocab_size = len(tokenizer_r)\n                 self.assertEqual(tokenizer_r.add_tokens(\"\"), 0)\n@@ -3558,7 +3589,7 @@ def test_add_tokens(self):\n     def test_offsets_mapping(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 text = \"Wonderful no inspiration example with subtoken\"\n                 pair = \"Along with an awesome pair\"\n@@ -3601,7 +3632,7 @@ def test_batch_encode_dynamic_overflowing(self):\n         This needs to be padded so that it can represented as a tensor\n         \"\"\"\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n-            tokenizer = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+            tokenizer = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name}, {tokenizer.__class__.__name__})\"):\n                 if is_torch_available():\n@@ -3663,8 +3694,8 @@ def test_compare_pretokenized_inputs(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 if hasattr(tokenizer_p, \"add_prefix_space\") and not tokenizer_p.add_prefix_space:\n                     continue  # Too hard to test for now\n@@ -3745,8 +3776,8 @@ def test_create_token_type_ids(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 input_simple = [1, 2, 3]\n                 input_pair = [1, 2, 3]\n \n@@ -3767,8 +3798,8 @@ def test_build_inputs_with_special_tokens(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 # # Input string\n                 # input_simple = tokenizer_p.tokenize(\"This is a sample input\", add_special_tokens=False)\n                 # input_pair = tokenizer_p.tokenize(\"This is a sample pair\", add_special_tokens=False)\n@@ -3812,8 +3843,8 @@ def test_padding(self, max_length=50):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n@@ -4038,8 +4069,8 @@ def test_padding_different_model_input_name(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 self.assertEqual(tokenizer_p.pad_token_id, tokenizer_r.pad_token_id)\n                 pad_token_id = tokenizer_p.pad_token_id\n \n@@ -4076,8 +4107,8 @@ def test_save_pretrained(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n \n                 tmpdirname2 = tempfile.mkdtemp()\n \n@@ -4151,8 +4182,8 @@ def test_embeded_special_tokens(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n                 sentence = \"A, <mask> AllenNLP sentence.\"\n                 tokens_r = tokenizer_r.encode_plus(\n                     sentence,\n@@ -4176,7 +4207,7 @@ def test_embeded_special_tokens(self):\n     def test_compare_add_special_tokens(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n \n                 simple_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=False)\n                 # pair_num_special_tokens_to_add = tokenizer_r.num_special_tokens_to_add(pair=True)\n@@ -4219,8 +4250,8 @@ def test_compare_prepare_for_model(self):\n \n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n-                tokenizer_p = self.tokenizer_class.from_pretrained(pretrained_name, **kwargs)\n+                tokenizer_r = self.get_rust_tokenizer(pretrained_name, **kwargs)\n+                tokenizer_p = self.get_tokenizer(pretrained_name, **kwargs)\n                 string_sequence = \"Asserting that both tokenizers are equal\"\n                 python_output = tokenizer_p.prepare_for_model(\n                     tokenizer_p.encode(string_sequence, add_special_tokens=False)\n@@ -4235,7 +4266,7 @@ def test_special_tokens_initialization(self):\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 added_tokens = [AddedToken(\"<special>\", lstrip=True)]\n-                tokenizer_r = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_r = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=added_tokens, **kwargs\n                 )\n                 r_output = tokenizer_r.encode(\"Hey this is a <special> token\")\n@@ -4246,12 +4277,10 @@ def test_special_tokens_initialization(self):\n \n                 if self.test_slow_tokenizer:\n                     # in rust fast, you lose the information of the AddedToken when initializing with `additional_special_tokens`\n-                    tokenizer_cr = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_cr = self.get_rust_tokenizer(\n                         pretrained_name, additional_special_tokens=added_tokens, **kwargs, from_slow=True\n                     )\n-                    tokenizer_p = self.tokenizer_class.from_pretrained(\n-                        pretrained_name, additional_special_tokens=added_tokens, **kwargs\n-                    )\n+                    tokenizer_p = self.get_tokenizer(pretrained_name, additional_special_tokens=added_tokens, **kwargs)\n \n                     p_output = tokenizer_p.encode(\"Hey this is a <special> token\")\n \n@@ -4498,7 +4527,7 @@ def test_saving_tokenizer_trainer(self):\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 with tempfile.TemporaryDirectory() as tmp_dir:\n                     # Save the fast tokenizer files in a temporary directory\n-                    tokenizer_old = self.rust_tokenizer_class.from_pretrained(pretrained_name, **kwargs, use_fast=True)\n+                    tokenizer_old = self.get_rust_tokenizer(pretrained_name, **kwargs, use_fast=True)\n                     tokenizer_old.save_pretrained(tmp_dir, legacy_format=False)  # save only fast version\n \n                     # Initialize toy model for the trainer\n@@ -4532,13 +4561,11 @@ def test_save_slow_from_fast_and_reload_fast(self):\n                 with tempfile.TemporaryDirectory() as tmp_dir_1:\n                     # Here we check that even if we have initialized a fast tokenizer with a tokenizer_file we can\n                     # still save only the slow version and use these saved files to rebuild a tokenizer\n-                    tokenizer_fast_old_1 = self.rust_tokenizer_class.from_pretrained(\n-                        pretrained_name, **kwargs, use_fast=True\n-                    )\n+                    tokenizer_fast_old_1 = self.get_rust_tokenizer(pretrained_name, **kwargs, use_fast=True)\n                     tokenizer_file = os.path.join(tmp_dir_1, \"tokenizer.json\")\n                     tokenizer_fast_old_1.backend_tokenizer.save(tokenizer_file)\n \n-                    tokenizer_fast_old_2 = self.rust_tokenizer_class.from_pretrained(\n+                    tokenizer_fast_old_2 = self.get_rust_tokenizer(\n                         pretrained_name, **kwargs, use_fast=True, tokenizer_file=tokenizer_file\n                     )\n \n@@ -4560,10 +4587,10 @@ def test_split_special_tokens(self):\n             special_token = \"<my_new_token>\"\n             special_sentence = f\"Hey this is a {special_token} token\"\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n-                tokenizer_rust = self.rust_tokenizer_class.from_pretrained(\n+                tokenizer_rust = self.get_rust_tokenizer(\n                     pretrained_name, additional_special_tokens=[special_token], split_special_tokens=True, **kwargs\n                 )\n-                tokenizer_py = self.tokenizer_class.from_pretrained(\n+                tokenizer_py = self.get_tokenizer(\n                     pretrained_name, additional_special_tokens=[special_token], split_special_tokens=True, **kwargs\n                 )\n \n@@ -4622,7 +4649,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n         for tokenizer, pretrained_name, kwargs in self.tokenizers_list:\n             with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n                 # Load a slow tokenizer from the hub, init with the new token for fast to also include it\n-                tokenizer = self.tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n+                tokenizer = self.get_tokenizer(pretrained_name, eos_token=new_eos)\n                 EXPECTED_ADDED_TOKENS_DECODER = tokenizer.added_tokens_decoder\n                 with self.subTest(\"Hub -> Slow: Test loading a slow tokenizer from the hub)\"):\n                     self.assertEqual(tokenizer._special_tokens_map[\"eos_token\"], new_eos)\n@@ -4662,7 +4689,7 @@ def _test_added_vocab_and_eos(expected, tokenizer_class, expected_eos, temp_dir)\n \n                 with self.subTest(\"Hub -> Fast: Test loading a fast tokenizer from the hub)\"):\n                     if self.rust_tokenizer_class is not None:\n-                        tokenizer_fast = self.rust_tokenizer_class.from_pretrained(pretrained_name, eos_token=new_eos)\n+                        tokenizer_fast = self.get_rust_tokenizer(pretrained_name, eos_token=new_eos)\n                         self.assertEqual(tokenizer_fast._special_tokens_map[\"eos_token\"], new_eos)\n                         self.assertIn(new_eos, list(tokenizer_fast.added_tokens_decoder.values()))\n                         # We can't test the following because for BC we kept the default rstrip lstrip in slow not fast. Will comment once normalization is alright"
        },
        {
            "sha": "40b945e272b774c774829d859dd99abb2c1195bd",
            "filename": "tests/tokenization/test_tokenization_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fcaad6df96c0d01ca47a2a48175c932ec354954/tests%2Ftokenization%2Ftest_tokenization_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_fast.py?ref=1fcaad6df96c0d01ca47a2a48175c932ec354954",
            "patch": "@@ -33,19 +33,20 @@ class PreTrainedTokenizationFastTest(TokenizerTesterMixin, unittest.TestCase):\n     test_rust_tokenizer = True\n     from_pretrained_vocab_key = \"tokenizer_file\"\n \n-    def setUp(self):\n-        self.test_rust_tokenizer = False  # because we don't have pretrained_vocab_files_map\n-        super().setUp()\n-        self.test_rust_tokenizer = True\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.test_rust_tokenizer = False  # because we don't have pretrained_vocab_files_map\n+        super().setUpClass()\n+        cls.test_rust_tokenizer = True\n \n         model_paths = [\"robot-test/dummy-tokenizer-fast\", \"robot-test/dummy-tokenizer-wordlevel\"]\n-        self.bytelevel_bpe_model_name = \"SaulLu/dummy-tokenizer-bytelevel-bpe\"\n+        cls.bytelevel_bpe_model_name = \"SaulLu/dummy-tokenizer-bytelevel-bpe\"\n \n         # Inclusion of 2 tokenizers to test different types of models (Unigram and WordLevel for the moment)\n-        self.tokenizers_list = [(PreTrainedTokenizerFast, model_path, {}) for model_path in model_paths]\n+        cls.tokenizers_list = [(PreTrainedTokenizerFast, model_path, {}) for model_path in model_paths]\n \n         tokenizer = PreTrainedTokenizerFast.from_pretrained(model_paths[0])\n-        tokenizer.save_pretrained(self.tmpdirname)\n+        tokenizer.save_pretrained(cls.tmpdirname)\n \n     @unittest.skip(\n         \"We disable this test for PreTrainedTokenizerFast because it is the only tokenizer that is not linked to any model\""
        }
    ],
    "stats": {
        "total": 2219,
        "additions": 1318,
        "deletions": 901
    }
}