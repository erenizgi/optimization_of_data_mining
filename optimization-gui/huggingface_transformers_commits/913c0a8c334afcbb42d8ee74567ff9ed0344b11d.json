{
    "author": "ariG23498",
    "message": "[docs] Zero Shot Object Detection Task (#40096)\n\n* refactor zsod task docs\n\n* keeping the image guided od section\n\n* Apply suggestions from code review\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update docs/source/en/tasks/zero_shot_object_detection.md\n\nCo-authored-by: Sergio Paniego Blanco <sergiopaniegoblanco@gmail.com>\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\nCo-authored-by: Sergio Paniego Blanco <sergiopaniegoblanco@gmail.com>",
    "sha": "913c0a8c334afcbb42d8ee74567ff9ed0344b11d",
    "files": [
        {
            "sha": "8635d71cf822ec18f9af1dde74ea6d37ebf73c11",
            "filename": "docs/source/en/tasks/zero_shot_object_detection.md",
            "status": "modified",
            "additions": 85,
            "deletions": 72,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/913c0a8c334afcbb42d8ee74567ff9ed0344b11d/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/913c0a8c334afcbb42d8ee74567ff9ed0344b11d/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md?ref=913c0a8c334afcbb42d8ee74567ff9ed0344b11d",
            "patch": "@@ -21,19 +21,14 @@ rendered properly in your Markdown viewer.\n Traditionally, models used for [object detection](object_detection) require labeled image datasets for training,\n and are limited to detecting the set of classes from the training data.\n \n-Zero-shot object detection is supported by the [OWL-ViT](../model_doc/owlvit) model which uses a different approach. OWL-ViT\n-is an open-vocabulary object detector. It means that it can detect objects in images based on free-text queries without\n-the need to fine-tune the model on labeled datasets.\n+Zero-shot object detection is a computer vision task to detect objects and their classes in images, without any\n+prior training or knowledge of the classes. Zero-shot object detection models receive an image as input, as well\n+as a list of candidate classes, and output the bounding boxes and labels where the objects have been detected.\n \n-OWL-ViT leverages multi-modal representations to perform open-vocabulary detection. It combines [CLIP](../model_doc/clip) with\n-lightweight object classification and localization heads. Open-vocabulary detection is achieved by embedding free-text queries with the text encoder of CLIP and using them as input to the object classification and localization heads,\n-which associate images with their corresponding textual descriptions, while ViT processes image patches as inputs. The authors\n-of OWL-ViT first trained CLIP from scratch and then fine-tuned OWL-ViT end to end on standard object detection datasets using\n-a bipartite matching loss.\n+> [!NOTE]\n+> Hugging Face houses many such [open vocabulary zero shot object detectors](https://huggingface.co/models?pipeline_tag=zero-shot-object-detection).\n \n-With this approach, the model can detect objects based on textual descriptions without prior training on labeled datasets.\n-\n-In this guide, you will learn how to use OWL-ViT:\n+In this guide, you will learn how to use such models:\n - to detect objects based on text prompts\n - for batch object detection\n - for image-guided object detection\n@@ -46,27 +41,25 @@ pip install -q transformers\n \n ## Zero-shot object detection pipeline\n \n-The simplest way to try out inference with OWL-ViT is to use it in a [`pipeline`]. Instantiate a pipeline\n-for zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit):\n+The simplest way to try out inference with models is to use it in a [`pipeline`]. Instantiate a pipeline\n+for zero-shot object detection from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-object-detection):\n \n ```python\n >>> from transformers import pipeline\n \n->>> checkpoint = \"google/owlv2-base-patch16-ensemble\"\n+>>> # Use any checkpoint from the hf.co/models?pipeline_tag=zero-shot-object-detection\n+>>> checkpoint = \"iSEE-Laboratory/llmdet_large\"\n >>> detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")\n ```\n \n Next, choose an image you'd like to detect objects in. Here we'll use the image of astronaut Eileen Collins that is\n a part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\n \n ```py\n->>> import skimage\n->>> import numpy as np\n->>> from PIL import Image\n-\n->>> image = skimage.data.astronaut()\n->>> image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n+>>> from transformers.image_utils import load_image\n \n+>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\"\n+>>> image = load_image(url)\n >>> image\n ```\n \n@@ -81,26 +74,30 @@ Here we pass the image directly; other suitable options include a local path to\n >>> predictions = detector(\n ...     image,\n ...     candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n+...     threshold=0.45,\n ... )\n >>> predictions\n-[{'score': 0.3571370542049408,\n+[{'score': 0.8409242033958435,\n   'label': 'human face',\n-  'box': {'xmin': 180, 'ymin': 71, 'xmax': 271, 'ymax': 178}},\n- {'score': 0.28099656105041504,\n-  'label': 'nasa badge',\n-  'box': {'xmin': 129, 'ymin': 348, 'xmax': 206, 'ymax': 427}},\n- {'score': 0.2110239565372467,\n+  'box': {'xmin': 179, 'ymin': 74, 'xmax': 272, 'ymax': 179}},\n+ {'score': 0.7380027770996094,\n   'label': 'rocket',\n-  'box': {'xmin': 350, 'ymin': -1, 'xmax': 468, 'ymax': 288}},\n- {'score': 0.13790413737297058,\n+  'box': {'xmin': 353, 'ymin': 0, 'xmax': 466, 'ymax': 284}},\n+ {'score': 0.5850900411605835,\n   'label': 'star-spangled banner',\n-  'box': {'xmin': 1, 'ymin': 1, 'xmax': 105, 'ymax': 509}},\n- {'score': 0.11950037628412247,\n+  'box': {'xmin': 0, 'ymin': 0, 'xmax': 96, 'ymax': 511}},\n+ {'score': 0.5697067975997925,\n+  'label': 'human face',\n+  'box': {'xmin': 18, 'ymin': 15, 'xmax': 366, 'ymax': 511}},\n+ {'score': 0.47813931107521057,\n+  'label': 'star-spangled banner',\n+  'box': {'xmin': 353, 'ymin': 0, 'xmax': 459, 'ymax': 274}},\n+ {'score': 0.46597740054130554,\n   'label': 'nasa badge',\n-  'box': {'xmin': 277, 'ymin': 338, 'xmax': 327, 'ymax': 380}},\n- {'score': 0.10649408400058746,\n-  'label': 'rocket',\n-  'box': {'xmin': 358, 'ymin': 64, 'xmax': 424, 'ymax': 280}}]\n+  'box': {'xmin': 353, 'ymin': 0, 'xmax': 462, 'ymax': 279}},\n+ {'score': 0.4585932493209839,\n+  'label': 'nasa badge',\n+  'box': {'xmin': 132, 'ymin': 348, 'xmax': 208, 'ymax': 423}}]\n ```\n \n Let's visualize the predictions:\n@@ -128,65 +125,63 @@ Let's visualize the predictions:\n \n ## Text-prompted zero-shot object detection by hand\n \n-Now that you've seen how to use the zero-shot object detection pipeline, let's replicate the same\n-result manually.\n+Now that you've seen how to use the zero-shot object detection pipeline, let's replicate the same result manually.\n \n-Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?other=owlvit).\n+Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](hf.co/iSEE-Laboratory/llmdet_large).\n Here we'll use the same checkpoint as before:\n \n ```py\n >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n \n->>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n+>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint, device_map=\"auto\")\n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n ```\n \n Let's take a different image to switch things up.\n \n ```py\n->>> import requests\n-\n->>> url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n->>> im = Image.open(requests.get(url, stream=True).raw)\n->>> im\n+>>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\"\n+>>> image = load_image(url)\n+>>> image\n ```\n \n <div class=\"flex justify-center\">\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\" alt=\"Beach photo\"/>\n </div>\n \n-Use the processor to prepare the inputs for the model. The processor combines an image processor that prepares the\n-image for the model by resizing and normalizing it, and a [`CLIPTokenizer`] that takes care of the text inputs.\n+Use the processor to prepare the inputs for the model.\n \n ```py\n->>> text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n->>> inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")\n+>>> text_labels = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n+>>> inputs = processor(text=text_labels, images=image, return_tensors=\"pt\")to(model.device)\n ```\n \n Pass the inputs through the model, post-process, and visualize the results. Since the image processor resized images before\n-feeding them to the model, you need to use the [`~OwlViTImageProcessor.post_process_object_detection`] method to make sure the predicted bounding\n+feeding them to the model, you need to use the `post_process_object_detection` method to make sure the predicted bounding\n boxes have the correct coordinates relative to the original image:\n \n ```py\n >>> import torch\n \n->>> with torch.no_grad():\n+>>> with torch.inference_mode():\n ...     outputs = model(**inputs)\n-...     target_sizes = torch.tensor([im.size[::-1]])\n-...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n \n->>> draw = ImageDraw.Draw(im)\n+>>> results = processor.post_process_grounded_object_detection(\n+...    outputs, threshold=0.50, target_sizes=[(image.height, image.width)], text_labels=text_labels,\n+...)[0]\n \n->>> scores = results[\"scores\"].tolist()\n->>> labels = results[\"labels\"].tolist()\n->>> boxes = results[\"boxes\"].tolist()\n+>>> draw = ImageDraw.Draw(image)\n+\n+>>> scores = results[\"scores\"]\n+>>> text_labels = results[\"text_labels\"]\n+>>> boxes = results[\"boxes\"]\n \n->>> for box, score, label in zip(boxes, scores, labels):\n+>>> for box, score, text_label in zip(boxes, scores, text_labels):\n ...     xmin, ymin, xmax, ymax = box\n ...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n-...     draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")\n+...     draw.text((xmin, ymin), f\"{text_label}: {round(score.item(),2)}\", fill=\"white\")\n \n->>> im\n+>>> image\n ```\n \n <div class=\"flex justify-center\">\n@@ -201,34 +196,43 @@ For batch processing, you should pass text queries as a nested list to the proce\n PyTorch tensors, or NumPy arrays.\n \n ```py\n->>> images = [image, im]\n+>>> url1 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png\"\n+>>> url2 = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png\"\n+>>> images = [load_image(url1), load_image(url2)]\n >>> text_queries = [\n ...     [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n-...     [\"hat\", \"book\", \"sunglasses\", \"camera\"],\n+...     [\"hat\", \"book\", \"sunglasses\", \"camera\", \"can\"],\n ... ]\n->>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")\n+>>> inputs = processor(text=text_queries, images=images, return_tensors=\"pt\", padding=True)\n ```\n \n Previously for post-processing you passed the single image's size as a tensor, but you can also pass a tuple, or, in case\n of several images, a list of tuples. Let's create predictions for the two examples, and visualize the second one (`image_idx = 1`).\n \n ```py\n >>> with torch.no_grad():\n-...     outputs = model(**inputs)\n-...     target_sizes = [x.size[::-1] for x in images]\n-...     results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\n+>>>     outputs = model(**inputs)\n \n+>>> target_sizes = [(image.height, image.width) for image in images]\n+>>> results = processor.post_process_grounded_object_detection(\n+...     outputs, threshold=0.3, target_sizes=target_sizes, text_labels=text_labels,\n+... )\n+```\n+\n+Let's visualize the results:\n+\n+```py\n >>> image_idx = 1\n >>> draw = ImageDraw.Draw(images[image_idx])\n \n >>> scores = results[image_idx][\"scores\"].tolist()\n->>> labels = results[image_idx][\"labels\"].tolist()\n+>>> text_labels = results[image_idx][\"text_labels\"]\n >>> boxes = results[image_idx][\"boxes\"].tolist()\n \n->>> for box, score, label in zip(boxes, scores, labels):\n-...     xmin, ymin, xmax, ymax = box\n-...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n-...     draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")\n+>>> for box, score, text_label in zip(boxes, scores, text_labels):\n+>>>     xmin, ymin, xmax, ymax = box\n+>>>     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n+>>>     draw.text((xmin, ymin), f\"{text_label}: {round(score,2)}\", fill=\"white\")\n \n >>> images[image_idx]\n ```\n@@ -239,8 +243,17 @@ of several images, a list of tuples. Let's create predictions for the two exampl\n \n ## Image-guided object detection\n \n-In addition to zero-shot object detection with text queries, OWL-ViT offers image-guided object detection. This means\n-you can use an image query to find similar objects in the target image.\n+In addition to zero-shot object detection with text queries, models like [OWL-ViT](https://huggingface.co/collections/ariG23498/owlvit-689b0d0872a7634a6ea17ae7) and [OWLv2](https://huggingface.co/collections/ariG23498/owlv2-689b0d27bd7d96ba3c7f7530) offers image-guided object detection. This means you can use an image query to find similar\n+objects in the target image.\n+\n+```py\n+>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n+\n+>>> checkpoint = \"google/owlv2-base-patch16-ensemble\"\n+>>> model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint, device_map=\"auto\")\n+>>> processor = AutoProcessor.from_pretrained(checkpoint)\n+```\n+\n Unlike text queries, only a single example image is allowed.\n \n Let's take an image with two cats on a couch as a target image, and an image of a single cat\n@@ -262,6 +275,7 @@ Let's take a quick look at the images:\n >>> fig, ax = plt.subplots(1, 2)\n >>> ax[0].imshow(image_target)\n >>> ax[1].imshow(query_image)\n+>>> fig.show()\n ```\n \n <div class=\"flex justify-center\">\n@@ -298,4 +312,3 @@ as before except now there are no labels.\n <div class=\"flex justify-center\">\n      <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png\" alt=\"Cats with bounding boxes\"/>\n </div>\n-"
        }
    ],
    "stats": {
        "total": 157,
        "additions": 85,
        "deletions": 72
    }
}