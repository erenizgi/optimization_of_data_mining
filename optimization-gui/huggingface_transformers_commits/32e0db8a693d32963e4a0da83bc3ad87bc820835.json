{
    "author": "tomaarsen",
    "message": "[`tokenizers`] Ensure that add_prefix_space is propagated to backend_tokenizer.pre_tokenizer (#35593)\n\n* Ensure that add_prefix_space is propagated to backend_tokenizer.pre_tokenizer\r\n\r\nin PreTrainedTokenizerFast, rather than relying on subclasses to take care of this.\r\n\r\n* Simplify setting self.add_prefix_space, ensure pre_tok exists\r\n\r\n* Wrap in try-except to catch 'Custom PreTokenizer cannot be serialized'\r\n\r\nhttps://github.com/huggingface/tokenizers/blob/862d1a346a99183017b1eb5ad1aa3133b466784f/bindings/python/src/pre_tokenizers.rs#L672 produces the Exception. They're triggered by the roformer tests, as the RoFormerTokenizerFast uses a custom PreTokenizer.\r\n\r\n* Propagate add_prefix_space in T5TokenizerFast to superclass",
    "sha": "32e0db8a693d32963e4a0da83bc3ad87bc820835",
    "files": [
        {
            "sha": "f7c92b9d22c99f1e4d69e7c3d36785ad9c6825d9",
            "filename": "src/transformers/models/bart/tokenization_bart_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Ftokenization_bart_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -16,7 +16,7 @@\n import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import AddedToken, BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -157,14 +157,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         # the pre_tokenizer is already updated in the GPT2TokenizerFast `__init__`\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)"
        },
        {
            "sha": "8667fe76349dfa786180a59581e2d3eff8040b67",
            "filename": "src/transformers/models/blenderbot/tokenization_blenderbot_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Ftokenization_blenderbot_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -17,7 +17,7 @@\n import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import AddedToken, BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -160,14 +160,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n         if tokenizer_component_instance:"
        },
        {
            "sha": "86782cf807074f2d28ea2b51c080ccf94899674d",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n-import json\n import re\n from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n@@ -29,7 +28,6 @@\n     if is_tf_available():\n         import tensorflow as tf\n \n-from tokenizers import pre_tokenizers\n \n from ...tokenization_utils_base import BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -137,14 +135,6 @@ def __init__(\n                 \" so that the fast tokenizer works correctly.\"\n             )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n     def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n         is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n         assert self.add_prefix_space or not is_split_into_words, ("
        },
        {
            "sha": "368f29b522bc48e218e7f9b782bd88cf818aec62",
            "filename": "src/transformers/models/deberta/tokenization_deberta_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Ftokenization_deberta_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -14,11 +14,8 @@\n # limitations under the License.\n \"\"\"Fast Tokenization class for model DeBERTa.\"\"\"\n \n-import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers\n-\n from ...tokenization_utils_base import AddedToken, BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import logging\n@@ -132,14 +129,6 @@ def __init__(\n         )\n         self.add_bos_token = kwargs.pop(\"add_bos_token\", False)\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n     @property\n     def mask_token(self) -> str:\n         \"\"\""
        },
        {
            "sha": "81e67a818deebeeeed8f9020510c9fbb6e0c1d5e",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -14,11 +14,8 @@\n # limitations under the License.\n \"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n \n-import json\n from typing import Optional, Tuple\n \n-from tokenizers import pre_tokenizers\n-\n from ...tokenization_utils_base import BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import logging\n@@ -109,14 +106,6 @@ def __init__(\n \n         self.add_bos_token = kwargs.pop(\"add_bos_token\", False)\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n     def _batch_encode_plus(self, *args, **kwargs) -> BatchEncoding:\n         is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n         assert self.add_prefix_space or not is_split_into_words, ("
        },
        {
            "sha": "d2ea1c3984fc3a243809ba79978b04ec2a2c2121",
            "filename": "src/transformers/models/gpt_neox/tokenization_gpt_neox_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Ftokenization_gpt_neox_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -14,10 +14,9 @@\n # limitations under the License.\n \"\"\"Tokenization classes for GPTNeoX.\"\"\"\n \n-import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils import logging\n@@ -122,14 +121,6 @@ def __init__(\n         self._add_eos_token = add_eos_token\n         self.update_post_processor()\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n     @property\n     def add_eos_token(self):\n         return self._add_eos_token"
        },
        {
            "sha": "ff67d233ffe97b9d7fde901337b28692cf22356b",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -20,7 +20,7 @@\n import json\n from typing import Dict, List, Optional, Tuple, Union\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import (\n     BatchEncoding,\n@@ -162,14 +162,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n         if tokenizer_component_instance:"
        },
        {
            "sha": "06e959e87542c505d756d95711b5c1bdeaa36e41",
            "filename": "src/transformers/models/led/tokenization_led_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -17,7 +17,7 @@\n import json\n from typing import Dict, List, Optional, Tuple, Union\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import AddedToken, BatchEncoding, EncodedInput\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -157,14 +157,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         # the pre_tokenizer is already updated in the GPT2TokenizerFast `__init__`\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)"
        },
        {
            "sha": "b8111b3d8a25f35bb1fae0f1aa28e28b35fa3cfd",
            "filename": "src/transformers/models/longformer/tokenization_longformer_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Ftokenization_longformer_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -17,7 +17,7 @@\n import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import AddedToken, BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -155,14 +155,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n         if tokenizer_component_instance:"
        },
        {
            "sha": "ec6808348abd4f869a83d229c8b98ec42dc5f095",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -21,7 +21,7 @@\n from functools import lru_cache\n from typing import Dict, List, Optional, Tuple, Union\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...file_utils import PaddingStrategy, TensorType, add_end_docstrings\n from ...tokenization_utils_base import (\n@@ -207,14 +207,6 @@ def __init__(\n \n         self.tags_dict = tags_dict\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n         if tokenizer_component_instance:"
        },
        {
            "sha": "ae226812c83a3f341a4a22a76bbefb9cd712ed79",
            "filename": "src/transformers/models/mvp/tokenization_mvp_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Ftokenization_mvp_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -16,7 +16,7 @@\n import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import AddedToken, BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -160,14 +160,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         # the pre_tokenizer is already updated in the GPT2TokenizerFast `__init__`\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)"
        },
        {
            "sha": "cf288f4d8c7c11d7482ea1e38da101f588215894",
            "filename": "src/transformers/models/roberta/tokenization_roberta_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Ftokenization_roberta_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -17,7 +17,7 @@\n import json\n from typing import List, Optional, Tuple\n \n-from tokenizers import pre_tokenizers, processors\n+from tokenizers import processors\n \n from ...tokenization_utils_base import AddedToken, BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -154,14 +154,6 @@ def __init__(\n             **kwargs,\n         )\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n-        self.add_prefix_space = add_prefix_space\n-\n         tokenizer_component = \"post_processor\"\n         tokenizer_component_instance = getattr(self.backend_tokenizer, tokenizer_component, None)\n         if tokenizer_component_instance:"
        },
        {
            "sha": "8eb652728bfac9bed797679df9adedfdacf0cefe",
            "filename": "src/transformers/models/t5/tokenization_t5_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -124,6 +124,7 @@ def __init__(\n             pad_token=pad_token,\n             extra_ids=extra_ids,\n             additional_special_tokens=additional_special_tokens,\n+            add_prefix_space=add_prefix_space,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "9a2c65254403a60a0e9dd4bb87a5d80b5dd16651",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -22,7 +22,7 @@\n from typing import List, Optional, Tuple\n \n import numpy as np\n-from tokenizers import AddedToken, pre_tokenizers, processors\n+from tokenizers import AddedToken, processors\n \n from ...tokenization_utils_base import BatchEncoding\n from ...tokenization_utils_fast import PreTrainedTokenizerFast\n@@ -128,19 +128,12 @@ def __init__(\n \n         self.add_bos_token = kwargs.pop(\"add_bos_token\", False)\n \n-        pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n-        if pre_tok_state.get(\"add_prefix_space\", add_prefix_space) != add_prefix_space:\n-            pre_tok_class = getattr(pre_tokenizers, pre_tok_state.pop(\"type\"))\n-            pre_tok_state[\"add_prefix_space\"] = add_prefix_space\n-            self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n-\n         if normalizer_file is not None:\n             with open(normalizer_file, encoding=\"utf-8\") as vocab_handle:\n                 self.english_spelling_normalizer = json.load(vocab_handle)\n         else:\n             self.english_spelling_normalizer = None\n \n-        self.add_prefix_space = add_prefix_space\n         self.timestamp_pat = re.compile(r\"<\\|(\\d+\\.\\d+)\\|>\")\n \n         self.language = language"
        },
        {
            "sha": "925069f2c2f9574c173f4d7ca399b0786dbc0f2e",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -102,6 +102,7 @@ def __init__(self, *args, **kwargs):\n         fast_tokenizer_file = kwargs.pop(\"tokenizer_file\", None)\n         from_slow = kwargs.pop(\"from_slow\", False)\n         added_tokens_decoder = kwargs.pop(\"added_tokens_decoder\", {})\n+        self.add_prefix_space = kwargs.get(\"add_prefix_space\", False)\n \n         if from_slow and slow_tokenizer is None and self.slow_tokenizer_class is None:\n             raise ValueError(\n@@ -206,6 +207,18 @@ def __init__(self, *args, **kwargs):\n             if tokens:\n                 self.add_tokens(tokens)\n \n+        try:\n+            pre_tok_state = json.loads(self.backend_tokenizer.pre_tokenizer.__getstate__())\n+            if pre_tok_state.get(\"add_prefix_space\", self.add_prefix_space) != self.add_prefix_space:\n+                pre_tok_class = getattr(pre_tokenizers_fast, pre_tok_state.pop(\"type\"))\n+                pre_tok_state[\"add_prefix_space\"] = self.add_prefix_space\n+                self.backend_tokenizer.pre_tokenizer = pre_tok_class(**pre_tok_state)\n+        except Exception:\n+            # We'll get an error if there is no pre_tokenizer, or if it's a custom pre_tokenizer that can\n+            # not be serialized. In those cases, we just ignore the error as there's no pre_tokenizer\n+            # for which we need to update the `add_prefix_space` attribute.\n+            pass\n+\n     @property\n     def is_fast(self) -> bool:\n         return True"
        },
        {
            "sha": "9bf90efd4b5c70d9a031110a1990aba17d10f677",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/32e0db8a693d32963e4a0da83bc3ad87bc820835/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32e0db8a693d32963e4a0da83bc3ad87bc820835/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=32e0db8a693d32963e4a0da83bc3ad87bc820835",
            "patch": "@@ -4684,3 +4684,15 @@ def test_tokenizer_initialization_with_conflicting_key(self):\n \n         with self.assertRaises(AttributeError, msg=\"conflicts with the method\"):\n             get_tokenizer_func(get_vocab=True)\n+\n+    @parameterized.expand([(True,), (False,)])\n+    def test_rust_tokenizer_add_prefix_space(self, add_prefix_space):\n+        if not self.test_rust_tokenizer:\n+            self.skipTest(reason=\"test_rust_tokenizer is set to False\")\n+\n+        for tokenizer, pretrained_name, _ in self.tokenizers_list:\n+            fast_tokenizer = tokenizer.from_pretrained(pretrained_name, add_prefix_space=add_prefix_space)\n+            self.assertEqual(fast_tokenizer.add_prefix_space, add_prefix_space)\n+            # Only the ByteLevel pre-tokenizer has the `add_prefix_space` attribute, we have to ensure that it's set correctly\n+            if hasattr(fast_tokenizer.backend_tokenizer.pre_tokenizer, \"add_prefix_space\"):\n+                self.assertEqual(fast_tokenizer.backend_tokenizer.pre_tokenizer.add_prefix_space, add_prefix_space)"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 36,
        "deletions": 122
    }
}