{
    "author": "MekkCyber",
    "message": "[kernels] Remove RWKV kernel finally ! (#41493)\n\n* rm kernel\n\n* fix style",
    "sha": "b543679d0ec057cb51a1d0be7b86df0e78556763",
    "files": [
        {
            "sha": "571d5a8a8307e95aac689eb3c9333d1ad350c7de",
            "filename": "src/transformers/kernels/rwkv/wkv_cuda.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 187,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac7777be1675a46a4355142f6f48bf641e069bfb/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_cuda.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac7777be1675a46a4355142f6f48bf641e069bfb/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_cuda.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_cuda.cu?ref=ac7777be1675a46a4355142f6f48bf641e069bfb",
            "patch": "@@ -1,187 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-\n-#define MIN_VALUE (-1e38)\n-\n-template <typename F>\n-__global__ void kernel_forward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-template <typename F>\n-__global__ void kernel_forward_with_state(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, F *__restrict__ const _y, F *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    F *__restrict__ const y = _y + _offset;\n-    F *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    F aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        y[ii] = (e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-template <typename F>\n-__global__ void kernel_backward(\n-    const int B, const int T, const int C, const F *__restrict__ const _w, const F *__restrict__ const _u,\n-    const F *__restrict__ const _k, const F *__restrict__ const _v, const F *__restrict__ const _y,\n-    const F *__restrict__ const _gy, F *__restrict__ const _gw, F *__restrict__ const _gu, F *__restrict__ const _gk,\n-    F *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    F u = _u[_c];\n-    F w = _w[_c];\n-    const F *__restrict__ const k = _k + _offset;\n-    const F *__restrict__ const v = _v + _offset;\n-    const F *__restrict__ const y = _y + _offset;\n-    const F *__restrict__ const gy = _gy + _offset;\n-    F *__restrict__ const gk = _gk + _offset;\n-    F *__restrict__ const gv = _gv + _offset;\n-\n-    F q[Tmax], r[Tmax];\n-\n-    F gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-\n-        F ww = u + kk;\n-        F p = max(pp, ww);\n-        F e1 = exp(pp - p);\n-        F e2 = exp(ww - p);\n-        const F qq = gy[ii] / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = gw * _w[_c]; // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = gu;\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const F kk = k[ii];\n-        const F vv = v[ii];\n-        const F yy = y[ii];\n-        const F qq = q[i];\n-        const F rr = r[i];\n-\n-        F e1 = qq * exp(rr);\n-        F e2 = exp(kk + pp);\n-        gk[ii] = e1 * (vv - yy) + e2 * (aa * vv + bb);\n-        gv[ii] = e1 + e2 * aa;\n-\n-        const F ww = w + pp;\n-        const F www = rr - u - kk;\n-        const F p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
        },
        {
            "sha": "042cb4aba1db98be5916aea1de86a7fed0b6510d",
            "filename": "src/transformers/kernels/rwkv/wkv_cuda_bf16.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 186,
            "changes": 186,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac7777be1675a46a4355142f6f48bf641e069bfb/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_cuda_bf16.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac7777be1675a46a4355142f6f48bf641e069bfb/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_cuda_bf16.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_cuda_bf16.cu?ref=ac7777be1675a46a4355142f6f48bf641e069bfb",
            "patch": "@@ -1,186 +0,0 @@\n-#include <stdio.h>\n-#include <assert.h>\n-#include \"ATen/ATen.h\"\n-#define MIN_VALUE (-1e38)\n-typedef at::BFloat16 bf16;\n-\n-__global__ void kernel_forward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16((e1 * aa + e2 * vv) / (e1 * bb + e2));\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-}\n-\n-__global__ void kernel_forward_with_state_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, bf16 *__restrict__ const _y,\n-    float *__restrict__ const _s\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset_s = _b * C * 3 + _c * 3;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    bf16 *__restrict__ const y = _y + _offset;\n-    float *__restrict__ const s = _s + _offset_s;\n-\n-    // aa and bb are running sums divided by exp(pp) (to avoid overflow)\n-    float aa = s[0], bb = s[1], pp = s[2];\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        y[ii] = bf16(e1 * aa + e2 * vv) / (e1 * bb + e2);\n-        \n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    s[0] = aa;\n-    s[1] = bb;\n-    s[2] = pp;\n-}\n-\n-__global__ void kernel_backward_bf16(\n-    const int B, const int T, const int C, const float *__restrict__ const _w, const bf16 *__restrict__ const _u,\n-    const bf16 *__restrict__ const _k, const bf16 *__restrict__ const _v, const bf16 *__restrict__ const _y,\n-    const bf16 *__restrict__ const _gy, bf16 *__restrict__ const _gw, bf16 *__restrict__ const _gu,\n-    bf16 *__restrict__ const _gk, bf16 *__restrict__ const _gv\n-) {\n-    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n-    const int _b = idx / C;\n-    const int _c = idx % C;\n-    const int _offset = _b * T * C + _c;\n-\n-    float u = float(_u[_c]);\n-    float w = _w[_c];\n-    const bf16 *__restrict__ const k = _k + _offset;\n-    const bf16 *__restrict__ const v = _v + _offset;\n-    const bf16 *__restrict__ const y = _y + _offset;\n-    const bf16 *__restrict__ const gy = _gy + _offset;\n-    bf16 *__restrict__ const gk = _gk + _offset;\n-    bf16 *__restrict__ const gv = _gv + _offset;\n-\n-    float q[Tmax], r[Tmax];\n-\n-    float gw = 0, gu = 0, aa = 0, bb = 0, ga = 0, gb = 0, pp = MIN_VALUE;\n-    for (int i = 0; i < T; i++) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-\n-        float ww = u + kk;\n-        float p = max(pp, ww);\n-        float e1 = exp(pp - p);\n-        float e2 = exp(ww - p);\n-        const float qq = float(gy[ii]) / (e1 * bb + e2);\n-        gw += (ga - gb * yy) * e1 * qq;\n-        gu += (vv - yy) * e2 * qq;\n-        q[i] = qq;\n-        r[i] = ww - p;\n-\n-        ww = w + pp;\n-        p = max(ww, kk);\n-        e1 = exp(ww - p);\n-        e2 = exp(kk - p);\n-        ga = e1 * (aa + ga);\n-        gb = e1 * (bb + gb);\n-        aa = e1 * aa + e2 * vv;\n-        bb = e1 * bb + e2;\n-        pp = p;\n-    }\n-    const int _offsetBC = _b * C + _c;\n-    _gw[_offsetBC] = bf16(gw * _w[_c]); // multiply by w because of w -> -exp(w) in python forward()\n-    _gu[_offsetBC] = bf16(gu);\n-\n-    aa = 0, bb = 0, pp = MIN_VALUE;\n-    for (int i = T - 1; i >= 0; i--) {\n-        const int ii = i * C;\n-        const float kk = float(k[ii]);\n-        const float vv = float(v[ii]);\n-        const float yy = float(y[ii]);\n-        const float qq = q[i];\n-        const float rr = r[i];\n-\n-        float e1 = qq * exp(rr);\n-        float e2 = exp(kk + pp);\n-        gk[ii] = bf16(e1 * (vv - yy) + e2 * (aa * vv + bb));\n-        gv[ii] = bf16(e1 + e2 * aa);\n-\n-        const float ww = w + pp;\n-        const float www = rr - u - kk;\n-        const float p = max(ww, www);\n-        e1 = exp(ww - p);\n-        e2 = qq * exp(www - p);\n-        aa = e1 * aa + e2;\n-        bb = e1 * bb - e2 * yy;\n-        pp = p;\n-    }\n-}\n-\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y);\n-}\n-\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_forward_with_state_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, s);\n-}\n-\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv) {\n-    dim3 threadsPerBlock( min(C, 32) ); // requires --maxrregcount 60 for optimal performance\n-    assert(B * C % threadsPerBlock.x == 0);\n-    dim3 numBlocks(B * C / threadsPerBlock.x);\n-    kernel_backward_bf16<<<numBlocks, threadsPerBlock>>>(B, T, C, w, u, k, v, y, gy, gw, gu, gk, gv);\n-}"
        },
        {
            "sha": "55e7280665927b523a88021d5111daf28a63c905",
            "filename": "src/transformers/kernels/rwkv/wkv_op.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac7777be1675a46a4355142f6f48bf641e069bfb/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_op.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac7777be1675a46a4355142f6f48bf641e069bfb/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_op.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Frwkv%2Fwkv_op.cpp?ref=ac7777be1675a46a4355142f6f48bf641e069bfb",
            "patch": "@@ -1,66 +0,0 @@\n-#include <torch/extension.h>\n-#include \"ATen/ATen.h\"\n-typedef at::BFloat16 bf16;\n-\n-void cuda_forward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y);\n-void cuda_forward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y);\n-void cuda_forward_with_state(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *s);\n-void cuda_forward_with_state_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, float *s);\n-void cuda_backward(int B, int T, int C, float *w, float *u, float *k, float *v, float *y, float *gy, float *gw, float *gu, float *gk, float *gv);\n-void cuda_backward_bf16(int B, int T, int C, float *w, bf16 *u, bf16 *k, bf16 *v, bf16 *y, bf16 *gy, bf16 *gw, bf16 *gu, bf16 *gk, bf16 *gv);\n-\n-void forward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>());\n-}\n-void forward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>());\n-}\n-void forward_with_state(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), s.data_ptr<float>());\n-}\n-void forward_with_state_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &s) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_forward_with_state_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(), s.data_ptr<float>());\n-}\n-void backward(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward(B, T, C, w.data_ptr<float>(), u.data_ptr<float>(), k.data_ptr<float>(), v.data_ptr<float>(), y.data_ptr<float>(), gy.data_ptr<float>(), gw.data_ptr<float>(), gu.data_ptr<float>(), gk.data_ptr<float>(), gv.data_ptr<float>());\n-}\n-void backward_bf16(torch::Tensor &w, torch::Tensor &u, torch::Tensor &k, torch::Tensor &v, torch::Tensor &y, torch::Tensor &gy, torch::Tensor &gw, torch::Tensor &gu, torch::Tensor &gk, torch::Tensor &gv) {\n-    const int B = k.size(0);\n-    const int T = k.size(1);\n-    const int C = k.size(2);\n-    cuda_backward_bf16(B, T, C, w.data_ptr<float>(), u.data_ptr<bf16>(), k.data_ptr<bf16>(), v.data_ptr<bf16>(), y.data_ptr<bf16>(),\n-        gy.data_ptr<bf16>(), gw.data_ptr<bf16>(), gu.data_ptr<bf16>(), gk.data_ptr<bf16>(), gv.data_ptr<bf16>());\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-    m.def(\"forward\", &forward, \"wkv forward\");\n-    m.def(\"forward_bf16\", &forward_bf16, \"wkv forward bf16\");\n-    m.def(\"forward_with_state\", &forward_with_state, \"wkv forward with state\");\n-    m.def(\"forward_with_state_bf16\", &forward_with_state_bf16, \"wkv forward with state bf16\");\n-    m.def(\"backward\", &backward, \"wkv backward\");\n-    m.def(\"backward_bf16\", &backward_bf16, \"wkv backward bf16\");\n-}\n-\n-TORCH_LIBRARY(wkv, m) {\n-    m.def(\"forward\", forward);\n-    m.def(\"forward_bf16\", forward_bf16);\n-    m.def(\"forward_with_state\", forward_with_state);\n-    m.def(\"forward_with_state_bf16\", forward_with_state_bf16);\n-    m.def(\"backward\", backward);\n-    m.def(\"backward_bf16\", backward_bf16);\n-}"
        },
        {
            "sha": "947b6890ce18c9486ad3da00f2cfd20626285328",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 6,
            "deletions": 27,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b543679d0ec057cb51a1d0be7b86df0e78556763/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=b543679d0ec057cb51a1d0be7b86df0e78556763",
            "patch": "@@ -17,7 +17,6 @@\n \n import math\n from dataclasses import dataclass\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n@@ -30,6 +29,7 @@\n     ModelOutput,\n     auto_docstring,\n     is_bitsandbytes_available,\n+    is_kernels_available,\n     is_ninja_available,\n     is_torch_cuda_available,\n     logging,\n@@ -44,34 +44,13 @@\n \n \n def load_wkv_cuda_kernel(context_length):\n-    from torch.utils.cpp_extension import load as load_kernel\n-\n     global rwkv_cuda_kernel\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+\n+    from kernels import get_kernel\n \n-    kernel_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"rwkv\"\n-    cuda_kernel_files = [kernel_folder / f for f in [\"wkv_op.cpp\", \"wkv_cuda.cu\", \"wkv_cuda_bf16.cu\"]]\n-\n-    # Only load the kernel if it's not been loaded yet or if we changed the context length\n-    if rwkv_cuda_kernel is not None and rwkv_cuda_kernel.max_seq_length == context_length:\n-        return\n-\n-    logger.info(f\"Loading CUDA kernel for RWKV at context length of {context_length}.\")\n-\n-    flags = [\n-        \"-res-usage\",\n-        \"--maxrregcount 60\",\n-        \"--use_fast_math\",\n-        \"-O3\",\n-        \"-Xptxas -O3\",\n-        \"--extra-device-vectorization\",\n-        f\"-DTmax={context_length}\",\n-    ]\n-    rwkv_cuda_kernel = load_kernel(\n-        name=f\"wkv_{context_length}\",\n-        sources=cuda_kernel_files,\n-        verbose=(logging.get_verbosity() == logging.DEBUG),\n-        extra_cuda_cflags=flags,\n-    )\n+    rwkv_cuda_kernel = get_kernel(\"kernels-community/rwkv\")\n     rwkv_cuda_kernel.max_seq_length = context_length\n \n "
        }
    ],
    "stats": {
        "total": 472,
        "additions": 6,
        "deletions": 466
    }
}