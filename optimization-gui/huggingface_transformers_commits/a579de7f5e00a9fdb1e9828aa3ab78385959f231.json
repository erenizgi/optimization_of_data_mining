{
    "author": "nithinraok",
    "message": "Add Parakeet (#39062)\n\n* first commit\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* update to handle masking for bs>1\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* Add tests and docs\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* update model ids\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* update docs and improve style\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* update librosa location\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* import guard torch too\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* ruff code checks fix\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* ruff format check\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* updated to parakeet names\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* update script\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* Add tokenizer decoding\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* Remove other model dependency\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* clean tests\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* fix tests\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* linting\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* fix ruff lint warnings\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* move to seperate folders\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* add parakeet ctc model code\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* simplify encoder structure\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* update documentation\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* add parakeet to toctree\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* fix tests\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* add parakeet doc\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* Address comments\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* Update featurizer to compute lens directly\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* fix ruff tests\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* fix encoding format\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* fix minor ctc decoding\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* revert modular_model_converter.py changes\n\n* revert check_config_attributes.py changes\n\n* refactor: fastconformer & parakeet_ctc -> parakeet\n\n* modeling update\n\n* test update\n\n* propagate feature extractor updates\n\n* propagate doc changes\n\n* propagate doc changes\n\n* propagate tokenization changes\n\n* propagate conversion changes\n\n* remove fastconformer tests\n\n* remove modular\n\n* update processor\n\n* update processor\n\n* tset update\n\n* diverse fixes\n\n* 100% macthing greedy batched\n\n* Update conversion script.\n\n* Refactor docs.\n\n* Reafactor auto loading.\n\n* Refactor and fix tokenization and processing.\n\n* Update integration test.\n\n* Modeling fixes:\n- ensure correct attention mask shape\n- ensure layer drop returns valid output\n- correct blank token ID when computing CTC loss\n\n* Format and repo consistency.\n\n* Update model doc.\n\n* Fix feature extraction tests.\n\n* Fix (most) tokenizer tests.\n\n* Add pipeline example.\n\n* Fixes\n\n* Use eager_attention_forward from Llama.\n\n* Small tweaks.\n\n* Replace Sequential with ModuleList\n\n* Add check if not all layers copied\n\n* Clean tokenizer.\n\n* Standardize FastSpeech2ConformerConvolutionModule for Parakeet.\n\n* Switch to modular for modeling and processing.\n\n* Add processor tests.\n\n* Fix modeling tests.\n\n* Formating and docstrings.\n\n* Add `return_attention_mask` like other feature extractors.\n\n* clean up after merging main.\n\n* nits on modeling\n\n* configuration update\n\n* nit\n\n* simplification: use PretrainedTokenizerFast, simplify processor\n\n* add dtype arg to mel_filter_bank\n\n* feature extraction: simplify!\n\n* modeling update\n\n* change to ParakeetTokenizerFast\n\n* correct attention mask handling\n\n* auto update\n\n* proc update\n\n* test update\n\n* feature extraction fixes\n\n* modeling update\n\n* conversion script update\n\n* udpate tests feature integration\n\n* update tokenization and tests\n\n* processor tests\n\n* revert audio_utils\n\n* config docstring update\n\n* blank_token -> pad_token\n\n* modeling udpate\n\n* doc update\n\n* fix tests\n\n* fix test\n\n* fix tests\n\n* address review comments\n\n* add comment\n\n* add comment\n\n* explicitly not support flash\n\n* atttention straightforward masking\n\n* fix\n\n* tokenizer update: skipping blank tokens by default\n\n* doc update\n\n* fix max_positions_embeddings handling\n\n* nits\n\n* change atol faeture extraction integration tests\n\n* doc update + fix loss\n\n* doc update\n\n* nit\n\n* update integration test for A10\n\n* repo id name\n\n* nit\n\n---------\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: Eric B <ebezzam@gmail.com>",
    "sha": "a579de7f5e00a9fdb1e9828aa3ab78385959f231",
    "files": [
        {
            "sha": "eb813ade6a092ce959e8ab3ee8ce6b52ffd7bd1a",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -935,6 +935,8 @@\n         title: MusicGen\n       - local: model_doc/musicgen_melody\n         title: MusicGen Melody\n+      - local: model_doc/parakeet\n+        title: Parakeet\n       - local: model_doc/pop2piano\n         title: Pop2Piano\n       - local: model_doc/seamless_m4t"
        },
        {
            "sha": "7713c8f61a4841196c873f01955b20d3eb3d88e8",
            "filename": "docs/source/en/model_doc/parakeet.md",
            "status": "added",
            "additions": 220,
            "deletions": 0,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fparakeet.md?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,220 @@\n+<!--Copyright 2025 The NVIDIA NeMo Team and The HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+# Parakeet\n+\n+## Overview\n+\n+Parakeet models, [introduced by NVIDIA NeMo](https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/), are models that combine a [Fast Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#fast-conformer) encoder with connectionist temporal classification (CTC), recurrent neural network transducer (RNNT) or token and duration transducer (TDT) decoder for automatic speech recognition.\n+\n+**Model Architecture**\n+- **Fast Conformer Encoder**: A linearly scalable Conformer architecture that processes mel-spectrogram features and reduces sequence length through subsampling. This is more efficient version of the Conformer Encoder found in [FastSpeech2Conformer](./fastspeech2_conformer.md) (see [`ParakeetEncoder`] for the encoder implementation and details).\n+- [**ParakeetForCTC**](#parakeetforctc): a Fast Conformer Encoder + a CTC decoder\n+    - **CTC Decoder**: Simple but effective decoder consisting of:\n+        - 1D convolution projection from encoder hidden size to vocabulary size (for optimal NeMo compatibility).\n+        - CTC loss computation for training.\n+        - Greedy CTC decoding for inference.\n+\n+The original implementation can be found in [NVIDIA NeMo](https://github.com/NVIDIA/NeMo).\n+Model checkpoints are to be found under [the NVIDIA organization](https://huggingface.co/nvidia/models?search=parakeet).\n+\n+This model was contributed by [Nithin Rao Koluguri](https://huggingface.co/nithinraok), [Eustache Le Bihan](https://huggingface.co/eustlb) and [Eric Bezzam](https://huggingface.co/bezzam).\n+\n+## Usage\n+\n+### Basic usage\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+from transformers import pipeline\n+\n+pipe = pipeline(\"automatic-speech-recognition\", model=\"nvidia/parakeet-ctc-1.1b\")\n+out = pipe(\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\")\n+print(out)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+from transformers import AutoModelForCTC, AutoProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+processor = AutoProcessor.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n+model = AutoModelForCTC.from_pretrained(\"nvidia/parakeet-ctc-1.1b\", dtype=\"auto\", device_map=device)\n+\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+speech_samples = [el['array'] for el in ds[\"audio\"][:5]]\n+\n+inputs = processor(speech_samples, sampling_rate=processor.feature_extractor.sampling_rate)\n+inputs.to(model.device, dtype=model.dtype)\n+outputs = model.generate(**inputs)\n+print(processor.batch_decode(outputs))\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+### Making The Model Go Brrr\n+\n+Parakeet supports full-graph compilation with CUDA graphs! This optimization is most effective when you know the maximum audio length you want to transcribe. The key idea is using static input shapes to avoid recompilation. For example, if you know your audio will be under 30 seconds, you can use the processor to pad all inputs to 30 seconds, preparing consistent input features and attention masks. See the example below!\n+\n+```python\n+from transformers import AutoModelForCTC, AutoProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+processor = AutoProcessor.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n+model = AutoModelForCTC.from_pretrained(\"nvidia/parakeet-ctc-1.1b\", dtype=\"auto\", device_map=device)\n+\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+speech_samples = [el['array'] for el in ds[\"audio\"][:5]]\n+\n+# Compile the generate method with fullgraph and CUDA graphs\n+model.generate = torch.compile(model.generate, fullgraph=True, mode=\"reduce-overhead\")\n+\n+# let's define processor kwargs to pad to 30 seconds\n+processor_kwargs = {\n+    \"padding\": \"max_length\",\n+    \"max_length\": 30 * processor.feature_extractor.sampling_rate,\n+}\n+\n+# Define a timing context using CUDA events\n+class TimerContext:\n+    def __init__(self, name=\"Execution\"):\n+        self.name = name\n+        self.start_event = None\n+        self.end_event = None\n+        \n+    def __enter__(self):\n+        # Use CUDA events for more accurate GPU timing\n+        self.start_event = torch.cuda.Event(enable_timing=True)\n+        self.end_event = torch.cuda.Event(enable_timing=True)\n+        self.start_event.record()\n+        return self\n+\n+    def __exit__(self, *args):\n+        self.end_event.record()\n+        torch.cuda.synchronize()\n+        elapsed_time = self.start_event.elapsed_time(self.end_event) / 1000.0\n+        print(f\"{self.name} time: {elapsed_time:.4f} seconds\")\n+\n+\n+inputs = processor(speech_samples[0], **processor_kwargs)\n+inputs.to(device, dtype=model.dtype)\n+print(\"\\n\" + \"=\"*50)\n+print(\"First generation - compiling...\")\n+# Generate with the compiled model\n+with TimerContext(\"First generation\"):\n+    outputs = model.generate(**inputs)\n+print(processor.batch_decode(outputs))\n+\n+inputs = processor(speech_samples[1], **processor_kwargs)\n+inputs.to(device, dtype=model.dtype)\n+print(\"\\n\" + \"=\"*50)\n+print(\"Second generation - recording CUDA graphs...\")\n+with TimerContext(\"Second generation\"):\n+    outputs = model.generate(**inputs)\n+print(processor.batch_decode(outputs))\n+\n+inputs = processor(speech_samples[2], **processor_kwargs)\n+inputs.to(device, dtype=model.dtype)\n+print(\"\\n\" + \"=\"*50)\n+print(\"Third generation - fast !!!\")\n+with TimerContext(\"Third generation\"):\n+    outputs = model.generate(**inputs)\n+print(processor.batch_decode(outputs))\n+\n+inputs = processor(speech_samples[3], **processor_kwargs)\n+inputs.to(device, dtype=model.dtype)\n+print(\"\\n\" + \"=\"*50)\n+print(\"Fourth generation - still fast !!!\")\n+with TimerContext(\"Fourth generation\"):\n+    outputs = model.generate(**inputs)\n+print(processor.batch_decode(outputs))\n+```\n+\n+### Training\n+\n+```python\n+from transformers import AutoModelForCTC, AutoProcessor\n+from datasets import load_dataset, Audio\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+processor = AutoProcessor.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n+model = AutoModelForCTC.from_pretrained(\"nvidia/parakeet-ctc-1.1b\", dtype=\"auto\", device_map=device)\n+\n+ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+speech_samples = [el['array'] for el in ds[\"audio\"][:5]]\n+text_samples = [el for el in ds[\"text\"][:5]]\n+\n+# passing `text` to the processor will prepare inputs' `labels` key\n+inputs = processor(audio=speech_samples, text=text_samples, sampling_rate=processor.feature_extractor.sampling_rate)\n+inputs.to(device, dtype=model.dtype)\n+\n+outputs = model(**inputs)\n+outputs.loss.backward()\n+```\n+\n+## ParakeetTokenizerFast\n+\n+[[autodoc]] ParakeetTokenizerFast \n+\n+## ParakeetFeatureExtractor\n+\n+[[autodoc]] ParakeetFeatureExtractor\n+    - __call__\n+\n+## ParakeetProcessor\n+\n+[[autodoc]] ParakeetProcessor\n+    - __call__\n+    - batch_decode\n+    - decode\n+\n+## ParakeetEncoderConfig\n+\n+[[autodoc]] ParakeetEncoderConfig \n+\n+## ParakeetCTCConfig\n+\n+[[autodoc]] ParakeetCTCConfig \n+\n+## ParakeetEncoder\n+\n+[[autodoc]] ParakeetEncoder\n+\n+## ParakeetForCTC\n+\n+[[autodoc]] ParakeetForCTC\n+"
        },
        {
            "sha": "aa32734ffb383878481e0d7bb205f0fcc8845198",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -1540,6 +1540,54 @@ def post_processor(self):\n         )\n \n \n+class ParakeetConverter(SpmConverter):\n+    handle_byte_fallback = True\n+\n+    def __init__(self, vocab_file=None, *args):\n+        self.vocab_file = vocab_file\n+\n+        requires_backends(self, \"protobuf\")\n+\n+        Converter.__init__(self, vocab_file)\n+\n+        model_pb2 = import_protobuf()\n+        m = model_pb2.ModelProto()\n+        with open(vocab_file, \"rb\") as f:\n+            m.ParseFromString(f.read())\n+        self.proto = m\n+\n+    def tokenizer(self, proto):\n+        vocab_scores = self.vocab(proto)\n+\n+        _, merges = self.SpmExtractor(self.vocab_file).extract(vocab_scores)\n+        bpe_vocab = {word: i for i, (word, score) in enumerate(vocab_scores)}\n+        tokenizer = Tokenizer(\n+            BPE(\n+                bpe_vocab,\n+                merges,\n+                unk_token=proto.trainer_spec.unk_piece,\n+                fuse_unk=True,\n+                byte_fallback=self.handle_byte_fallback,\n+                dropout=None,\n+            )\n+        )\n+\n+        # Add user defined symbols and control tokens from sentencepiece model\n+        spm_added_tokens = [\n+            (id, p.piece, p.type == 3 or p.piece in self.special_tokens)\n+            for id, p in enumerate(proto.pieces)\n+            if p.type in [3, 4]\n+        ]\n+        tokenizer.add_tokens(\n+            [\n+                AddedToken(token, normalized=False, special=special)\n+                for id, token, special in sorted(spm_added_tokens, key=lambda x: x[0])\n+            ]\n+        )\n+\n+        return tokenizer\n+\n+\n # Copied from transformers.models.gpt2.tokenization_gpt2.bytes_to_unicode\n def bytes_to_unicode():\n     \"\"\""
        },
        {
            "sha": "2905a842612e114adb1c387e3b9f19be472a2294",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -253,6 +253,7 @@\n     from .owlv2 import *\n     from .owlvit import *\n     from .paligemma import *\n+    from .parakeet import *\n     from .patchtsmixer import *\n     from .patchtst import *\n     from .pegasus import *"
        },
        {
            "sha": "c40b5a37b02a44842fd8b03d5d7fff31656ac419",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -296,6 +296,8 @@\n         (\"owlv2\", \"Owlv2Config\"),\n         (\"owlvit\", \"OwlViTConfig\"),\n         (\"paligemma\", \"PaliGemmaConfig\"),\n+        (\"parakeet_ctc\", \"ParakeetCTCConfig\"),\n+        (\"parakeet_encoder\", \"ParakeetEncoderConfig\"),\n         (\"patchtsmixer\", \"PatchTSMixerConfig\"),\n         (\"patchtst\", \"PatchTSTConfig\"),\n         (\"pegasus\", \"PegasusConfig\"),\n@@ -745,6 +747,9 @@\n         (\"owlv2\", \"OWLv2\"),\n         (\"owlvit\", \"OWL-ViT\"),\n         (\"paligemma\", \"PaliGemma\"),\n+        (\"parakeet\", \"Parakeet\"),\n+        (\"parakeet_ctc\", \"Parakeet\"),\n+        (\"parakeet_encoder\", \"ParakeetEncoder\"),\n         (\"patchtsmixer\", \"PatchTSMixer\"),\n         (\"patchtst\", \"PatchTST\"),\n         (\"pegasus\", \"Pegasus\"),\n@@ -984,6 +989,8 @@\n         (\"blip_2_qformer\", \"blip_2\"),\n         (\"fastspeech2_conformer_with_hifigan\", \"fastspeech2_conformer\"),\n         (\"perception_encoder\", \"perception_lm\"),\n+        (\"parakeet_encoder\", \"parakeet\"),\n+        (\"parakeet_ctc\", \"parakeet\"),\n     ]\n )\n "
        },
        {
            "sha": "6d4c4f554d9dcbaceadff65ff84d5fbe818fa96a",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -81,6 +81,8 @@\n         (\"moshi\", \"EncodecFeatureExtractor\"),\n         (\"nat\", \"ViTFeatureExtractor\"),\n         (\"owlvit\", \"OwlViTFeatureExtractor\"),\n+        (\"parakeet_ctc\", \"ParakeetFeatureExtractor\"),\n+        (\"parakeet_encoder\", \"ParakeetFeatureExtractor\"),\n         (\"perceiver\", \"PerceiverFeatureExtractor\"),\n         (\"phi4_multimodal\", \"Phi4MultimodalFeatureExtractor\"),\n         (\"poolformer\", \"PoolFormerFeatureExtractor\"),"
        },
        {
            "sha": "297d4890d1317af738d47f733e6b8250723fd5f6",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -295,6 +295,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"owlv2\", \"Owlv2Model\"),\n         (\"owlvit\", \"OwlViTModel\"),\n         (\"paligemma\", \"PaliGemmaModel\"),\n+        (\"parakeet_ctc\", \"ParakeetForCTC\"),\n+        (\"parakeet_encoder\", \"ParakeetEncoder\"),\n         (\"patchtsmixer\", \"PatchTSMixerModel\"),\n         (\"patchtst\", \"PatchTSTModel\"),\n         (\"pegasus\", \"PegasusModel\"),\n@@ -1601,6 +1603,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"data2vec-audio\", \"Data2VecAudioForCTC\"),\n         (\"hubert\", \"HubertForCTC\"),\n         (\"mctct\", \"MCTCTForCTC\"),\n+        (\"parakeet_ctc\", \"ParakeetForCTC\"),\n         (\"sew\", \"SEWForCTC\"),\n         (\"sew-d\", \"SEWDForCTC\"),\n         (\"unispeech\", \"UniSpeechForCTC\"),"
        },
        {
            "sha": "5aecc119560e3cace20cdc9da16d9b3b7faa1430",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -502,6 +502,7 @@\n         (\"owlv2\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"owlvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"paligemma\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"parakeet\", (\"ParakeetCTCTokenizer\", None)),\n         (\n             \"pegasus\",\n             ("
        },
        {
            "sha": "5a2dc39385b3c953eb256bb03c04d6456c8f8890",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 25,
            "deletions": 8,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n@@ -472,24 +473,37 @@ def forward(\n \n \n class FastSpeech2ConformerConvolutionModule(nn.Module):\n-    def __init__(self, config: FastSpeech2ConformerConfig, module_config):\n+    def __init__(self, config: FastSpeech2ConformerConfig, module_config=None):\n+        \"\"\"\n+        Args:\n+            config (FastSpeech2ConformerConfig): Configuration for the model.\n+            module_config (dict): Configuration for the module (e.g., encoder or decoder).\n+        \"\"\"\n         super().__init__()\n-        # kernel_size should be an odd number for 'SAME' padding\n         channels = config.hidden_size\n-        kernel_size = module_config[\"kernel_size\"]\n+        # kernel_size should be an odd number for 'SAME' padding\n+        if module_config is None:\n+            # e.g. using `ParakeetEncoderConfig` in src/transformers/models/parakeet/configuration_parakeet.py\n+            kernel_size = config.conv_kernel_size\n+            self.activation = ACT2FN[getattr(config, \"hidden_act\", \"silu\")]\n+        else:\n+            kernel_size = module_config[\"kernel_size\"]\n+            self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n+        self.padding = (kernel_size - 1) // 2\n         self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n         self.depthwise_conv = nn.Conv1d(\n-            channels, channels, kernel_size, stride=1, padding=(kernel_size - 1) // 2, groups=channels, bias=True\n+            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n         )\n         self.norm = nn.BatchNorm1d(channels)\n         self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states, attention_mask=None):\n         \"\"\"\n         Compute convolution module.\n \n         Args:\n             hidden_states (`torch.Tensor` of shape `(batch, time, channels)`): Input tensor.\n+            attention_mask (`torch.Tensor` of shape `(batch, 1, time)`): Attention mask.\n \n         Returns:\n             `torch.Tensor`: Output tensor of shape `(batch, time, channels)`.\n@@ -503,12 +517,15 @@ def forward(self, hidden_states):\n         # (batch_size, channel, dim)\n         hidden_states = nn.functional.glu(hidden_states, dim=1)\n \n+        # Apply padding mask before convolution\n+        if attention_mask is not None:\n+            all_masked_rows = torch.all(~attention_mask, dim=-1)\n+            hidden_states = hidden_states.masked_fill(all_masked_rows, 0.0)\n+\n         # 1D Depthwise Conv\n         hidden_states = self.depthwise_conv(hidden_states)\n         hidden_states = self.norm(hidden_states)\n-\n-        hidden_states = hidden_states * torch.sigmoid(hidden_states)\n-\n+        hidden_states = self.activation(hidden_states)\n         hidden_states = self.pointwise_conv2(hidden_states)\n \n         return hidden_states.transpose(1, 2)"
        },
        {
            "sha": "5c54b2e2eadb2432844098e3c3ea75f129377a19",
            "filename": "src/transformers/models/parakeet/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2F__init__.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_parakeet import *\n+    from .feature_extraction_parakeet import *\n+    from .modeling_parakeet import *\n+    from .tokenization_parakeet_fast import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "3612da58006a8fc89d72471dddef4a63bc7646fc",
            "filename": "src/transformers/models/parakeet/configuration_parakeet.py",
            "status": "added",
            "additions": 235,
            "deletions": 0,
            "changes": 235,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconfiguration_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconfiguration_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconfiguration_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,235 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Parakeet model configuration.\"\"\"\n+\n+from typing import Union\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ParakeetEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`ParakeetEncoder`]. It is used to instantiate a\n+    `ParakeetEncoder` model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the layers and the hidden states.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        intermediate_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in the Transformer encoder.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler.\n+        attention_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the attention layers.\n+        conv_kernel_size (`int`, *optional*, defaults to 9):\n+            The kernel size of the convolution layers in the Conformer block.\n+        subsampling_factor (`int`, *optional*, defaults to 8):\n+            The factor by which the input sequence is subsampled.\n+        subsampling_conv_channels (`int`, *optional*, defaults to 256):\n+            The number of channels in the subsampling convolution layers.\n+        num_mel_bins (`int`, *optional*, defaults to 80):\n+            Number of mel features.\n+        subsampling_conv_kernel_size (`int`, *optional*, defaults to 3):\n+            The kernel size of the subsampling convolution layers.\n+        subsampling_conv_stride (`int`, *optional*, defaults to 2):\n+            The stride of the subsampling convolution layers.\n+        dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for all fully connected layers in the embeddings, encoder, and pooler.\n+        dropout_positions (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the positions in the input sequence.\n+        layerdrop (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for the layers in the encoder.\n+        activation_dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for activations inside the fully connected layer.\n+        attention_dropout (`float`, *optional*, defaults to 0.1):\n+            The dropout ratio for the attention layers.\n+        max_position_embeddings (`int`, *optional*, defaults to 5000):\n+            The maximum sequence length that this model might ever be used with.\n+        scale_input (`bool`, *optional*, defaults to `True`):\n+            Whether to scale the input embeddings.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+    Example:\n+        ```python\n+        >>> from transformers import ParakeetEncoderModel, ParakeetEncoderConfig\n+\n+        >>> # Initializing a `ParakeetEncoder` configuration\n+        >>> configuration = ParakeetEncoderConfig()\n+\n+        >>> # Initializing a model from the configuration\n+        >>> model = ParakeetEncoderModel(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+\n+    This configuration class is based on the ParakeetEncoder architecture from NVIDIA NeMo. You can find more details\n+    and pre-trained models at [nvidia/parakeet-ctc-1.1b](https://huggingface.co/nvidia/parakeet-ctc-1.1b).\n+    \"\"\"\n+\n+    model_type = \"parakeet_encoder\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        num_hidden_layers=24,\n+        num_attention_heads=8,\n+        intermediate_size=4096,\n+        hidden_act=\"silu\",\n+        attention_bias=True,\n+        conv_kernel_size=9,\n+        subsampling_factor=8,\n+        subsampling_conv_channels=256,\n+        num_mel_bins=80,\n+        subsampling_conv_kernel_size=3,\n+        subsampling_conv_stride=2,\n+        dropout=0.1,\n+        dropout_positions=0.0,\n+        layerdrop=0.1,\n+        activation_dropout=0.1,\n+        attention_dropout=0.1,\n+        max_position_embeddings=5000,\n+        scale_input=True,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            **kwargs,\n+        )\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_attention_heads  # LlamaAttention compatibility\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.attention_bias = attention_bias\n+\n+        if (conv_kernel_size - 1) % 2 != 0:\n+            raise ValueError(f\"conv_kernel_size must be odd, got {conv_kernel_size}\")\n+        self.conv_kernel_size = conv_kernel_size\n+\n+        self.subsampling_conv_kernel_size = subsampling_conv_kernel_size\n+        self.subsampling_conv_stride = subsampling_conv_stride\n+\n+        self.subsampling_factor = subsampling_factor\n+        self.subsampling_conv_channels = subsampling_conv_channels\n+        self.num_mel_bins = num_mel_bins\n+\n+        self.dropout = dropout\n+        self.dropout_positions = dropout_positions\n+        self.layerdrop = layerdrop\n+        self.activation_dropout = activation_dropout\n+        self.attention_dropout = attention_dropout\n+        self.max_position_embeddings = max_position_embeddings\n+        self.scale_input = scale_input\n+        self.initializer_range = initializer_range\n+\n+\n+class ParakeetCTCConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`ParakeetForCTC`]. It is used to instantiate a\n+    Parakeet CTC model according to the specified arguments, defining the model architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+            vocab_size (`int`, *optional*, defaults to 1025):\n+                Vocabulary size of the model.\n+            ctc_loss_reduction (`str`, *optional*, defaults to `\"mean\"`):\n+                Specifies the reduction to apply to the output of `torch.nn.CTCLoss`. Only relevant when training an\n+                instance of [`ParakeetForCTC`].\n+            ctc_zero_infinity (`bool`, *optional*, defaults to `True`):\n+                Whether to zero infinite losses and the associated gradients of `torch.nn.CTCLoss`. Infinite losses mainly\n+                occur when the inputs are too short to be aligned to the targets. Only relevant when training an instance\n+                of [`ParakeetForCTC`].\n+            encoder_config (`Union[dict, ParakeetEncoderConfig]`, *optional*):\n+                The config object or dictionary of the encoder.\n+            pad_token_id (`int`, *optional*, defaults to 1024):\n+                Padding token id. Also used as blank token id.\n+\n+    Example:\n+        ```python\n+        >>> from transformers import ParakeetForCTC, ParakeetCTCConfig\n+\n+        >>> # Initializing a Parakeet configuration\n+        >>> configuration = ParakeetCTCConfig()\n+\n+        >>> # Initializing a model from the configuration\n+        >>> model = ParakeetForCTC(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+\n+    This configuration class is based on the Parakeet CTC architecture from NVIDIA NeMo. You can find more details\n+    and pre-trained models at [nvidia/parakeet-ctc-1.1b](https://huggingface.co/nvidia/parakeet-ctc-1.1b).\n+    \"\"\"\n+\n+    model_type = \"parakeet_ctc\"\n+    sub_configs = {\"encoder_config\": ParakeetEncoderConfig}\n+\n+    def __init__(\n+        self,\n+        vocab_size=1025,\n+        ctc_loss_reduction=\"mean\",\n+        ctc_zero_infinity=True,\n+        encoder_config: Union[dict, ParakeetEncoderConfig] = None,\n+        pad_token_id=1024,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.ctc_loss_reduction = ctc_loss_reduction\n+        self.ctc_zero_infinity = ctc_zero_infinity\n+\n+        if isinstance(encoder_config, dict):\n+            self.encoder_config = ParakeetEncoderConfig(**encoder_config)\n+        elif encoder_config is None:\n+            self.encoder_config = ParakeetEncoderConfig()\n+\n+        self.encoder_config = self.encoder_config\n+        self.initializer_range = self.encoder_config.initializer_range\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            **kwargs,\n+        )\n+\n+    @classmethod\n+    def from_encoder_config(cls, encoder_config: ParakeetEncoderConfig, **kwargs):\n+        r\"\"\"\n+        Instantiate a [`ParakeetCTCConfig`] (or a derived class) from parakeet encoder model configuration.\n+\n+        Returns:\n+            [`ParakeetCTCConfig`]: An instance of a configuration object\n+        \"\"\"\n+\n+        return cls(encoder_config=encoder_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"ParakeetCTCConfig\", \"ParakeetEncoderConfig\"]"
        },
        {
            "sha": "f1998fbd81b8fd43d5f770086df3995edad8c0ea",
            "filename": "src/transformers/models/parakeet/convert_nemo_to_hf.py",
            "status": "added",
            "additions": 315,
            "deletions": 0,
            "changes": 315,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconvert_nemo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconvert_nemo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fconvert_nemo_to_hf.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,315 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import os\n+import re\n+import tarfile\n+\n+import torch\n+import yaml\n+from tokenizers import AddedToken\n+\n+from transformers import (\n+    ParakeetCTCConfig,\n+    ParakeetFeatureExtractor,\n+    ParakeetForCTC,\n+    ParakeetProcessor,\n+    ParakeetTokenizerFast,\n+)\n+from transformers.convert_slow_tokenizer import ParakeetConverter\n+from transformers.utils.hub import cached_file\n+\n+\n+NEMO_TO_HF_WEIGHT_MAPPING = {\n+    r\"encoder\\.pre_encode\\.conv\\.\": r\"encoder.subsampling.layers.\",\n+    r\"encoder\\.pre_encode\\.out\\.\": r\"encoder.subsampling.linear.\",\n+    r\"encoder\\.pos_enc\\.\": r\"encoder.encode_positions.\",\n+    r\"encoder\\.layers\\.(\\d+)\\.conv\\.batch_norm\\.\": r\"encoder.layers.\\1.conv.norm.\",\n+    r\"decoder\\.decoder_layers\\.0\\.(weight|bias)\": r\"ctc_head.\\1\",\n+    r\"linear_([kv])\": r\"\\1_proj\",\n+    r\"linear_out\": r\"o_proj\",\n+    r\"linear_q\": r\"q_proj\",\n+    r\"pos_bias_([uv])\": r\"bias_\\1\",\n+    r\"linear_pos\": r\"relative_k_proj\",\n+}\n+\n+\n+def convert_key(key, mapping):\n+    for pattern, replacement in mapping.items():\n+        key = re.sub(pattern, replacement, key)\n+    return key\n+\n+\n+def extract_nemo_archive(nemo_file_path: str, extract_dir: str) -> dict[str, str]:\n+    \"\"\"\n+    Extract .nemo file (tar archive) and return paths to important files.\n+\n+    Args:\n+        nemo_file_path: Path to .nemo file\n+        extract_dir: Directory to extract to\n+\n+    Returns:\n+        Dictionary with paths to model.pt, model_config.yaml, etc.\n+    \"\"\"\n+    print(f\"Extracting NeMo archive: {nemo_file_path}\")\n+\n+    with tarfile.open(nemo_file_path, \"r\", encoding=\"utf-8\") as tar:\n+        tar.extractall(extract_dir)\n+\n+    # Log all extracted files for debugging\n+    all_files = []\n+    for root, dirs, files in os.walk(extract_dir):\n+        for file in files:\n+            file_path = os.path.join(root, file)\n+            all_files.append(file_path)\n+\n+    print(f\"All extracted files: {[os.path.basename(f) for f in all_files]}\")\n+\n+    # Find important files with more robust detection\n+    model_files = {}\n+    for root, dirs, files in os.walk(extract_dir):\n+        for file in files:\n+            file_path = os.path.join(root, file)\n+            file_lower = file.lower()\n+\n+            # Look for model weights with various common names\n+            if (\n+                file.endswith(\".pt\")\n+                or file.endswith(\".pth\")\n+                or file.endswith(\".ckpt\")\n+                or file.endswith(\".bin\")\n+                or \"model\" in file_lower\n+                and (\"weight\" in file_lower or \"state\" in file_lower)\n+                or file_lower == \"model.pt\"\n+                or file_lower == \"pytorch_model.bin\"\n+                or file_lower == \"model_weights.ckpt\"\n+            ):\n+                model_files[\"model_weights\"] = file_path\n+                print(f\"Found model weights: {file}\")\n+\n+            # Look for config files\n+            elif (\n+                file == \"model_config.yaml\"\n+                or file == \"config.yaml\"\n+                or (file.endswith(\".yaml\") and \"config\" in file_lower)\n+            ):\n+                if \"model_config\" not in model_files:  # Prefer model_config.yaml\n+                    model_files[\"model_config\"] = file_path\n+                    print(f\"Found config file: {file}\")\n+                if file == \"model_config.yaml\":\n+                    model_files[\"model_config\"] = file_path  # Override with preferred name\n+\n+            # Look for vocabulary files\n+            elif (\n+                file.endswith(\".vocab\")\n+                or file.endswith(\".model\")\n+                or file.endswith(\".txt\")\n+                or (\"tokenizer\" in file_lower and (file.endswith(\".vocab\") or file.endswith(\".model\")))\n+            ):\n+                # Prefer .vocab files over others\n+                if \"tokenizer_model_file\" not in model_files or file.endswith(\".model\"):\n+                    model_files[\"tokenizer_model_file\"] = file_path\n+                    print(f\"Found tokenizer model file: {file}\")\n+                else:\n+                    print(f\"Found additional vocabulary file (using existing): {file}\")\n+\n+    print(f\"Found model files: {list(model_files.keys())}\")\n+\n+    # Validate that we found the required files\n+    if \"model_weights\" not in model_files:\n+        raise FileNotFoundError(\n+            f\"Could not find model weights file in {nemo_file_path}. \"\n+            f\"Expected files with extensions: .pt, .pth, .ckpt, .bin. \"\n+            f\"Found files: {[os.path.basename(f) for f in all_files]}\"\n+        )\n+\n+    if \"model_config\" not in model_files:\n+        raise FileNotFoundError(\n+            f\"Could not find model config file in {nemo_file_path}. \"\n+            f\"Expected: model_config.yaml or config.yaml. \"\n+            f\"Found files: {[os.path.basename(f) for f in all_files]}\"\n+        )\n+\n+    return model_files\n+\n+\n+def write_processor(nemo_config: dict, model_files, output_dir, push_to_repo_id=None):\n+    tokenizer_converted = ParakeetConverter(model_files[\"tokenizer_model_file\"]).converted()\n+    tokenizer_converted_fast = ParakeetTokenizerFast(\n+        tokenizer_object=tokenizer_converted,\n+        clean_up_tokenization_spaces=False,\n+    )\n+    tokenizer_converted_fast.add_tokens(\n+        [AddedToken(\"<unk>\", normalized=False, special=True), AddedToken(\"<pad>\", normalized=False, special=True)]\n+    )\n+    tokenizer_converted_fast.add_special_tokens(\n+        {\n+            \"pad_token\": AddedToken(\"<pad>\", normalized=False, special=True),\n+            \"unk_token\": AddedToken(\"<unk>\", normalized=False, special=True),\n+        }\n+    )\n+\n+    feature_extractor_keys_to_ignore = [\"_target_\", \"pad_to\", \"frame_splicing\", \"dither\", \"normalize\", \"window\", \"log\"]\n+    feature_extractor_config_keys_mapping = {\n+        \"sample_rate\": \"sampling_rate\",\n+        \"window_size\": \"win_length\",\n+        \"window_stride\": \"hop_length\",\n+        \"window\": \"window\",\n+        \"n_fft\": \"n_fft\",\n+        \"log\": \"log\",\n+        \"features\": \"feature_size\",\n+        \"dither\": \"dither\",\n+        \"pad_to\": \"pad_to\",\n+        \"pad_value\": \"padding_value\",\n+        \"frame_splicing\": \"frame_splicing\",\n+        \"preemphasis\": \"preemphasis\",\n+        \"hop_length\": \"hop_length\",\n+    }\n+    converted_feature_extractor_config = {}\n+\n+    for key, value in nemo_config[\"preprocessor\"].items():\n+        if key in feature_extractor_keys_to_ignore:\n+            continue\n+        if key in feature_extractor_config_keys_mapping:\n+            if key in [\"window_size\", \"window_stride\"]:\n+                value = int(value * nemo_config[\"preprocessor\"][\"sample_rate\"])\n+            converted_feature_extractor_config[feature_extractor_config_keys_mapping[key]] = value\n+        else:\n+            raise ValueError(f\"Key {key} not found in feature_extractor_keys_mapping\")\n+\n+    feature_extractor = ParakeetFeatureExtractor(**converted_feature_extractor_config)\n+\n+    processor = ParakeetProcessor(\n+        feature_extractor=feature_extractor,\n+        tokenizer=tokenizer_converted_fast,\n+    )\n+    processor.save_pretrained(output_dir)\n+\n+    if push_to_repo_id:\n+        processor.push_to_hub(push_to_repo_id)\n+\n+\n+def write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id=None):\n+    encoder_keys_to_ignore = [\n+        \"att_context_size\",\n+        \"causal_downsampling\",\n+        \"stochastic_depth_start_layer\",\n+        \"feat_out\",\n+        \"stochastic_depth_drop_prob\",\n+        \"_target_\",\n+        \"ff_expansion_factor\",\n+        \"untie_biases\",\n+        \"att_context_style\",\n+        \"self_attention_model\",\n+        \"conv_norm_type\",\n+        \"subsampling\",\n+        \"stochastic_depth_mode\",\n+        \"conv_context_size\",\n+        \"dropout_pre_encoder\",\n+    ]\n+    enocder_config_keys_mapping = {\n+        \"d_model\": \"hidden_size\",\n+        \"n_heads\": \"num_attention_heads\",\n+        \"n_layers\": \"num_hidden_layers\",\n+        \"feat_in\": \"num_mel_bins\",\n+        \"conv_kernel_size\": \"conv_kernel_size\",\n+        \"subsampling_factor\": \"subsampling_factor\",\n+        \"subsampling_conv_channels\": \"subsampling_conv_channels\",\n+        \"pos_emb_max_len\": \"max_position_embeddings\",\n+        \"dropout\": \"dropout\",\n+        \"dropout_emb\": \"dropout_positions\",\n+        \"dropout_att\": \"attention_dropout\",\n+        \"xscaling\": \"scale_input\",\n+    }\n+    converted_encoder_config = {}\n+\n+    for key, value in nemo_config[\"encoder\"].items():\n+        if key in encoder_keys_to_ignore:\n+            continue\n+        if key in enocder_config_keys_mapping:\n+            converted_encoder_config[enocder_config_keys_mapping[key]] = value\n+        else:\n+            raise ValueError(f\"Key {key} not found in enocder_config_keys_mapping\")\n+\n+    state_dict = torch.load(model_files[\"model_weights\"], map_location=\"cpu\", weights_only=True)\n+    converted_state_dict = {}\n+    for key, value in state_dict.items():\n+        # Skip preprocessing weights (featurizer components)\n+        if key.endswith(\"featurizer.window\") or key.endswith(\"featurizer.fb\"):\n+            print(f\"Skipping preprocessing weight: {key}\")\n+            continue\n+        converted_key = convert_key(key, NEMO_TO_HF_WEIGHT_MAPPING)\n+        converted_state_dict[converted_key] = value\n+\n+    if model_type == \"ctc\":\n+        model_config = ParakeetCTCConfig(\n+            encoder_config=converted_encoder_config,\n+        )\n+        print(\"Loading the checkpoint in a Parakeet CTC model.\")\n+        with torch.device(\"meta\"):\n+            model = ParakeetForCTC(model_config)\n+        model.load_state_dict(converted_state_dict, strict=True, assign=True)\n+        print(\"Checkpoint loaded successfully.\")\n+        del model.config._name_or_path\n+\n+        print(\"Saving the model.\")\n+        model.save_pretrained(output_dir)\n+\n+        if push_to_repo_id:\n+            model.push_to_hub(push_to_repo_id)\n+\n+        del converted_state_dict, model\n+\n+        # Safety check: reload the converted model\n+        gc.collect()\n+        print(\"Reloading the model to check if it's saved correctly.\")\n+        ParakeetForCTC.from_pretrained(output_dir, dtype=torch.bfloat16, device_map=\"auto\")\n+        print(\"Model reloaded successfully.\")\n+\n+    else:\n+        raise ValueError(f\"Model type {model_type} not supported.\")\n+\n+\n+def main(\n+    hf_repo_id,\n+    output_dir,\n+    model_type,\n+    push_to_repo_id=None,\n+):\n+    nemo_filename = f\"{hf_repo_id.split('/')[-1]}.nemo\"\n+    filepath = cached_file(hf_repo_id, nemo_filename)\n+\n+    model_files = extract_nemo_archive(filepath, os.path.dirname(filepath))\n+    nemo_config = yaml.load(open(model_files[\"model_config\"], \"r\"), Loader=yaml.FullLoader)\n+\n+    write_processor(nemo_config, model_files, output_dir, push_to_repo_id)\n+    write_model(nemo_config, model_files, model_type, output_dir, push_to_repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--hf_repo_id\", required=True, help=\"Model repo on huggingface.co\")\n+    parser.add_argument(\"--model_type\", required=True, choices=[\"ctc\"], help=\"Model type (`ctc`, `tdt`)\")\n+    parser.add_argument(\"--output_dir\", required=True, help=\"Output directory for HuggingFace model\")\n+    parser.add_argument(\"--push_to_repo_id\", help=\"Repository ID to push the model to on the Hub\")\n+    args = parser.parse_args()\n+    main(\n+        args.hf_repo_id,\n+        args.output_dir,\n+        args.model_type,\n+        args.push_to_repo_id,\n+    )"
        },
        {
            "sha": "d28f1a214a217e8e9e7446f73dbe03a7530c3b6d",
            "filename": "src/transformers/models/parakeet/feature_extraction_parakeet.py",
            "status": "added",
            "additions": 287,
            "deletions": 0,
            "changes": 287,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Ffeature_extraction_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Ffeature_extraction_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Ffeature_extraction_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,287 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from ...feature_extraction_utils import BatchFeature\n+from ...utils import TensorType, is_librosa_available, logging\n+from ...utils.import_utils import requires\n+\n+\n+if is_librosa_available():\n+    import librosa\n+\n+\n+EPSILON = 1e-5\n+LOG_ZERO_GUARD_VALUE = 2**-24\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@requires(backends=(\"torch\", \"librosa\"))\n+class ParakeetFeatureExtractor(SequenceFeatureExtractor):\n+    r\"\"\"\n+    Constructs a Parakeet feature extractor.\n+\n+    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains\n+    most of the main methods. Users should refer to this superclass for more information regarding those methods.\n+\n+    This class extracts mel-filter bank features from raw speech using a custom numpy implementation of the `Short Time\n+    Fourier Transform` which should match pytorch's `torch.stft` equivalent.\n+\n+    Args:\n+        feature_size (`int`, *optional*, defaults to 80):\n+            The feature dimension of the extracted features.\n+        sampling_rate (`int`, *optional*, defaults to 16000):\n+            The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n+        hop_length (`int`, *optional*, defaults to 160):\n+            Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n+        n_fft (`int`, *optional*, defaults to 512):\n+            Size of the Fourier transform.\n+        win_length (`int`, *optional*, defaults to 400):\n+            The window length for the STFT computation.\n+        preemphasis (`float`, *optional*, defaults to 0.97):\n+            A preemphasis filter coefficient. 0.0 means no preemphasis filter.\n+        padding_value (`float`, *optional*, defaults to 0.0):\n+            Padding value used to pad the audio. Should correspond to silences.\n+    \"\"\"\n+\n+    model_input_names = [\"input_features\", \"attention_mask\"]\n+\n+    def __init__(\n+        self,\n+        feature_size=80,\n+        sampling_rate=16000,\n+        hop_length=160,\n+        n_fft=512,\n+        win_length=400,\n+        preemphasis=0.97,\n+        padding_value=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)\n+\n+        self.hop_length = hop_length\n+        self.n_fft = n_fft\n+        self.win_length = win_length\n+        self.preemphasis = preemphasis\n+\n+        # TODO: @eustlb, for now we use librosa to compute the mel filters\n+        # indeed mel_filter_bank uses np.float64 (while librosa uses np.float32), giving numerical differences\n+        # self.mel_filters = mel_filter_bank(\n+        #     num_frequency_bins=n_fft // 2 + 1,\n+        #     num_mel_filters=feature_size,\n+        #     min_frequency=0.0,\n+        #     max_frequency=sampling_rate / 2,\n+        #     sampling_rate=sampling_rate,\n+        #     norm=\"slaney\",\n+        #     mel_scale=\"slaney\",\n+        # )\n+        mel_filters = librosa.filters.mel(\n+            sr=sampling_rate, n_fft=n_fft, n_mels=feature_size, fmin=0.0, fmax=sampling_rate / 2, norm=\"slaney\"\n+        )\n+        self.mel_filters = torch.from_numpy(mel_filters).to(torch.float32)\n+\n+    def _torch_extract_fbank_features(self, waveform, device=\"cpu\"):\n+        # spectrogram\n+        window = torch.hann_window(self.win_length, periodic=False, device=device)\n+        stft = torch.stft(\n+            waveform,\n+            self.n_fft,\n+            hop_length=self.hop_length,\n+            win_length=self.win_length,\n+            window=window,\n+            return_complex=True,\n+            pad_mode=\"constant\",\n+        )\n+        # Let's math original implementation\n+        # magnitudes = torch.abs(stft) ** 2\n+        magnitudes = torch.view_as_real(stft)\n+        magnitudes = torch.sqrt(magnitudes.pow(2).sum(-1))\n+        magnitudes = magnitudes.pow(2)\n+\n+        # log mel spectrogram\n+        mel_filters = self.mel_filters.to(device)\n+        mel_spec = mel_filters @ magnitudes\n+        mel_spec = torch.log(mel_spec + LOG_ZERO_GUARD_VALUE)\n+\n+        # (batch_size, num_mel_filters, num_frames) -> (batch_size, num_frames, num_mel_filters)\n+        mel_spec = mel_spec.permute(0, 2, 1)\n+\n+        return mel_spec\n+\n+    def __call__(\n+        self,\n+        raw_speech: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n+        truncation: bool = False,\n+        pad_to_multiple_of: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        return_attention_mask: Optional[bool] = None,\n+        padding: Optional[str] = \"longest\",\n+        max_length: Optional[int] = None,\n+        sampling_rate: Optional[int] = None,\n+        do_normalize: Optional[bool] = None,\n+        device: Optional[str] = \"cpu\",\n+        return_token_timestamps: Optional[bool] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to featurize and prepare for the model one or several sequence(s). Implementation uses PyTorch for\n+        the STFT computation if available, otherwise a slower NumPy based one.\n+\n+        Args:\n+            raw_speech (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n+                The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n+                values, a list of numpy arrays or a list of list of float values. Must be mono channel audio, not\n+                stereo, i.e. single float per timestep.\n+            truncation (`bool`, *optional*, default to `True`):\n+                Activates truncation to cut input sequences longer than *max_length* to *max_length*.\n+            pad_to_multiple_of (`int`, *optional*, defaults to None):\n+                If set will pad the sequence to a multiple of the provided value.\n+\n+                This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n+                `>= 7.5` (Volta), or on TPUs which benefit from having sequence lengths be a multiple of 128.\n+            return_attention_mask (`bool`, *optional*):\n+                Whether to return the attention mask. If left to the default, will return the attention mask according\n+                to the specific feature_extractor's default.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+\n+                <Tip>\n+\n+                For Parakeet models, `attention_mask` should always be passed for batched inference, to avoid subtle\n+                bugs.\n+\n+                </Tip>\n+\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate at which the `raw_speech` input was sampled. It is strongly recommended to pass\n+                `sampling_rate` at the forward call to prevent silent errors and allow automatic speech recognition\n+                pipeline.\n+            padding_value (`float`, *optional*, defaults to 0.0):\n+                The value that is used to fill the padding values / vectors.\n+            do_normalize (`bool`, *optional*, defaults to `False`):\n+                Whether or not to zero-mean unit-variance normalize the input. Normalizing can help to significantly\n+                improve the performance of the model.\n+            device (`str`, *optional*, defaults to `'cpu'`):\n+                Specifies the device for computation of the log-mel spectrogram of audio signals in the\n+                `_torch_extract_fbank_features` method. (e.g., \"cpu\", \"cuda\")\n+            return_token_timestamps (`bool`, *optional*, defaults to `None`):\n+                Deprecated. Use `return_attention_mask` instead from which the number of frames can be inferred.\n+\n+                Whether or not to return the number of frames of the input raw_speech.\n+                These num_frames can be used by the model to compute word level timestamps.\n+        \"\"\"\n+        if sampling_rate is not None:\n+            if sampling_rate != self.sampling_rate:\n+                raise ValueError(\n+                    f\"The model corresponding to this feature extractor: {self.__class__.__name__} was trained using a\"\n+                    f\" sampling rate of {self.sampling_rate}. Please make sure that the provided `raw_speech` input\"\n+                    f\" was sampled with {self.sampling_rate} and not {sampling_rate}.\"\n+                )\n+        else:\n+            logger.warning(\n+                f\"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. \"\n+                \"Failing to do so can result in silent errors that might be hard to debug.\"\n+            )\n+\n+        # Convert to torch tensor\n+        if isinstance(raw_speech, np.ndarray):\n+            raw_speech = torch.tensor(raw_speech)\n+        elif isinstance(raw_speech, (list, tuple)) and isinstance(raw_speech[0], np.ndarray):\n+            raw_speech = [torch.tensor(speech) for speech in raw_speech]\n+\n+        is_batched_torch = isinstance(raw_speech, torch.Tensor) and len(raw_speech.shape) > 1\n+        if is_batched_torch and len(raw_speech.shape) > 2:\n+            logger.warning(\n+                f\"Only mono-channel audio is supported for input to {self.__class__.__name__}. \"\n+                \"We will take the mean of the channels to convert to mono.\"\n+            )\n+            raw_speech = raw_speech.mean(-1)\n+\n+        is_batched_sequence = isinstance(raw_speech, (list, tuple))\n+        if is_batched_sequence:\n+            for speech in raw_speech:\n+                if len(speech.shape) > 1:\n+                    logger.warning(\n+                        f\"Only mono-channel audio is supported for input to {self.__class__.__name__}. \"\n+                        \"We will take the mean of the channels to convert to mono.\"\n+                    )\n+                    speech = speech.mean(-1)\n+\n+        if is_batched_torch or is_batched_sequence:\n+            raw_speech = [speech[:, None].to(torch.float32) for speech in raw_speech]\n+        else:\n+            raw_speech = [raw_speech[:, None].to(torch.float32)]\n+\n+        audio_lengths = [len(speech) for speech in raw_speech]\n+        batched_speech = BatchFeature({\"input_features\": raw_speech, \"audio_lengths\": audio_lengths})\n+\n+        padded_inputs = self.pad(\n+            batched_speech,\n+            padding=padding,\n+            max_length=max_length,\n+            truncation=truncation,\n+            pad_to_multiple_of=pad_to_multiple_of,\n+            return_tensors=\"pt\",\n+        )\n+        input_features = padded_inputs.input_features.squeeze(-1)\n+\n+        # preemphasis\n+        if self.preemphasis is not None:\n+            timemask = torch.arange(input_features.shape[1], device=input_features.device).unsqueeze(\n+                0\n+            ) < padded_inputs.audio_lengths.unsqueeze(1)\n+            input_features = torch.cat(\n+                [input_features[:, :1], input_features[:, 1:] - self.preemphasis * input_features[:, :-1]], dim=1\n+            )\n+            input_features = input_features.masked_fill(~timemask, 0.0)\n+\n+        input_features = self._torch_extract_fbank_features(input_features, device)\n+        features_lengths = torch.floor_divide(\n+            padded_inputs.audio_lengths + self.n_fft // 2 * 2 - self.n_fft, self.hop_length\n+        )\n+        attention_mask = torch.arange(input_features.shape[1], device=device)[None, :] < features_lengths[:, None]\n+\n+        # normalize mel features, ignoring padding\n+        mask = attention_mask.unsqueeze(-1)\n+        input_features_masked = input_features * mask\n+        mean = input_features_masked.sum(dim=1) / features_lengths.unsqueeze(-1)\n+        mean = mean.unsqueeze(1)\n+        variance = ((input_features_masked - mean) ** 2 * mask).sum(dim=1) / (features_lengths - 1).unsqueeze(-1)\n+        std = torch.sqrt(variance).unsqueeze(1)\n+        input_features = (input_features - mean) / (std + EPSILON)\n+        input_features *= mask\n+\n+        return BatchFeature(\n+            data={\n+                \"input_features\": input_features,\n+                \"attention_mask\": attention_mask,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+\n+__all__ = [\"ParakeetFeatureExtractor\"]"
        },
        {
            "sha": "4190517b48fd1dde9b3bd92a38e06f02cfca8934",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "added",
            "additions": 744,
            "deletions": 0,
            "changes": 744,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,744 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/parakeet/modular_parakeet.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_parakeet.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n+\n+\n+class ParakeetEncoderRelPositionalEncoding(nn.Module):\n+    \"\"\"Relative positional encoding for Parakeet.\"\"\"\n+\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: ParakeetEncoderConfig, device=None):\n+        super().__init__()\n+        self.max_position_embeddings = config.max_position_embeddings\n+        base = 10000.0\n+        inv_freq = 1.0 / (\n+            base\n+            ** (\n+                torch.arange(0, config.hidden_size, 2, dtype=torch.int64).to(device=device, dtype=torch.float)\n+                / config.hidden_size\n+            )\n+        )\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, hidden_states: torch.Tensor):\n+        seq_length = hidden_states.shape[1]\n+        if seq_length > self.max_position_embeddings:\n+            raise ValueError(\n+                f\"Sequence Length: {seq_length} has to be less or equal than \"\n+                f\"config.max_position_embeddings {self.max_position_embeddings}.\"\n+            )\n+\n+        position_ids = torch.arange(seq_length - 1, -seq_length, -1, device=hidden_states.device)\n+        inv_freq_expanded = (\n+            self.inv_freq[None, :, None].float().expand(hidden_states.shape[0], -1, 1).to(hidden_states.device)\n+        )\n+        position_ids_expanded = position_ids[None, None, :].float()\n+\n+        device_type = (\n+            hidden_states.device.type\n+            if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n+            else \"cpu\"\n+        )\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            sin = freqs.sin()\n+            cos = freqs.cos()\n+            # interleave sin and cos\n+            pos_embed = torch.stack([sin, cos], dim=-1)\n+            pos_embed = pos_embed.reshape(*pos_embed.shape[:-2], -1)\n+\n+        return pos_embed.to(dtype=hidden_states.dtype)\n+\n+\n+class ParakeetEncoderFeedForward(nn.Module):\n+    def __init__(self, config: ParakeetEncoderConfig):\n+        super().__init__()\n+        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.attention_bias)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.attention_bias)\n+        self.activation_dropout = config.activation_dropout\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.activation(self.linear1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.linear2(hidden_states)\n+        return hidden_states\n+\n+\n+class ParakeetEncoderConvolutionModule(nn.Module):\n+    def __init__(self, config: ParakeetEncoderConfig, module_config=None):\n+        \"\"\"\n+        Args:\n+            config (ParakeetEncoderConfig): Configuration for the model.\n+            module_config (dict): Configuration for the module (e.g., encoder or decoder).\n+        \"\"\"\n+        super().__init__()\n+        channels = config.hidden_size\n+        # kernel_size should be an odd number for 'SAME' padding\n+        if module_config is None:\n+            # e.g. using `ParakeetEncoderEncoderConfig` in src/transformers/models/parakeet_encoder/configuration_parakeet_encoder.py\n+            kernel_size = config.conv_kernel_size\n+            self.activation = ACT2FN[getattr(config, \"hidden_act\", \"silu\")]\n+        else:\n+            kernel_size = module_config[\"kernel_size\"]\n+            self.activation = ACT2FN[module_config.get(\"activation\", \"silu\")]\n+        self.padding = (kernel_size - 1) // 2\n+        self.pointwise_conv1 = nn.Conv1d(channels, 2 * channels, kernel_size=1, stride=1, padding=0, bias=True)\n+        self.depthwise_conv = nn.Conv1d(\n+            channels, channels, kernel_size, stride=1, padding=self.padding, groups=channels, bias=True\n+        )\n+        self.norm = nn.BatchNorm1d(channels)\n+        self.pointwise_conv2 = nn.Conv1d(channels, channels, kernel_size=1, stride=1, padding=0, bias=True)\n+\n+    def forward(self, hidden_states, attention_mask=None):\n+        \"\"\"\n+        Compute convolution module.\n+\n+        Args:\n+            hidden_states (`torch.Tensor` of shape `(batch, time, channels)`): Input tensor.\n+            attention_mask (`torch.Tensor` of shape `(batch, 1, time)`): Attention mask.\n+\n+        Returns:\n+            `torch.Tensor`: Output tensor of shape `(batch, time, channels)`.\n+\n+        \"\"\"\n+        # exchange the temporal dimension and the feature dimension\n+        hidden_states = hidden_states.transpose(1, 2)\n+\n+        # GLU mechanism, (batch_size, 2*channel, dim)\n+        hidden_states = self.pointwise_conv1(hidden_states)\n+        # (batch_size, channel, dim)\n+        hidden_states = nn.functional.glu(hidden_states, dim=1)\n+\n+        # Apply padding mask before convolution\n+        if attention_mask is not None:\n+            all_masked_rows = torch.all(~attention_mask, dim=-1)\n+            hidden_states = hidden_states.masked_fill(all_masked_rows, 0.0)\n+\n+        # 1D Depthwise Conv\n+        hidden_states = self.depthwise_conv(hidden_states)\n+        hidden_states = self.norm(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        hidden_states = self.pointwise_conv2(hidden_states)\n+\n+        return hidden_states.transpose(1, 2)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class ParakeetEncoderAttention(nn.Module):\n+    \"\"\"Multi-head attention with relative positional encoding. See section 3.3 of https://huggingface.co/papers/1901.02860.\"\"\"\n+\n+    def __init__(self, config: ParakeetEncoderConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = False\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        # W_{k,R} projection\n+        self.relative_k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        # global content bias\n+        self.bias_u = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))\n+        # global positional bias\n+        self.bias_v = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        batch_size, seq_length = input_shape\n+        hidden_shape = (batch_size, seq_length, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        query_states_with_bias_u = query_states + self.bias_u.view(\n+            1, self.config.num_attention_heads, 1, self.head_dim\n+        )\n+        query_states_with_bias_v = query_states + self.bias_v.view(\n+            1, self.config.num_attention_heads, 1, self.head_dim\n+        )\n+\n+        relative_key_states = self.relative_k_proj(position_embeddings)\n+        relative_key_states = relative_key_states.view(batch_size, -1, self.config.num_attention_heads, self.head_dim)\n+\n+        # terms (b) and (d)\n+        matrix_bd = query_states_with_bias_v @ relative_key_states.permute(0, 2, 3, 1)\n+        matrix_bd = self._rel_shift(matrix_bd)\n+        matrix_bd = matrix_bd[..., :seq_length]\n+        matrix_bd = matrix_bd * self.scaling\n+\n+        if attention_mask is not None:\n+            # here the original codebase uses -10000.0 rather than float(\"-inf\") and then manual masked fill with 0.0s\n+            # see: https://github.com/NVIDIA-NeMo/NeMo/blob/8cfedd7203462cb251a914e700e5605444277561/nemo/collections/asr/parts/submodules/multi_head_attention.py#L320-L340\n+            # we rather went for a straight-forward approach with float(\"-inf\")\n+            matrix_bd = matrix_bd.masked_fill_(attention_mask.logical_not(), float(\"-inf\"))\n+\n+        # will compute matrix_ac - terms (a) and (c) - and add matrix_bd\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query=query_states_with_bias_u,\n+            key=key_states,\n+            value=value_states,\n+            attention_mask=matrix_bd,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+    def _rel_shift(self, attention_scores):\n+        \"\"\"Relative position shift for Shaw et al. style attention. See appendix B of https://huggingface.co/papers/1901.02860.\"\"\"\n+        batch_size, num_heads, query_length, position_length = attention_scores.shape\n+        attention_scores = nn.functional.pad(attention_scores, pad=(1, 0))\n+        attention_scores = attention_scores.view(batch_size, num_heads, -1, query_length)\n+        attention_scores = attention_scores[:, :, 1:].view(batch_size, num_heads, query_length, position_length)\n+        return attention_scores\n+\n+\n+class ParakeetEncoderSubsamplingConv2D(nn.Module):\n+    def __init__(self, config: ParakeetEncoderConfig):\n+        super().__init__()\n+\n+        self.kernel_size = config.subsampling_conv_kernel_size\n+        self.stride = config.subsampling_conv_stride\n+        self.channels = config.subsampling_conv_channels\n+        self.padding = (self.kernel_size - 1) // 2\n+        self.num_layers = int(math.log2(config.subsampling_factor))\n+\n+        # define layers\n+        self.layers = nn.ModuleList()\n+        self.layers.append(\n+            nn.Conv2d(1, self.channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n+        )\n+        self.layers.append(nn.ReLU())\n+        for i in range(self.num_layers - 1):\n+            # depthwise conv\n+            self.layers.append(\n+                nn.Conv2d(\n+                    self.channels,\n+                    self.channels,\n+                    kernel_size=self.kernel_size,\n+                    stride=self.stride,\n+                    padding=self.padding,\n+                    groups=self.channels,\n+                )\n+            )\n+            # pointwise conv\n+            self.layers.append(nn.Conv2d(self.channels, self.channels, kernel_size=1))\n+            # activation\n+            self.layers.append(nn.ReLU())\n+\n+        out_length = config.num_mel_bins // (self.stride**self.num_layers)\n+        self.linear = nn.Linear(config.subsampling_conv_channels * out_length, config.hidden_size, bias=True)\n+\n+    def _get_output_length(self, input_lengths: torch.Tensor, conv_layer: nn.Conv2d):\n+        if hasattr(conv_layer, \"stride\") and conv_layer.stride != (1, 1):\n+            padding = conv_layer.padding\n+            kernel_size = conv_layer.kernel_size[0]\n+            stride = conv_layer.stride[0]\n+\n+            output_lengths = (input_lengths + padding[0] + padding[1] - kernel_size) // stride + 1\n+            return output_lengths\n+\n+        return input_lengths\n+\n+    def forward(self, input_features: torch.Tensor, attention_mask: torch.Tensor = None):\n+        hidden_states = input_features.unsqueeze(1)\n+        current_lengths = attention_mask.sum(-1) if attention_mask is not None else None\n+\n+        for layer in self.layers:\n+            hidden_states = layer(hidden_states)\n+\n+            # mask the hidden states\n+            if isinstance(layer, nn.Conv2d) and attention_mask is not None:\n+                current_lengths = self._get_output_length(current_lengths, layer)\n+                current_seq_length = hidden_states.shape[2]\n+                channel_mask = (\n+                    torch.arange(current_seq_length, device=attention_mask.device) < current_lengths[:, None]\n+                )\n+                hidden_states *= channel_mask[:, None, :, None]\n+\n+        hidden_states = hidden_states.transpose(1, 2).reshape(hidden_states.shape[0], hidden_states.shape[2], -1)\n+        hidden_states = self.linear(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class ParakeetEncoderBlock(GradientCheckpointingLayer):\n+    def __init__(self, config: ParakeetEncoderConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.gradient_checkpointing = False\n+\n+        self.feed_forward1 = ParakeetEncoderFeedForward(config)\n+        self.self_attn = ParakeetEncoderAttention(config, layer_idx)\n+        self.conv = ParakeetEncoderConvolutionModule(config)\n+        self.feed_forward2 = ParakeetEncoderFeedForward(config)\n+\n+        self.norm_feed_forward1 = nn.LayerNorm(config.hidden_size)\n+        self.norm_self_att = nn.LayerNorm(config.hidden_size)\n+        self.norm_conv = nn.LayerNorm(config.hidden_size)\n+        self.norm_feed_forward2 = nn.LayerNorm(config.hidden_size)\n+        self.norm_out = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.feed_forward1(self.norm_feed_forward1(hidden_states))\n+        hidden_states = residual + 0.5 * hidden_states  # the conformer architecture uses a factor of 0.5\n+\n+        normalized_hidden_states = self.norm_self_att(hidden_states)\n+        attn_output, _ = self.self_attn(\n+            hidden_states=normalized_hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + attn_output\n+\n+        conv_output = self.conv(self.norm_conv(hidden_states), attention_mask=attention_mask)\n+        hidden_states = hidden_states + conv_output\n+\n+        ff2_output = self.feed_forward2(self.norm_feed_forward2(hidden_states))\n+        hidden_states = hidden_states + 0.5 * ff2_output  # the conformer architecture uses a factor of 0.5\n+\n+        hidden_states = self.norm_out(hidden_states)\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class ParakeetPreTrainedModel(PreTrainedModel):\n+    config: ParakeetCTCConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"input_features\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"ParakeetEncoderBlock\"]\n+    _supports_flat_attention_mask = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    # TODO: @eustlb, add support when flash attention supports custom attention bias\n+    _supports_flash_attn = False\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": ParakeetEncoderBlock,\n+        \"attentions\": ParakeetEncoderAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+        if hasattr(self.config, \"initializer_range\"):\n+            std = self.config.initializer_range\n+        else:\n+            # 0.02 is the standard default value accross the library\n+            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n+\n+        if isinstance(module, ParakeetEncoderAttention):\n+            # Initialize positional bias parameters\n+            module.bias_u.data.normal_(mean=0.0, std=std)\n+            module.bias_v.data.normal_(mean=0.0, std=std)\n+\n+    def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n+        encoder_config = self.config.encoder_config if isinstance(self.config, ParakeetCTCConfig) else self.config\n+\n+        kernel_size = encoder_config.subsampling_conv_kernel_size\n+        stride = encoder_config.subsampling_conv_stride\n+        num_layers = int(math.log2(encoder_config.subsampling_factor))\n+\n+        all_paddings = (kernel_size - 1) // 2 * 2\n+        add_pad = all_paddings - kernel_size\n+        lengths = input_lengths\n+\n+        for _ in range(num_layers):\n+            lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + 1.0\n+            lengths = torch.floor(lengths)\n+\n+        return lengths.to(dtype=torch.int)\n+\n+    def _get_output_attention_mask(self, attention_mask: torch.Tensor, target_length: Optional[int] = None):\n+        \"\"\"\n+        Convert the input attention mask to its subsampled form. `target_length` sets the desired output length, useful\n+        when the attention mask length differs from `sum(-1).max()` (i.e., when the longest sequence in the batch is padded)\n+        \"\"\"\n+        output_lengths = self._get_subsampling_output_length(attention_mask.sum(-1))\n+        # Use target_length if provided, otherwise use max length in batch\n+        max_length = target_length if target_length is not None else output_lengths.max()\n+        attention_mask = torch.arange(max_length, device=attention_mask.device) < output_lengths[:, None]\n+        return attention_mask\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Parakeet Encoder model, based on the [Fast Conformer architecture](https://huggingface.co/papers/2305.05084).\n+    \"\"\"\n+)\n+class ParakeetEncoder(ParakeetPreTrainedModel):\n+    config: ParakeetEncoderConfig\n+    base_model_prefix = \"encoder\"\n+\n+    def __init__(self, config: ParakeetEncoderConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.gradient_checkpointing = False\n+\n+        self.dropout = config.dropout\n+        self.dropout_positions = config.dropout_positions\n+        self.layerdrop = config.layerdrop\n+\n+        self.input_scale = math.sqrt(config.hidden_size) if config.scale_input else 1.0\n+        self.subsampling = ParakeetEncoderSubsamplingConv2D(config)\n+        self.encode_positions = ParakeetEncoderRelPositionalEncoding(config)\n+\n+        self.layers = nn.ModuleList(\n+            [ParakeetEncoderBlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, ParakeetEncoder\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/parakeet-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> encoder = ParakeetEncoder.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"])\n+        >>> encoder_outputs = encoder(**inputs)\n+\n+        >>> print(encoder_outputs.last_hidden_state.shape)\n+        ```\n+        \"\"\"\n+\n+        hidden_states = self.subsampling(input_features, attention_mask)\n+        hidden_states = hidden_states * self.input_scale\n+        position_embeddings = self.encode_positions(hidden_states)\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        position_embeddings = nn.functional.dropout(\n+            position_embeddings, p=self.dropout_positions, training=self.training\n+        )\n+\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n+            attention_mask = attention_mask.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)\n+            attention_mask = attention_mask & attention_mask.transpose(1, 2)\n+            attention_mask = attention_mask.unsqueeze(1)\n+\n+        for encoder_layer in self.layers:\n+            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n+            to_drop = False\n+            if self.training:\n+                dropout_probability = torch.rand([])\n+                if dropout_probability < self.layerdrop:  # skip the layer\n+                    to_drop = True\n+\n+            if not to_drop:\n+                hidden_states = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_embeddings=position_embeddings,\n+                    **kwargs,\n+                )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+@dataclass\n+class ParakeetGenerateOutput(ModelOutput):\n+    \"\"\"\n+    Outputs of Parakeet models.\n+\n+    Args:\n+        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n+            if all batches finished early due to the `eos_token_id`.\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n+            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n+            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n+            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n+        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n+        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n+    \"\"\"\n+\n+    sequences: torch.LongTensor\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Parakeet Encoder with a Connectionist Temporal Classification (CTC) head.\n+    \"\"\"\n+)\n+class ParakeetForCTC(ParakeetPreTrainedModel):\n+    config: ParakeetCTCConfig\n+\n+    def __init__(self, config: ParakeetCTCConfig):\n+        super().__init__(config)\n+        self.encoder = ParakeetEncoder(config.encoder_config)\n+        # Conv rather than linear to be consistent with NeMO decoding layer\n+        self.ctc_head = nn.Conv1d(config.encoder_config.hidden_size, config.vocab_size, kernel_size=1)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, ParakeetForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/parakeet-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = ParakeetForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> outputs = model(**inputs)\n+\n+        >>> print(outputs.loss)\n+        ```\"\"\"\n+\n+        encoder_outputs = self.encoder(\n+            input_features=input_features,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+\n+        hidden_states = encoder_outputs.last_hidden_state\n+        logits = self.ctc_head(hidden_states.transpose(1, 2)).transpose(1, 2)\n+\n+        loss = None\n+        if labels is not None:\n+            # retrieve loss input_lengths from attention_mask\n+            attention_mask = (\n+                attention_mask if attention_mask is not None else torch.ones_like(input_features, dtype=torch.long)\n+            )\n+            input_lengths = self._get_subsampling_output_length(attention_mask.sum(-1))\n+\n+            # assuming that padded tokens are filled with -100\n+            # when not being attended to\n+            labels_mask = labels != self.config.pad_token_id\n+            target_lengths = labels_mask.sum(-1)\n+            flattened_targets = labels.masked_select(labels_mask)\n+\n+            # ctc_loss doesn't support fp16\n+            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n+\n+            with torch.backends.cudnn.flags(enabled=False):\n+                loss = nn.functional.ctc_loss(\n+                    log_probs,\n+                    flattened_targets,\n+                    input_lengths,\n+                    target_lengths,\n+                    blank=self.config.pad_token_id,\n+                    reduction=self.config.ctc_loss_reduction,\n+                    zero_infinity=self.config.ctc_zero_infinity,\n+                )\n+\n+        return CausalLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+    @torch.no_grad()\n+    def generate(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        return_dict_in_generate: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[ParakeetGenerateOutput, torch.LongTensor]:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, ParakeetForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/parakeet-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = ParakeetForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> predicted_ids = model.generate(**inputs)\n+        >>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+\n+        >>> print(transcription)\n+        ```\n+        \"\"\"\n+        kwargs[\"return_dict\"] = True\n+        outputs: CausalLMOutput = self.forward(\n+            input_features=input_features,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+\n+        # greedy decoding\n+        sequences = outputs.logits.argmax(dim=-1)\n+\n+        # mask out padded tokens\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=sequences.shape[1])\n+            sequences[~attention_mask] = self.config.pad_token_id\n+\n+        if return_dict_in_generate:\n+            return ParakeetGenerateOutput(\n+                sequences=sequences,\n+                logits=outputs.logits,\n+                attentions=outputs.attentions,\n+                hidden_states=outputs.hidden_states,\n+            )\n+\n+        return sequences\n+\n+\n+__all__ = [\"ParakeetForCTC\", \"ParakeetEncoder\", \"ParakeetPreTrainedModel\"]"
        },
        {
            "sha": "489e0f9cc056344709bd54fd397ccd2643de8b56",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "added",
            "additions": 628,
            "deletions": 0,
            "changes": 628,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,628 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Parakeet model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from ..fastspeech2_conformer.modeling_fastspeech2_conformer import FastSpeech2ConformerConvolutionModule\n+from ..llama.modeling_llama import LlamaAttention, eager_attention_forward\n+from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n+\n+\n+class ParakeetEncoderRelPositionalEncoding(nn.Module):\n+    \"\"\"Relative positional encoding for Parakeet.\"\"\"\n+\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: ParakeetEncoderConfig, device=None):\n+        super().__init__()\n+        self.max_position_embeddings = config.max_position_embeddings\n+        base = 10000.0\n+        inv_freq = 1.0 / (\n+            base\n+            ** (\n+                torch.arange(0, config.hidden_size, 2, dtype=torch.int64).to(device=device, dtype=torch.float)\n+                / config.hidden_size\n+            )\n+        )\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, hidden_states: torch.Tensor):\n+        seq_length = hidden_states.shape[1]\n+        if seq_length > self.max_position_embeddings:\n+            raise ValueError(\n+                f\"Sequence Length: {seq_length} has to be less or equal than \"\n+                f\"config.max_position_embeddings {self.max_position_embeddings}.\"\n+            )\n+\n+        position_ids = torch.arange(seq_length - 1, -seq_length, -1, device=hidden_states.device)\n+        inv_freq_expanded = (\n+            self.inv_freq[None, :, None].float().expand(hidden_states.shape[0], -1, 1).to(hidden_states.device)\n+        )\n+        position_ids_expanded = position_ids[None, None, :].float()\n+\n+        device_type = (\n+            hidden_states.device.type\n+            if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n+            else \"cpu\"\n+        )\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            sin = freqs.sin()\n+            cos = freqs.cos()\n+            # interleave sin and cos\n+            pos_embed = torch.stack([sin, cos], dim=-1)\n+            pos_embed = pos_embed.reshape(*pos_embed.shape[:-2], -1)\n+\n+        return pos_embed.to(dtype=hidden_states.dtype)\n+\n+\n+class ParakeetEncoderFeedForward(nn.Module):\n+    def __init__(self, config: ParakeetEncoderConfig):\n+        super().__init__()\n+        self.linear1 = nn.Linear(config.hidden_size, config.intermediate_size, bias=config.attention_bias)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.linear2 = nn.Linear(config.intermediate_size, config.hidden_size, bias=config.attention_bias)\n+        self.activation_dropout = config.activation_dropout\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.activation(self.linear1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.linear2(hidden_states)\n+        return hidden_states\n+\n+\n+class ParakeetEncoderConvolutionModule(FastSpeech2ConformerConvolutionModule):\n+    def __init__(self, config: ParakeetEncoderConfig, module_config=None):\n+        super().__init__(config, module_config)\n+\n+\n+class ParakeetEncoderAttention(LlamaAttention):\n+    \"\"\"Multi-head attention with relative positional encoding. See section 3.3 of https://huggingface.co/papers/1901.02860.\"\"\"\n+\n+    def __init__(self, config: ParakeetEncoderConfig, layer_idx: int):\n+        super().__init__(config, layer_idx=layer_idx)\n+        self.is_causal = False\n+        # W_{k,R} projection\n+        self.relative_k_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        # global content bias\n+        self.bias_u = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))\n+        # global positional bias\n+        self.bias_v = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        batch_size, seq_length = input_shape\n+        hidden_shape = (batch_size, seq_length, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        query_states_with_bias_u = query_states + self.bias_u.view(\n+            1, self.config.num_attention_heads, 1, self.head_dim\n+        )\n+        query_states_with_bias_v = query_states + self.bias_v.view(\n+            1, self.config.num_attention_heads, 1, self.head_dim\n+        )\n+\n+        relative_key_states = self.relative_k_proj(position_embeddings)\n+        relative_key_states = relative_key_states.view(batch_size, -1, self.config.num_attention_heads, self.head_dim)\n+\n+        # terms (b) and (d)\n+        matrix_bd = query_states_with_bias_v @ relative_key_states.permute(0, 2, 3, 1)\n+        matrix_bd = self._rel_shift(matrix_bd)\n+        matrix_bd = matrix_bd[..., :seq_length]\n+        matrix_bd = matrix_bd * self.scaling\n+\n+        if attention_mask is not None:\n+            # here the original codebase uses -10000.0 rather than float(\"-inf\") and then manual masked fill with 0.0s\n+            # see: https://github.com/NVIDIA-NeMo/NeMo/blob/8cfedd7203462cb251a914e700e5605444277561/nemo/collections/asr/parts/submodules/multi_head_attention.py#L320-L340\n+            # we rather went for a straight-forward approach with float(\"-inf\")\n+            matrix_bd = matrix_bd.masked_fill_(attention_mask.logical_not(), float(\"-inf\"))\n+\n+        # will compute matrix_ac - terms (a) and (c) - and add matrix_bd\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query=query_states_with_bias_u,\n+            key=key_states,\n+            value=value_states,\n+            attention_mask=matrix_bd,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+    def _rel_shift(self, attention_scores):\n+        \"\"\"Relative position shift for Shaw et al. style attention. See appendix B of https://huggingface.co/papers/1901.02860.\"\"\"\n+        batch_size, num_heads, query_length, position_length = attention_scores.shape\n+        attention_scores = nn.functional.pad(attention_scores, pad=(1, 0))\n+        attention_scores = attention_scores.view(batch_size, num_heads, -1, query_length)\n+        attention_scores = attention_scores[:, :, 1:].view(batch_size, num_heads, query_length, position_length)\n+        return attention_scores\n+\n+\n+class ParakeetEncoderSubsamplingConv2D(nn.Module):\n+    def __init__(self, config: ParakeetEncoderConfig):\n+        super().__init__()\n+\n+        self.kernel_size = config.subsampling_conv_kernel_size\n+        self.stride = config.subsampling_conv_stride\n+        self.channels = config.subsampling_conv_channels\n+        self.padding = (self.kernel_size - 1) // 2\n+        self.num_layers = int(math.log2(config.subsampling_factor))\n+\n+        # define layers\n+        self.layers = nn.ModuleList()\n+        self.layers.append(\n+            nn.Conv2d(1, self.channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n+        )\n+        self.layers.append(nn.ReLU())\n+        for i in range(self.num_layers - 1):\n+            # depthwise conv\n+            self.layers.append(\n+                nn.Conv2d(\n+                    self.channels,\n+                    self.channels,\n+                    kernel_size=self.kernel_size,\n+                    stride=self.stride,\n+                    padding=self.padding,\n+                    groups=self.channels,\n+                )\n+            )\n+            # pointwise conv\n+            self.layers.append(nn.Conv2d(self.channels, self.channels, kernel_size=1))\n+            # activation\n+            self.layers.append(nn.ReLU())\n+\n+        out_length = config.num_mel_bins // (self.stride**self.num_layers)\n+        self.linear = nn.Linear(config.subsampling_conv_channels * out_length, config.hidden_size, bias=True)\n+\n+    def _get_output_length(self, input_lengths: torch.Tensor, conv_layer: nn.Conv2d):\n+        if hasattr(conv_layer, \"stride\") and conv_layer.stride != (1, 1):\n+            padding = conv_layer.padding\n+            kernel_size = conv_layer.kernel_size[0]\n+            stride = conv_layer.stride[0]\n+\n+            output_lengths = (input_lengths + padding[0] + padding[1] - kernel_size) // stride + 1\n+            return output_lengths\n+\n+        return input_lengths\n+\n+    def forward(self, input_features: torch.Tensor, attention_mask: torch.Tensor = None):\n+        hidden_states = input_features.unsqueeze(1)\n+        current_lengths = attention_mask.sum(-1) if attention_mask is not None else None\n+\n+        for layer in self.layers:\n+            hidden_states = layer(hidden_states)\n+\n+            # mask the hidden states\n+            if isinstance(layer, nn.Conv2d) and attention_mask is not None:\n+                current_lengths = self._get_output_length(current_lengths, layer)\n+                current_seq_length = hidden_states.shape[2]\n+                channel_mask = (\n+                    torch.arange(current_seq_length, device=attention_mask.device) < current_lengths[:, None]\n+                )\n+                hidden_states *= channel_mask[:, None, :, None]\n+\n+        hidden_states = hidden_states.transpose(1, 2).reshape(hidden_states.shape[0], hidden_states.shape[2], -1)\n+        hidden_states = self.linear(hidden_states)\n+\n+        return hidden_states\n+\n+\n+class ParakeetEncoderBlock(GradientCheckpointingLayer):\n+    def __init__(self, config: ParakeetEncoderConfig, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.gradient_checkpointing = False\n+\n+        self.feed_forward1 = ParakeetEncoderFeedForward(config)\n+        self.self_attn = ParakeetEncoderAttention(config, layer_idx)\n+        self.conv = ParakeetEncoderConvolutionModule(config)\n+        self.feed_forward2 = ParakeetEncoderFeedForward(config)\n+\n+        self.norm_feed_forward1 = nn.LayerNorm(config.hidden_size)\n+        self.norm_self_att = nn.LayerNorm(config.hidden_size)\n+        self.norm_conv = nn.LayerNorm(config.hidden_size)\n+        self.norm_feed_forward2 = nn.LayerNorm(config.hidden_size)\n+        self.norm_out = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.feed_forward1(self.norm_feed_forward1(hidden_states))\n+        hidden_states = residual + 0.5 * hidden_states  # the conformer architecture uses a factor of 0.5\n+\n+        normalized_hidden_states = self.norm_self_att(hidden_states)\n+        attn_output, _ = self.self_attn(\n+            hidden_states=normalized_hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + attn_output\n+\n+        conv_output = self.conv(self.norm_conv(hidden_states), attention_mask=attention_mask)\n+        hidden_states = hidden_states + conv_output\n+\n+        ff2_output = self.feed_forward2(self.norm_feed_forward2(hidden_states))\n+        hidden_states = hidden_states + 0.5 * ff2_output  # the conformer architecture uses a factor of 0.5\n+\n+        hidden_states = self.norm_out(hidden_states)\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class ParakeetPreTrainedModel(PreTrainedModel):\n+    config: ParakeetCTCConfig\n+    base_model_prefix = \"model\"\n+    main_input_name = \"input_features\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"ParakeetEncoderBlock\"]\n+    _supports_flat_attention_mask = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    # TODO: @eustlb, add support when flash attention supports custom attention bias\n+    _supports_flash_attn = False\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": ParakeetEncoderBlock,\n+        \"attentions\": ParakeetEncoderAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+\n+        if hasattr(self.config, \"initializer_range\"):\n+            std = self.config.initializer_range\n+        else:\n+            # 0.02 is the standard default value accross the library\n+            std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n+\n+        if isinstance(module, ParakeetEncoderAttention):\n+            # Initialize positional bias parameters\n+            module.bias_u.data.normal_(mean=0.0, std=std)\n+            module.bias_v.data.normal_(mean=0.0, std=std)\n+\n+    def _get_subsampling_output_length(self, input_lengths: torch.Tensor):\n+        encoder_config = self.config.encoder_config if isinstance(self.config, ParakeetCTCConfig) else self.config\n+\n+        kernel_size = encoder_config.subsampling_conv_kernel_size\n+        stride = encoder_config.subsampling_conv_stride\n+        num_layers = int(math.log2(encoder_config.subsampling_factor))\n+\n+        all_paddings = (kernel_size - 1) // 2 * 2\n+        add_pad = all_paddings - kernel_size\n+        lengths = input_lengths\n+\n+        for _ in range(num_layers):\n+            lengths = torch.div(lengths.to(dtype=torch.float) + add_pad, stride) + 1.0\n+            lengths = torch.floor(lengths)\n+\n+        return lengths.to(dtype=torch.int)\n+\n+    def _get_output_attention_mask(self, attention_mask: torch.Tensor, target_length: Optional[int] = None):\n+        \"\"\"\n+        Convert the input attention mask to its subsampled form. `target_length` sets the desired output length, useful\n+        when the attention mask length differs from `sum(-1).max()` (i.e., when the longest sequence in the batch is padded)\n+        \"\"\"\n+        output_lengths = self._get_subsampling_output_length(attention_mask.sum(-1))\n+        # Use target_length if provided, otherwise use max length in batch\n+        max_length = target_length if target_length is not None else output_lengths.max()\n+        attention_mask = torch.arange(max_length, device=attention_mask.device) < output_lengths[:, None]\n+        return attention_mask\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Parakeet Encoder model, based on the [Fast Conformer architecture](https://huggingface.co/papers/2305.05084).\n+    \"\"\"\n+)\n+class ParakeetEncoder(ParakeetPreTrainedModel):\n+    config: ParakeetEncoderConfig\n+    base_model_prefix = \"encoder\"\n+\n+    def __init__(self, config: ParakeetEncoderConfig):\n+        super().__init__(config)\n+        self.config = config\n+        self.gradient_checkpointing = False\n+\n+        self.dropout = config.dropout\n+        self.dropout_positions = config.dropout_positions\n+        self.layerdrop = config.layerdrop\n+\n+        self.input_scale = math.sqrt(config.hidden_size) if config.scale_input else 1.0\n+        self.subsampling = ParakeetEncoderSubsamplingConv2D(config)\n+        self.encode_positions = ParakeetEncoderRelPositionalEncoding(config)\n+\n+        self.layers = nn.ModuleList(\n+            [ParakeetEncoderBlock(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, ParakeetEncoder\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/parakeet-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> encoder = ParakeetEncoder.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"])\n+        >>> encoder_outputs = encoder(**inputs)\n+\n+        >>> print(encoder_outputs.last_hidden_state.shape)\n+        ```\n+        \"\"\"\n+\n+        hidden_states = self.subsampling(input_features, attention_mask)\n+        hidden_states = hidden_states * self.input_scale\n+        position_embeddings = self.encode_positions(hidden_states)\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        position_embeddings = nn.functional.dropout(\n+            position_embeddings, p=self.dropout_positions, training=self.training\n+        )\n+\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n+            attention_mask = attention_mask.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)\n+            attention_mask = attention_mask & attention_mask.transpose(1, 2)\n+            attention_mask = attention_mask.unsqueeze(1)\n+\n+        for encoder_layer in self.layers:\n+            # add LayerDrop (see https://huggingface.co/papers/1909.11556 for description)\n+            to_drop = False\n+            if self.training:\n+                dropout_probability = torch.rand([])\n+                if dropout_probability < self.layerdrop:  # skip the layer\n+                    to_drop = True\n+\n+            if not to_drop:\n+                hidden_states = encoder_layer(\n+                    hidden_states,\n+                    attention_mask=attention_mask,\n+                    position_embeddings=position_embeddings,\n+                    **kwargs,\n+                )\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n+\n+@dataclass\n+class ParakeetGenerateOutput(ModelOutput):\n+    \"\"\"\n+    Outputs of Parakeet models.\n+\n+    Args:\n+        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n+            if all batches finished early due to the `eos_token_id`.\n+        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n+            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n+            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n+            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n+        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n+        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n+            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+            `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n+    \"\"\"\n+\n+    sequences: torch.LongTensor\n+    logits: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Parakeet Encoder with a Connectionist Temporal Classification (CTC) head.\n+    \"\"\"\n+)\n+class ParakeetForCTC(ParakeetPreTrainedModel):\n+    config: ParakeetCTCConfig\n+\n+    def __init__(self, config: ParakeetCTCConfig):\n+        super().__init__(config)\n+        self.encoder = ParakeetEncoder(config.encoder_config)\n+        # Conv rather than linear to be consistent with NeMO decoding layer\n+        self.ctc_head = nn.Conv1d(config.encoder_config.hidden_size, config.vocab_size, kernel_size=1)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutput:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, ParakeetForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/parakeet-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = ParakeetForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> outputs = model(**inputs)\n+\n+        >>> print(outputs.loss)\n+        ```\"\"\"\n+\n+        encoder_outputs = self.encoder(\n+            input_features=input_features,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+\n+        hidden_states = encoder_outputs.last_hidden_state\n+        logits = self.ctc_head(hidden_states.transpose(1, 2)).transpose(1, 2)\n+\n+        loss = None\n+        if labels is not None:\n+            # retrieve loss input_lengths from attention_mask\n+            attention_mask = (\n+                attention_mask if attention_mask is not None else torch.ones_like(input_features, dtype=torch.long)\n+            )\n+            input_lengths = self._get_subsampling_output_length(attention_mask.sum(-1))\n+\n+            # assuming that padded tokens are filled with -100\n+            # when not being attended to\n+            labels_mask = labels != self.config.pad_token_id\n+            target_lengths = labels_mask.sum(-1)\n+            flattened_targets = labels.masked_select(labels_mask)\n+\n+            # ctc_loss doesn't support fp16\n+            log_probs = nn.functional.log_softmax(logits, dim=-1, dtype=torch.float32).transpose(0, 1)\n+\n+            with torch.backends.cudnn.flags(enabled=False):\n+                loss = nn.functional.ctc_loss(\n+                    log_probs,\n+                    flattened_targets,\n+                    input_lengths,\n+                    target_lengths,\n+                    blank=self.config.pad_token_id,\n+                    reduction=self.config.ctc_loss_reduction,\n+                    zero_infinity=self.config.ctc_zero_infinity,\n+                )\n+\n+        return CausalLMOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+    @torch.no_grad()\n+    def generate(\n+        self,\n+        input_features: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        return_dict_in_generate: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[ParakeetGenerateOutput, torch.LongTensor]:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoProcessor, ParakeetForCTC\n+        >>> from datasets import load_dataset, Audio\n+\n+        >>> model_id = \"nvidia/parakeet-ctc-1.1b\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = ParakeetForCTC.from_pretrained(model_id)\n+\n+        >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=processor.feature_extractor.sampling_rate))\n+\n+        >>> inputs = processor(ds[0][\"audio\"][\"array\"], text=ds[0][\"text\"])\n+        >>> predicted_ids = model.generate(**inputs)\n+        >>> transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+\n+        >>> print(transcription)\n+        ```\n+        \"\"\"\n+        kwargs[\"return_dict\"] = True\n+        outputs: CausalLMOutput = self.forward(\n+            input_features=input_features,\n+            attention_mask=attention_mask,\n+            **kwargs,\n+        )\n+\n+        # greedy decoding\n+        sequences = outputs.logits.argmax(dim=-1)\n+\n+        # mask out padded tokens\n+        if attention_mask is not None:\n+            attention_mask = self._get_output_attention_mask(attention_mask, target_length=sequences.shape[1])\n+            sequences[~attention_mask] = self.config.pad_token_id\n+\n+        if return_dict_in_generate:\n+            return ParakeetGenerateOutput(\n+                sequences=sequences,\n+                logits=outputs.logits,\n+                attentions=outputs.attentions,\n+                hidden_states=outputs.hidden_states,\n+            )\n+\n+        return sequences\n+\n+\n+__all__ = [\"ParakeetForCTC\", \"ParakeetEncoder\", \"ParakeetPreTrainedModel\"]"
        },
        {
            "sha": "20b86a28393bd58164113d0d4ee80783a71023f2",
            "filename": "src/transformers/models/parakeet/processing_parakeet.py",
            "status": "added",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,87 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Union\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ParakeetProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"audio_kwargs\": {\n+            \"sampling_rate\": 16000,\n+            \"padding\": \"longest\",\n+        },\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+            \"padding_side\": \"right\",\n+            \"add_special_tokens\": False,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class ParakeetProcessor(ProcessorMixin):\n+    attributes = [\"feature_extractor\", \"tokenizer\"]\n+    feature_extractor_class = \"ParakeetFeatureExtractor\"\n+    tokenizer_class = \"ParakeetTokenizerFast\"\n+\n+    def __call__(\n+        self,\n+        audio: AudioInput,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = None,\n+        sampling_rate: Optional[int] = None,\n+        **kwargs: Unpack[ParakeetProcessorKwargs],\n+    ):\n+        audio = make_list_of_audio(audio)\n+\n+        output_kwargs = self._merge_kwargs(\n+            ParakeetProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if sampling_rate is None:\n+            logger.warning_once(\n+                f\"You've provided audio without specifying the sampling rate. It will be assumed to be {output_kwargs['audio_kwargs']['sampling_rate']}, which can result in silent errors.\"\n+            )\n+        elif sampling_rate != output_kwargs[\"audio_kwargs\"][\"sampling_rate\"]:\n+            raise ValueError(\n+                f\"The sampling rate of the audio ({sampling_rate}) does not match the sampling rate of the processor ({output_kwargs['audio_kwargs']['sampling_rate']}). Please provide resampled the audio to the expected sampling rate.\"\n+            )\n+\n+        if audio is not None:\n+            inputs = self.feature_extractor(audio, **output_kwargs[\"audio_kwargs\"])\n+        if text is not None:\n+            encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+\n+        if text is None:\n+            return inputs\n+        else:\n+            inputs[\"labels\"] = encodings[\"input_ids\"]\n+            return inputs\n+\n+    @property\n+    def model_input_names(self):\n+        feature_extractor_input_names = self.feature_extractor.model_input_names\n+        return feature_extractor_input_names + [\"labels\"]\n+\n+\n+__all__ = [\"ParakeetProcessor\"]"
        },
        {
            "sha": "d53eb9c68ad487360d97331f062002ff6e89a984",
            "filename": "src/transformers/models/parakeet/tokenization_parakeet_fast.py",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Ftokenization_parakeet_fast.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,54 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import itertools\n+from typing import Optional, Union\n+\n+from ...tokenization_utils_fast import PreTrainedTokenizerFast\n+\n+\n+class ParakeetTokenizerFast(PreTrainedTokenizerFast):\n+    \"\"\"\n+    Inherits all methods from [`PreTrainedTokenizerFast`]. Users should refer to this superclass for more information regarding those methods,\n+    except for `_decode` which is overridden to adapt it to CTC decoding:\n+    1. Group consecutive tokens\n+    2. Filter out the blank token\n+    \"\"\"\n+\n+    def _decode(\n+        self,\n+        token_ids: Union[int, list[int]],\n+        skip_special_tokens: bool = False,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n+        group_tokens: bool = True,\n+        **kwargs,\n+    ) -> str:\n+        if isinstance(token_ids, int):\n+            token_ids = [token_ids]\n+        if group_tokens:\n+            token_ids = [token_group[0] for token_group in itertools.groupby(token_ids)]\n+\n+        # for CTC we filter out the blank token, which is the pad token\n+        token_ids = [token for token in token_ids if token != self.pad_token_id]\n+\n+        return super()._decode(\n+            token_ids=token_ids,\n+            skip_special_tokens=skip_special_tokens,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"ParakeetTokenizerFast\"]"
        },
        {
            "sha": "2ca30b96d85a42b6d2dc5cf6a428a1cf0c37b2a1",
            "filename": "tests/fixtures/parakeet/expected_results_batch.json",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Ffixtures%2Fparakeet%2Fexpected_results_batch.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Ffixtures%2Fparakeet%2Fexpected_results_batch.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffixtures%2Fparakeet%2Fexpected_results_batch.json?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1 @@\n+{\"transcriptions\": [\"mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\", \"nor is mister quilter's manner less interesting than his matter\", \"he tells us that at this festive season of the year with christmas and roast beef looming before us similes drawn from eating and its results occur most readily to the mind\", \"he has grave doubts whether sir frederick leighton's work is really greek after all and can discover in it but little of rocky ithaca\", \"linnell's pictures are a sort of up guards and adam paintings and mason's exquisite idylls are as national as a jingo poem mr burket foster's landscapes smile at one much in the same way that mr carker used to flash his teeth and mr john collier gives his sitter a cheerful slap on the back before he says like a shampooer in a turkish bath next man\"], \"token_ids\": [[1024, 1024, 1024, 1024, 1024, 1024, 19, 37, 132, 1024, 1024, 264, 128, 1024, 1024, 1024, 132, 1024, 58, 1024, 5, 645, 1024, 1000, 82, 52, 1024, 34, 1024, 5, 19, 68, 1007, 52, 1024, 235, 1024, 388, 1024, 27, 1024, 25, 1024, 56, 1024, 103, 1024, 1024, 727, 112, 1024, 22, 1024, 56, 1006, 1009, 405, 1024, 1024, 217, 1024, 1024, 95, 1003, 1024, 133, 1006, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024], [1024, 1024, 1024, 1024, 1024, 1024, 1024, 42, 28, 1024, 1024, 58, 1024, 19, 37, 1024, 132, 1024, 264, 128, 1024, 1024, 132, 1024, 1019, 1003, 1024, 284, 1024, 896, 1024, 32, 154, 1024, 715, 1024, 1024, 1024, 1024, 21, 1024, 322, 1024, 1024, 1024, 217, 1024, 1024, 1024, 1024, 19, 1024, 710, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024], [1024, 1024, 1024, 1024, 1024, 1024, 1024, 67, 1024, 634, 1024, 1024, 1003, 1024, 208, 1024, 1024, 39, 1024, 1024, 124, 1024, 1024, 77, 1024, 1024, 1024, 20, 156, 1024, 1024, 171, 1024, 1024, 101, 1024, 667, 1024, 1024, 34, 1024, 5, 1024, 696, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 93, 1024, 1024, 1024, 1024, 121, 1004, 172, 1024, 1010, 43, 1024, 25, 1024, 343, 250, 1024, 1024, 1024, 50, 1024, 846, 1024, 1024, 304, 44, 1024, 1024, 21, 1024, 1024, 497, 1024, 1024, 208, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 596, 1024, 1024, 1024, 128, 1024, 1024, 27, 1024, 26, 96, 447, 1024, 176, 1024, 48, 1024, 1024, 599, 1024, 25, 1024, 525, 1024, 1024, 338, 1024, 411, 1003, 1024, 1024, 9, 1009, 1024, 1024, 1009, 83, 1024, 1024, 463, 1024, 788, 1024, 1024, 522, 1024, 22, 1024, 5, 1024, 19, 191, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024], [1024, 1024, 1024, 1024, 1024, 1024, 67, 1024, 1024, 244, 1024, 1024, 657, 1024, 47, 1024, 1024, 26, 13, 1016, 998, 1003, 1024, 789, 1024, 1024, 8, 94, 1024, 20, 265, 1024, 12, 1024, 363, 184, 120, 1024, 1024, 1024, 18, 1024, 1019, 1003, 337, 1024, 1024, 58, 1024, 1024, 254, 1024, 1024, 1024, 1024, 1024, 41, 302, 1018, 1024, 1024, 451, 1024, 1024, 1024, 1024, 142, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 25, 1024, 1024, 117, 1024, 1024, 1024, 321, 1024, 394, 1024, 71, 1024, 35, 1024, 45, 1024, 106, 1024, 1024, 1024, 401, 1024, 1024, 1024, 34, 1024, 1024, 1024, 343, 1024, 137, 1024, 1024, 1011, 1024, 45, 1005, 1024, 765, 1024, 1024, 999, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024], [1024, 1024, 1024, 1024, 1024, 1024, 32, 1024, 10, 728, 728, 30, 1024, 1024, 1019, 1003, 1024, 24, 433, 1024, 799, 1024, 1024, 103, 1024, 1024, 3, 1024, 903, 1024, 1024, 34, 1024, 1024, 1024, 1024, 1024, 190, 1024, 1024, 1024, 415, 203, 1024, 1003, 1003, 25, 1024, 273, 1024, 1024, 104, 1024, 1024, 1024, 24, 164, 1024, 1024, 467, 1003, 1024, 1024, 1024, 1024, 1024, 25, 1024, 1024, 19, 1024, 1024, 1024, 667, 1024, 1019, 1003, 1024, 146, 1024, 162, 37, 1024, 320, 1024, 4, 1007, 1011, 1011, 30, 1024, 1003, 1024, 103, 1024, 1024, 88, 1024, 1024, 1024, 42, 1024, 1024, 1024, 895, 1024, 88, 1024, 1024, 3, 1024, 92, 1024, 21, 1024, 1024, 1000, 1024, 1024, 325, 1024, 1024, 215, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 747, 1024, 1024, 1024, 16, 83, 1024, 1018, 1024, 63, 1024, 453, 1024, 82, 1024, 12, 1024, 1019, 1003, 32, 187, 1003, 1024, 1009, 354, 27, 1024, 1024, 1024, 1024, 524, 1024, 429, 1024, 1024, 124, 1024, 1024, 165, 1024, 1024, 1024, 1024, 417, 1024, 1024, 35, 5, 1024, 545, 1024, 1024, 317, 1024, 1024, 39, 1024, 747, 1024, 1024, 1024, 1024, 15, 1024, 475, 1024, 1024, 1024, 12, 1024, 1024, 713, 1024, 1024, 1024, 22, 1024, 428, 1024, 958, 1024, 1024, 217, 1024, 1024, 261, 63, 1005, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 25, 1024, 1024, 747, 1024, 1024, 1024, 1024, 494, 1005, 1002, 1024, 737, 1024, 1024, 1001, 1024, 12, 1024, 1024, 1024, 41, 300, 1024, 27, 1024, 217, 1024, 882, 1024, 1024, 132, 1024, 1024, 3, 1024, 1024, 681, 12, 1024, 1024, 535, 1024, 1024, 635, 1024, 354, 1024, 1024, 1024, 62, 1024, 5, 1024, 344, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 497, 1024, 1024, 67, 1024, 1024, 858, 1024, 1024, 1024, 1024, 144, 1024, 3, 1024, 1024, 1024, 100, 104, 1024, 1015, 1024, 127, 1024, 12, 1024, 35, 1024, 3, 1, 83, 1018, 1024, 391, 1024, 1024, 16, 563, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 608, 1024, 1024, 1024, 1024, 284, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024, 1024]]}\n\\ No newline at end of file"
        },
        {
            "sha": "b6b686fa4223a4c1623790d39be607c4279114f7",
            "filename": "tests/fixtures/parakeet/expected_results_single.json",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Ffixtures%2Fparakeet%2Fexpected_results_single.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Ffixtures%2Fparakeet%2Fexpected_results_single.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffixtures%2Fparakeet%2Fexpected_results_single.json?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1 @@\n+{\"transcriptions\": [\"mister quilter is the apostle of the middle classes and we are glad to welcome his gospel\"], \"scores\": [-0.08922013640403748], \"token_ids\": [[1024, 1024, 1024, 1024, 1024, 1024, 19, 37, 132, 1024, 1024, 264, 128, 1024, 1024, 1024, 132, 1024, 58, 1024, 5, 645, 1024, 1000, 82, 52, 1024, 34, 1024, 5, 19, 68, 1007, 52, 1024, 235, 1024, 388, 1024, 27, 1024, 25, 1024, 56, 1024, 103, 1024, 1024, 727, 112, 1024, 22, 1024, 56, 1006, 1009, 405, 1024, 1024, 217, 1024, 1024, 95, 1003, 1024, 133, 1006, 1024, 1024, 1024, 1024, 1024, 1024, 1024]]}\n\\ No newline at end of file"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/parakeet/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fparakeet%2F__init__.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231"
        },
        {
            "sha": "25cab9d8f41d8172cff742c8f4ff0130c3e59b9c",
            "filename": "tests/models/parakeet/test_feature_extraction_parakeet.py",
            "status": "added",
            "additions": 197,
            "deletions": 0,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_feature_extraction_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_feature_extraction_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fparakeet%2Ftest_feature_extraction_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,197 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the Parakeet feature extraction.\"\"\"\n+\n+import itertools\n+import random\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import ParakeetFeatureExtractor\n+from transformers.testing_utils import require_torch\n+from transformers.utils import is_datasets_available, is_torch_available\n+\n+from ...test_sequence_feature_extraction_common import SequenceFeatureExtractionTestMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_datasets_available():\n+    from datasets import load_dataset\n+\n+global_rng = random.Random()\n+\n+\n+def floats_list(shape, scale=1.0, rng=None, name=None):\n+    \"\"\"Creates a random float32 tensor\"\"\"\n+    if rng is None:\n+        rng = global_rng\n+\n+    values = []\n+    for batch_idx in range(shape[0]):\n+        values.append([])\n+        for _ in range(shape[1]):\n+            values[-1].append(rng.random() * scale)\n+\n+    return values\n+\n+\n+class ParakeetFeatureExtractionTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        min_seq_length=400,\n+        max_seq_length=2000,\n+        feature_size=80,\n+        hop_length=160,\n+        win_length=400,\n+        n_fft=512,\n+        sampling_rate=16000,\n+        padding_value=0.0,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.min_seq_length = min_seq_length\n+        self.max_seq_length = max_seq_length\n+        self.seq_length_diff = (self.max_seq_length - self.min_seq_length) // (self.batch_size - 1)\n+        self.feature_size = feature_size\n+        self.hop_length = hop_length\n+        self.win_length = win_length\n+        self.n_fft = n_fft\n+        self.sampling_rate = sampling_rate\n+        self.padding_value = padding_value\n+\n+    def prepare_feat_extract_dict(self):\n+        return {\n+            \"feature_size\": self.feature_size,\n+            \"hop_length\": self.hop_length,\n+            \"win_length\": self.win_length,\n+            \"n_fft\": self.n_fft,\n+            \"sampling_rate\": self.sampling_rate,\n+            \"padding_value\": self.padding_value,\n+        }\n+\n+    # Copied from tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTester.prepare_inputs_for_common\n+    def prepare_inputs_for_common(self, equal_length=False, numpify=False):\n+        def _flatten(list_of_lists):\n+            return list(itertools.chain(*list_of_lists))\n+\n+        if equal_length:\n+            speech_inputs = [floats_list((self.max_seq_length, self.feature_size)) for _ in range(self.batch_size)]\n+        else:\n+            # make sure that inputs increase in size\n+            speech_inputs = [\n+                floats_list((x, self.feature_size))\n+                for x in range(self.min_seq_length, self.max_seq_length, self.seq_length_diff)\n+            ]\n+        if numpify:\n+            speech_inputs = [np.asarray(x) for x in speech_inputs]\n+        return speech_inputs\n+\n+\n+class ParakeetFeatureExtractionTest(SequenceFeatureExtractionTestMixin, unittest.TestCase):\n+    feature_extraction_class = ParakeetFeatureExtractor\n+\n+    def setUp(self):\n+        self.feat_extract_tester = ParakeetFeatureExtractionTester(self)\n+\n+    def _load_datasamples(self, num_samples):\n+        ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+        # automatic decoding with librispeech\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n+\n+        return [x[\"array\"] for x in speech_samples]\n+\n+    @require_torch\n+    def test_torch_integration(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c4a0999e54466b7e8d8b040d8e0900df\n+        \"\"\"\n+        # fmt: off\n+        EXPECTED_INPUT_FEATURES = torch.tensor(\n+            [\n+                0.60935932, 1.18187428, 1.29877627, 1.36461377, 1.09311509, 1.39821815,\n+                1.63753450, 1.37100816, 1.26510608, 1.70332706, 1.69067430, 1.28770995,\n+                1.52999651, 1.77962756, 1.71420062, 1.21944094, 1.30884087, 1.44343364,\n+                1.17694926, 1.42690814, 1.78877723, 1.68655288, 1.27155364, 1.66103351,\n+                1.75820673, 1.41575801, 1.40622294, 1.70603478, 1.63117850, 1.13353217,\n+            ]\n+        )\n+        # fmt: on\n+\n+        input_speech = self._load_datasamples(1)\n+        feature_extractor = ParakeetFeatureExtractor()\n+        inputs = feature_extractor(input_speech, return_tensors=\"pt\")\n+\n+        self.assertEqual(inputs.input_features.shape, (1, 586, 80))\n+        torch.testing.assert_close(inputs.input_features[0, 100, :30], EXPECTED_INPUT_FEATURES, atol=1e-4, rtol=1e-4)\n+\n+        self.assertEqual(inputs.attention_mask.shape, (1, 586))\n+        # last frame should be masked\n+        self.assertEqual(inputs.attention_mask.sum(), 585)\n+\n+    @require_torch\n+    def test_torch_integration_batch(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c4a0999e54466b7e8d8b040d8e0900df\n+        \"\"\"\n+        # fmt: off\n+        EXPECTED_INPUT_FEATURES = torch.tensor(\n+            [\n+                [ 0.60935932,  1.18187428,  1.29877627,  1.36461377,  1.09311533,\n+                  1.39821827,  1.63753450,  1.37100816,  1.26510608,  1.70332706,\n+                  1.69067478,  1.28770995,  1.52999651,  1.77962780,  1.71420062,\n+                  1.21944094,  1.30884087,  1.44343400,  1.17694926,  1.42690814,\n+                  1.78877664,  1.68655288,  1.27155364,  1.66103351,  1.75820673,\n+                  1.41575801,  1.40622294,  1.70603478,  1.63117862,  1.13353217],\n+                [ 0.58339858,  0.54317272,  0.46222782,  0.34154415,  0.17806509,\n+                  0.32182255,  0.28909618,  0.02141305, -0.09710173, -0.35818669,\n+                 -0.48172510, -0.52942866, -0.58029658, -0.70519227, -0.67929971,\n+                 -0.54698551, -0.28611183, -0.24780270, -0.31363955, -0.41913241,\n+                 -0.32394424, -0.44897896, -0.68657434, -0.62047797, -0.46886450,\n+                 -0.65987164, -1.02435589, -0.58527517, -0.56095684, -0.73582536],\n+                [-0.91937613, -0.97933632, -1.06843162, -1.02642107, -0.94232899,\n+                 -0.83840621, -0.82306921, -0.45763230, -0.45182887, -0.75917768,\n+                 -0.42541453, -0.28512970, -0.39637473, -0.66478080, -0.68004298,\n+                 -0.49690303, -0.31799242, -0.12917191,  0.13149273,  0.10163058,\n+                 -0.40041649,  0.05001565,  0.23906317,  0.28816083,  0.14308788,\n+                 -0.29588422, -0.05428466,  0.14418560,  0.28865972, -0.12138986],\n+                [ 0.73217624,  0.84484011,  0.79323846,  0.66315967,  0.41556871,\n+                  0.88633078,  0.90718138,  0.91268104,  1.15920067,  1.26141894,\n+                  1.10222173,  0.92990804,  0.96352047,  0.88142169,  0.56635213,\n+                  0.71491158,  0.81301254,  0.67301887,  0.74780160,  0.64429688,\n+                  0.22885245,  0.47035533,  0.46498337,  0.17544533,  0.44458991,\n+                  0.79245001,  0.57207537,  0.85768145,  1.00491571,  0.93360955],\n+                [ 1.40496337,  1.32492661,  1.16519547,  0.98379827,  0.77614164,\n+                  0.95871657,  0.81910741,  1.23010278,  1.33011520,  1.16538525,\n+                  1.28319681,  1.45041633,  1.33421600,  0.91677380,  0.67107433,\n+                  0.52890682,  0.82009870,  1.15821445,  1.15343642,  1.10958862,\n+                  1.44962490,  1.44485891,  1.46043479,  1.90800595,  1.95863307,\n+                  1.63670933,  1.49021459,  1.18701911,  0.74906683,  0.84700620]\n+            ]\n+        )\n+        # fmt: on\n+\n+        input_speech = self._load_datasamples(5)\n+        feature_extractor = ParakeetFeatureExtractor()\n+        inputs = feature_extractor(input_speech, return_tensors=\"pt\")\n+\n+        self.assertEqual(inputs.input_features.shape, (5, 2941, 80))\n+        torch.testing.assert_close(inputs.input_features[:, 100, :30], EXPECTED_INPUT_FEATURES, atol=1e-4, rtol=1e-4)\n+\n+        self.assertEqual(inputs.attention_mask.shape, (5, 2941))\n+        self.assertTrue(inputs.attention_mask.sum(dim=-1).tolist(), [585, 481, 1248, 990, 2940])"
        },
        {
            "sha": "8b845b213f91788e650a6e2a2fd14ac907da006c",
            "filename": "tests/models/parakeet/test_modeling_parakeet.py",
            "status": "added",
            "additions": 380,
            "deletions": 0,
            "changes": 380,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_modeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_modeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fparakeet%2Ftest_modeling_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,380 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Parakeet model.\"\"\"\n+\n+import json\n+import tempfile\n+import unittest\n+from pathlib import Path\n+\n+from transformers import is_datasets_available, is_torch_available\n+from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+\n+\n+if is_datasets_available():\n+    from datasets import Audio, load_dataset\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoProcessor,\n+        ParakeetCTCConfig,\n+        ParakeetEncoder,\n+        ParakeetEncoderConfig,\n+        ParakeetForCTC,\n+    )\n+\n+\n+class ParakeetEncoderModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=1024,\n+        is_training=True,\n+        hidden_size=64,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=256,\n+        hidden_act=\"silu\",\n+        dropout=0,  # so gradient checkpointing doesn't fail\n+        conv_kernel_size=9,\n+        subsampling_factor=8,\n+        subsampling_conv_channels=32,\n+        use_bias=True,\n+        num_mel_bins=80,\n+        scale_input=True,\n+    ):\n+        # testing suite parameters\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.num_mel_bins = num_mel_bins\n+        self.is_training = is_training\n+\n+        # config parameters\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.dropout = dropout\n+        self.conv_kernel_size = conv_kernel_size\n+        self.subsampling_factor = subsampling_factor\n+        self.subsampling_conv_channels = subsampling_conv_channels\n+        self.use_bias = use_bias\n+        self.num_mel_bins = num_mel_bins\n+        self.scale_input = scale_input\n+\n+        # Calculate output sequence length after subsampling\n+        self.output_seq_length = seq_length // subsampling_factor\n+        self.encoder_seq_length = self.output_seq_length\n+        self.key_length = self.output_seq_length\n+\n+    def prepare_config_and_inputs(self):\n+        input_features = floats_tensor([self.batch_size, self.seq_length, self.num_mel_bins])\n+        attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n+        config = self.get_config()\n+\n+        return config, input_features, attention_mask\n+\n+    def get_config(self):\n+        return ParakeetEncoderConfig(\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            dropout=self.dropout,\n+            dropout_positions=self.dropout,\n+            layerdrop=self.dropout,\n+            activation_dropout=self.dropout,\n+            attention_dropout=self.dropout,\n+            conv_kernel_size=self.conv_kernel_size,\n+            subsampling_factor=self.subsampling_factor,\n+            subsampling_conv_channels=self.subsampling_conv_channels,\n+            use_bias=self.use_bias,\n+            num_mel_bins=self.num_mel_bins,\n+            scale_input=self.scale_input,\n+        )\n+\n+    def create_and_check_model(self, config, input_features, attention_mask):\n+        model = ParakeetEncoder(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_features, attention_mask=attention_mask)\n+\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, config.hidden_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_features, attention_mask = self.prepare_config_and_inputs()\n+        inputs_dict = {\n+            \"input_features\": input_features,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def check_ctc_loss(self, config, input_values, *args):\n+        model = ParakeetForCTC(config=config)\n+        model.to(torch_device)\n+\n+        # make sure that dropout is disabled\n+        model.eval()\n+\n+        input_values = input_values[:3]\n+        attention_mask = torch.ones(input_values.shape, device=torch_device, dtype=torch.long)\n+\n+        input_lengths = [input_values.shape[-1] // i for i in [4, 2, 1]]\n+        max_length_labels = model._get_feat_extract_output_lengths(torch.tensor(input_lengths))\n+        labels = ids_tensor((input_values.shape[0], min(max_length_labels) - 1), model.config.vocab_size)\n+\n+        # pad input\n+        for i in range(len(input_lengths)):\n+            input_values[i, input_lengths[i] :] = 0.0\n+            attention_mask[i, input_lengths[i] :] = 0\n+\n+        model.config.ctc_loss_reduction = \"sum\"\n+        sum_loss = model(input_values, attention_mask=attention_mask, labels=labels).loss.item()\n+\n+        model.config.ctc_loss_reduction = \"mean\"\n+        mean_loss = model(input_values, attention_mask=attention_mask, labels=labels).loss.item()\n+\n+        self.parent.assertTrue(isinstance(sum_loss, float))\n+        self.parent.assertTrue(isinstance(mean_loss, float))\n+\n+\n+@require_torch\n+class ParakeetEncoderModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (ParakeetEncoder,) if is_torch_available() else ()\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = ParakeetEncoderModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=ParakeetEncoderConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"ParakeetEncoder does not use inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+\n+class ParakeetForCTCModelTester:\n+    def __init__(self, parent, encoder_kwargs=None, is_training=True, vocab_size=128, pad_token_id=0):\n+        if encoder_kwargs is None:\n+            encoder_kwargs = {}\n+\n+        self.parent = parent\n+        self.encoder_model_tester = ParakeetEncoderModelTester(parent, **encoder_kwargs)\n+        self.is_training = is_training\n+\n+        self.batch_size = self.encoder_model_tester.batch_size\n+        self.output_seq_length = self.encoder_model_tester.output_seq_length\n+        self.num_hidden_layers = self.encoder_model_tester.num_hidden_layers\n+        self.seq_length = vocab_size\n+        self.hidden_size = self.encoder_model_tester.hidden_size\n+\n+        self.vocab_size = vocab_size\n+        self.pad_token_id = pad_token_id\n+\n+    def prepare_config_and_inputs(self):\n+        _, input_features, attention_mask = self.encoder_model_tester.prepare_config_and_inputs()\n+        config = self.get_config()\n+        return config, input_features, attention_mask\n+\n+    def get_config(self):\n+        return ParakeetCTCConfig.from_encoder_config(\n+            encoder_config=self.encoder_model_tester.get_config(),\n+            vocab_size=self.vocab_size,\n+            pad_token_id=self.pad_token_id,\n+        )\n+\n+    def create_and_check_model(self, config, input_features, attention_mask):\n+        model = ParakeetForCTC(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_features, attention_mask=attention_mask)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.output_seq_length, self.vocab_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_features, attention_mask = self.prepare_config_and_inputs()\n+        inputs_dict = {\n+            \"input_features\": input_features,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+    def test_ctc_loss_inference(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.encoder_model_tester.check_ctc_loss(*config_and_inputs)\n+\n+\n+@require_torch\n+class ParakeetForCTCModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (ParakeetForCTC,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": ParakeetEncoder,\n+            \"automatic-speech-recognition\": ParakeetForCTC,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_attention_outputs = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torch_exportable = True\n+\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = ParakeetForCTCModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=ParakeetCTCConfig)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"ParakeetEncoder does not use inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    # Original function assumes vision+text model, so overwrite since Parakeet is audio+text\n+    # Below is modified from `tests/models/granite_speech/test_modeling_granite_speech.py`\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+@require_torch\n+class ParakeetForCTCIntegrationTest(unittest.TestCase):\n+    _dataset = None\n+\n+    @classmethod\n+    def setUp(cls):\n+        cls.checkpoint_name = \"nvidia/parakeet-ctc-1.1b\"\n+        cls.dtype = torch.bfloat16\n+        cls.processor = AutoProcessor.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            cls._dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+            cls._dataset = cls._dataset.cast_column(\n+                \"audio\", Audio(sampling_rate=cls.processor.feature_extractor.sampling_rate)\n+            )\n+\n+    def _load_datasamples(self, num_samples):\n+        self._load_dataset()\n+        ds = self._dataset\n+        speech_samples = ds.sort(\"id\")[:num_samples][\"audio\"]\n+        return [x[\"array\"] for x in speech_samples]\n+\n+    @slow\n+    def test_1b_model_integration(self):\n+        \"\"\"\n+        bezzam reproducer (creates JSON directly in repo): https://gist.github.com/ebezzam/6382bdabfc64bb2541ca9f77deb7678d#file-reproducer_single-py\n+        eustlb reproducer: https://gist.github.com/eustlb/6e9e3aa85de3f7c340ec3c36e65f2fe6\n+        \"\"\"\n+        RESULTS_PATH = Path(__file__).parent.parent.parent / \"fixtures/parakeet/expected_results_single.json\"\n+        with open(RESULTS_PATH, \"r\") as f:\n+            raw_data = json.load(f)\n+        EXPECTED_TOKEN_IDS = torch.tensor(raw_data[\"token_ids\"])\n+        EXPECTED_TRANSCRIPTIONS = raw_data[\"transcriptions\"]\n+\n+        samples = self._load_datasamples(1)\n+        model = ParakeetForCTC.from_pretrained(self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device)\n+        model.eval()\n+        model.to(torch_device)\n+\n+        # -- apply\n+        inputs = self.processor(samples)\n+        inputs.to(torch_device, dtype=self.dtype)\n+        predicted_ids = model.generate(**inputs)\n+        torch.testing.assert_close(predicted_ids.cpu(), EXPECTED_TOKEN_IDS)\n+        predicted_transcripts = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+        self.assertListEqual(predicted_transcripts, EXPECTED_TRANSCRIPTIONS)\n+\n+    @slow\n+    def test_1b_model_integration_batched(self):\n+        \"\"\"\n+        bezzam reproducer (creates JSON directly in repo): https://gist.github.com/ebezzam/6382bdabfc64bb2541ca9f77deb7678d#file-reproducer_batched-py\n+        eustlb reproducer: https://gist.github.com/eustlb/575b5da58de34a70116a1955b1183596\n+        \"\"\"\n+\n+        RESULTS_PATH = Path(__file__).parent.parent.parent / \"fixtures/parakeet/expected_results_batch.json\"\n+        with open(RESULTS_PATH, \"r\") as f:\n+            raw_data = json.load(f)\n+        EXPECTED_TOKEN_IDS = torch.tensor(raw_data[\"token_ids\"])\n+        EXPECTED_TRANSCRIPTIONS = raw_data[\"transcriptions\"]\n+\n+        samples = self._load_datasamples(5)\n+        model = ParakeetForCTC.from_pretrained(self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device)\n+        model.eval()\n+        model.to(torch_device)\n+\n+        # -- apply\n+        inputs = self.processor(samples)\n+        inputs.to(torch_device, dtype=self.dtype)\n+        predicted_ids = model.generate(**inputs)\n+        torch.testing.assert_close(predicted_ids.cpu(), EXPECTED_TOKEN_IDS)\n+        predicted_transcripts = self.processor.batch_decode(predicted_ids, skip_special_tokens=True)\n+        self.assertListEqual(predicted_transcripts, EXPECTED_TRANSCRIPTIONS)"
        },
        {
            "sha": "05fe57e75729dbd593f2819c7db3e2a9b4bf2c59",
            "filename": "tests/models/parakeet/test_processing_parakeet.py",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_processing_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_processing_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fparakeet%2Ftest_processing_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,49 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import AutoProcessor, ParakeetProcessor\n+from transformers.testing_utils import require_torch, require_torchaudio\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+@require_torch\n+@require_torchaudio\n+class ParakeetProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = ParakeetProcessor\n+    text_input_name = \"labels\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+        cls.checkpoint = \"nvidia/parakeet-ctc-1.1b\"\n+        processor = ParakeetProcessor.from_pretrained(cls.checkpoint)\n+        processor.save_pretrained(cls.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_feature_extractor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).feature_extractor\n+\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)"
        },
        {
            "sha": "c5612e09391dfc5a9f5f15cd1037e98e63cfe6fc",
            "filename": "tests/models/parakeet/test_tokenization_parakeet.py",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_tokenization_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Fmodels%2Fparakeet%2Ftest_tokenization_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fparakeet%2Ftest_tokenization_parakeet.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -0,0 +1,53 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the ParakeetCTC tokenizer.\"\"\"\n+\n+import unittest\n+\n+from transformers.models.parakeet import ParakeetTokenizerFast\n+\n+from ...test_tokenization_common import TokenizerTesterMixin\n+\n+\n+class ParakeetTokenizationTest(TokenizerTesterMixin, unittest.TestCase):\n+    slow_tokenizer_class = None\n+    rust_tokenizer_class = ParakeetTokenizerFast\n+    tokenizer_class = ParakeetTokenizerFast\n+    test_slow_tokenizer = False\n+    test_rust_tokenizer = True\n+    from_pretrained_id = \"nvidia/parakeet-ctc-1.1b\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        super().setUpClass()\n+        tokenizer = ParakeetTokenizerFast.from_pretrained(\"nvidia/parakeet-ctc-1.1b\")\n+        tokenizer.save_pretrained(cls.tmpdirname)\n+\n+    @unittest.skip(\n+        reason=\"This test does not apply to ParakeetTokenizerFast. More details in the test docstring itself.\"\n+    )\n+    def test_added_tokens_do_lower_case(self):\n+        \"\"\"\n+        Precompiled normalization from sentencepiece is `nmt_nfkc_cf` that includes lowercasing. Yet, ParakeetTokenizerFast does not have a do_lower_case attribute.\n+        This result in the test failing.\n+        \"\"\"\n+        pass\n+\n+    @unittest.skip(reason=\"This needs a slow tokenizer. Parakeet does not have one!\")\n+    def test_encode_decode_with_spaces(self):\n+        return\n+\n+    @unittest.skip(reason=\"ParakeetTokenizerFast doesn't have tokenizer_file in its signature.\")\n+    def test_rust_tokenizer_signature(self):\n+        pass"
        },
        {
            "sha": "f1074001f625ae774a95defbd7ecd28ccf229ca8",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -55,6 +55,7 @@\n     MODEL_FOR_BACKBONE_MAPPING_NAMES,\n     MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING_NAMES,\n     MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n+    MODEL_FOR_CTC_MAPPING_NAMES,\n     MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES,\n     MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n     MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n@@ -657,6 +658,7 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n                 *get_values(MODEL_FOR_MASKED_LM_MAPPING_NAMES),\n                 *get_values(MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES),\n                 *get_values(MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES),\n+                *get_values(MODEL_FOR_CTC_MAPPING_NAMES),\n             ]:\n                 inputs_dict[\"labels\"] = torch.zeros(\n                     (self.model_tester.batch_size, self.model_tester.seq_length), dtype=torch.long, device=torch_device"
        },
        {
            "sha": "613368e1477e8222a5e2324798145572ea38dd4f",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a579de7f5e00a9fdb1e9828aa3ab78385959f231/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a579de7f5e00a9fdb1e9828aa3ab78385959f231/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=a579de7f5e00a9fdb1e9828aa3ab78385959f231",
            "patch": "@@ -315,6 +315,7 @@\n     \"OpenAIGPTTokenizerFast\",\n     \"OpenLlamaConfig\",\n     \"PLBartConfig\",\n+    \"ParakeetCTCConfig\",\n     \"PegasusConfig\",\n     \"PegasusTokenizer\",\n     \"PegasusTokenizerFast\","
        }
    ],
    "stats": {
        "total": 3380,
        "additions": 3372,
        "deletions": 8
    }
}