{
    "author": "ydshieh",
    "message": "Update expected values (after switching to A10) - part 6 (#39207)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "037755ed54208eefa77673b0af2a0b13e51f2fb1",
    "files": [
        {
            "sha": "36ec831ddbdbc190e8e7bd2106ea6ae0d67d26c6",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/037755ed54208eefa77673b0af2a0b13e51f2fb1/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/037755ed54208eefa77673b0af2a0b13e51f2fb1/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=037755ed54208eefa77673b0af2a0b13e51f2fb1",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Aria model.\"\"\"\n \n-import gc\n import unittest\n \n import requests\n@@ -32,7 +31,7 @@\n from transformers.models.idefics3 import Idefics3VisionConfig\n from transformers.testing_utils import (\n     Expectations,\n-    backend_empty_cache,\n+    cleanup,\n     require_bitsandbytes,\n     require_torch,\n     require_torch_large_accelerator,\n@@ -252,14 +251,23 @@ def test_disk_offload_safetensors(self):\n         pass\n \n \n+SKIP = False\n+torch_accelerator_module = getattr(torch, torch_device)\n+memory = 23  # skip on T4 / A10\n+if hasattr(torch_accelerator_module, \"get_device_properties\"):\n+    if torch_accelerator_module.get_device_properties(0).total_memory / 1024**3 < memory:\n+        SKIP = True\n+\n+\n+@unittest.skipIf(SKIP, reason=\"A10 doesn't have enough GPU memory for this tests\")\n @require_torch\n class AriaForConditionalGenerationIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n+        cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n-        gc.collect()\n-        backend_empty_cache(torch_device)\n+        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_torch_large_accelerator"
        },
        {
            "sha": "f8bc302a456357478ba428e3ed78b0dc9c73cdb8",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 73,
            "deletions": 17,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/037755ed54208eefa77673b0af2a0b13e51f2fb1/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/037755ed54208eefa77673b0af2a0b13e51f2fb1/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=037755ed54208eefa77673b0af2a0b13e51f2fb1",
            "patch": "@@ -115,9 +115,12 @@ class GemmaIntegrationTest(unittest.TestCase):\n     def setUpClass(cls):\n         cls.device_properties = get_device_properties()\n \n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def tearDown(self):\n         # See LlamaIntegrationTest.tearDown(). Can be removed once LlamaIntegrationTest.tearDown() is removed.\n-        cleanup(torch_device, gc_collect=False)\n+        cleanup(torch_device, gc_collect=True)\n \n     @require_read_token\n     def test_model_2b_fp16(self):\n@@ -276,7 +279,7 @@ def test_model_7b_bf16(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"cuda\", 7): [\"\"\"Hello I am doing a project on a 1991 240sx and I am trying to find\"\"\", \"Hi today I am going to show you how to make a very simple and easy to make a very simple and\",],\n-                (\"cuda\", 8): [\"Hello I am doing a project for my school and I am trying to make a program that will read a .txt file\", \"Hi today I am going to show you how to make a very simple and easy to make a very simple and\",],\n+                (\"cuda\", 8): ['Hello I am doing a project for my school and I am trying to make a game in which you have to get a', 'Hi today I am going to show you how to make a very simple and easy to make a very simple and'],\n                 (\"rocm\", 9): [\"Hello I am doing a project for my school and I am trying to get a servo to move a certain amount of degrees\", \"Hi today I am going to show you how to make a very simple and easy to make DIY light up sign\",],\n             }\n         )\n@@ -298,10 +301,20 @@ def test_model_7b_fp16_static_cache(self):\n             self.skipTest(\"This test is failing (`torch.compile` fails) on Nvidia T4 GPU (OOM).\")\n \n         model_id = \"google/gemma-7b\"\n-        EXPECTED_TEXTS = [\n-            \"\"\"Hello I am doing a project on a 1999 4.0L 4x4. I\"\"\",\n-            \"Hi today I am going to show you how to make a simple and easy to make a DIY 3D\",\n-        ]\n+\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    \"Hello I am doing a project on a 1999 4.0L 4x4. I\",\n+                    \"Hi today I am going to show you how to make a simple and easy to make a DIY 3D\",\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on a 1995 3000gt SL. I have a\",\n+                    \"Hi today I am going to show you how to make a simple and easy to make a DIY 3D\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXTS = expectations.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(torch_device)\n \n@@ -317,10 +330,20 @@ def test_model_7b_fp16_static_cache(self):\n     @require_read_token\n     def test_model_7b_4bit(self):\n         model_id = \"google/gemma-7b\"\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing a project for my school and I am trying to make a program that will take a number and then\",\n-            \"Hi today I am going to talk about the best way to get rid of acne. miniaturing is a very\",\n-        ]\n+\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    \"Hello I am doing a project for my school and I am trying to make a program that will take a number and then\",\n+                    \"Hi today I am going to talk about the best way to get rid of acne. miniaturing is a very\",\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project for my school and I am trying to make a program that will take a number and then\",\n+                    'Hi today I am going to talk about the new update for the game called \"The new update!:)!:)!:)',\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXTS = expectations.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True)\n \n@@ -382,9 +405,19 @@ def test_export_static_cache(self):\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\", pad_token=\"</s>\", padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have looked on the internet and I have found\",\n-        ]\n+\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have looked on the internet and I have found\"\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have been looking on the internet and I have\"\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT_COMPLETION = expectations.get_expectation()\n+\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]\n@@ -432,15 +465,38 @@ def test_export_static_cache(self):\n             exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n         )\n         ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+\n+        # After switching to A10 on 2025/06/29, we get slightly different outputs when using export\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have looked on the internet and I have found\"\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have looked on the internet and I have found\"\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT_COMPLETION = expectations.get_expectation()\n+\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n \n     def test_model_2b_bf16_dola(self):\n         model_id = \"google/gemma-2b\"\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n-        EXPECTED_TEXTS = [\n-            \"Hello I am doing an experiment and need to get the mass of a block. The problem is, it has no scale\",\n-            \"Hi today we have the review for a <strong>2016/2017</strong> season of\",\n-        ]\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    \"Hello I am doing an experiment and need to get the mass of a block. The problem is, it has no scale\",\n+                    \"Hi today we have the review for a <strong>2016/2017</strong> season of\",\n+                ],\n+                (\"cuda\", 8): [\n+                    \"Hello I am doing an experiment and need to get the mass of a block. The only tool I have is a scale\",\n+                    \"Hi today we have the review for a <strong>2016/2017</strong> season of\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXTS = expectations.get_expectation()\n \n         model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(torch_device)\n "
        }
    ],
    "stats": {
        "total": 106,
        "additions": 85,
        "deletions": 21
    }
}