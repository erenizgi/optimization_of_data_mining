{
    "author": "BenjaminBossan",
    "message": "[PEFT] Better Trainer error when prompt learning with loading best model at the end (#35087)\n\nOriginal issue: https://github.com/huggingface/peft/issues/2256\r\n\r\nThere is a potential error when using load_best_model_at_end=True with a\r\nprompt learning PEFT method. This is because Trainer uses load_adapter\r\nunder the hood but with some prompt learning methods, there is an\r\noptimization on the saved model to remove parameters that are not\r\nrequired for inference, which in turn requires a change to the model\r\narchitecture. This is why load_adapter will fail in such cases and users\r\nshould instead set load_best_model_at_end=False and use\r\nPeftModel.from_pretrained. As this is not obvious, we now intercept the\r\nerror and add a helpful error message.",
    "sha": "bcc50cc7ce12c458ef9e82e189652efe9150a4d0",
    "files": [
        {
            "sha": "a708d8deb4efcc43020669a963c07651cd8ace9a",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc50cc7ce12c458ef9e82e189652efe9150a4d0/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc50cc7ce12c458ef9e82e189652efe9150a4d0/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=bcc50cc7ce12c458ef9e82e189652efe9150a4d0",
            "patch": "@@ -2938,7 +2938,22 @@ def _load_best_model(self):\n                             active_adapter = model.active_adapter\n \n                         if os.path.exists(best_adapter_model_path) or os.path.exists(best_safe_adapter_model_path):\n-                            model.load_adapter(self.state.best_model_checkpoint, active_adapter)\n+                            try:\n+                                model.load_adapter(self.state.best_model_checkpoint, active_adapter)\n+                            except RuntimeError as exc:\n+                                if model.peft_config[active_adapter].is_prompt_learning:\n+                                    # for context: https://github.com/huggingface/peft/issues/2256\n+                                    msg = (\n+                                        \"When using prompt learning PEFT methods such as \"\n+                                        f\"{model.peft_config[active_adapter].peft_type.value}, setting \"\n+                                        \"load_best_model_at_end=True can lead to errors, it is recommended \"\n+                                        \"to set this to False and to load the model manually from the checkpoint \"\n+                                        \"directory using PeftModel.from_pretrained(base_model, <path>) after training \"\n+                                        \"has finished.\"\n+                                    )\n+                                    raise RuntimeError(msg) from exc\n+                                else:\n+                                    raise\n                             # Load_adapter has no return value present, modify it when appropriate.\n                             from torch.nn.modules.module import _IncompatibleKeys\n "
        },
        {
            "sha": "bdbccee5ad364cee6dbf00252c9aa0ffb050e91c",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 83,
            "deletions": 1,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc50cc7ce12c458ef9e82e189652efe9150a4d0/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc50cc7ce12c458ef9e82e189652efe9150a4d0/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=bcc50cc7ce12c458ef9e82e189652efe9150a4d0",
            "patch": "@@ -17,10 +17,19 @@\n import tempfile\n import unittest\n \n+from datasets import Dataset, DatasetDict\n from huggingface_hub import hf_hub_download\n from packaging import version\n \n-from transformers import AutoModelForCausalLM, OPTForCausalLM, logging\n+from transformers import (\n+    AutoModelForCausalLM,\n+    AutoModelForSequenceClassification,\n+    AutoTokenizer,\n+    OPTForCausalLM,\n+    Trainer,\n+    TrainingArguments,\n+    logging,\n+)\n from transformers.testing_utils import (\n     CaptureLogger,\n     require_bitsandbytes,\n@@ -665,3 +674,76 @@ def test_peft_load_adapter_training_inference_mode_false(self):\n                         else:\n                             assert not module.training\n                             assert all(not p.requires_grad for p in module.parameters())\n+\n+    def test_prefix_tuning_trainer_load_best_model_at_end_error(self):\n+        # Original issue: https://github.com/huggingface/peft/issues/2256\n+        # There is a potential error when using load_best_model_at_end=True with a prompt learning PEFT method. This is\n+        # because Trainer uses load_adapter under the hood but with some prompt learning methods, there is an\n+        # optimization on the saved model to remove parameters that are not required for inference, which in turn\n+        # requires a change to the model architecture. This is why load_adapter will fail in such cases and users should\n+        # instead set load_best_model_at_end=False and use PeftModel.from_pretrained. As this is not obvious, we now\n+        # intercept the error and add a helpful error message.\n+        # This test checks this error message. It also tests the \"happy path\" (i.e. no error) when using LoRA.\n+        from peft import LoraConfig, PrefixTuningConfig, TaskType, get_peft_model\n+\n+        # create a small sequence classification dataset (binary classification)\n+        dataset = []\n+        for i, row in enumerate(os.__doc__.splitlines()):\n+            dataset.append({\"text\": row, \"label\": i % 2})\n+        ds_train = Dataset.from_list(dataset)\n+        ds_valid = ds_train\n+        datasets = DatasetDict(\n+            {\n+                \"train\": ds_train,\n+                \"val\": ds_valid,\n+            }\n+        )\n+\n+        # tokenizer for peft-internal-testing/tiny-OPTForCausalLM-lora cannot be loaded, thus using\n+        # hf-internal-testing/tiny-random-OPTForCausalLM\n+        model_id = \"hf-internal-testing/tiny-random-OPTForCausalLM\"\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\", model_type=\"opt\")\n+\n+        def tokenize_function(examples):\n+            return tokenizer(examples[\"text\"], max_length=128, truncation=True, padding=\"max_length\")\n+\n+        tokenized_datasets = datasets.map(tokenize_function, batched=True)\n+        # lora works, prefix-tuning is expected to raise an error\n+        peft_configs = {\n+            \"lora\": LoraConfig(task_type=TaskType.SEQ_CLS),\n+            \"prefix-tuning\": PrefixTuningConfig(\n+                task_type=TaskType.SEQ_CLS,\n+                inference_mode=False,\n+                prefix_projection=True,\n+                num_virtual_tokens=10,\n+            ),\n+        }\n+\n+        for peft_type, peft_config in peft_configs.items():\n+            base_model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n+            base_model.config.pad_token_id = tokenizer.pad_token_id\n+            peft_model = get_peft_model(base_model, peft_config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                training_args = TrainingArguments(\n+                    output_dir=tmpdirname,\n+                    num_train_epochs=3,\n+                    eval_strategy=\"epoch\",\n+                    save_strategy=\"epoch\",\n+                    load_best_model_at_end=True,\n+                )\n+                trainer = Trainer(\n+                    model=peft_model,\n+                    args=training_args,\n+                    train_dataset=tokenized_datasets[\"train\"],\n+                    eval_dataset=tokenized_datasets[\"val\"],\n+                )\n+\n+                if peft_type == \"lora\":\n+                    # LoRA works with load_best_model_at_end\n+                    trainer.train()\n+                else:\n+                    # prefix tuning does not work, but at least users should get a helpful error message\n+                    msg = \"When using prompt learning PEFT methods such as PREFIX_TUNING\"\n+                    with self.assertRaisesRegex(RuntimeError, msg):\n+                        trainer.train()"
        }
    ],
    "stats": {
        "total": 101,
        "additions": 99,
        "deletions": 2
    }
}