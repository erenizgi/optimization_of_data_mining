{
    "author": "muellerzr",
    "message": "Fix loading zero3 weights (#36455)\n\n* Check if fixes\n\n* Fix zero3 loading\n\n* Quality\n\n* Fix marc nit\n\n* Add fast tests\n\n* Migrate to integrations.deepspeed rather than modeling_utils\n\n* Style",
    "sha": "4d8259d24592f27603efc3827f60794904f14aad",
    "files": [
        {
            "sha": "1b51a531645d9fd172083f8a13f3fad1890b7409",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d8259d24592f27603efc3827f60794904f14aad/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d8259d24592f27603efc3827f60794904f14aad/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=4d8259d24592f27603efc3827f60794904f14aad",
            "patch": "@@ -27,6 +27,7 @@\n \n if is_torch_available():\n     import torch\n+    from torch import nn\n \n \n logger = logging.get_logger(__name__)\n@@ -305,6 +306,57 @@ def deepspeed_config():\n         return None\n \n \n+def _load_state_dict_into_zero3_model(model_to_load, state_dict, start_prefix, assign_to_params_buffers=False):\n+    \"\"\"\n+    Loads state dict into a model specifically for Zero3, since DeepSpeed does not support the `transformers`\n+    tensor parallelism API.\n+\n+    Nearly identical code to PyTorch's `_load_from_state_dict`\n+    \"\"\"\n+    # copy state_dict so `_load_state_dict_into_zero3_model` can modify it\n+    metadata = getattr(state_dict, \"_metadata\", None)\n+    state_dict = state_dict.copy()\n+    if metadata is not None:\n+        state_dict._metadata = metadata\n+\n+    error_msgs = []\n+\n+    # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n+    # so we need to apply the function recursively.\n+    def load(module: nn.Module, state_dict, prefix=\"\", assign_to_params_buffers=False):\n+        local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n+        local_metadata[\"assign_to_params_buffers\"] = assign_to_params_buffers\n+\n+        args = (state_dict, prefix, local_metadata, True, [], [], error_msgs)\n+        # Parameters of module and children will start with prefix. We can exit early if there are none in this\n+        # state_dict\n+        if is_deepspeed_zero3_enabled() and len([key for key in state_dict if key.startswith(prefix)]) > 0:\n+            import deepspeed\n+\n+            # In sharded models, each shard has only part of the full state_dict, so only gather\n+            # parameters that are in the current state_dict.\n+            named_parameters = dict(module.named_parameters(prefix=prefix[:-1], recurse=False))\n+            params_to_gather = [named_parameters[k] for k in state_dict.keys() if k in named_parameters]\n+            if len(params_to_gather) > 0:\n+                # because zero3 puts placeholders in model params, this context\n+                # manager gathers (unpartitions) the params of the current layer, then loads from\n+                # the state dict and then re-partitions them again\n+                with deepspeed.zero.GatheredParameters(params_to_gather, modifier_rank=0):\n+                    if torch.distributed.get_rank() == 0:\n+                        module._load_from_state_dict(*args)\n+\n+        for name, child in module._modules.items():\n+            if child is not None:\n+                load(child, state_dict, prefix + name + \".\", assign_to_params_buffers)\n+\n+    load(model_to_load, state_dict, prefix=start_prefix, assign_to_params_buffers=assign_to_params_buffers)\n+    # Delete `state_dict` so it could be collected by GC earlier. Note that `state_dict` is a copy of the argument, so\n+    # it's safe to delete it.\n+    del state_dict\n+\n+    return error_msgs\n+\n+\n def deepspeed_optim_sched(trainer, hf_deepspeed_config, args, num_training_steps, model_parameters):\n     \"\"\"\n     A convenience wrapper that deals with optimizer and lr scheduler configuration."
        },
        {
            "sha": "a016f6013f1e3db92d5189706251fe246bbe8340",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d8259d24592f27603efc3827f60794904f14aad/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d8259d24592f27603efc3827f60794904f14aad/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4d8259d24592f27603efc3827f60794904f14aad",
            "patch": "@@ -50,6 +50,7 @@\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig, GenerationMixin\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n+from .integrations.deepspeed import _load_state_dict_into_zero3_model\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.sdpa_attention import sdpa_attention_forward\n@@ -4918,7 +4919,13 @@ def _load_pretrained_model(\n                 mismatched_names = [name for name, _, _ in mismatched_keys]\n                 fixed_state_dict = {k: v for k, v in state_dict.items() if k not in mismatched_names}\n                 fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(fixed_state_dict)\n-                model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n+\n+                if is_deepspeed_zero3_enabled():\n+                    error_msgs += _load_state_dict_into_zero3_model(\n+                        model_to_load, fixed_state_dict, start_prefix, assign_to_params_buffers\n+                    )\n+                else:\n+                    model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n         else:\n             # This should always be a list but, just to be sure.\n             if not isinstance(resolved_archive_file, list):\n@@ -5009,7 +5016,12 @@ def _load_pretrained_model(\n                             model_to_load, state_dict, start_prefix\n                         )\n                     fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(state_dict)\n-                    model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n+                    if is_deepspeed_zero3_enabled():\n+                        error_msgs += _load_state_dict_into_zero3_model(\n+                            model_to_load, fixed_state_dict, start_prefix, assign_to_params_buffers\n+                        )\n+                    else:\n+                        model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n                 # force memory release\n                 del state_dict\n                 gc.collect()"
        },
        {
            "sha": "80a926f08db5be4bb0f1d41c222f2b5ef9b97bcd",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 39,
            "deletions": 1,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d8259d24592f27603efc3827f60794904f14aad/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d8259d24592f27603efc3827f60794904f14aad/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=4d8259d24592f27603efc3827f60794904f14aad",
            "patch": "@@ -170,7 +170,6 @@ def parameterized_custom_name_func(func, param_num, param):\n \n \n @require_deepspeed\n-@require_torch_accelerator\n class CoreIntegrationDeepSpeed(TestCasePlus, TrainerIntegrationCommon):\n     \"\"\"\n     Testing non-Trainer DeepSpeed integration\n@@ -194,13 +193,52 @@ def tearDown(self):\n         # reset the ds config global so that tests state doesn't leak\n         unset_hf_deepspeed_config()\n \n+    def test_init_zero3(self):\n+        # test that zero.Init() works correctly\n+        ds_config = {\n+            \"train_batch_size\": 1,\n+            \"zero_optimization\": {\n+                \"stage\": 3,\n+            },\n+        }\n+\n+        dschf = HfDeepSpeedConfig(ds_config)\n+\n+        self.assertTrue(dschf.is_zero3())\n+        self.assertTrue(is_deepspeed_zero3_enabled())\n+\n+        with LoggingLevel(logging.INFO):\n+            with mockenv_context(**self.dist_env_1_gpu):\n+                logger = logging.get_logger(\"transformers.modeling_utils\")\n+                with CaptureLogger(logger) as cl:\n+                    AutoModel.from_pretrained(T5_TINY)\n+        self.assertIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n+\n+        # now remove zero optimization\n+        del ds_config[\"zero_optimization\"]\n+        dschf = HfDeepSpeedConfig(ds_config)\n+\n+        self.assertFalse(dschf.is_zero3())\n+        self.assertFalse(is_deepspeed_zero3_enabled())\n+\n+        with LoggingLevel(logging.INFO):\n+            with mockenv_context(**self.dist_env_1_gpu):\n+                logger = logging.get_logger(\"transformers.modeling_utils\")\n+                with CaptureLogger(logger) as cl:\n+                    AutoModel.from_pretrained(T5_TINY)\n+        self.assertNotIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n+\n+    @require_torch_accelerator\n     def test_init_zero3_fp16(self):\n         # test that zero.Init() works correctly under zero3/fp16\n         ds_config = {\n             \"train_batch_size\": 1,\n             \"zero_optimization\": {\n                 \"stage\": 3,\n             },\n+            \"fp16\": {\n+                \"enabled\": True,\n+            },\n         }\n \n         dschf = HfDeepSpeedConfig(ds_config)"
        }
    ],
    "stats": {
        "total": 108,
        "additions": 105,
        "deletions": 3
    }
}