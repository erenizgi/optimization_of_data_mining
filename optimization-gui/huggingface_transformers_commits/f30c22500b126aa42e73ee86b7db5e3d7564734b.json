{
    "author": "albertvillanova",
    "message": "Fix logic error in `prepare_inputs_for_generation` cache slicing condition (#41764)\n\nFix logic error in cache slicing condition\n\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "f30c22500b126aa42e73ee86b7db5e3d7564734b",
    "files": [
        {
            "sha": "c724fa3d57153a3e143ff275368f7bd4040aff27",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f30c22500b126aa42e73ee86b7db5e3d7564734b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f30c22500b126aa42e73ee86b7db5e3d7564734b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f30c22500b126aa42e73ee86b7db5e3d7564734b",
            "patch": "@@ -608,7 +608,7 @@ def prepare_inputs_for_generation(\n         use_cache = kwargs.get(\"use_cache\")\n         if use_cache is None:\n             use_cache = getattr(self.config, \"use_cache\", False)\n-        if past_key_values is None or use_cache:\n+        if past_key_values is not None or use_cache:\n             # TODO (joao): handle the case where cache length == input_ids length. The function below results in an\n             # exception because we get empty input_ids after slicing. In essence, we need to roll back the cache 1\n             # token to recompute the logits for the first token to be generated (but not all caches support roll backs)"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}