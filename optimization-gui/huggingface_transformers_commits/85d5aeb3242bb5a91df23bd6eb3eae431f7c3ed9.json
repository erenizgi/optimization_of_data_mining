{
    "author": "S1ro1",
    "message": "Fix: add back base model plan (#39733)\n\n* Fix: add back base model plan\n\n* Fix: typo\n\n* fixup\n\n* remove unused import\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "85d5aeb3242bb5a91df23bd6eb3eae431f7c3ed9",
    "files": [
        {
            "sha": "e61fc2135070afa5cfb934ab6202ed29a04cdf0f",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/85d5aeb3242bb5a91df23bd6eb3eae431f7c3ed9/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85d5aeb3242bb5a91df23bd6eb3eae431f7c3ed9/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=85d5aeb3242bb5a91df23bd6eb3eae431f7c3ed9",
            "patch": "@@ -821,8 +821,6 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         param = param[ep_rank * local_num_experts : (ep_rank + 1) * local_num_experts].to(param_casting_dtype)\n         if to_contiguous:\n             param = param.contiguous()\n-        if \"gate_up\" in param_type and False:\n-            param = torch.cat([param[..., ::2], param[..., 1::2]], dim=-1)\n         return param\n \n \n@@ -1014,7 +1012,8 @@ def shard_and_distribute_module(\n \n     \"\"\"\n     param_name, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n-    tp_plan = model._tp_plan\n+    tp_plan = model._tp_plan or {}\n+    tp_plan.update(getattr(type(model), \"_tp_plan\", {}))\n     module_to_tp = model.get_submodule(param_name)  # TODO: can i loop over modules?\n     rank = int(rank)\n     current_shard_plan = _get_parameter_tp_plan(parameter_name, tp_plan)\n@@ -1080,11 +1079,14 @@ def verify_tp_plan(expected_keys: list[str], tp_plan: dict[str, str] | None):\n \n def distribute_model(model, distributed_config, device_mesh, tp_size):\n     _plan = \"_tp_plan\"\n+    tp_plan = getattr(model, \"_tp_plan\", {}).copy()\n     model._tp_plan = getattr(model.config, \"base_model_tp_plan\").copy()\n+    model._tp_plan.update(tp_plan)\n     model._tp_size = tp_size\n     model._device_mesh = device_mesh\n     if distributed_config is not None:\n-        distributed_config = DistributedConfig.from_config(distributed_config)\n+        if isinstance(distributed_config, dict):\n+            distributed_config = DistributedConfig.from_dict(distributed_config)\n         if distributed_config.enable_expert_parallel:\n             _plan = \"_ep_plan\"\n             model._tp_plan = getattr(model.config, \"base_model_ep_plan\", model._tp_plan).copy()"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}