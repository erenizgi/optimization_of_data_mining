{
    "author": "IlyasMoutawwakil",
    "message": "HPU support (#36424)\n\n* test\n\n* fix\n\n* fix\n\n* skip some and run some first\n\n* test fsdp\n\n* fix\n\n* patches for generate\n\n* test distributed\n\n* copy\n\n* don't test distributed loss for hpu\n\n* require fp16 and run first\n\n* changes from marc's PR fixing zero3\n\n* better alternative\n\n* return True when fp16 support on gaudi without creating bridge\n\n* fix\n\n* fix tested dtype in deepspeed inference test\n\n* test\n\n* fix\n\n* test\n\n* fix\n\n* skip\n\n* require fp16\n\n* run first fsdp\n\n* Apply suggestions from code review\n\n* address comments\n\n* address comments and refactor test\n\n* reduce precison\n\n* avoid doing gaudi1 specific stuff in the genreation loop\n\n* document test_gradient_accumulation_loss_alignment_with_model_loss test a bit more",
    "sha": "89f6956015a42ab32b35de2a6055ea65b5ca53d4",
    "files": [
        {
            "sha": "fcb62c6157909b5abf0ac77286374c824328e12a",
            "filename": "setup.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -152,6 +152,7 @@\n     \"pytest-asyncio\",\n     \"pytest-timeout\",\n     \"pytest-xdist\",\n+    \"pytest-order\",\n     \"python>=3.9.0\",\n     \"ray[tune]>=2.7.0\",\n     \"regex!=2019.12.17\",\n@@ -324,6 +325,7 @@ def run(self):\n         \"pytest-asyncio\",\n         \"pytest-rich\",\n         \"pytest-xdist\",\n+        \"pytest-order\",\n         \"timeout-decorator\",\n         \"parameterized\",\n         \"psutil\","
        },
        {
            "sha": "da8b1cacaa750b88814bf3f08c191761df723d4d",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -1016,6 +1016,7 @@\n         \"is_timm_available\",\n         \"is_tokenizers_available\",\n         \"is_torch_available\",\n+        \"is_torch_hpu_available\",\n         \"is_torch_mlu_available\",\n         \"is_torch_musa_available\",\n         \"is_torch_neuroncore_available\",\n@@ -6243,6 +6244,7 @@\n         is_timm_available,\n         is_tokenizers_available,\n         is_torch_available,\n+        is_torch_hpu_available,\n         is_torch_mlu_available,\n         is_torch_musa_available,\n         is_torch_neuroncore_available,"
        },
        {
            "sha": "4162f21e954403bace9f66580f056b47ead8e92d",
            "filename": "src/transformers/commands/env.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fcommands%2Fenv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fcommands%2Fenv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fenv.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -30,6 +30,7 @@\n     is_safetensors_available,\n     is_tf_available,\n     is_torch_available,\n+    is_torch_hpu_available,\n     is_torch_npu_available,\n )\n from . import BaseTransformersCLICommand\n@@ -94,6 +95,7 @@ def run(self):\n             pt_version = torch.__version__\n             pt_cuda_available = torch.cuda.is_available()\n             pt_npu_available = is_torch_npu_available()\n+            pt_hpu_available = is_torch_hpu_available()\n \n         tf_version = \"not installed\"\n         tf_cuda_available = \"NA\"\n@@ -149,6 +151,9 @@ def run(self):\n             if pt_cuda_available:\n                 info[\"Using GPU in script?\"] = \"<fill in>\"\n                 info[\"GPU type\"] = torch.cuda.get_device_name()\n+            elif pt_hpu_available:\n+                info[\"Using HPU in script?\"] = \"<fill in>\"\n+                info[\"HPU type\"] = torch.hpu.get_device_name()\n             elif pt_npu_available:\n                 info[\"Using NPU in script?\"] = \"<fill in>\"\n                 info[\"NPU type\"] = torch.npu.get_device_name()"
        },
        {
            "sha": "28ae4463667779b1980e202fa83e45facc597291",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -58,6 +58,7 @@\n     \"pytest-asyncio\": \"pytest-asyncio\",\n     \"pytest-timeout\": \"pytest-timeout\",\n     \"pytest-xdist\": \"pytest-xdist\",\n+    \"pytest-order\": \"pytest-order\",\n     \"python\": \"python>=3.9.0\",\n     \"ray[tune]\": \"ray[tune]>=2.7.0\",\n     \"regex\": \"regex!=2019.12.17\","
        },
        {
            "sha": "decc4f8df0fd9c9bbf6d48d5c178ec6c8562a4a6",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -598,6 +598,8 @@ def forward(\n         kwargs_decoder = {\n             argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n         }\n+        if \"num_items_in_batch\" in kwargs_encoder:\n+            kwargs_decoder[\"num_items_in_batch\"] = kwargs_encoder.pop(\"num_items_in_batch\", None)\n \n         if encoder_outputs is None:\n             encoder_outputs = self.encoder("
        },
        {
            "sha": "70b2ec8ba52c885937c2ea61057d847dd107e72a",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -45,6 +45,7 @@\n     is_tf_available,\n     is_torch_available,\n     is_torch_cuda_available,\n+    is_torch_hpu_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n     is_torch_musa_available,\n@@ -963,6 +964,8 @@ def __init__(\n                 self.device = torch.device(f\"cuda:{device}\")\n             elif is_torch_npu_available():\n                 self.device = torch.device(f\"npu:{device}\")\n+            elif is_torch_hpu_available():\n+                self.device = torch.device(f\"hpu:{device}\")\n             elif is_torch_xpu_available(check_device=True):\n                 self.device = torch.device(f\"xpu:{device}\")\n             elif is_torch_mps_available():"
        },
        {
            "sha": "ab04a2954602893842ca7757e059f2980b35ec7f",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -29,6 +29,7 @@\n     is_accelerate_available,\n     is_bitsandbytes_available,\n     is_torch_available,\n+    is_torch_hpu_available,\n     is_torch_npu_available,\n     is_torch_xpu_available,\n     logging,\n@@ -269,6 +270,8 @@ def update_device_map(self, device_map):\n                 device_map = {\"\": torch.cuda.current_device()}\n             elif is_torch_npu_available():\n                 device_map = {\"\": f\"npu:{torch.npu.current_device()}\"}\n+            elif is_torch_hpu_available():\n+                device_map = {\"\": f\"hpu:{torch.hpu.current_device()}\"}\n             elif is_torch_xpu_available():\n                 device_map = {\"\": f\"xpu:{torch.xpu.current_device()}\"}\n             else:"
        },
        {
            "sha": "f6577469bf480ea964e7793b82806e8a43f14154",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -141,6 +141,7 @@\n     is_torch_deterministic,\n     is_torch_fp16_available_on_device,\n     is_torch_greater_or_equal,\n+    is_torch_hpu_available,\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n     is_torch_sdpa_available,\n@@ -858,6 +859,13 @@ def require_torch_multi_npu(test_case):\n     return unittest.skipUnless(torch.npu.device_count() > 1, \"test requires multiple NPUs\")(test_case)\n \n \n+def require_non_hpu(test_case):\n+    \"\"\"\n+    Decorator marking a test that should be skipped for HPU.\n+    \"\"\"\n+    return unittest.skipUnless(torch_device != \"hpu\", \"test requires a non-HPU\")(test_case)\n+\n+\n def require_torch_xpu(test_case):\n     \"\"\"\n     Decorator marking a test that requires XPU (in PyTorch).\n@@ -889,6 +897,19 @@ def require_torch_multi_xpu(test_case):\n     return unittest.skipUnless(torch.xpu.device_count() > 1, \"test requires multiple XPUs\")(test_case)\n \n \n+def require_torch_multi_hpu(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires a multi-HPU setup (in PyTorch). These tests are skipped on a machine without\n+    multiple HPUs.\n+\n+    To run *only* the multi_hpu tests, assuming all test names contain multi_hpu: $ pytest -sv ./tests -k \"multi_hpu\"\n+    \"\"\"\n+    if not is_torch_hpu_available():\n+        return unittest.skip(reason=\"test requires PyTorch HPU\")(test_case)\n+\n+    return unittest.skipUnless(torch.hpu.device_count() > 1, \"test requires multiple HPUs\")(test_case)\n+\n+\n if is_torch_available():\n     # Set env var CUDA_VISIBLE_DEVICES=\"\" to force cpu-mode\n     import torch\n@@ -917,6 +938,10 @@ def require_torch_multi_xpu(test_case):\n             raise ValueError(\n                 f\"TRANSFORMERS_TEST_DEVICE={torch_device}, but NPU is unavailable. Please double-check your testing environment.\"\n             )\n+        if torch_device == \"hpu\" and not is_torch_hpu_available():\n+            raise ValueError(\n+                f\"TRANSFORMERS_TEST_DEVICE={torch_device}, but HPU is unavailable. Please double-check your testing environment.\"\n+            )\n \n         try:\n             # try creating device to see if provided device is valid\n@@ -929,6 +954,8 @@ def require_torch_multi_xpu(test_case):\n         torch_device = \"cuda\"\n     elif _run_third_party_device_tests and is_torch_npu_available():\n         torch_device = \"npu\"\n+    elif _run_third_party_device_tests and is_torch_hpu_available():\n+        torch_device = \"hpu\"\n     elif _run_third_party_device_tests and is_torch_xpu_available():\n         torch_device = \"xpu\"\n     else:\n@@ -2565,6 +2592,20 @@ def wrapper(*args, **kwargs):\n     return decorator\n \n \n+def run_first(test_case):\n+    \"\"\"\n+    Decorator marking a test with order(1). When pytest-order plugin is installed, tests marked with this decorator\n+    are garanteed to run first.\n+\n+    This is especially useful in some test settings like on a Gaudi instance where a Gaudi device can only be used by a\n+    single process at a time. So we make sure all tests that run in a subprocess are launched first, to avoid device\n+    allocation conflicts.\n+    \"\"\"\n+    import pytest\n+\n+    return pytest.mark.order(1)(test_case)\n+\n+\n def run_test_in_subprocess(test_case, target_func, inputs=None, timeout=None):\n     \"\"\"\n     To run a test in a subprocess. In particular, this can avoid (GPU) memory issue.\n@@ -2853,6 +2894,25 @@ def _device_agnostic_dispatch(device: str, dispatch_table: Dict[str, Callable],\n     BACKEND_EMPTY_CACHE = {\"default\": None}\n     BACKEND_DEVICE_COUNT = {\"default\": lambda: 0}\n \n+if is_torch_hpu_available():\n+    BACKEND_MANUAL_SEED[\"hpu\"] = torch.hpu.manual_seed\n+    BACKEND_DEVICE_COUNT[\"hpu\"] = torch.hpu.device_count\n+\n+if is_torch_npu_available():\n+    BACKEND_EMPTY_CACHE[\"npu\"] = torch.npu.empty_cache\n+    BACKEND_MANUAL_SEED[\"npu\"] = torch.npu.manual_seed\n+    BACKEND_DEVICE_COUNT[\"npu\"] = torch.npu.device_count\n+\n+if is_torch_xpu_available():\n+    BACKEND_EMPTY_CACHE[\"xpu\"] = torch.xpu.empty_cache\n+    BACKEND_MANUAL_SEED[\"xpu\"] = torch.xpu.manual_seed\n+    BACKEND_DEVICE_COUNT[\"xpu\"] = torch.xpu.device_count\n+\n+if is_torch_xla_available():\n+    BACKEND_EMPTY_CACHE[\"xla\"] = torch.cuda.empty_cache\n+    BACKEND_MANUAL_SEED[\"xla\"] = torch.cuda.manual_seed\n+    BACKEND_DEVICE_COUNT[\"xla\"] = torch.cuda.device_count\n+\n \n def backend_manual_seed(device: str, seed: int):\n     return _device_agnostic_dispatch(device, BACKEND_MANUAL_SEED, seed)"
        },
        {
            "sha": "79109fbd667114b53d56d4eb3e1ff081916c5356",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -166,6 +166,7 @@\n     is_sagemaker_mp_enabled,\n     is_schedulefree_available,\n     is_torch_compile_available,\n+    is_torch_hpu_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n     is_torch_musa_available,\n@@ -3141,9 +3142,10 @@ def _load_rng_state(self, checkpoint):\n             set_rng_state_for_device(\"CUDA\", torch.cuda, checkpoint_rng_state, is_distributed)\n         if is_torch_npu_available():\n             set_rng_state_for_device(\"NPU\", torch.npu, checkpoint_rng_state, is_distributed)\n+        if is_torch_hpu_available():\n+            set_rng_state_for_device(\"HPU\", torch.hpu, checkpoint_rng_state, is_distributed)\n         if is_torch_mlu_available():\n             set_rng_state_for_device(\"MLU\", torch.mlu, checkpoint_rng_state, is_distributed)\n-\n         if is_torch_musa_available():\n             set_rng_state_for_device(\"MUSA\", torch.musa, checkpoint_rng_state, is_distributed)\n \n@@ -3255,6 +3257,12 @@ def _save_rng_state(self, output_dir):\n             else:\n                 rng_states[\"npu\"] = torch.npu.random.get_rng_state()\n \n+        if is_torch_hpu_available():\n+            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n+                rng_states[\"hpu\"] = torch.hpu.random.get_rng_state_all()\n+            else:\n+                rng_states[\"hpu\"] = torch.hpu.random.get_rng_state()\n+\n         if is_torch_mlu_available():\n             if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n                 rng_states[\"mlu\"] = torch.mlu.random.get_rng_state_all()\n@@ -3725,6 +3733,10 @@ def training_step(\n                 torch.npu.empty_cache()\n             elif is_torch_mps_available(min_version=\"2.0\"):\n                 torch.mps.empty_cache()\n+            elif is_torch_hpu_available():\n+                logger.warning(\n+                    \"`torch_empty_cache_steps` is set but HPU device/backend does not support empty_cache().\"\n+                )\n             else:\n                 torch.cuda.empty_cache()\n "
        },
        {
            "sha": "982f7b7c0282ec1869bae91da4f9cc347525bda1",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -35,6 +35,7 @@\n     is_tf_available,\n     is_torch_available,\n     is_torch_cuda_available,\n+    is_torch_hpu_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n     is_torch_musa_available,\n@@ -113,6 +114,8 @@ def set_seed(seed: int, deterministic: bool = False):\n         torch.musa.manual_seed_all(seed)\n     if is_torch_npu_available():\n         torch.npu.manual_seed_all(seed)\n+    if is_torch_hpu_available():\n+        torch.hpu.manual_seed_all(seed)\n     if is_torch_xpu_available():\n         torch.xpu.manual_seed_all(seed)\n     if is_tf_available():\n@@ -506,6 +509,11 @@ def __init__(self, skip_memory_metrics=False):\n         elif is_torch_npu_available():\n             import torch\n \n+            self.torch = torch\n+            self.gpu = {}\n+        elif is_torch_hpu_available():\n+            import torch\n+\n             self.torch = torch\n             self.gpu = {}\n         else:\n@@ -573,6 +581,10 @@ def start(self):\n             elif is_torch_npu_available():\n                 self.torch.npu.reset_peak_memory_stats()\n                 self.torch.npu.empty_cache()\n+            elif is_torch_hpu_available():\n+                self.torch.hpu.reset_peak_memory_stats()\n+                # not available on hpu as it reserves all device memory for the current process\n+                # self.torch.hpu.empty_cache()\n             elif is_torch_mps_available():\n                 self.torch.mps.empty_cache()\n \n@@ -588,6 +600,8 @@ def start(self):\n                 self.gpu_mem_used_at_start = self.torch.xpu.memory_allocated()\n             elif is_torch_npu_available():\n                 self.gpu_mem_used_at_start = self.torch.npu.memory_allocated()\n+            elif is_torch_hpu_available():\n+                self.gpu_mem_used_at_start = self.torch.hpu.memory_allocated()\n             elif is_torch_mps_available():\n                 self.gpu_mem_used_at_start = self.torch.mps.current_allocated_memory()\n \n@@ -623,6 +637,10 @@ def stop(self, stage):\n                 self.torch.xpu.empty_cache()\n             elif is_torch_npu_available():\n                 self.torch.npu.empty_cache()\n+            elif is_torch_hpu_available():\n+                # not available on hpu as it reserves all device memory for the current process\n+                # self.torch.npu.empty_cache()\n+                pass\n             elif is_torch_mps_available():\n                 self.torch.mps.empty_cache()\n \n@@ -648,6 +666,9 @@ def stop(self, stage):\n             elif is_torch_npu_available():\n                 self.gpu_mem_used_now = self.torch.npu.memory_allocated()\n                 self.gpu_mem_used_peak = self.torch.npu.max_memory_allocated()\n+            elif is_torch_hpu_available():\n+                self.gpu_mem_used_now = self.torch.hpu.memory_allocated()\n+                self.gpu_mem_used_peak = self.torch.hpu.max_memory_allocated()\n             elif is_torch_mps_available():\n                 self.gpu_mem_used_now = self.torch.mps.current_allocated_memory()\n                 # self.torch.mps.max_memory_allocated() does not exist yet"
        },
        {
            "sha": "d2ec76091f3cb7f67494be7a0fa229a242855f79",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -48,6 +48,7 @@\n     is_torch_available,\n     is_torch_bf16_cpu_available,\n     is_torch_bf16_gpu_available,\n+    is_torch_hpu_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n     is_torch_musa_available,\n@@ -260,9 +261,9 @@ class TrainingArguments:\n         prediction_loss_only (`bool`, *optional*, defaults to `False`):\n             When performing evaluation and generating predictions, only returns the loss.\n         per_device_train_batch_size (`int`, *optional*, defaults to 8):\n-            The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.\n+            The batch size per device accelerator core/CPU for training.\n         per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n-            The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.\n+            The batch size per device accelerator core/CPU for evaluation.\n         gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n             Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n \n@@ -275,7 +276,7 @@ class TrainingArguments:\n \n         eval_accumulation_steps (`int`, *optional*):\n             Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n-            left unset, the whole predictions are accumulated on GPU/NPU/TPU before being moved to the CPU (faster but\n+            left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n             requires more memory).\n         eval_delay (`float`, *optional*):\n             Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n@@ -853,10 +854,10 @@ class TrainingArguments:\n     )\n \n     per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for training.\"}\n+        default=8, metadata={\"help\": \"Batch size per device accelerator core/CPU for training.\"}\n     )\n     per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU/MPS/NPU core/CPU for evaluation.\"}\n+        default=8, metadata={\"help\": \"Batch size per device accelerator core/CPU for evaluation.\"}\n     )\n \n     per_gpu_train_batch_size: Optional[int] = field(\n@@ -1044,7 +1045,7 @@ class TrainingArguments:\n     use_cpu: bool = field(\n         default=False,\n         metadata={\n-            \"help\": \"Whether or not to use cpu. If set to False, we will use cuda/tpu/mps/npu device if available.\"\n+            \"help\": \"Whether or not to use cpu. If left to False, we will use the available torch device/backend (cuda/mps/xpu/hpu etc.)\"\n         },\n     )\n     use_mps_device: bool = field(\n@@ -1830,7 +1831,10 @@ def __post_init__(self):\n         if (self.torch_compile_mode is not None or self.torch_compile_backend is not None) and not self.torch_compile:\n             self.torch_compile = True\n         if self.torch_compile and self.torch_compile_backend is None:\n-            self.torch_compile_backend = \"inductor\"\n+            if not self.use_cpu and is_torch_hpu_available():\n+                self.torch_compile_backend = \"hpu_backend\"\n+            else:\n+                self.torch_compile_backend = \"inductor\"\n \n         # accelerate integration for torch compile\n         if self.torch_compile:\n@@ -2312,6 +2316,9 @@ def _setup_devices(self) -> \"torch.device\":\n             elif is_torch_npu_available():\n                 device = torch.device(\"npu:0\")\n                 torch.npu.set_device(device)\n+            elif is_torch_hpu_available():\n+                device = torch.device(\"hpu:0\")\n+                torch.hpu.set_device(device)\n             else:\n                 # if n_gpu is > 1 we'll use nn.DataParallel.\n                 # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`"
        },
        {
            "sha": "9561666db76ebd0aa30b306765509ca6906fa4f7",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -148,6 +148,7 @@\n     is_gguf_available,\n     is_gptqmodel_available,\n     is_grokadamw_available,\n+    is_habana_gaudi1,\n     is_hadamard_available,\n     is_hqq_available,\n     is_in_notebook,\n@@ -218,6 +219,7 @@\n     is_torch_fx_available,\n     is_torch_fx_proxy,\n     is_torch_greater_or_equal,\n+    is_torch_hpu_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n     is_torch_musa_available,\n@@ -316,6 +318,9 @@ def get_available_devices() -> FrozenSet[str]:\n     if is_torch_npu_available():\n         devices.add(\"npu\")\n \n+    if is_torch_hpu_available():\n+        devices.add(\"hpu\")\n+\n     if is_torch_mlu_available():\n         devices.add(\"mlu\")\n "
        },
        {
            "sha": "f114b925482758db285eec9b9a98ed7cdd6b5fd3",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 64,
            "deletions": 0,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -542,6 +542,12 @@ def is_torch_fp16_available_on_device(device):\n     if not is_torch_available():\n         return False\n \n+    if is_torch_hpu_available():\n+        if is_habana_gaudi1():\n+            return False\n+        else:\n+            return True\n+\n     import torch\n \n     try:\n@@ -573,6 +579,9 @@ def is_torch_bf16_available_on_device(device):\n     if device == \"cuda\":\n         return is_torch_bf16_gpu_available()\n \n+    if device == \"hpu\":\n+        return True\n+\n     try:\n         x = torch.zeros(2, 2, dtype=torch.bfloat16).to(device)\n         _ = x @ x\n@@ -773,6 +782,61 @@ def is_torch_musa_available(check_device=False):\n     return hasattr(torch, \"musa\") and torch.musa.is_available()\n \n \n+@lru_cache\n+def is_torch_hpu_available():\n+    \"Checks if `torch.hpu` is available and potentially if a HPU is in the environment\"\n+    if (\n+        not _torch_available\n+        or importlib.util.find_spec(\"habana_frameworks\") is None\n+        or importlib.util.find_spec(\"habana_frameworks.torch\") is None\n+    ):\n+        return False\n+\n+    torch_hpu_min_version = \"1.5.0\"\n+    if _accelerate_available and version.parse(_accelerate_version) < version.parse(torch_hpu_min_version):\n+        return False\n+\n+    import torch\n+\n+    if not hasattr(torch, \"hpu\") or not torch.hpu.is_available():\n+        return False\n+\n+    import habana_frameworks.torch.utils.experimental as htexp  # noqa: F401\n+\n+    # IlyasMoutawwakil: We patch masked_fill_ for int64 tensors to avoid a bug on Gaudi1\n+    # synNodeCreateWithId failed for node: masked_fill_fwd_i64 with synStatus 26 [Generic failure]\n+    # This can be removed once Gaudi1 support is discontinued but for now we need it to keep using\n+    # dl1.24xlarge Gaudi1 instances on AWS for testing.\n+    # check if the device is Gaudi1 (vs Gaudi2, Gaudi3).\n+    if htexp._get_device_type() == htexp.synDeviceType.synDeviceGaudi:\n+        original_masked_fill_ = torch.Tensor.masked_fill_\n+\n+        def patched_masked_fill_(self, mask, value):\n+            if self.dtype == torch.int64:\n+                logger.warning(\n+                    \"In-place tensor.masked_fill_(mask, value) is not supported for int64 tensors on Gaudi1. \"\n+                    \"This operation will be performed out-of-place using tensor[mask] = value.\"\n+                )\n+                self[mask] = value\n+            else:\n+                original_masked_fill_(self, mask, value)\n+\n+        torch.Tensor.masked_fill_ = patched_masked_fill_\n+\n+    return True\n+\n+\n+@lru_cache\n+def is_habana_gaudi1():\n+    if not is_torch_hpu_available():\n+        return False\n+\n+    import habana_frameworks.torch.utils.experimental as htexp  # noqa: F401\n+\n+    # Check if the device is Gaudi1 (vs Gaudi2, Gaudi3)\n+    return htexp._get_device_type() == htexp.synDeviceType.synDeviceGaudi\n+\n+\n def is_torchdynamo_available():\n     if not is_torch_available():\n         return False"
        },
        {
            "sha": "003e635a10892835e315d78b95d0d63f6735e695",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 24,
            "deletions": 8,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -45,12 +45,14 @@\n     require_deepspeed,\n     require_optuna,\n     require_torch_accelerator,\n+    require_torch_fp16,\n     require_torch_multi_accelerator,\n+    run_first,\n     slow,\n     torch_device,\n )\n from transformers.trainer_utils import get_last_checkpoint, set_seed\n-from transformers.utils import SAFE_WEIGHTS_NAME, is_torch_bf16_available_on_device\n+from transformers.utils import SAFE_WEIGHTS_NAME, is_torch_bf16_available_on_device, is_torch_fp16_available_on_device\n \n \n if is_torch_available():\n@@ -150,10 +152,12 @@ def get_launcher(distributed=False):\n schedulers = [HF_SCHEDULER, DS_SCHEDULER]\n \n stages = [ZERO2, ZERO3]\n+\n+dtypes = []\n if is_torch_bf16_available_on_device(torch_device):\n-    dtypes = [FP16, BF16]\n-else:\n-    dtypes = [FP16]\n+    dtypes.append(BF16)\n+if is_torch_fp16_available_on_device(torch_device):\n+    dtypes.append(FP16)\n \n \n def parameterized_custom_name_func(func, param_num, param):\n@@ -228,6 +232,7 @@ def test_init_zero3(self):\n                     AutoModel.from_pretrained(T5_TINY)\n         self.assertNotIn(\"Detected DeepSpeed ZeRO-3\", cl.out)\n \n+    @require_torch_fp16\n     @require_torch_accelerator\n     def test_init_zero3_fp16(self):\n         # test that zero.Init() works correctly under zero3/fp16\n@@ -456,6 +461,7 @@ def get_config_dict(self, stage):\n \n \n @require_deepspeed\n+@require_torch_fp16\n @require_torch_accelerator\n class TrainerIntegrationDeepSpeed(TrainerIntegrationDeepSpeedWithCustomConfig, TrainerIntegrationCommon):\n     \"\"\"\n@@ -714,7 +720,7 @@ def test_gradient_accumulation(self, stage, dtype):\n         # dynamic loss scale value set to:\n         #   \"fp16.initial_scale_power\": 1\n         # plus having the same WarmupLR's warmup_min_lr == warmup_max_lr in the config file\n-        # but for some reason going to train_len=64 the weights, weights start to mismatch with this setup.\n+        # but for some reason going to train_len=64, the weights start to mismatch with this setup.\n         # the culprit seems to be `initial_scale_power` - putting it back to its default 32 keeps the weights identical\n \n         train_len = 64\n@@ -757,8 +763,12 @@ def test_gradient_accumulation(self, stage, dtype):\n \n         # training with half the batch size but accumulation steps as 2 should give the same\n         # weights, but sometimes get a slight difference still of 1e-6\n-        self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, places=5)\n-        self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, places=5)\n+        if torch_device == \"hpu\":\n+            self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, delta=1e-4)\n+            self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, delta=1e-4)\n+        else:\n+            self.assertAlmostEqual(no_grad_accum_a, yes_grad_accum_a, places=5)\n+            self.assertAlmostEqual(no_grad_accum_b, yes_grad_accum_b, places=5)\n \n         # Relative difference. See the note above how to get identical loss on a small bs\n         self.assertTrue((no_grad_accum_loss - yes_grad_accum_loss) / (no_grad_accum_loss + 1e-15) <= 1e-3)\n@@ -1100,6 +1110,7 @@ def get_dataset():\n \n \n @slow\n+@run_first\n @require_deepspeed\n @require_torch_accelerator\n class TestDeepSpeedWithLauncher(TestCasePlus):\n@@ -1126,6 +1137,7 @@ class TestDeepSpeedWithLauncher(TestCasePlus):\n     def test_basic_distributed(self, stage, dtype):\n         self.run_and_check(stage=stage, dtype=dtype, distributed=True)\n \n+    @require_torch_fp16\n     def test_do_eval_no_train(self):\n         # testing only zero3 since zero2 makes no sense with inference\n         self.run_and_check(\n@@ -1199,12 +1211,15 @@ def test_inference(self, dtype):\n         if dtype == \"bf16\" and not is_torch_bf16_available_on_device(torch_device):\n             self.skipTest(reason=\"test requires bfloat16 hardware support\")\n \n+        if dtype == \"fp16\" and not is_torch_fp16_available_on_device(torch_device):\n+            self.skipTest(reason=\"test requires fp16 hardware support\")\n+\n         # this is just inference, so no optimizer should be loaded\n         # it only works for z3 (makes no sense with z1-z2)\n         fp32 = True if dtype == \"fp32\" else False\n         self.run_and_check(\n             stage=ZERO3,\n-            dtype=FP16,\n+            dtype=dtype,\n             model_name=T5_TINY,\n             distributed=True,\n             do_train=False,\n@@ -1381,6 +1396,7 @@ def test_clm(self, stage, dtype):\n         # print(\" \".join([f\"\\nPYTHONPATH={self.src_dir_str}\"] +cmd)); die\n         execute_subprocess_async(cmd, env=self.get_env())\n \n+    @require_torch_fp16\n     def test_clm_from_config_zero3_fp16(self):\n         # this test exercises AutoModel.from_config(config) - to ensure zero.Init is called\n "
        },
        {
            "sha": "cce33cc7e6cd0f9c32f14b414d195576c3840524",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 30,
            "deletions": 13,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -33,12 +33,17 @@\n     require_fsdp,\n     require_torch_accelerator,\n     require_torch_multi_accelerator,\n+    run_first,\n     slow,\n     torch_device,\n )\n from transformers.trainer_callback import TrainerState\n from transformers.trainer_utils import FSDPOption, set_seed\n-from transformers.utils import is_accelerate_available, is_torch_bf16_available_on_device\n+from transformers.utils import (\n+    is_accelerate_available,\n+    is_torch_bf16_available_on_device,\n+    is_torch_fp16_available_on_device,\n+)\n \n \n if is_torch_available():\n@@ -49,14 +54,19 @@\n \n # default torch.distributed port\n DEFAULT_MASTER_PORT = \"10999\"\n-dtypes = [\"fp16\"]\n+\n+dtypes = []\n if is_torch_bf16_available_on_device(torch_device):\n     dtypes += [\"bf16\"]\n+if is_torch_fp16_available_on_device(torch_device):\n+    dtypes += [\"fp16\"]\n+\n sharding_strategies = [\"full_shard\", \"shard_grad_op\"]\n state_dict_types = [\"FULL_STATE_DICT\", \"SHARDED_STATE_DICT\"]\n-set_seed(42)\n params = list(itertools.product(sharding_strategies, dtypes))\n \n+set_seed(42)\n+\n \n def get_master_port(real_launcher=False):\n     \"\"\"\n@@ -140,13 +150,13 @@ def setUp(self):\n         }\n \n         self.fsdp_config = {\n-            \"backward_prefetch\": \"backward_pre\",\n-            \"forward_prefetch\": \"False\",\n-            \"limit_all_gathers\": \"False\",\n-            \"use_orig_params\": \"True\",\n-            \"sync_module_states\": \"True\",\n-            \"cpu_ram_efficient_loading\": \"True\",\n-            \"activation_checkpointing\": \"False\",\n+            \"backward_prefetch\": \"BACKWARD_PRE\",\n+            \"forward_prefetch\": \"false\",\n+            \"limit_all_gathers\": \"false\",\n+            \"use_orig_params\": \"true\",\n+            \"sync_module_states\": \"true\",\n+            \"cpu_ram_efficient_loading\": \"true\",\n+            \"activation_checkpointing\": \"false\",\n             \"min_num_params\": 1,\n         }\n \n@@ -202,7 +212,7 @@ def test_fsdp_config_transformers_auto_wrap(self, sharding_strategy, dtype):\n             self.assertEqual(\n                 os.environ[f\"{prefix}TRANSFORMER_CLS_TO_WRAP\"], \",\".join(fsdp_config[\"transformer_layer_cls_to_wrap\"])\n             )\n-            self.assertEqual(os.environ[f\"{prefix}BACKWARD_PREFETCH\"], fsdp_config[\"backward_prefetch\"].upper())\n+            self.assertEqual(os.environ[f\"{prefix}BACKWARD_PREFETCH\"], fsdp_config[\"backward_prefetch\"])\n             self.assertEqual(os.environ[f\"{prefix}FORWARD_PREFETCH\"], fsdp_config[\"forward_prefetch\"])\n             self.assertEqual(os.environ[f\"{prefix}USE_ORIG_PARAMS\"], fsdp_config[\"use_orig_params\"])\n             self.assertEqual(os.environ[f\"{prefix}SYNC_MODULE_STATES\"], fsdp_config[\"sync_module_states\"])\n@@ -213,6 +223,7 @@ def test_fsdp_config_transformers_auto_wrap(self, sharding_strategy, dtype):\n \n     @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n     def test_basic_run(self, sharding_strategy, dtype):\n         launcher = get_launcher(distributed=True, use_accelerate=False)\n@@ -225,6 +236,7 @@ def test_basic_run(self, sharding_strategy, dtype):\n \n     @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n     def test_basic_run_with_gradient_accumulation(self, sharding_strategy, dtype):\n         launcher = get_launcher(distributed=True, use_accelerate=False)\n@@ -237,6 +249,7 @@ def test_basic_run_with_gradient_accumulation(self, sharding_strategy, dtype):\n \n     @parameterized.expand(dtypes)\n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n     @unittest.skipIf(not is_torch_greater_or_equal_than_2_1, reason=\"This test on pytorch 2.0 takes 4 hours.\")\n     def test_basic_run_with_cpu_offload(self, dtype):\n@@ -250,6 +263,7 @@ def test_basic_run_with_cpu_offload(self, dtype):\n \n     @parameterized.expand(state_dict_types, name_func=_parameterized_custom_name_func)\n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n     def test_training_and_can_resume_normally(self, state_dict_type):\n         output_dir = self.get_auto_remove_tmp_dir(\"./xxx\", after=False)\n@@ -286,10 +300,13 @@ def test_training_and_can_resume_normally(self, state_dict_type):\n                 self.assertAlmostEqual(log[\"learning_rate\"], log1[\"learning_rate\"], delta=1e-5)\n \n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n-    @require_torch_accelerator\n-    @require_fsdp\n     def test_fsdp_cpu_offloading(self):\n+        # TODO: This file is missing and should be added or the test should be removed\n+        if not os.path.exists(\"utils/testing_scripts/fsdp_cpu_offloading.py\"):\n+            raise unittest.SkipTest(\"FSDP CPU offloading script not found!\")\n+\n         try:\n             subprocess.run(\n                 \"accelerate launch utils/testing_scripts/fsdp_cpu_offloading.py --config utils/testing_scripts/dummy_fsdp_config.yml\","
        },
        {
            "sha": "b8c41c4ed497ce9cc60d6647815141c29281f443",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -2770,7 +2770,7 @@ def check_device_map_is_respected(self, model, device_map):\n             elif param_device in [\"mps\"]:\n                 self.assertEqual(param.device, torch.device(\"mps\"))\n             else:\n-                # when loaded with device_map, `param_device` are integer values for cuda/xpu/npu/mlu\n+                # when loaded with device_map, `param_device` are integer values for cuda/xpu/hpu/npu/mlu\n                 self.assertEqual(param.device, torch.device(f\"{torch_device}:{param_device}\"))\n \n     @require_accelerate"
        },
        {
            "sha": "beee7fcb48a496ec008878b45fe3057af7a20cb4",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 44,
            "deletions": 30,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -75,6 +75,7 @@\n     require_intel_extension_for_pytorch,\n     require_liger_kernel,\n     require_lomo,\n+    require_non_hpu,\n     require_non_xpu,\n     require_optuna,\n     require_peft,\n@@ -88,6 +89,7 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_bf16,\n+    require_torch_fp16,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_non_multi_accelerator,\n@@ -98,6 +100,7 @@\n     require_torchdynamo,\n     require_vision,\n     require_wandb,\n+    run_first,\n     run_test_using_subprocess,\n     slow,\n     torch_device,\n@@ -119,6 +122,13 @@\n from transformers.utils.hp_naming import TrialShortNamer\n \n \n+if torch_device == \"hpu\":\n+    RTOL = 1e-3\n+    ATOL = 1e-3\n+else:\n+    RTOL = 1e-5\n+    ATOL = 1e-5\n+\n if is_torch_available():\n     import torch\n     from torch import nn\n@@ -726,11 +736,11 @@ def setUp(self):\n             trainer.train()\n             self.alternate_trained_model = (trainer.model.a, trainer.model.b)\n \n-    def check_trained_model(self, model, alternate_seed=False):\n+    def check_trained_model(self, model, alternate_seed=False, **kwargs):\n         # Checks a training seeded with learning_rate = 0.1\n         (a, b) = self.alternate_trained_model if alternate_seed else self.default_trained_model\n-        torch.testing.assert_close(model.a, a)\n-        torch.testing.assert_close(model.b, b)\n+        torch.testing.assert_close(model.a, a, **kwargs)\n+        torch.testing.assert_close(model.b, b, **kwargs)\n \n     def test_reproducible_training(self):\n         # Checks that training worked, model trained and seed made a reproducible training.\n@@ -812,11 +822,6 @@ def tokenize_function(examples):\n \n         data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n \n-        model = AutoModelForCausalLM.from_pretrained(model_name)\n-        state_dict = model.state_dict()\n-\n-        base_loss_callback = StoreLossCallback()\n-\n         args_kwargs = {\n             \"report_to\": \"none\",\n             \"logging_steps\": 1,\n@@ -830,6 +835,10 @@ def tokenize_function(examples):\n                 tmp_dir,\n                 **args_kwargs,\n             )\n+            # train with base loss\n+            set_seed(42)\n+            model = AutoModelForCausalLM.from_pretrained(model_name)\n+            base_loss_callback = StoreLossCallback()\n             trainer = Trainer(\n                 model,\n                 args,\n@@ -840,27 +849,30 @@ def tokenize_function(examples):\n             assert trainer.model_accepts_loss_kwargs\n             trainer.train()\n \n-        grad_accum_loss_callback = StoreLossCallback()\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n             args = TrainingArguments(\n                 tmp_dir,\n                 **args_kwargs,\n                 gradient_accumulation_steps=2,\n                 per_device_train_batch_size=4,\n             )\n+\n+            # train with gradient accumulation\n             set_seed(42)\n             model = AutoModelForCausalLM.from_pretrained(model_name)\n+            grad_accum_loss_callback = StoreLossCallback()\n             trainer = Trainer(\n                 model,\n                 args,\n                 train_dataset=tokenized_dataset,\n                 callbacks=[grad_accum_loss_callback],\n                 data_collator=data_collator,\n             )\n+            assert trainer.model_accepts_loss_kwargs\n             trainer.train()\n \n+            # train with broken loss\n             set_seed(42)\n-            model.load_state_dict(state_dict)\n+            model = AutoModelForCausalLM.from_pretrained(model_name)\n             broken_loss_callback = StoreLossCallback()\n             trainer = Trainer(\n                 model,\n@@ -869,30 +881,28 @@ def tokenize_function(examples):\n                 callbacks=[broken_loss_callback],\n                 data_collator=data_collator,\n             )\n-            # disable model_accepts_loss_kwargs\n+            # disable model_accepts_loss_kwargs so that \"num_items_in_batch\" is not passed to the model\n             trainer.model_accepts_loss_kwargs = False\n             trainer.train()\n \n-            # Calculate the difference between the base loss and the grad_accum loss\n-            diff_truth = [\n-                abs(base - grad) for base, grad in zip(base_loss_callback.losses, grad_accum_loss_callback.losses)\n-            ]\n-            diff_broken = [\n-                abs(base - grad) for base, grad in zip(base_loss_callback.losses, broken_loss_callback.losses)\n-            ]\n+        # Calculate the difference between the base loss and the grad_accum loss\n+        diff_truth = [\n+            abs(base - grad) for base, grad in zip(base_loss_callback.losses, grad_accum_loss_callback.losses)\n+        ]\n+        diff_broken = [abs(base - grad) for base, grad in zip(base_loss_callback.losses, broken_loss_callback.losses)]\n \n-            # all diff truth should be quite close\n-            self.assertLess(max(diff_truth), 0.01, f\"Difference {max(diff_truth)} is not within 0.01\")\n+        # all diff truth should be quite close\n+        self.assertLess(max(diff_truth), 0.01, f\"Difference {max(diff_truth)} is not within 0.01\")\n \n-            # max diff broken should be very off\n-            self.assertGreater(max(diff_broken), 1.5, f\"Difference {max(diff_broken)} is not greater than 2\")\n+        # max diff broken should be very off\n+        self.assertGreater(max(diff_broken), 1.3, f\"Difference {max(diff_broken)} is not greater than 1.3\")\n \n-            loss_base = sum(base_loss_callback.losses)\n-            loss_broken = sum(broken_loss_callback.losses)\n+        loss_base = sum(base_loss_callback.losses)\n+        loss_broken = sum(broken_loss_callback.losses)\n \n-            # mean/sum loss should not vary too much.\n-            relative_diff = abs(loss_base - loss_broken) / max(loss_base, loss_broken)\n-            self.assertLess(relative_diff, 0.2, f\"Relative difference {relative_diff} is not within 0.2\")\n+        # mean/sum loss should not vary too much.\n+        relative_diff = abs(loss_base - loss_broken) / max(loss_base, loss_broken)\n+        self.assertLess(relative_diff, 0.2, f\"Relative difference {relative_diff} is not within 0.2\")\n \n     def test_gradient_accumulation_loss_alignment_with_loss_func(self):\n         set_seed(42)\n@@ -1214,14 +1224,14 @@ def test_adafactor_lr_none(self):\n             self.assertFalse(torch.allclose(trainer.model.b, b))\n             self.assertGreater(trainer.optimizer.state_dict()[\"param_groups\"][0][\"lr\"], 0)\n \n-    @require_torch_accelerator\n     @require_torch_bf16\n+    @require_torch_accelerator\n     def test_mixed_bf16(self):\n         # very basic test\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(learning_rate=0.1, bf16=True, output_dir=tmp_dir)\n             trainer.train()\n-            self.check_trained_model(trainer.model)\n+            self.check_trained_model(trainer.model, atol=ATOL, rtol=RTOL)\n \n         # --bf16 --half_precision_backend apex can't be used together\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -3582,6 +3592,7 @@ def test_load_best_model_from_safetensors(self):\n                 )\n \n     @slow\n+    @run_first\n     def test_trainer_eval_mrpc(self):\n         MODEL_ID = \"google-bert/bert-base-cased-finetuned-mrpc\"\n         tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n@@ -3598,6 +3609,7 @@ def test_trainer_eval_mrpc(self):\n             self.assertLess(result[\"eval_loss\"], 0.2)\n \n     @slow\n+    @run_first\n     def test_trainer_eval_multiple(self):\n         MODEL_ID = \"openai-community/gpt2\"\n         tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n@@ -3897,6 +3909,7 @@ def test_mem_metrics(self):\n             trainer = get_regression_trainer(skip_memory_metrics=True, output_dir=tmp_dir)\n             self.check_mem_metrics(trainer, self.assertNotIn)\n \n+    @require_torch_fp16\n     @require_torch_accelerator\n     def test_fp16_full_eval(self):\n         # this is a sensitive test so let's keep debugging printouts in place for quick diagnosis.\n@@ -4152,6 +4165,7 @@ def test_no_wd_param_group(self):\n             self.assertListEqual(trainer.optimizer.param_groups[1][\"params\"], no_wd_params)\n \n     @slow\n+    @require_non_hpu\n     @require_torch_multi_accelerator\n     def test_end_to_end_example(self):\n         # Tests that `translation.py` will run without issues"
        },
        {
            "sha": "f7f34b83e7c0d20047bb9b145fa88923842d6ca0",
            "filename": "tests/trainer/test_trainer_distributed.py",
            "status": "modified",
            "additions": 5,
            "deletions": 48,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -19,12 +19,11 @@\n from transformers import EvalPrediction, HfArgumentParser, TrainingArguments, is_torch_available\n from transformers.testing_utils import (\n     TestCasePlus,\n+    backend_device_count,\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n-    require_torch_multi_gpu,\n-    require_torch_multi_xpu,\n-    require_torch_neuroncore,\n-    require_torch_npu,\n+    require_torch_multi_accelerator,\n+    torch_device,\n )\n from transformers.training_args import ParallelMode\n from transformers.utils import logging\n@@ -117,38 +116,10 @@ def __getitem__(self, i):\n             return result\n \n \n-class TestTrainerDistributedNeuronCore(TestCasePlus):\n-    @require_torch_neuroncore\n-    def test_trainer(self):\n-        distributed_args = f\"\"\"--nproc_per_node=2\n-            --master_port={get_torch_dist_unique_port()}\n-            {self.test_file_dir}/test_trainer_distributed.py\n-        \"\"\".split()\n-        output_dir = self.get_auto_remove_tmp_dir()\n-        args = f\"--output_dir {output_dir}\".split()\n-        cmd = [\"torchrun\"] + distributed_args + args\n-        execute_subprocess_async(cmd, env=self.get_env())\n-        # successful return here == success - any errors would have caused an error in the sub-call\n-\n-\n-class TestTrainerDistributedNPU(TestCasePlus):\n-    @require_torch_npu\n-    def test_trainer(self):\n-        distributed_args = f\"\"\"--nproc_per_node=2\n-            --master_port={get_torch_dist_unique_port()}\n-            {self.test_file_dir}/test_trainer_distributed.py\n-        \"\"\".split()\n-        output_dir = self.get_auto_remove_tmp_dir()\n-        args = f\"--output_dir {output_dir}\".split()\n-        cmd = [\"torchrun\"] + distributed_args + args\n-        execute_subprocess_async(cmd, env=self.get_env())\n-        # successful return here == success - any errors would have caused an error in the sub-call\n-\n-\n class TestTrainerDistributed(TestCasePlus):\n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_trainer(self):\n-        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n+        distributed_args = f\"\"\"--nproc_per_node={backend_device_count(torch_device)}\n             --master_port={get_torch_dist_unique_port()}\n             {self.test_file_dir}/test_trainer_distributed.py\n         \"\"\".split()\n@@ -159,20 +130,6 @@ def test_trainer(self):\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n \n-@require_torch_multi_xpu\n-class TestTrainerDistributedXPU(TestCasePlus):\n-    def test_trainer(self):\n-        distributed_args = f\"\"\"--nproc_per_node={torch.xpu.device_count()}\n-            --master_port={get_torch_dist_unique_port()}\n-            {self.test_file_dir}/test_trainer_distributed.py\n-        \"\"\".split()\n-        output_dir = self.get_auto_remove_tmp_dir()\n-        args = f\"--output_dir {output_dir}\".split()\n-        cmd = [\"torchrun\"] + distributed_args + args\n-        execute_subprocess_async(cmd, env=self.get_env())\n-        # successful return here == success - any errors would have caused an error in the sub-call\n-\n-\n if __name__ == \"__main__\":\n     # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:\n     #"
        },
        {
            "sha": "255739a2d7f3958e20829bf0088009ad48b03048",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "modified",
            "additions": 38,
            "deletions": 31,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89f6956015a42ab32b35de2a6055ea65b5ca53d4/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=89f6956015a42ab32b35de2a6055ea65b5ca53d4",
            "patch": "@@ -17,12 +17,15 @@\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     TestCasePlus,\n+    backend_device_count,\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_accelerate,\n     require_fp8,\n     require_fsdp,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n+    run_first,\n+    torch_device,\n )\n \n \n@@ -64,9 +67,10 @@ def __getitem__(self, i: int) -> str:\n \n \n class TestFSDPTrainer(TestCasePlus):\n+    @require_torch_multi_accelerator\n     @require_accelerate\n-    @require_torch_multi_gpu\n     @require_fsdp\n+    @run_first\n     def test_trainer(self):\n         output_dir = self.get_auto_remove_tmp_dir()\n         cmd = [\n@@ -76,7 +80,7 @@ def test_trainer(self):\n             \"--main_process_port\",\n             f\"{get_torch_dist_unique_port()}\",\n             \"--num_processes\",\n-            f\"{torch.cuda.device_count()}\",\n+            f\"{backend_device_count(torch_device)}\",\n             \"--fsdp_transformer_layer_cls_to_wrap\",\n             \"GPT2Block\",\n             f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n@@ -90,10 +94,11 @@ def test_trainer(self):\n \n \n class TestFSDPTrainerFP8(TestCasePlus):\n+    @require_torch_multi_accelerator\n     @require_accelerate\n-    @require_torch_multi_gpu\n     @require_fsdp\n     @require_fp8\n+    @run_first\n     def test_trainer(self):\n         output_dir = self.get_auto_remove_tmp_dir()\n         cmd = [\n@@ -103,7 +108,7 @@ def test_trainer(self):\n             \"--main_process_port\",\n             f\"{get_torch_dist_unique_port()}\",\n             \"--num_processes\",\n-            f\"{torch.cuda.device_count()}\",\n+            f\"{backend_device_count(torch_device)}\",\n             \"--mixed_precision\",\n             \"fp8\",\n             \"--fsdp_transformer_layer_cls_to_wrap\",\n@@ -117,32 +122,34 @@ def test_trainer(self):\n         execute_subprocess_async(cmd, env=self.get_env())\n         # successful return here == success - any errors would have caused an error in the sub-call\n \n-    class TestFSDPTrainerWrap(TestCasePlus):\n-        @require_accelerate\n-        @require_torch_multi_gpu\n-        @require_fsdp\n-        def test_trainer(self):\n-            output_dir = self.get_auto_remove_tmp_dir()\n-            cmd = [\n-                \"accelerate\",\n-                \"launch\",\n-                \"--use_fsdp\",\n-                \"--main_process_port\",\n-                f\"{get_torch_dist_unique_port()}\",\n-                \"--num_processes\",\n-                f\"{torch.cuda.device_count()}\",\n-                \"--fsdp_transformer_layer_cls_to_wrap\",\n-                \"GPT2Block\",\n-                f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n-                \"--output_dir\",\n-                f\"{output_dir}\",\n-                \"--report_to\",\n-                \"none\",\n-                \"--auto_find_batch_size\",\n-                \"True\",\n-            ]\n-            execute_subprocess_async(cmd, env=self.get_env())\n-            # successful return here == success - any errors would have caused an error in the sub-call\n+\n+class TestFSDPTrainerWrap(TestCasePlus):\n+    @require_torch_multi_accelerator\n+    @require_accelerate\n+    @require_fsdp\n+    @run_first\n+    def test_trainer(self):\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        cmd = [\n+            \"accelerate\",\n+            \"launch\",\n+            \"--use_fsdp\",\n+            \"--main_process_port\",\n+            f\"{get_torch_dist_unique_port()}\",\n+            \"--num_processes\",\n+            f\"{backend_device_count(torch_device)}\",\n+            \"--fsdp_transformer_layer_cls_to_wrap\",\n+            \"GPT2Block\",\n+            f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n+            \"--output_dir\",\n+            f\"{output_dir}\",\n+            \"--report_to\",\n+            \"none\",\n+            \"--auto_find_batch_size\",\n+            \"True\",\n+        ]\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 476,
        "additions": 337,
        "deletions": 139
    }
}