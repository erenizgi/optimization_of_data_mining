{
    "author": "sywangyi",
    "message": "fix failure of llava/pixtral (#42985)\n\n* fix failure of llava/pixtral\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* also fix the issue of Mistral-Small-3.1-24B-Instruct-2503-only-processor tokenizer\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* update\n\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>\n\n* update\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n---------\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\nSigned-off-by: Wang, Yi A <yi.a.wang@intel.com>",
    "sha": "457048fbfdba9a7dee8bd03328c62f49e57b95f9",
    "files": [
        {
            "sha": "d32f38bee69517e967c73954a3342b8ff3cb2412",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 41,
            "deletions": 23,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/457048fbfdba9a7dee8bd03328c62f49e57b95f9/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/457048fbfdba9a7dee8bd03328c62f49e57b95f9/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=457048fbfdba9a7dee8bd03328c62f49e57b95f9",
            "patch": "@@ -33,6 +33,7 @@\n     Expectations,\n     cleanup,\n     require_bitsandbytes,\n+    require_deterministic_for_xpu,\n     require_torch,\n     require_vision,\n     slow,\n@@ -298,16 +299,20 @@ def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @require_bitsandbytes\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_test(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model = LlavaForConditionalGeneration.from_pretrained(\n-            \"llava-hf/bakLlava-v1-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+            \"llava-hf/bakLlava-v1-hf\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            dtype=\"float16\",\n+            device_map=torch_device,\n         )\n \n         prompt = \"<image>\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT:\"\n         image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n         raw_image = Image.open(requests.get(image_file, stream=True).raw)\n-        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device)\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n         expected_decoded_texts = Expectations({\n@@ -323,6 +328,7 @@ def test_small_model_integration_test(self):\n         )\n \n     @require_bitsandbytes\n+    @require_deterministic_for_xpu\n     def test_small_model_integration_test_llama_single(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model_id = \"llava-hf/llava-1.5-7b-hf\"\n@@ -341,7 +347,7 @@ def test_small_model_integration_test_llama_single(self):\n \n         EXPECTED_DECODED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): 'USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Lastly, be respectful of the environment and other visitors, as the pier is a shared space where people can enjoy the view, relax, or engage in recreational activities.',\n+                (\"xpu\", 3): 'USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the presence of boats or other watercraft in the area, as they may be moving at high speeds or making sudden turns, which could pose a risk to pedestrians. Lastly, be aware of any posted signs or warnings, as they may provide important information about the pier\\'s conditions or potential dangers.',\n                 (\"cuda\", 7): 'USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Lastly, be respectful of the environment and other visitors, as the pier is a shared space where people can enjoy the view, relax, or engage in recreational activities.',\n                 (\"cuda\", 8): 'USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock extending over a body of water, there are a few things to be cautious about. First, be aware of the weather conditions, as sudden changes in weather can make the pier unsafe to walk on. Second, be mindful of the water depth and any potential hazards, such as submerged rocks or debris, that could cause accidents or injuries. Additionally, be cautious of the tides and currents, as they can change rapidly and pose a risk to swimmers or those who venture too close to the edge of the pier. Lastly, be respectful of the environment and other visitors, as the pier is a shared space where people can enjoy the view, relax, or engage in recreational activities.',\n                 (\"rocm\", (9, 5)): 'USER:  \\nWhat are the things I should be cautious about when I visit this place? ASSISTANT: When visiting this place, which is a pier or dock overlooking a lake, you should be cautious about the following:\\n\\n1. Safety: Ensure that the pier or dock is stable and secure before stepping onto it. Avoid walking on the edge of the pier or dock, as it could be unstable or unsafe.\\n\\n2. Weather conditions: Be aware of the weather forecast before visiting the area. Strong winds, heavy rain, or storms can make the pier or dock unsafe to use.\\n\\n3. Wildlife: Be mindful of the wildlife in the area, such as birds or aquatic animals. Avoid disturbing their natural habitat or causing harm to the local ecosystem.\\n\\n4. Water safety: If you plan to go swimming or engage in water activities, be aware of the water conditions, such as currents, tides, or potential hazards like submerged objects.\\n\\n5. Personal belongings: Keep an eye on your personal belongings, such as bags or backpacks, to prevent theft or loss.\\n\\n6. Leave no trace: When visiting the area, make sure to clean up after yourself and leave no trace of your presence to preserve the natural environment.',\n@@ -407,7 +413,10 @@ def test_small_model_integration_test_llama_batched(self):\n     def test_small_model_integration_test_batch(self):\n         # Let's make sure we test the preprocessing to replace what is used\n         model = LlavaForConditionalGeneration.from_pretrained(\n-            \"llava-hf/bakLlava-v1-hf\", quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+            \"llava-hf/bakLlava-v1-hf\",\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            dtype=\"float16\",\n+            device_map=torch_device,\n         )\n         # The first batch is longer in terms of text, but only has 1 image. The second batch will be padded in text, but the first will be padded because images take more space!.\n         prompts = [\n@@ -418,7 +427,7 @@ def test_small_model_integration_test_batch(self):\n         image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n \n         inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True).to(\n-            torch_device\n+            torch_device, torch.float16\n         )\n \n         output = model.generate(**inputs, max_new_tokens=20)\n@@ -509,6 +518,7 @@ def test_small_model_integration_test_llama_batched_regression(self):\n     @require_torch\n     @require_vision\n     @require_bitsandbytes\n+    @require_deterministic_for_xpu\n     def test_batched_generation(self):\n         model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", device_map=\"auto\")\n \n@@ -534,24 +544,24 @@ def test_batched_generation(self):\n         EXPECTED_OUTPUTS = Expectations(\n             {\n                 (\"xpu\", 3): [\n-                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one shows a dog standing on a grassy field, while\",\n-                    '\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small',\n-                    '\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the'\n+                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: In the two images, the primary difference is the presence of a green plant in one image and a\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a small, fluffy dog sitting on a sidewalk. The dog is holding\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone, adult llama standing on a grassy hill. The llama\",\n                 ],\n                 (\"cuda\", 7): [\n-                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one of them has a dog standing on a field, while\",\n-                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a brown and white dog sitting on a sidewalk. The dog is holding a small\",\n-                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the\",\n+                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT:  In the two images, the primary difference is the presence of a green plant in one and a yellow\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a small, fluffy dog sitting on a sidewalk. The dog is holding\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone, adult llama standing on a grassy hill. The llama\",\n                 ],\n                 (\"cuda\", 8): [\n-                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one of them has a dog standing on a field, while\",\n-                    '\\nUSER: Describe the image.\\nASSISTANT: The image features a beautiful blonde dog sitting on a sidewalk. The dog is holding a small',\n-                    '\\nUSER: Describe the image.\\nASSISTANT: The image features a lone llama standing on a grassy hill. The llama is the',\n+                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: In the two images, the primary difference is the presence of a green plant in one and a yellow\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a small, fluffy dog sitting on a sidewalk. The dog is holding\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone, adult llama standing on a grassy hill. The llama\",\n                 ],\n                 (\"rocm\", (9, 5)): [\n-                    \"\\n \\nUSER: What's the difference of two images?\\nASSISTANT: The difference between the two images is that one of them is a black and white photo, while the\",\n-                    '\\nUSER: Describe the image.\\nASSISTANT: The image features a brown dog sitting on a sidewalk, holding a green rose in its mouth.',\n-                    '\\nUSER: Describe the image.\\nASSISTANT: The image features a lone, adult llama standing on a grassy hill. The llama',\n+                    \"\\n\\nUSER: What's the difference of two images?\\nASSISTANT: In the two images, the primary difference is the presence of a green plant in one and a yellow\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a small, fluffy dog sitting on a sidewalk. The dog is holding\",\n+                    \"\\nUSER: Describe the image.\\nASSISTANT: The image features a lone, adult llama standing on a grassy hill. The llama\",\n                 ],\n             }\n         )  # fmt: skip\n@@ -612,18 +622,18 @@ def test_generation_siglip_backbone(self):\n \n         EXPECTED_DECODED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): \"user\\n\\nWhat are these?\\nassistant These are two cats, one with a green collar and the other with a black collar. They are lying on a pink blanket and appear to be sleeping\",\n+                (\"xpu\", 3): \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\",\n                 (\"cuda\", None): \"user\\n\\nWhat are these?\\nassistant The image shows two cats, one on the left and one on the right. They appear to be resting or sleeping on a pink blanket. The cat\",\n             }\n         )  # fmt: skip\n         EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n-\n         decoded_text = processor.batch_decode(output, skip_special_tokens=True)[0]\n         self.assertEqual(decoded_text, EXPECTED_DECODED_TEXT)\n \n+    @require_deterministic_for_xpu\n     def test_pixtral(self):\n         model_id = \"mistral-community/pixtral-12b\"\n-        model = LlavaForConditionalGeneration.from_pretrained(model_id)\n+        model = LlavaForConditionalGeneration.from_pretrained(model_id, dtype=\"float16\", device_map=torch_device)\n         processor = AutoProcessor.from_pretrained(model_id)\n \n         IMG_URLS = [\n@@ -633,7 +643,7 @@ def test_pixtral(self):\n         PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][/INST]\"\n \n         # image = Image.open(requests.get(url, stream=True).raw)\n-        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(model.device)\n+        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(torch_device, torch.float16)\n         generate_ids = model.generate(**inputs, max_new_tokens=100)\n         output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n \n@@ -651,10 +661,14 @@ def test_pixtral(self):\n         self.assertEqual(output, EXPECTED_GENERATION)\n \n     @require_bitsandbytes\n+    @require_deterministic_for_xpu\n     def test_pixtral_4bit(self):\n         model_id = \"mistral-community/pixtral-12b\"\n         model = LlavaForConditionalGeneration.from_pretrained(\n-            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+            model_id,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            dtype=\"float16\",\n+            device_map=torch_device,\n         )\n         processor = AutoProcessor.from_pretrained(model_id)\n \n@@ -679,10 +693,14 @@ def test_pixtral_4bit(self):\n         self.assertTrue(output in EXPECTED_GENERATION)\n \n     @require_bitsandbytes\n+    @require_deterministic_for_xpu\n     def test_pixtral_batched(self):\n         model_id = \"mistral-community/pixtral-12b\"\n         model = LlavaForConditionalGeneration.from_pretrained(\n-            model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n+            model_id,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            dtype=\"float16\",\n+            device_map=torch_device,\n         )\n         processor = AutoProcessor.from_pretrained(model_id)\n         processor.tokenizer.pad_token_id = processor.tokenizer.eos_token_id"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 41,
        "deletions": 23
    }
}