{
    "author": "RUFFY-369",
    "message": "[Fix] ViViT interpolate_pos_encoding (#33815)\n\n* fix:test_inference_interpolate_pos_encoding\r\n\r\n* style:make style;make fixup\r\n\r\n* test: add suggestion to test_modeling_vivit\r\n\r\n* chore:add suggestions\r\n\r\n* style:make style\r\n\r\n* [run_slow] vivit\r\n\r\n* ci:slow test fix\r\n\r\n* [run_slow] vivit",
    "sha": "68a2b500690e684ff38da410363ffac24dbe6048",
    "files": [
        {
            "sha": "3d5435032844898c3e6181f47f68eff2ca4de331",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a2b500690e684ff38da410363ffac24dbe6048/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a2b500690e684ff38da410363ffac24dbe6048/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=68a2b500690e684ff38da410363ffac24dbe6048",
            "patch": "@@ -104,9 +104,10 @@ def __init__(self, config):\n             torch.zeros(1, self.patch_embeddings.num_patches + 1, config.hidden_size)\n         )\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.tubelet_size[1:]\n         self.config = config\n \n-    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n+    # Adapted from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n@@ -129,8 +130,8 @@ def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width:\n \n         dim = embeddings.shape[-1]\n \n-        new_height = height // self.patch_size\n-        new_width = width // self.patch_size\n+        new_height = height // self.patch_size[0]\n+        new_width = width // self.patch_size[1]\n \n         sqrt_num_positions = torch_int(num_positions**0.5)\n         patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)"
        },
        {
            "sha": "7cce77e6fc0019d1825ae0f7639ed9ba2dcd1732",
            "filename": "tests/models/vivit/test_modeling_vivit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/68a2b500690e684ff38da410363ffac24dbe6048/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/68a2b500690e684ff38da410363ffac24dbe6048/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py?ref=68a2b500690e684ff38da410363ffac24dbe6048",
            "patch": "@@ -359,12 +359,12 @@ def test_inference_interpolate_pos_encoding(self):\n         # allowing to interpolate the pre-trained position embeddings in order to use\n         # the model on higher resolutions. The DINO model by Facebook AI leverages this\n         # to visualize self-attention on higher resolution images.\n-        model = VivitModel.from_pretrained(\"google/vivit-b-16x2\").to(torch_device)\n+        model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\").to(torch_device)\n \n-        image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2\")\n+        image_processor = VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\")\n         video = prepare_video()\n         inputs = image_processor(\n-            video, size={\"shortest_edge\": 480}, crop_size={\"height\": 480, \"width\": 480}, return_tensors=\"pt\"\n+            video, size={\"shortest_edge\": 480}, crop_size={\"height\": 232, \"width\": 232}, return_tensors=\"pt\"\n         )\n         pixel_values = inputs.pixel_values.to(torch_device)\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 7,
        "deletions": 6
    }
}