{
    "author": "NielsRogge",
    "message": "Add ViTPose (#30530)\n\n* First draft\r\n\r\n* Make fixup\r\n\r\n* Make forward pass worké\r\n\r\n* Improve code\r\n\r\n* More improvements\r\n\r\n* More improvements\r\n\r\n* Make predictions match\r\n\r\n* More improvements\r\n\r\n* Improve image processor\r\n\r\n* Fix model tests\r\n\r\n* Add classic decoder\r\n\r\n* Convert classic decoder\r\n\r\n* Verify image processor\r\n\r\n* Fix classic decoder logits\r\n\r\n* Clean up\r\n\r\n* Add post_process_pose_estimation\r\n\r\n* Improve post_process_pose_estimation\r\n\r\n* Use AutoBackbone\r\n\r\n* Add support for MoE models\r\n\r\n* Fix tests, improve num_experts%\r\n\r\n* Improve variable names\r\n\r\n* Make fixup\r\n\r\n* More improvements\r\n\r\n* Improve post_process_pose_estimation\r\n\r\n* Compute centers and scales\r\n\r\n* Improve postprocessing\r\n\r\n* More improvements\r\n\r\n* Fix ViTPoseBackbone tests\r\n\r\n* Add docstrings, fix image processor tests\r\n\r\n* Update index\r\n\r\n* Use is_cv2_available\r\n\r\n* Add model to toctree\r\n\r\n* Add cv2 to doc tests\r\n\r\n* Remove script\r\n\r\n* Improve conversion script\r\n\r\n* Add coco_to_pascal_voc\r\n\r\n* Add box_to_center_and_scale to image_transforms\r\n\r\n* Update tests\r\n\r\n* Add integration test\r\n\r\n* Fix merge\r\n\r\n* Address comments\r\n\r\n* Replace numpy by pytorch, improve docstrings\r\n\r\n* Remove get_input_embeddings\r\n\r\n* Address comments\r\n\r\n* Move coco_to_pascal_voc\r\n\r\n* Address comment\r\n\r\n* Fix style\r\n\r\n* Address comments\r\n\r\n* Fix test\r\n\r\n* Address comment\r\n\r\n* Remove udp\r\n\r\n* Remove comment\r\n\r\n* [WIP] need to check if the numpy function is same as cv\r\n\r\n* add scipy affine_transform\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* refactor convert\r\n\r\n* add output_shape\r\n\r\n* add atol 5e-2\r\n\r\n* Use hf_hub_download in conversion script\r\n\r\n* make box_to_center more applicable\r\n\r\n* skipt test_get_set_embedding\r\n\r\n* fix to accept array and fix CI\r\n\r\n* add co-contributor\r\n\r\n* make it to tensor type output\r\n\r\n* add torch\r\n\r\n* change to torch tensor\r\n\r\n* add more test\r\n\r\n* minor change\r\n\r\n* CI test change\r\n\r\n* import torch should be above ImageProcessor\r\n\r\n* make style\r\n\r\n* try not use torch in def\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose_backbone/configuration_vitpose_backbone.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose/modeling_vitpose.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* add caution\r\n\r\n* make more detail about dataset_index\r\n\r\n* Update src/transformers/models/vitpose/modeling_vitpose.py\r\n\r\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\r\n\r\n* add docs\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\n* Update src/transformers/models/vitpose/configuration_vitpose.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* Update src/transformers/__init__.py\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* Revert \"Update src/transformers/__init__.py\"\r\n\r\nThis reverts commit 7ffa504450bb9dbccf9c7ea668441b98a1939d5c.\r\n\r\n* change name\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update tests/models/vitpose/test_modeling_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose/modeling_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* move vitpose only function to image_processor\r\n\r\n* raise valueerror when using timm backbone\r\n\r\n* use out_indices\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* remove camel-case of def flip_back\r\n\r\n* rename vitposeEstimatorOutput\r\n\r\n* Update src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* fix confused camelcase of MLP\r\n\r\n* remove in-place logic\r\n\r\n* clear scale description\r\n\r\n* make consistent batch format\r\n\r\n* docs update\r\n\r\n* formatting docstring\r\n\r\n* add batch tests\r\n\r\n* test docs change\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\n* Update src/transformers/models/vitpose/configuration_vitpose.py\r\n\r\n* chagne ViT to Vit\r\n\r\n* change to enable MoE\r\n\r\n* make fix-copies\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* extract udp\r\n\r\n* add more described docs\r\n\r\n* simple fix\r\n\r\n* change to accept target_size\r\n\r\n* make style\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/vitpose/configuration_vitpose.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* change to `verify_backbone_config_arguments`\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* remove unnecessary copy\r\n\r\n* make config immutable\r\n\r\n* enable gradient checkpointing\r\n\r\n* update inappropriate docstring\r\n\r\n* linting docs\r\n\r\n* split function for visibility\r\n\r\n* make style\r\n\r\n* check isinstances\r\n\r\n* change to acceptable use_pretrained_backbone\r\n\r\n* make style\r\n\r\n* remove copy in docs\r\n\r\n* Update src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/models/vitpose/modeling_vitpose.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* simple fix + make style\r\n\r\n* change input config of activation function to string\r\n\r\n* Update docs/source/en/model_doc/vitpose.md\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* tmp docs\r\n\r\n* delete index.md\r\n\r\n* make fix-copies\r\n\r\n* simple fix\r\n\r\n* change conversion to sam2/mllama style\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/models/vitpose/image_processing_vitpose.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* refactor convert\r\n\r\n* add supervision\r\n\r\n* Update src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* remove reduntant def\r\n\r\n* seperate code block for visualization\r\n\r\n* add validation for num_moe\r\n\r\n* final commit\r\n\r\n* add labels\r\n\r\n* [run-slow] vitpose, vitpose_backbone\r\n\r\n* Update src/transformers/models/vitpose/convert_vitpose_to_hf.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* enable all conversion\r\n\r\n* final commit\r\n\r\n* [run-slow] vitpose, vitpose_backbone\r\n\r\n* ruff check --fix\r\n\r\n* [run-slow] vitpose, vitpose_backbone\r\n\r\n* rename split module\r\n\r\n* [run-slow] vitpose, vitpose_backbone\r\n\r\n* fix pos_embed\r\n\r\n* Simplify init\r\n\r\n* Revert \"fix pos_embed\"\r\n\r\nThis reverts commit 2c56a4806e30bc9b5753b142fa04b913306c54ff.\r\n\r\n* refactor single loop\r\n\r\n* allow flag to enable custom model\r\n\r\n* efficiency of MoE to not use unused experts\r\n\r\n* make style\r\n\r\n* Fix range -> arange to avoid warning\r\n\r\n* Revert MOE router, a new one does not work\r\n\r\n* Fix postprocessing a bit (labels)\r\n\r\n* Fix type hint\r\n\r\n* Fix docs snippets\r\n\r\n* Fix links to checkpoints\r\n\r\n* Fix checkpoints in tests\r\n\r\n* Fix test\r\n\r\n* Add image to docs\r\n\r\n---------\r\n\r\nCo-authored-by: Niels Rogge <nielsrogge@nielss-mbp.home>\r\nCo-authored-by: Niels Rogge <nielsrogge@Nielss-MacBook-Pro.local>\r\nCo-authored-by: sangbumchoi <danielsejong55@gmail.com>\r\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "8490d3159ce37087151e30211e33f59008a44233",
    "files": [
        {
            "sha": "61a3938252e91eaad03ebf329047e644523edc26",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -741,6 +741,8 @@\n         title: ViTMatte\n       - local: model_doc/vit_msn\n         title: ViTMSN\n+      - local: model_doc/vitpose\n+        title: ViTPose\n       - local: model_doc/yolos\n         title: YOLOS\n       - local: model_doc/zoedepth"
        },
        {
            "sha": "0135f8f0eb919c5aa7dc0d94a93f43e5e2476eb6",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -356,6 +356,8 @@ Flax), PyTorch, and/or TensorFlow.\n |                       [ViTMAE](model_doc/vit_mae)                        |       ✅        |         ✅         |      ❌      |\n |                      [ViTMatte](model_doc/vitmatte)                      |       ✅        |         ❌         |      ❌      |\n |                       [ViTMSN](model_doc/vit_msn)                        |       ✅        |         ❌         |      ❌      |\n+|                       [VitPose](model_doc/vitpose)                       |       ✅        |         ❌         |      ❌      |\n+|              [VitPoseBackbone](model_doc/vitpose_backbone)               |       ✅        |         ❌         |      ❌      |\n |                          [VITS](model_doc/vits)                          |       ✅        |         ❌         |      ❌      |\n |                         [ViViT](model_doc/vivit)                         |       ✅        |         ❌         |      ❌      |\n |                      [Wav2Vec2](model_doc/wav2vec2)                      |       ✅        |         ✅         |      ✅      |"
        },
        {
            "sha": "361f8e30c75d2b72599bfa46d48bd0b51b231f63",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "added",
            "additions": 254,
            "deletions": 0,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,254 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# VitPose\n+\n+## Overview\n+\n+The VitPose model was proposed in [ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation](https://arxiv.org/abs/2204.12484) by Yufei Xu, Jing Zhang, Qiming Zhang, Dacheng Tao. VitPose employs a standard, non-hierarchical [Vision Transformer](https://arxiv.org/pdf/2010.11929v2) as backbone for the task of keypoint estimation. A simple decoder head is added on top to predict the heatmaps from a given image. Despite its simplicity, the model gets state-of-the-art results on the challenging MS COCO Keypoint Detection benchmark.\n+\n+The abstract from the paper is the following:\n+\n+*Although no specific domain knowledge is considered in the design, plain vision transformers have shown excellent performance in visual recognition tasks. However, little effort has been made to reveal the potential of such simple structures for pose estimation tasks. In this paper, we show the surprisingly good capabilities of plain vision transformers for pose estimation from various aspects, namely simplicity in model structure, scalability in model size, flexibility in training paradigm, and transferability of knowledge between models, through a simple baseline model called ViTPose. Specifically, ViTPose employs plain and non-hierarchical vision transformers as backbones to extract features for a given person instance and a lightweight decoder for pose estimation. It can be scaled up from 100M to 1B parameters by taking the advantages of the scalable model capacity and high parallelism of transformers, setting a new Pareto front between throughput and performance. Besides, ViTPose is very flexible regarding the attention type, input resolution, pre-training and finetuning strategy, as well as dealing with multiple pose tasks. We also empirically demonstrate that the knowledge of large ViTPose models can be easily transferred to small ones via a simple knowledge token. Experimental results show that our basic ViTPose model outperforms representative methods on the challenging MS COCO Keypoint Detection benchmark, while the largest model sets a new state-of-the-art.*\n+\n+![vitpose-architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-architecture.png)\n+\n+This model was contributed by [nielsr](https://huggingface.co/nielsr) and [sangbumchoi](https://github.com/SangbumChoi).\n+The original code can be found [here](https://github.com/ViTAE-Transformer/ViTPose).\n+\n+## Usage Tips\n+\n+ViTPose is a so-called top-down keypoint detection model. This means that one first uses an object detector, like [RT-DETR](rt_detr.md), to detect people (or other instances) in an image. Next, ViTPose takes the cropped images as input and predicts the keypoints.\n+\n+```py\n+import torch\n+import requests\n+import numpy as np\n+\n+from PIL import Image\n+\n+from transformers import (\n+    AutoProcessor,\n+    RTDetrForObjectDetection,\n+    VitPoseForPoseEstimation,\n+)\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+\n+# ------------------------------------------------------------------------\n+# Stage 1. Detect humans on the image\n+# ------------------------------------------------------------------------\n+\n+# You can choose detector by your choice\n+person_image_processor = AutoProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n+person_model = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\", device_map=device)\n+\n+inputs = person_image_processor(images=image, return_tensors=\"pt\").to(device)\n+\n+with torch.no_grad():\n+    outputs = person_model(**inputs)\n+\n+results = person_image_processor.post_process_object_detection(\n+    outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.3\n+)\n+result = results[0]  # take first image results\n+\n+# Human label refers 0 index in COCO dataset\n+person_boxes = result[\"boxes\"][result[\"labels\"] == 0]\n+person_boxes = person_boxes.cpu().numpy()\n+\n+# Convert boxes from VOC (x1, y1, x2, y2) to COCO (x1, y1, w, h) format\n+person_boxes[:, 2] = person_boxes[:, 2] - person_boxes[:, 0]\n+person_boxes[:, 3] = person_boxes[:, 3] - person_boxes[:, 1]\n+\n+# ------------------------------------------------------------------------\n+# Stage 2. Detect keypoints for each person found\n+# ------------------------------------------------------------------------\n+\n+image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n+model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\", device_map=device)\n+\n+inputs = image_processor(image, boxes=[person_boxes], return_tensors=\"pt\").to(device)\n+\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+pose_results = image_processor.post_process_pose_estimation(outputs, boxes=[person_boxes])\n+image_pose_result = pose_results[0]  # results for first image\n+```\n+\n+\n+### Visualization for supervision user\n+```py\n+import supervision as sv\n+\n+xy = torch.stack([pose_result['keypoints'] for pose_result in image_pose_result]).cpu().numpy()\n+scores = torch.stack([pose_result['scores'] for pose_result in image_pose_result]).cpu().numpy()\n+\n+key_points = sv.KeyPoints(\n+    xy=xy, confidence=scores\n+)\n+\n+edge_annotator = sv.EdgeAnnotator(\n+    color=sv.Color.GREEN,\n+    thickness=1\n+)\n+vertex_annotator = sv.VertexAnnotator(\n+    color=sv.Color.RED,\n+    radius=2\n+)\n+annotated_frame = edge_annotator.annotate(\n+    scene=image.copy(),\n+    key_points=key_points\n+)\n+annotated_frame = vertex_annotator.annotate(\n+    scene=annotated_frame,\n+    key_points=key_points\n+)\n+```\n+\n+### Visualization for advanced user\n+```py\n+import math\n+import cv2\n+\n+def draw_points(image, keypoints, scores, pose_keypoint_color, keypoint_score_threshold, radius, show_keypoint_weight):\n+    if pose_keypoint_color is not None:\n+        assert len(pose_keypoint_color) == len(keypoints)\n+    for kid, (kpt, kpt_score) in enumerate(zip(keypoints, scores)):\n+        x_coord, y_coord = int(kpt[0]), int(kpt[1])\n+        if kpt_score > keypoint_score_threshold:\n+            color = tuple(int(c) for c in pose_keypoint_color[kid])\n+            if show_keypoint_weight:\n+                cv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\n+                transparency = max(0, min(1, kpt_score))\n+                cv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\n+            else:\n+                cv2.circle(image, (int(x_coord), int(y_coord)), radius, color, -1)\n+\n+def draw_links(image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold, thickness, show_keypoint_weight, stick_width = 2):\n+    height, width, _ = image.shape\n+    if keypoint_edges is not None and link_colors is not None:\n+        assert len(link_colors) == len(keypoint_edges)\n+        for sk_id, sk in enumerate(keypoint_edges):\n+            x1, y1, score1 = (int(keypoints[sk[0], 0]), int(keypoints[sk[0], 1]), scores[sk[0]])\n+            x2, y2, score2 = (int(keypoints[sk[1], 0]), int(keypoints[sk[1], 1]), scores[sk[1]])\n+            if (\n+                x1 > 0\n+                and x1 < width\n+                and y1 > 0\n+                and y1 < height\n+                and x2 > 0\n+                and x2 < width\n+                and y2 > 0\n+                and y2 < height\n+                and score1 > keypoint_score_threshold\n+                and score2 > keypoint_score_threshold\n+            ):\n+                color = tuple(int(c) for c in link_colors[sk_id])\n+                if show_keypoint_weight:\n+                    X = (x1, x2)\n+                    Y = (y1, y2)\n+                    mean_x = np.mean(X)\n+                    mean_y = np.mean(Y)\n+                    length = ((Y[0] - Y[1]) ** 2 + (X[0] - X[1]) ** 2) ** 0.5\n+                    angle = math.degrees(math.atan2(Y[0] - Y[1], X[0] - X[1]))\n+                    polygon = cv2.ellipse2Poly(\n+                        (int(mean_x), int(mean_y)), (int(length / 2), int(stick_width)), int(angle), 0, 360, 1\n+                    )\n+                    cv2.fillConvexPoly(image, polygon, color)\n+                    transparency = max(0, min(1, 0.5 * (keypoints[sk[0], 2] + keypoints[sk[1], 2])))\n+                    cv2.addWeighted(image, transparency, image, 1 - transparency, 0, dst=image)\n+                else:\n+                    cv2.line(image, (x1, y1), (x2, y2), color, thickness=thickness)\n+\n+\n+# Note: keypoint_edges and color palette are dataset-specific\n+keypoint_edges = model.config.edges\n+\n+palette = np.array(\n+    [\n+        [255, 128, 0],\n+        [255, 153, 51],\n+        [255, 178, 102],\n+        [230, 230, 0],\n+        [255, 153, 255],\n+        [153, 204, 255],\n+        [255, 102, 255],\n+        [255, 51, 255],\n+        [102, 178, 255],\n+        [51, 153, 255],\n+        [255, 153, 153],\n+        [255, 102, 102],\n+        [255, 51, 51],\n+        [153, 255, 153],\n+        [102, 255, 102],\n+        [51, 255, 51],\n+        [0, 255, 0],\n+        [0, 0, 255],\n+        [255, 0, 0],\n+        [255, 255, 255],\n+    ]\n+)\n+\n+link_colors = palette[[0, 0, 0, 0, 7, 7, 7, 9, 9, 9, 9, 9, 16, 16, 16, 16, 16, 16, 16]]\n+keypoint_colors = palette[[16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 0, 0, 0, 0, 0, 0]]\n+\n+numpy_image = np.array(image)\n+\n+for pose_result in image_pose_result:\n+    scores = np.array(pose_result[\"scores\"])\n+    keypoints = np.array(pose_result[\"keypoints\"])\n+\n+    # draw each point on image\n+    draw_points(numpy_image, keypoints, scores, keypoint_colors, keypoint_score_threshold=0.3, radius=4, show_keypoint_weight=False)\n+\n+    # draw links\n+    draw_links(numpy_image, keypoints, scores, keypoint_edges, link_colors, keypoint_score_threshold=0.3, thickness=1, show_keypoint_weight=False)\n+\n+pose_image = Image.fromarray(numpy_image)\n+pose_image\n+```\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vitpose-coco.jpg\" alt=\"drawing\" width=\"600\"/>\n+\n+### MoE backbone\n+\n+To enable MoE (Mixture of Experts) function in the backbone, user has to give appropriate configuration such as `num_experts` and input value `dataset_index` to the backbone model.  However, it is not used in default parameters. Below is the code snippet for usage of MoE function.\n+\n+```py\n+>>> from transformers import VitPoseBackboneConfig, VitPoseBackbone\n+>>> import torch\n+\n+>>> config = VitPoseBackboneConfig(num_experts=3, out_indices=[-1])\n+>>> model = VitPoseBackbone(config)\n+\n+>>> pixel_values = torch.randn(3, 3, 256, 192)\n+>>> dataset_index = torch.tensor([1, 2, 3])\n+>>> outputs = model(pixel_values, dataset_index)\n+```\n+\n+## VitPoseImageProcessor\n+\n+[[autodoc]] VitPoseImageProcessor\n+    - preprocess\n+\n+## VitPoseConfig\n+\n+[[autodoc]] VitPoseConfig\n+\n+## VitPoseForPoseEstimation\n+\n+[[autodoc]] VitPoseForPoseEstimation\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "2b4980306c53ce0b2c9d7be7af8ded1c8a66f66b",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -834,6 +834,8 @@\n     \"models.vit_msn\": [\"ViTMSNConfig\"],\n     \"models.vitdet\": [\"VitDetConfig\"],\n     \"models.vitmatte\": [\"VitMatteConfig\"],\n+    \"models.vitpose\": [\"VitPoseConfig\"],\n+    \"models.vitpose_backbone\": [\"VitPoseBackboneConfig\"],\n     \"models.vits\": [\n         \"VitsConfig\",\n         \"VitsTokenizer\",\n@@ -1266,6 +1268,7 @@\n     _import_structure[\"models.vilt\"].extend([\"ViltFeatureExtractor\", \"ViltImageProcessor\", \"ViltProcessor\"])\n     _import_structure[\"models.vit\"].extend([\"ViTFeatureExtractor\", \"ViTImageProcessor\"])\n     _import_structure[\"models.vitmatte\"].append(\"VitMatteImageProcessor\")\n+    _import_structure[\"models.vitpose\"].append(\"VitPoseImageProcessor\")\n     _import_structure[\"models.vivit\"].append(\"VivitImageProcessor\")\n     _import_structure[\"models.yolos\"].extend([\"YolosFeatureExtractor\", \"YolosImageProcessor\"])\n     _import_structure[\"models.zoedepth\"].append(\"ZoeDepthImageProcessor\")\n@@ -3755,6 +3758,18 @@\n             \"VitMattePreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.vitpose\"].extend(\n+        [\n+            \"VitPoseForPoseEstimation\",\n+            \"VitPosePreTrainedModel\",\n+        ]\n+    )\n+    _import_structure[\"models.vitpose_backbone\"].extend(\n+        [\n+            \"VitPoseBackbone\",\n+            \"VitPoseBackbonePreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.vits\"].extend(\n         [\n             \"VitsModel\",\n@@ -5877,6 +5892,8 @@\n     from .models.vit_msn import ViTMSNConfig\n     from .models.vitdet import VitDetConfig\n     from .models.vitmatte import VitMatteConfig\n+    from .models.vitpose import VitPoseConfig\n+    from .models.vitpose_backbone import VitPoseBackboneConfig\n     from .models.vits import (\n         VitsConfig,\n         VitsTokenizer,\n@@ -6311,6 +6328,7 @@\n         from .models.vilt import ViltFeatureExtractor, ViltImageProcessor, ViltProcessor\n         from .models.vit import ViTFeatureExtractor, ViTImageProcessor\n         from .models.vitmatte import VitMatteImageProcessor\n+        from .models.vitpose import VitPoseImageProcessor\n         from .models.vivit import VivitImageProcessor\n         from .models.yolos import YolosFeatureExtractor, YolosImageProcessor\n         from .models.zoedepth import ZoeDepthImageProcessor\n@@ -8294,6 +8312,11 @@\n             VitMatteForImageMatting,\n             VitMattePreTrainedModel,\n         )\n+        from .models.vitpose import (\n+            VitPoseForPoseEstimation,\n+            VitPosePreTrainedModel,\n+        )\n+        from .models.vitpose_backbone import VitPoseBackbone, VitPoseBackbonePreTrainedModel\n         from .models.vits import (\n             VitsModel,\n             VitsPreTrainedModel,"
        },
        {
            "sha": "150eca78e38834c66415086548437c5ff959c49b",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -277,6 +277,8 @@\n     vit_msn,\n     vitdet,\n     vitmatte,\n+    vitpose,\n+    vitpose_backbone,\n     vits,\n     vivit,\n     wav2vec2,"
        },
        {
            "sha": "e8deb34018e58c301ef583b44fd3d538d0361c11",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -309,6 +309,8 @@\n         (\"vit_msn\", \"ViTMSNConfig\"),\n         (\"vitdet\", \"VitDetConfig\"),\n         (\"vitmatte\", \"VitMatteConfig\"),\n+        (\"vitpose\", \"VitPoseConfig\"),\n+        (\"vitpose_backbone\", \"VitPoseBackboneConfig\"),\n         (\"vits\", \"VitsConfig\"),\n         (\"vivit\", \"VivitConfig\"),\n         (\"wav2vec2\", \"Wav2Vec2Config\"),\n@@ -642,6 +644,8 @@\n         (\"vit_msn\", \"ViTMSN\"),\n         (\"vitdet\", \"VitDet\"),\n         (\"vitmatte\", \"ViTMatte\"),\n+        (\"vitpose\", \"VitPose\"),\n+        (\"vitpose_backbone\", \"VitPoseBackbone\"),\n         (\"vits\", \"VITS\"),\n         (\"vivit\", \"ViViT\"),\n         (\"wav2vec2\", \"Wav2Vec2\"),"
        },
        {
            "sha": "d2d52a77579ace87a9fd10fec624f9ad61061ff0",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -1396,6 +1396,7 @@\n         (\"textnet\", \"TextNetBackbone\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"vitdet\", \"VitDetBackbone\"),\n+        (\"vitpose_backbone\", \"VitPoseBackbone\"),\n     ]\n )\n "
        },
        {
            "sha": "4a57524cce2143830e294c1920e395436d670add",
            "filename": "src/transformers/models/vitpose/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2F__init__.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_vitpose import *\n+    from .image_processing_vitpose import *\n+    from .modeling_vitpose import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "763c1f1bd7bdd771b81383e678e64121378bb585",
            "filename": "src/transformers/models/vitpose/configuration_vitpose.py",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,124 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"VitPose model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto.configuration_auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VitPoseConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`VitPoseForPoseEstimation`]. It is used to instantiate a\n+    VitPose model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the VitPose\n+    [usyd-community/vitpose-base-simple](https://huggingface.co/usyd-community/vitpose-base-simple) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        backbone_config (`PretrainedConfig` or `dict`, *optional*, defaults to `VitPoseBackboneConfig()`):\n+            The configuration of the backbone model. Currently, only `backbone_config` with `vitpose_backbone` as `model_type` is supported.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        scale_factor (`int`, *optional*, defaults to 4):\n+            Factor to upscale the feature maps coming from the ViT backbone.\n+        use_simple_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether to use a `VitPoseSimpleDecoder` to decode the feature maps from the backbone into heatmaps. Otherwise it uses `VitPoseClassicDecoder`.\n+\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import VitPoseConfig, VitPoseForPoseEstimation\n+\n+    >>> # Initializing a VitPose configuration\n+    >>> configuration = VitPoseConfig()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = VitPoseForPoseEstimation(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"vitpose\"\n+\n+    def __init__(\n+        self,\n+        backbone_config: PretrainedConfig = None,\n+        backbone: str = None,\n+        use_pretrained_backbone: bool = False,\n+        use_timm_backbone: bool = False,\n+        backbone_kwargs: dict = None,\n+        initializer_range: float = 0.02,\n+        scale_factor: int = 4,\n+        use_simple_decoder: bool = True,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if use_pretrained_backbone:\n+            logger.info(\n+                \"`use_pretrained_backbone` is `True`. For the pure inference purpose of VitPose weight do not set this value.\"\n+            )\n+        if use_timm_backbone:\n+            raise ValueError(\"use_timm_backbone set `True` is not supported at the moment.\")\n+\n+        if backbone_config is None and backbone is None:\n+            logger.info(\"`backbone_config` is `None`. Initializing the config with the default `VitPose` backbone.\")\n+            backbone_config = CONFIG_MAPPING[\"vitpose_backbone\"](out_indices=[4])\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.get(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.backbone_kwargs = backbone_kwargs\n+\n+        self.initializer_range = initializer_range\n+        self.scale_factor = scale_factor\n+        self.use_simple_decoder = use_simple_decoder\n+\n+\n+__all__ = [\"VitPoseConfig\"]"
        },
        {
            "sha": "f151adebbce79e8d45db233aebfb4efd30499720",
            "filename": "src/transformers/models/vitpose/convert_vitpose_to_hf.py",
            "status": "added",
            "additions": 355,
            "deletions": 0,
            "changes": 355,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconvert_vitpose_to_hf.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,355 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert VitPose checkpoints from the original repository.\n+\n+URL: https://github.com/vitae-transformer/vitpose\n+\"\"\"\n+\n+import argparse\n+import os\n+import re\n+\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+\n+from transformers import VitPoseBackboneConfig, VitPoseConfig, VitPoseForPoseEstimation, VitPoseImageProcessor\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"patch_embed.proj\": \"embeddings.patch_embeddings.projection\",\n+    r\"pos_embed\": \"embeddings.position_embeddings\",\n+    r\"blocks\": \"encoder.layer\",\n+    r\"attn.proj\": \"attention.output.dense\",\n+    r\"attn\": \"attention.self\",\n+    r\"norm1\": \"layernorm_before\",\n+    r\"norm2\": \"layernorm_after\",\n+    r\"last_norm\": \"layernorm\",\n+    r\"keypoint_head\": \"head\",\n+    r\"final_layer\": \"conv\",\n+}\n+\n+MODEL_TO_FILE_NAME_MAPPING = {\n+    \"vitpose-base-simple\": \"vitpose-b-simple.pth\",\n+    \"vitpose-base\": \"vitpose-b.pth\",\n+    \"vitpose-base-coco-aic-mpii\": \"vitpose_base_coco_aic_mpii.pth\",\n+    \"vitpose-plus-base\": \"vitpose+_base.pth\",\n+}\n+\n+\n+def get_config(model_name):\n+    num_experts = 6 if \"plus\" in model_name else 1\n+    part_features = 192 if \"plus\" in model_name else 0\n+\n+    backbone_config = VitPoseBackboneConfig(out_indices=[12], num_experts=num_experts, part_features=part_features)\n+    # size of the architecture\n+    if \"small\" in model_name:\n+        backbone_config.hidden_size = 768\n+        backbone_config.intermediate_size = 2304\n+        backbone_config.num_hidden_layers = 8\n+        backbone_config.num_attention_heads = 8\n+    elif \"large\" in model_name:\n+        backbone_config.hidden_size = 1024\n+        backbone_config.intermediate_size = 4096\n+        backbone_config.num_hidden_layers = 24\n+        backbone_config.num_attention_heads = 16\n+    elif \"huge\" in model_name:\n+        backbone_config.hidden_size = 1280\n+        backbone_config.intermediate_size = 5120\n+        backbone_config.num_hidden_layers = 32\n+        backbone_config.num_attention_heads = 16\n+\n+    use_simple_decoder = \"simple\" in model_name\n+\n+    edges = [\n+        [15, 13],\n+        [13, 11],\n+        [16, 14],\n+        [14, 12],\n+        [11, 12],\n+        [5, 11],\n+        [6, 12],\n+        [5, 6],\n+        [5, 7],\n+        [6, 8],\n+        [7, 9],\n+        [8, 10],\n+        [1, 2],\n+        [0, 1],\n+        [0, 2],\n+        [1, 3],\n+        [2, 4],\n+        [3, 5],\n+        [4, 6],\n+    ]\n+    id2label = {\n+        0: \"Nose\",\n+        1: \"L_Eye\",\n+        2: \"R_Eye\",\n+        3: \"L_Ear\",\n+        4: \"R_Ear\",\n+        5: \"L_Shoulder\",\n+        6: \"R_Shoulder\",\n+        7: \"L_Elbow\",\n+        8: \"R_Elbow\",\n+        9: \"L_Wrist\",\n+        10: \"R_Wrist\",\n+        11: \"L_Hip\",\n+        12: \"R_Hip\",\n+        13: \"L_Knee\",\n+        14: \"R_Knee\",\n+        15: \"L_Ankle\",\n+        16: \"R_Ankle\",\n+    }\n+\n+    label2id = {v: k for k, v in id2label.items()}\n+\n+    config = VitPoseConfig(\n+        backbone_config=backbone_config,\n+        num_labels=17,\n+        use_simple_decoder=use_simple_decoder,\n+        edges=edges,\n+        id2label=id2label,\n+        label2id=label2id,\n+    )\n+\n+    return config\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+# We will verify our results on a COCO image\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw)\n+    return image\n+\n+\n+@torch.no_grad()\n+def write_model(model_path, model_name, push_to_hub, check_logits=True):\n+    os.makedirs(model_path, exist_ok=True)\n+\n+    # ------------------------------------------------------------\n+    # Vision model params and config\n+    # ------------------------------------------------------------\n+\n+    # params from config\n+    config = get_config(model_name)\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    # load original state_dict\n+    filename = MODEL_TO_FILE_NAME_MAPPING[model_name]\n+    print(f\"Fetching all parameters from the checkpoint at {filename}...\")\n+\n+    checkpoint_path = hf_hub_download(\n+        repo_id=\"nielsr/vitpose-original-checkpoints\", filename=filename, repo_type=\"model\"\n+    )\n+\n+    print(\"Converting model...\")\n+    original_state_dict = torch.load(checkpoint_path, map_location=\"cpu\")[\"state_dict\"]\n+    all_keys = list(original_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    dim = config.backbone_config.hidden_size\n+\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        value = original_state_dict[key]\n+\n+        if re.search(\"associate_heads\", new_key) or re.search(\"backbone.cls_token\", new_key):\n+            # This associated_heads is concept of auxiliary head so does not require in inference stage.\n+            # backbone.cls_token is optional forward function for dynamically change of size, see detail in https://github.com/ViTAE-Transformer/ViTPose/issues/34\n+            pass\n+        elif re.search(\"qkv\", new_key):\n+            state_dict[new_key.replace(\"self.qkv\", \"attention.query\")] = value[:dim]\n+            state_dict[new_key.replace(\"self.qkv\", \"attention.key\")] = value[dim : dim * 2]\n+            state_dict[new_key.replace(\"self.qkv\", \"attention.value\")] = value[-dim:]\n+        elif re.search(\"head\", new_key) and not config.use_simple_decoder:\n+            # Pattern for deconvolution layers\n+            deconv_pattern = r\"deconv_layers\\.(0|3)\\.weight\"\n+            new_key = re.sub(deconv_pattern, lambda m: f\"deconv{int(m.group(1))//3 + 1}.weight\", new_key)\n+            # Pattern for batch normalization layers\n+            bn_patterns = [\n+                (r\"deconv_layers\\.(\\d+)\\.weight\", r\"batchnorm\\1.weight\"),\n+                (r\"deconv_layers\\.(\\d+)\\.bias\", r\"batchnorm\\1.bias\"),\n+                (r\"deconv_layers\\.(\\d+)\\.running_mean\", r\"batchnorm\\1.running_mean\"),\n+                (r\"deconv_layers\\.(\\d+)\\.running_var\", r\"batchnorm\\1.running_var\"),\n+                (r\"deconv_layers\\.(\\d+)\\.num_batches_tracked\", r\"batchnorm\\1.num_batches_tracked\"),\n+            ]\n+\n+            for pattern, replacement in bn_patterns:\n+                if re.search(pattern, new_key):\n+                    # Convert the layer number to the correct batch norm index\n+                    layer_num = int(re.search(pattern, key).group(1))\n+                    bn_num = layer_num // 3 + 1\n+                    new_key = re.sub(pattern, replacement.replace(r\"\\1\", str(bn_num)), new_key)\n+            state_dict[new_key] = value\n+        else:\n+            state_dict[new_key] = value\n+\n+    print(\"Loading the checkpoint in a Vitpose model.\")\n+    model = VitPoseForPoseEstimation(config)\n+    model.eval()\n+    model.load_state_dict(state_dict)\n+    print(\"Checkpoint loaded successfully.\")\n+\n+    # create image processor\n+    image_processor = VitPoseImageProcessor()\n+\n+    # verify image processor\n+    image = prepare_img()\n+    boxes = [[[412.8, 157.61, 53.05, 138.01], [384.43, 172.21, 15.12, 35.74]]]\n+    pixel_values = image_processor(images=image, boxes=boxes, return_tensors=\"pt\").pixel_values\n+\n+    filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=\"vitpose_batch_data.pt\", repo_type=\"dataset\")\n+    original_pixel_values = torch.load(filepath, map_location=\"cpu\")[\"img\"]\n+    assert torch.allclose(pixel_values, original_pixel_values, atol=1e-1)\n+\n+    dataset_index = torch.tensor([0])\n+\n+    with torch.no_grad():\n+        # first forward pass\n+        outputs = model(pixel_values, dataset_index=dataset_index)\n+        output_heatmap = outputs.heatmaps\n+\n+        # second forward pass (flipped)\n+        # this is done since the model uses `flip_test=True` in its test config\n+        pixel_values_flipped = torch.flip(pixel_values, [3])\n+        outputs_flipped = model(\n+            pixel_values_flipped,\n+            dataset_index=dataset_index,\n+            flip_pairs=torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]]),\n+        )\n+        output_flipped_heatmap = outputs_flipped.heatmaps\n+\n+    outputs.heatmaps = (output_heatmap + output_flipped_heatmap) * 0.5\n+\n+    # Verify pose_results\n+    pose_results = image_processor.post_process_pose_estimation(outputs, boxes=boxes)[0]\n+\n+    if check_logits:\n+        if model_name == \"vitpose-base-simple\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([3.98180511e02, 1.81808380e02]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor([8.66642594e-01]),\n+                atol=5e-2,\n+            )\n+        elif model_name == \"vitpose-base\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([3.9807913e02, 1.8182812e02]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor([8.8235235e-01]),\n+                atol=5e-2,\n+            )\n+        elif model_name == \"vitpose-base-coco-aic-mpii\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([3.98305542e02, 1.81741592e02]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor([8.69966745e-01]),\n+                atol=5e-2,\n+            )\n+        elif model_name == \"vitpose-plus-base\":\n+            assert torch.allclose(\n+                pose_results[1][\"keypoints\"][0],\n+                torch.tensor([3.98201294e02, 1.81728302e02]),\n+                atol=5e-2,\n+            )\n+            assert torch.allclose(\n+                pose_results[1][\"scores\"][0],\n+                torch.tensor([8.75046968e-01]),\n+                atol=5e-2,\n+            )\n+        else:\n+            raise ValueError(\"Model not supported\")\n+    print(\"Conversion successfully done.\")\n+\n+    # save the model to a local directory\n+    model.save_pretrained(model_path)\n+    image_processor.save_pretrained(model_path)\n+\n+    if push_to_hub:\n+        print(f\"Pushing model and image processor for {model_name} to hub\")\n+        model.push_to_hub(f\"danelcsb/{model_name}\")\n+        image_processor.push_to_hub(f\"danelcsb/{model_name}\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"vitpose-base-simple\",\n+        choices=MODEL_TO_FILE_NAME_MAPPING.keys(),\n+        type=str,\n+        help=\"Name of the VitPose model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the output PyTorch model directory.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the 🤗 hub.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        default=True,\n+        type=bool,\n+        help=\"Whether to check the logits of public converted model to the 🤗 hub. You can disable when using custom model.\",\n+    )\n+\n+    args = parser.parse_args()\n+    write_model(\n+        model_path=args.pytorch_dump_folder_path,\n+        model_name=args.model_name,\n+        push_to_hub=args.push_to_hub,\n+        check_logits=args.check_logits,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "e7c5c524cb056d2dec27c89de789b6125a673ce3",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "added",
            "additions": 684,
            "deletions": 0,
            "changes": 684,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,684 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for VitPose.\"\"\"\n+\n+import itertools\n+import math\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature\n+from ...image_transforms import to_channel_dimension_format\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+)\n+from ...utils import TensorType, is_scipy_available, is_torch_available, is_vision_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    import PIL\n+\n+if is_scipy_available():\n+    from scipy.linalg import inv\n+    from scipy.ndimage import affine_transform, gaussian_filter\n+\n+if TYPE_CHECKING:\n+    from .modeling_vitpose import VitPoseEstimatorOutput\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# inspired by https://github.com/ViTAE-Transformer/ViTPose/blob/d5216452796c90c6bc29f5c5ec0bdba94366768a/mmpose/datasets/datasets/base/kpt_2d_sview_rgb_img_top_down_dataset.py#L132\n+def box_to_center_and_scale(\n+    box: Union[Tuple, List, np.ndarray],\n+    image_width: int,\n+    image_height: int,\n+    normalize_factor: float = 200.0,\n+    padding_factor: float = 1.25,\n+):\n+    \"\"\"\n+    Encodes a bounding box in COCO format into (center, scale).\n+\n+    Args:\n+        box (`Tuple`, `List`, or `np.ndarray`):\n+            Bounding box in COCO format (top_left_x, top_left_y, width, height).\n+        image_width (`int`):\n+            Image width.\n+        image_height (`int`):\n+            Image height.\n+        normalize_factor (`float`):\n+            Width and height scale factor.\n+        padding_factor (`float`):\n+            Bounding box padding factor.\n+\n+    Returns:\n+        tuple: A tuple containing center and scale.\n+\n+        - `np.ndarray` [float32](2,): Center of the bbox (x, y).\n+        - `np.ndarray` [float32](2,): Scale of the bbox width & height.\n+    \"\"\"\n+\n+    top_left_x, top_left_y, width, height = box[:4]\n+    aspect_ratio = image_width / image_height\n+    center = np.array([top_left_x + width * 0.5, top_left_y + height * 0.5], dtype=np.float32)\n+\n+    if width > aspect_ratio * height:\n+        height = width * 1.0 / aspect_ratio\n+    elif width < aspect_ratio * height:\n+        width = height * aspect_ratio\n+\n+    scale = np.array([width / normalize_factor, height / normalize_factor], dtype=np.float32)\n+    scale = scale * padding_factor\n+\n+    return center, scale\n+\n+\n+def coco_to_pascal_voc(bboxes: np.ndarray) -> np.ndarray:\n+    \"\"\"\n+    Converts bounding boxes from the COCO format to the Pascal VOC format.\n+\n+    In other words, converts from (top_left_x, top_left_y, width, height) format\n+    to (top_left_x, top_left_y, bottom_right_x, bottom_right_y).\n+\n+    Args:\n+        bboxes (`np.ndarray` of shape `(batch_size, 4)):\n+            Bounding boxes in COCO format.\n+\n+    Returns:\n+        `np.ndarray` of shape `(batch_size, 4) in Pascal VOC format.\n+    \"\"\"\n+    bboxes[:, 2] = bboxes[:, 2] + bboxes[:, 0] - 1\n+    bboxes[:, 3] = bboxes[:, 3] + bboxes[:, 1] - 1\n+\n+    return bboxes\n+\n+\n+def get_keypoint_predictions(heatmaps: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n+    \"\"\"Get keypoint predictions from score maps.\n+\n+    Args:\n+        heatmaps (`np.ndarray` of shape `(batch_size, num_keypoints, height, width)`):\n+            Model predicted heatmaps.\n+\n+    Returns:\n+        tuple: A tuple containing aggregated results.\n+\n+        - coords (`np.ndarray` of shape `(batch_size, num_keypoints, 2)`):\n+            Predicted keypoint location.\n+        - scores (`np.ndarray` of shape `(batch_size, num_keypoints, 1)`):\n+            Scores (confidence) of the keypoints.\n+    \"\"\"\n+    if not isinstance(heatmaps, np.ndarray):\n+        raise ValueError(\"Heatmaps should be np.ndarray\")\n+    if heatmaps.ndim != 4:\n+        raise ValueError(\"Heatmaps should be 4-dimensional\")\n+\n+    batch_size, num_keypoints, _, width = heatmaps.shape\n+    heatmaps_reshaped = heatmaps.reshape((batch_size, num_keypoints, -1))\n+    idx = np.argmax(heatmaps_reshaped, 2).reshape((batch_size, num_keypoints, 1))\n+    scores = np.amax(heatmaps_reshaped, 2).reshape((batch_size, num_keypoints, 1))\n+\n+    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n+    preds[:, :, 0] = preds[:, :, 0] % width\n+    preds[:, :, 1] = preds[:, :, 1] // width\n+\n+    preds = np.where(np.tile(scores, (1, 1, 2)) > 0.0, preds, -1)\n+    return preds, scores\n+\n+\n+def post_dark_unbiased_data_processing(coords: np.ndarray, batch_heatmaps: np.ndarray, kernel: int = 3) -> np.ndarray:\n+    \"\"\"DARK post-pocessing. Implemented by unbiased_data_processing.\n+\n+    Paper references:\n+    - Huang et al. The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation (CVPR 2020).\n+    - Zhang et al. Distribution-Aware Coordinate Representation for Human Pose Estimation (CVPR 2020).\n+\n+    Args:\n+        coords (`np.ndarray` of shape `(num_persons, num_keypoints, 2)`):\n+            Initial coordinates of human pose.\n+        batch_heatmaps (`np.ndarray` of shape `(batch_size, num_keypoints, height, width)`):\n+            Batched heatmaps as predicted by the model.\n+            A batch_size of 1 is used for the bottom up paradigm where all persons share the same heatmap.\n+            A batch_size of `num_persons` is used for the top down paradigm where each person has its own heatmaps.\n+        kernel (`int`, *optional*, defaults to 3):\n+            Gaussian kernel size (K) for modulation.\n+\n+    Returns:\n+        `np.ndarray` of shape `(num_persons, num_keypoints, 2)` ):\n+            Refined coordinates.\n+    \"\"\"\n+    batch_size, num_keypoints, height, width = batch_heatmaps.shape\n+    num_coords = coords.shape[0]\n+    if not (batch_size == 1 or batch_size == num_coords):\n+        raise ValueError(\"The batch size of heatmaps should be 1 or equal to the batch size of coordinates.\")\n+    radius = int((kernel - 1) // 2)\n+    batch_heatmaps = np.array(\n+        [\n+            [gaussian_filter(heatmap, sigma=0.8, radius=(radius, radius), axes=(0, 1)) for heatmap in heatmaps]\n+            for heatmaps in batch_heatmaps\n+        ]\n+    )\n+    batch_heatmaps = np.clip(batch_heatmaps, 0.001, 50)\n+    batch_heatmaps = np.log(batch_heatmaps)\n+\n+    batch_heatmaps_pad = np.pad(batch_heatmaps, ((0, 0), (0, 0), (1, 1), (1, 1)), mode=\"edge\").flatten()\n+\n+    # calculate indices for coordinates\n+    index = coords[..., 0] + 1 + (coords[..., 1] + 1) * (width + 2)\n+    index += (width + 2) * (height + 2) * np.arange(0, batch_size * num_keypoints).reshape(-1, num_keypoints)\n+    index = index.astype(int).reshape(-1, 1)\n+    i_ = batch_heatmaps_pad[index]\n+    ix1 = batch_heatmaps_pad[index + 1]\n+    iy1 = batch_heatmaps_pad[index + width + 2]\n+    ix1y1 = batch_heatmaps_pad[index + width + 3]\n+    ix1_y1_ = batch_heatmaps_pad[index - width - 3]\n+    ix1_ = batch_heatmaps_pad[index - 1]\n+    iy1_ = batch_heatmaps_pad[index - 2 - width]\n+\n+    # calculate refined coordinates using Newton's method\n+    dx = 0.5 * (ix1 - ix1_)\n+    dy = 0.5 * (iy1 - iy1_)\n+    derivative = np.concatenate([dx, dy], axis=1)\n+    derivative = derivative.reshape(num_coords, num_keypoints, 2, 1)\n+    dxx = ix1 - 2 * i_ + ix1_\n+    dyy = iy1 - 2 * i_ + iy1_\n+    dxy = 0.5 * (ix1y1 - ix1 - iy1 + i_ + i_ - ix1_ - iy1_ + ix1_y1_)\n+    hessian = np.concatenate([dxx, dxy, dxy, dyy], axis=1)\n+    hessian = hessian.reshape(num_coords, num_keypoints, 2, 2)\n+    hessian = np.linalg.inv(hessian + np.finfo(np.float32).eps * np.eye(2))\n+    coords -= np.einsum(\"ijmn,ijnk->ijmk\", hessian, derivative).squeeze()\n+    return coords\n+\n+\n+def transform_preds(coords: np.ndarray, center: np.ndarray, scale: np.ndarray, output_size: np.ndarray) -> np.ndarray:\n+    \"\"\"Get final keypoint predictions from heatmaps and apply scaling and\n+    translation to map them back to the image.\n+\n+    Note:\n+        num_keypoints: K\n+\n+    Args:\n+        coords (`np.ndarray` of shape `(num_keypoints, ndims)`):\n+\n+            * If ndims=2, corrds are predicted keypoint location.\n+            * If ndims=4, corrds are composed of (x, y, scores, tags)\n+            * If ndims=5, corrds are composed of (x, y, scores, tags,\n+              flipped_tags)\n+\n+        center (`np.ndarray` of shape `(2,)`):\n+            Center of the bounding box (x, y).\n+        scale (`np.ndarray` of shape `(2,)`):\n+            Scale of the bounding box wrt original image of width and height.\n+        output_size (`np.ndarray` of shape `(2,)`):\n+            Size of the destination heatmaps in (height, width) format.\n+\n+    Returns:\n+        np.ndarray: Predicted coordinates in the images.\n+    \"\"\"\n+    if coords.shape[1] not in (2, 4, 5):\n+        raise ValueError(\"Coordinates need to have either 2, 4 or 5 dimensions.\")\n+    if len(center) != 2:\n+        raise ValueError(\"Center needs to have 2 elements, one for x and one for y.\")\n+    if len(scale) != 2:\n+        raise ValueError(\"Scale needs to consist of a width and height\")\n+    if len(output_size) != 2:\n+        raise ValueError(\"Output size needs to consist of a height and width\")\n+\n+    # Recover the scale which is normalized by a factor of 200.\n+    scale = scale * 200.0\n+\n+    # We use unbiased data processing\n+    scale_y = scale[1] / (output_size[0] - 1.0)\n+    scale_x = scale[0] / (output_size[1] - 1.0)\n+\n+    target_coords = np.ones_like(coords)\n+    target_coords[:, 0] = coords[:, 0] * scale_x + center[0] - scale[0] * 0.5\n+    target_coords[:, 1] = coords[:, 1] * scale_y + center[1] - scale[1] * 0.5\n+\n+    return target_coords\n+\n+\n+def get_warp_matrix(theta: float, size_input: np.ndarray, size_dst: np.ndarray, size_target: np.ndarray):\n+    \"\"\"\n+    Calculate the transformation matrix under the constraint of unbiased. Paper ref: Huang et al. The Devil is in the\n+    Details: Delving into Unbiased Data Processing for Human Pose Estimation (CVPR 2020).\n+\n+    Source: https://github.com/open-mmlab/mmpose/blob/master/mmpose/core/post_processing/post_transforms.py\n+\n+    Args:\n+        theta (`float`):\n+            Rotation angle in degrees.\n+        size_input (`np.ndarray`):\n+            Size of input image [width, height].\n+        size_dst (`np.ndarray`):\n+            Size of output image [width, height].\n+        size_target (`np.ndarray`):\n+            Size of ROI in input plane [w, h].\n+\n+    Returns:\n+        `np.ndarray`: A matrix for transformation.\n+    \"\"\"\n+    theta = np.deg2rad(theta)\n+    matrix = np.zeros((2, 3), dtype=np.float32)\n+    scale_x = size_dst[0] / size_target[0]\n+    scale_y = size_dst[1] / size_target[1]\n+    matrix[0, 0] = math.cos(theta) * scale_x\n+    matrix[0, 1] = -math.sin(theta) * scale_x\n+    matrix[0, 2] = scale_x * (\n+        -0.5 * size_input[0] * math.cos(theta) + 0.5 * size_input[1] * math.sin(theta) + 0.5 * size_target[0]\n+    )\n+    matrix[1, 0] = math.sin(theta) * scale_y\n+    matrix[1, 1] = math.cos(theta) * scale_y\n+    matrix[1, 2] = scale_y * (\n+        -0.5 * size_input[0] * math.sin(theta) - 0.5 * size_input[1] * math.cos(theta) + 0.5 * size_target[1]\n+    )\n+    return matrix\n+\n+\n+def scipy_warp_affine(src, M, size):\n+    \"\"\"\n+    This function implements cv2.warpAffine function using affine_transform in scipy. See https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.affine_transform.html and https://docs.opencv.org/4.x/d4/d61/tutorial_warp_affine.html for more details.\n+\n+    Note: the original implementation of cv2.warpAffine uses cv2.INTER_LINEAR.\n+    \"\"\"\n+    channels = [src[..., i] for i in range(src.shape[-1])]\n+\n+    # Convert to a 3x3 matrix used by SciPy\n+    M_scipy = np.vstack([M, [0, 0, 1]])\n+    # If you have a matrix for the ‘push’ transformation, use its inverse (numpy.linalg.inv) in this function.\n+    M_inv = inv(M_scipy)\n+    M_inv[0, 0], M_inv[0, 1], M_inv[1, 0], M_inv[1, 1], M_inv[0, 2], M_inv[1, 2] = (\n+        M_inv[1, 1],\n+        M_inv[1, 0],\n+        M_inv[0, 1],\n+        M_inv[0, 0],\n+        M_inv[1, 2],\n+        M_inv[0, 2],\n+    )\n+\n+    new_src = [affine_transform(channel, M_inv, output_shape=size, order=1) for channel in channels]\n+    new_src = np.stack(new_src, axis=-1)\n+    return new_src\n+\n+\n+class VitPoseImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a VitPose image processor.\n+\n+    Args:\n+        do_affine_transform (`bool`, *optional*, defaults to `True`):\n+            Whether to apply an affine transformation to the input images.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 256, \"width\": 192}`):\n+            Resolution of the image after `affine_transform` is applied. Only has an effect if `do_affine_transform` is set to `True`. Can\n+            be overriden by `size` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether or not to apply the scaling factor (to make pixel values floats between 0. and 1.).\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether or not to normalize the input with mean and standard deviation.\n+        image_mean (`List[int]`, defaults to `[0.485, 0.456, 0.406]`, *optional*):\n+            The sequence of means for each channel, to be used when normalizing images.\n+        image_std (`List[int]`, defaults to `[0.229, 0.224, 0.225]`, *optional*):\n+            The sequence of standard deviations for each channel, to be used when normalizing images.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_affine_transform: bool = True,\n+        size: Dict[str, int] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.do_affine_transform = do_affine_transform\n+        self.size = size if size is not None else {\"height\": 256, \"width\": 192}\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.normalize_factor = 200.0\n+\n+    def affine_transform(\n+        self,\n+        image: np.array,\n+        center: Tuple[float],\n+        scale: Tuple[float],\n+        rotation: float,\n+        size: Dict[str, int],\n+        data_format: Optional[ChannelDimension] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Apply an affine transformation to an image.\n+\n+        Args:\n+            image (`np.array`):\n+                Image to transform.\n+            center (`Tuple[float]`):\n+                Center of the bounding box (x, y).\n+            scale (`Tuple[float]`):\n+                Scale of the bounding box with respect to height/width.\n+            rotation (`float`):\n+                Rotation angle in degrees.\n+            size (`Dict[str, int]`):\n+                Size of the destination image.\n+            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format of the output image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the input image.\n+        \"\"\"\n+\n+        data_format = input_data_format if data_format is None else data_format\n+\n+        size = (size[\"width\"], size[\"height\"])\n+\n+        # one uses a pixel standard deviation of 200 pixels\n+        transformation = get_warp_matrix(rotation, center * 2.0, np.array(size) - 1.0, scale * 200.0)\n+\n+        # input image requires channels last format\n+        image = (\n+            image\n+            if input_data_format == ChannelDimension.LAST\n+            else to_channel_dimension_format(image, ChannelDimension.LAST, input_data_format)\n+        )\n+        image = scipy_warp_affine(src=image, M=transformation, size=(size[1], size[0]))\n+\n+        image = to_channel_dimension_format(image, data_format, ChannelDimension.LAST)\n+\n+        return image\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        boxes: Union[List[List[float]], np.ndarray],\n+        do_affine_transform: bool = None,\n+        size: Dict[str, int] = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+\n+            boxes (`List[List[List[float]]]` or `np.ndarray`):\n+                List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the bounding\n+                box coordinates in COCO format (top_left_x, top_left_y, width, height).\n+\n+            do_affine_transform (`bool`, *optional*, defaults to `self.do_affine_transform`):\n+                Whether to apply an affine transformation to the input images.\n+            size (`Dict[str, int]` *optional*, defaults to `self.size`):\n+                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the output image after\n+                resizing.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use if `do_normalize` is set to `True`.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to `'np'`):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **pixel_values** -- Pixel values to be fed to a model, of shape (batch_size, num_channels, height,\n+              width).\n+        \"\"\"\n+        do_affine_transform = do_affine_transform if do_affine_transform is not None else self.do_affine_transform\n+        size = size if size is not None else self.size\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+\n+        images = make_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        if isinstance(boxes, list) and len(images) != len(boxes):\n+            raise ValueError(f\"Batch of images and boxes mismatch : {len(images)} != {len(boxes)}\")\n+        elif isinstance(boxes, np.ndarray) and len(images) != boxes.shape[0]:\n+            raise ValueError(f\"Batch of images and boxes mismatch : {len(images)} != {boxes.shape[0]}\")\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        # transformations (affine transformation + rescaling + normalization)\n+        if self.do_affine_transform:\n+            new_images = []\n+            for image, image_boxes in zip(images, boxes):\n+                for box in image_boxes:\n+                    center, scale = box_to_center_and_scale(\n+                        box,\n+                        image_width=size[\"width\"],\n+                        image_height=size[\"height\"],\n+                        normalize_factor=self.normalize_factor,\n+                    )\n+                    transformed_image = self.affine_transform(\n+                        image, center, scale, rotation=0, size=size, input_data_format=input_data_format\n+                    )\n+                    new_images.append(transformed_image)\n+            images = new_images\n+\n+        # For batch processing, the number of boxes must be consistent across all images in the batch.\n+        # When using a list input, the number of boxes can vary dynamically per image.\n+        # The image processor creates pixel_values of shape (batch_size*num_persons, num_channels, height, width)\n+\n+        all_images = []\n+        for image in images:\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n+        ]\n+\n+        data = {\"pixel_values\": images}\n+        encoded_inputs = BatchFeature(data=data, tensor_type=return_tensors)\n+\n+        return encoded_inputs\n+\n+    def keypoints_from_heatmaps(\n+        self,\n+        heatmaps: np.ndarray,\n+        center: np.ndarray,\n+        scale: np.ndarray,\n+        kernel: int = 11,\n+    ):\n+        \"\"\"\n+        Get final keypoint predictions from heatmaps and transform them back to\n+        the image.\n+\n+        Args:\n+            heatmaps (`np.ndarray` of shape `(batch_size, num_keypoints, height, width])`):\n+                Model predicted heatmaps.\n+            center (`np.ndarray` of shape `(batch_size, 2)`):\n+                Center of the bounding box (x, y).\n+            scale (`np.ndarray` of shape `(batch_size, 2)`):\n+                Scale of the bounding box wrt original images of width and height.\n+            kernel (int, *optional*, defaults to 11):\n+                Gaussian kernel size (K) for modulation, which should match the heatmap gaussian sigma when training.\n+                K=17 for sigma=3 and k=11 for sigma=2.\n+\n+        Returns:\n+            tuple: A tuple containing keypoint predictions and scores.\n+\n+            - preds (`np.ndarray` of shape `(batch_size, num_keypoints, 2)`):\n+                Predicted keypoint location in images.\n+            - scores (`np.ndarray` of shape `(batch_size, num_keypoints, 1)`):\n+                Scores (confidence) of the keypoints.\n+        \"\"\"\n+        batch_size, _, height, width = heatmaps.shape\n+\n+        coords, scores = get_keypoint_predictions(heatmaps)\n+\n+        preds = post_dark_unbiased_data_processing(coords, heatmaps, kernel=kernel)\n+\n+        # Transform back to the image\n+        for i in range(batch_size):\n+            preds[i] = transform_preds(preds[i], center=center[i], scale=scale[i], output_size=[height, width])\n+\n+        return preds, scores\n+\n+    def post_process_pose_estimation(\n+        self,\n+        outputs: \"VitPoseEstimatorOutput\",\n+        boxes: Union[List[List[List[float]]], np.ndarray],\n+        kernel_size: int = 11,\n+        threshold: float = None,\n+        target_sizes: Union[TensorType, List[Tuple]] = None,\n+    ):\n+        \"\"\"\n+        Transform the heatmaps into keypoint predictions and transform them back to the image.\n+\n+        Args:\n+            outputs (`VitPoseEstimatorOutput`):\n+                VitPoseForPoseEstimation model outputs.\n+            boxes (`List[List[List[float]]]` or `np.ndarray`):\n+                List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the bounding\n+                box coordinates in COCO format (top_left_x, top_left_y, width, height).\n+            kernel_size (`int`, *optional*, defaults to 11):\n+                Gaussian kernel size (K) for modulation.\n+            threshold (`float`, *optional*, defaults to None):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will be resize with the default value.\n+        Returns:\n+            `List[List[Dict]]`: A list of dictionaries, each dictionary containing the keypoints and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+\n+        # First compute centers and scales for each bounding box\n+        batch_size, num_keypoints, _, _ = outputs.heatmaps.shape\n+\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        centers = np.zeros((batch_size, 2), dtype=np.float32)\n+        scales = np.zeros((batch_size, 2), dtype=np.float32)\n+        flattened_boxes = list(itertools.chain(*boxes))\n+        for i in range(batch_size):\n+            if target_sizes is not None:\n+                image_width, image_height = target_sizes[i][0], target_sizes[i][1]\n+                scale_factor = np.array([image_width, image_height, image_width, image_height])\n+                flattened_boxes[i] = flattened_boxes[i] * scale_factor\n+            width, height = self.size[\"width\"], self.size[\"height\"]\n+            center, scale = box_to_center_and_scale(flattened_boxes[i], image_width=width, image_height=height)\n+            centers[i, :] = center\n+            scales[i, :] = scale\n+\n+        preds, scores = self.keypoints_from_heatmaps(\n+            outputs.heatmaps.cpu().numpy(), centers, scales, kernel=kernel_size\n+        )\n+\n+        all_boxes = np.zeros((batch_size, 4), dtype=np.float32)\n+        all_boxes[:, 0:2] = centers[:, 0:2]\n+        all_boxes[:, 2:4] = scales[:, 0:2]\n+\n+        poses = torch.tensor(preds)\n+        scores = torch.tensor(scores)\n+        labels = torch.arange(0, num_keypoints)\n+        bboxes_xyxy = torch.tensor(coco_to_pascal_voc(all_boxes))\n+\n+        results: List[List[Dict[str, torch.Tensor]]] = []\n+\n+        pose_bbox_pairs = zip(poses, scores, bboxes_xyxy)\n+\n+        for image_bboxes in boxes:\n+            image_results: List[Dict[str, torch.Tensor]] = []\n+            for _ in image_bboxes:\n+                # Unpack the next pose and bbox_xyxy from the iterator\n+                pose, score, bbox_xyxy = next(pose_bbox_pairs)\n+                score = score.squeeze()\n+                keypoints_labels = labels\n+                if threshold is not None:\n+                    keep = score > threshold\n+                    pose = pose[keep]\n+                    score = score[keep]\n+                    keypoints_labels = keypoints_labels[keep]\n+                pose_result = {\"keypoints\": pose, \"scores\": score, \"labels\": keypoints_labels, \"bbox\": bbox_xyxy}\n+                image_results.append(pose_result)\n+            results.append(image_results)\n+\n+        return results\n+\n+\n+__all__ = [\"VitPoseImageProcessor\"]"
        },
        {
            "sha": "b5dd274654ac1b08f9ec70ea85c95d9b98667a8e",
            "filename": "src/transformers/models/vitpose/modeling_vitpose.py",
            "status": "added",
            "additions": 340,
            "deletions": 0,
            "changes": 340,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,340 @@\n+# coding=utf-8\n+# Copyright 2024 University of Sydney and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch VitPose model.\"\"\"\n+\n+from dataclasses import dataclass\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.backbone_utils import load_backbone\n+from .configuration_vitpose import VitPoseConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"VitPoseConfig\"\n+\n+\n+@dataclass\n+class VitPoseEstimatorOutput(ModelOutput):\n+    \"\"\"\n+    Class for outputs of pose estimation models.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Loss is not supported at this moment. See https://github.com/ViTAE-Transformer/ViTPose/tree/main/mmpose/models/losses for further detail.\n+        heatmaps (`torch.FloatTensor` of shape `(batch_size, num_keypoints, height, width)`):\n+            Heatmaps as predicted by the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each stage) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states\n+            (also called feature maps) of the model at the output of each stage.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, patch_size,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    heatmaps: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class VitPosePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = VitPoseConfig\n+    base_model_prefix = \"vit\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+VITPOSE_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`VitPoseConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+VITPOSE_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`VitPoseImageProcessor`]. See\n+            [`VitPoseImageProcessor.__call__`] for details.\n+\n+        dataset_index (`torch.Tensor` of shape `(batch_size,)`):\n+            Index to use in the Mixture-of-Experts (MoE) blocks of the backbone.\n+\n+            This corresponds to the dataset index used during training, e.g. For the single dataset index 0 refers to the corresponding dataset. For the multiple datasets index 0 refers to dataset A (e.g. MPII) and index 1 refers to dataset B (e.g. CrowdPose).\n+\n+        flip_pairs (`torch.tensor`, *optional*):\n+            Whether to mirror pairs of keypoints (for example, left ear -- right ear).\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+def flip_back(output_flipped, flip_pairs, target_type=\"gaussian-heatmap\"):\n+    \"\"\"Flip the flipped heatmaps back to the original form.\n+\n+    Args:\n+        output_flipped (`torch.tensor` of shape `(batch_size, num_keypoints, height, width)`):\n+            The output heatmaps obtained from the flipped images.\n+        flip_pairs (`torch.Tensor` of shape `(num_keypoints, 2)`):\n+            Pairs of keypoints which are mirrored (for example, left ear -- right ear).\n+        target_type (`str`, *optional*, defaults to `\"gaussian-heatmap\"`):\n+            Target type to use. Can be gaussian-heatmap or combined-target.\n+            gaussian-heatmap: Classification target with gaussian distribution.\n+            combined-target: The combination of classification target (response map) and regression target (offset map).\n+            Paper ref: Huang et al. The Devil is in the Details: Delving into Unbiased Data Processing for Human Pose Estimation (CVPR 2020).\n+\n+    Returns:\n+        torch.Tensor: heatmaps that flipped back to the original image\n+    \"\"\"\n+    if target_type not in [\"gaussian-heatmap\", \"combined-target\"]:\n+        raise ValueError(\"target_type should be gaussian-heatmap or combined-target\")\n+\n+    if output_flipped.ndim != 4:\n+        raise ValueError(\"output_flipped should be [batch_size, num_keypoints, height, width]\")\n+    batch_size, num_keypoints, height, width = output_flipped.shape\n+    channels = 1\n+    if target_type == \"combined-target\":\n+        channels = 3\n+        output_flipped[:, 1::3, ...] = -output_flipped[:, 1::3, ...]\n+    output_flipped = output_flipped.reshape(batch_size, -1, channels, height, width)\n+    output_flipped_back = output_flipped.clone()\n+\n+    # Swap left-right parts\n+    for left, right in flip_pairs.tolist():\n+        output_flipped_back[:, left, ...] = output_flipped[:, right, ...]\n+        output_flipped_back[:, right, ...] = output_flipped[:, left, ...]\n+    output_flipped_back = output_flipped_back.reshape((batch_size, num_keypoints, height, width))\n+    # Flip horizontally\n+    output_flipped_back = output_flipped_back.flip(-1)\n+    return output_flipped_back\n+\n+\n+class VitPoseSimpleDecoder(nn.Module):\n+    \"\"\"\n+    Simple decoding head consisting of a ReLU activation, 4x upsampling and a 3x3 convolution, turning the\n+    feature maps into heatmaps.\n+    \"\"\"\n+\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+\n+        self.activation = nn.ReLU()\n+        self.upsampling = nn.Upsample(scale_factor=config.scale_factor, mode=\"bilinear\", align_corners=False)\n+        self.conv = nn.Conv2d(\n+            config.backbone_config.hidden_size, config.num_labels, kernel_size=3, stride=1, padding=1\n+        )\n+\n+    def forward(self, hidden_state: torch.Tensor, flip_pairs: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        # Transform input: ReLU + upsample\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.upsampling(hidden_state)\n+        heatmaps = self.conv(hidden_state)\n+\n+        if flip_pairs is not None:\n+            heatmaps = flip_back(heatmaps, flip_pairs)\n+\n+        return heatmaps\n+\n+\n+class VitPoseClassicDecoder(nn.Module):\n+    \"\"\"\n+    Classic decoding head consisting of a 2 deconvolutional blocks, followed by a 1x1 convolution layer,\n+    turning the feature maps into heatmaps.\n+    \"\"\"\n+\n+    def __init__(self, config: VitPoseConfig):\n+        super().__init__()\n+\n+        self.deconv1 = nn.ConvTranspose2d(\n+            config.backbone_config.hidden_size, 256, kernel_size=4, stride=2, padding=1, bias=False\n+        )\n+        self.batchnorm1 = nn.BatchNorm2d(256)\n+        self.relu1 = nn.ReLU()\n+\n+        self.deconv2 = nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1, bias=False)\n+        self.batchnorm2 = nn.BatchNorm2d(256)\n+        self.relu2 = nn.ReLU()\n+\n+        self.conv = nn.Conv2d(256, config.num_labels, kernel_size=1, stride=1, padding=0)\n+\n+    def forward(self, hidden_state: torch.Tensor, flip_pairs: Optional[torch.Tensor] = None):\n+        hidden_state = self.deconv1(hidden_state)\n+        hidden_state = self.batchnorm1(hidden_state)\n+        hidden_state = self.relu1(hidden_state)\n+\n+        hidden_state = self.deconv2(hidden_state)\n+        hidden_state = self.batchnorm2(hidden_state)\n+        hidden_state = self.relu2(hidden_state)\n+\n+        heatmaps = self.conv(hidden_state)\n+\n+        if flip_pairs is not None:\n+            heatmaps = flip_back(heatmaps, flip_pairs)\n+\n+        return heatmaps\n+\n+\n+@add_start_docstrings(\n+    \"The VitPose model with a pose estimation head on top.\",\n+    VITPOSE_START_DOCSTRING,\n+)\n+class VitPoseForPoseEstimation(VitPosePreTrainedModel):\n+    def __init__(self, config: VitPoseConfig) -> None:\n+        super().__init__(config)\n+\n+        self.backbone = load_backbone(config)\n+\n+        # add backbone attributes\n+        if not hasattr(self.backbone.config, \"hidden_size\"):\n+            raise ValueError(\"The backbone should have a hidden_size attribute\")\n+        if not hasattr(self.backbone.config, \"image_size\"):\n+            raise ValueError(\"The backbone should have an image_size attribute\")\n+        if not hasattr(self.backbone.config, \"patch_size\"):\n+            raise ValueError(\"The backbone should have a patch_size attribute\")\n+\n+        self.head = VitPoseSimpleDecoder(config) if config.use_simple_decoder else VitPoseClassicDecoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(VITPOSE_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=VitPoseEstimatorOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        dataset_index: Optional[torch.Tensor] = None,\n+        flip_pairs: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[tuple, VitPoseEstimatorOutput]:\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, VitPoseForPoseEstimation\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n+        >>> model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> boxes = [[[412.8, 157.61, 53.05, 138.01], [384.43, 172.21, 15.12, 35.74]]]\n+        >>> inputs = processor(image, boxes=boxes, return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+        >>> heatmaps = outputs.heatmaps\n+        ```\"\"\"\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        loss = None\n+        if labels is not None:\n+            raise NotImplementedError(\"Training is not yet supported\")\n+\n+        outputs = self.backbone.forward_with_filtered_kwargs(\n+            pixel_values,\n+            dataset_index=dataset_index,\n+            output_hidden_states=output_hidden_states,\n+            output_attentions=output_attentions,\n+            return_dict=return_dict,\n+        )\n+\n+        # Turn output hidden states in tensor of shape (batch_size, num_channels, height, width)\n+        sequence_output = outputs.feature_maps[-1] if return_dict else outputs[0][-1]\n+        batch_size = sequence_output.shape[0]\n+        patch_height = self.config.backbone_config.image_size[0] // self.config.backbone_config.patch_size[0]\n+        patch_width = self.config.backbone_config.image_size[1] // self.config.backbone_config.patch_size[1]\n+        sequence_output = (\n+            sequence_output.permute(0, 2, 1).reshape(batch_size, -1, patch_height, patch_width).contiguous()\n+        )\n+\n+        heatmaps = self.head(sequence_output, flip_pairs=flip_pairs)\n+\n+        if not return_dict:\n+            if output_hidden_states:\n+                output = (heatmaps,) + outputs[1:]\n+            else:\n+                output = (heatmaps,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return VitPoseEstimatorOutput(\n+            loss=loss,\n+            heatmaps=heatmaps,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"VitPosePreTrainedModel\", \"VitPoseForPoseEstimation\"]"
        },
        {
            "sha": "f69ff8762af0e322d7038e00b892a1dbb0914e45",
            "filename": "src/transformers/models/vitpose_backbone/__init__.py",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2F__init__.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,54 @@\n+# flake8: noqa\n+# There's no way to ignore \"F401 '...' imported but unused\" warnings in this\n+# module, but to preserve other warnings. So, don't check this module at all.\n+\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n+\n+\n+_import_structure = {\"configuration_vitpose_backbone\": [\"VitPoseBackboneConfig\"]}\n+\n+\n+try:\n+    if not is_torch_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"modeling_vitpose_backbone\"] = [\n+        \"VitPoseBackbonePreTrainedModel\",\n+        \"VitPoseBackbone\",\n+    ]\n+\n+if TYPE_CHECKING:\n+    from .configuration_vitpose_backbone import VitPoseBackboneConfig\n+\n+    try:\n+        if not is_torch_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .modeling_vitpose_backbone import (\n+            VitPoseBackbone,\n+            VitPoseBackbonePreTrainedModel,\n+        )\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)"
        },
        {
            "sha": "2872d39a2a30df940307978b4c258a2c6cdfb811",
            "filename": "src/transformers/models/vitpose_backbone/configuration_vitpose_backbone.py",
            "status": "added",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fconfiguration_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fconfiguration_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fconfiguration_vitpose_backbone.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,136 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"VitPose backbone configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VitPoseBackboneConfig(BackboneConfigMixin, PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`VitPoseBackbone`]. It is used to instantiate a\n+    VitPose model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the VitPose\n+    [usyd-community/vitpose-base-simple](https://huggingface.co/usyd-community/vitpose-base-simple) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        image_size (`int`, *optional*, defaults to `[256, 192]`):\n+            The size (resolution) of each image.\n+        patch_size (`List[int]`, *optional*, defaults to `[16, 16]`):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            The ratio of the hidden size in the feedforward network to the hidden size in the attention layers.\n+        num_experts (`int`, *optional*, defaults to 1):\n+            The number of experts in the MoE layer.\n+        part_features (`int`, *optional*):\n+            The number of part features to output. Only used in case `num_experts` is greater than 1.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_probs_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-12):\n+            The epsilon used by the layer normalization layers.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n+        out_features (`List[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`List[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import VitPoseBackboneConfig, VitPoseBackbone\n+\n+    >>> # Initializing a VitPose configuration\n+    >>> configuration = VitPoseBackboneConfig()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = VitPoseBackbone(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"vitpose_backbone\"\n+\n+    def __init__(\n+        self,\n+        image_size=[256, 192],\n+        patch_size=[16, 16],\n+        num_channels=3,\n+        hidden_size=768,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        mlp_ratio=4,\n+        num_experts=1,\n+        part_features=256,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        attention_probs_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-12,\n+        qkv_bias=True,\n+        out_features=None,\n+        out_indices=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.mlp_ratio = mlp_ratio\n+        self.num_experts = num_experts\n+        self.part_features = part_features\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.qkv_bias = qkv_bias\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, num_hidden_layers + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )"
        },
        {
            "sha": "d3f057988370f06ded97df5c4555276ca06ea7d8",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "added",
            "additions": 542,
            "deletions": 0,
            "changes": 542,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,542 @@\n+# coding=utf-8\n+# Copyright 2024 University of Sydney and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch VitPose backbone model.\n+\n+This code is the same as the original Vision Transformer (ViT) with 2 modifications:\n+- use of padding=2 in the patch embedding layer\n+- addition of a mixture-of-experts MLP layer\n+\"\"\"\n+\n+import collections.abc\n+import math\n+from typing import Optional, Set, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BackboneOutput, BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ...utils.backbone_utils import BackboneMixin\n+from .configuration_vitpose_backbone import VitPoseBackboneConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"VitPoseBackboneConfig\"\n+\n+\n+class VitPoseBackbonePatchEmbeddings(nn.Module):\n+    \"\"\"Image to Patch Embedding.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        image_size = config.image_size\n+        patch_size = config.patch_size\n+        num_channels = config.num_channels\n+        embed_dim = config.hidden_size\n+\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, embed_dim, kernel_size=patch_size, stride=patch_size, padding=2)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        height, width = pixel_values.shape[-2:]\n+        if height != self.image_size[0] or width != self.image_size[1]:\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n+            )\n+        embeddings = self.projection(pixel_values)\n+\n+        embeddings = embeddings.flatten(2).transpose(1, 2)\n+        return embeddings\n+\n+\n+class VitPoseBackboneEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+\n+        self.patch_embeddings = VitPoseBackbonePatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        embeddings = self.patch_embeddings(pixel_values)\n+\n+        # add positional encoding to each token\n+        embeddings = embeddings + self.position_embeddings[:, 1:] + self.position_embeddings[:, :1]\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+# Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->VitPoseBackbone\n+class VitPoseBackboneSelfAttention(nn.Module):\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n+                f\"heads {config.num_attention_heads}.\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n+\n+\n+# Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->VitPoseBackbone\n+class VitPoseBackboneSelfOutput(nn.Module):\n+    \"\"\"\n+    The residual connection is defined in VitPoseBackboneLayer instead of here (as is the case with other models), due to the\n+    layernorm applied before each block.\n+    \"\"\"\n+\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+\n+        return hidden_states\n+\n+\n+# Copied from transformers.models.vit.modeling_vit.ViTAttention with ViT->VitPoseBackbone\n+class VitPoseBackboneAttention(nn.Module):\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+        self.attention = VitPoseBackboneSelfAttention(config)\n+        self.output = VitPoseBackboneSelfOutput(config)\n+        self.pruned_heads = set()\n+\n+    def prune_heads(self, heads: Set[int]) -> None:\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.attention.query = prune_linear_layer(self.attention.query, index)\n+        self.attention.key = prune_linear_layer(self.attention.key, index)\n+        self.attention.value = prune_linear_layer(self.attention.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)\n+        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        self_outputs = self.attention(hidden_states, head_mask, output_attentions)\n+\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+class VitPoseBackboneMoeMLP(nn.Module):\n+    def __init__(self, config: VitPoseBackboneConfig):\n+        super().__init__()\n+\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+\n+        num_experts = config.num_experts\n+        part_features = config.part_features\n+\n+        self.part_features = part_features\n+        self.fc1 = nn.Linear(in_features, hidden_features)\n+        self.act = ACT2FN[config.hidden_act]\n+        self.fc2 = nn.Linear(hidden_features, out_features - part_features)\n+        self.drop = nn.Dropout(config.hidden_dropout_prob)\n+\n+        self.num_experts = num_experts\n+        experts = [nn.Linear(hidden_features, part_features) for _ in range(num_experts)]\n+        self.experts = nn.ModuleList(experts)\n+\n+    def forward(self, hidden_state: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n+        expert_hidden_state = torch.zeros_like(hidden_state[:, :, -self.part_features :])\n+\n+        hidden_state = self.fc1(hidden_state)\n+        hidden_state = self.act(hidden_state)\n+        shared_hidden_state = self.fc2(hidden_state)\n+        indices = indices.view(-1, 1, 1)\n+\n+        # to support ddp training\n+        for i in range(self.num_experts):\n+            selected_index = indices == i\n+            current_hidden_state = self.experts[i](hidden_state) * selected_index\n+            expert_hidden_state = expert_hidden_state + current_hidden_state\n+\n+        hidden_state = torch.cat([shared_hidden_state, expert_hidden_state], dim=-1)\n+\n+        return hidden_state\n+\n+\n+class VitPoseBackboneMLP(nn.Module):\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+        self.fc1 = nn.Linear(in_features, hidden_features, bias=True)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.fc2 = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.fc1(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.fc2(hidden_state)\n+        return hidden_state\n+\n+\n+class VitPoseBackboneLayer(nn.Module):\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+        self.num_experts = config.num_experts\n+        self.attention = VitPoseBackboneAttention(config)\n+        self.mlp = VitPoseBackboneMLP(config) if self.num_experts == 1 else VitPoseBackboneMoeMLP(config)\n+        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        dataset_index: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n+        # Validate dataset_index when using multiple experts\n+        if self.num_experts > 1 and dataset_index is None:\n+            raise ValueError(\n+                \"dataset_index must be provided when using multiple experts \"\n+                f\"(num_experts={self.num_experts}). Please provide dataset_index \"\n+                \"to the forward pass.\"\n+            )\n+        self_attention_outputs = self.attention(\n+            self.layernorm_before(hidden_states),  # in VitPoseBackbone, layernorm is applied before self-attention\n+            head_mask,\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self_attention_outputs[0]\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        # first residual connection\n+        hidden_states = attention_output + hidden_states\n+\n+        layer_output = self.layernorm_after(hidden_states)\n+        if self.num_experts == 1:\n+            layer_output = self.mlp(layer_output)\n+        else:\n+            layer_output = self.mlp(layer_output, indices=dataset_index)\n+\n+        # second residual connection\n+        layer_output = layer_output + hidden_states\n+\n+        outputs = (layer_output,) + outputs\n+\n+        return outputs\n+\n+\n+# Copied from transformers.models.vit.modeling_vit.ViTEncoder with ViT->VitPoseBackbone\n+class VitPoseBackboneEncoder(nn.Module):\n+    def __init__(self, config: VitPoseBackboneConfig) -> None:\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([VitPoseBackboneLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        dataset_index: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+        return_dict: bool = True,\n+    ) -> Union[tuple, BaseModelOutput]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                    dataset_index,\n+                    layer_head_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, dataset_index, layer_head_mask, output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = VitPoseBackboneConfig\n+    base_model_prefix = \"vit\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n+\n+    def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+            # `trunc_normal_cpu` not implemented in `half` issues\n+            module.weight.data = nn.init.trunc_normal_(\n+                module.weight.data.to(torch.float32), mean=0.0, std=self.config.initializer_range\n+            ).to(module.weight.dtype)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, VitPoseBackboneEmbeddings):\n+            module.position_embeddings.data = nn.init.trunc_normal_(\n+                module.position_embeddings.data.to(torch.float32),\n+                mean=0.0,\n+                std=self.config.initializer_range,\n+            ).to(module.position_embeddings.dtype)\n+\n+\n+VITPOSE_BACKBONE_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`VitPoseBackboneConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+VITPOSE_BACKBONE_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values.\n+\n+        dataset_index (`torch.Tensor` of shape `(batch_size,)`):\n+            Index to use in the Mixture-of-Experts (MoE) blocks of the backbone.\n+\n+            This corresponds to the dataset index used during training, e.g. index 0 refers to COCO.\n+\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The VitPose backbone useful for downstream tasks.\",\n+    VITPOSE_BACKBONE_START_DOCSTRING,\n+)\n+class VitPoseBackbone(VitPoseBackbonePreTrainedModel, BackboneMixin):\n+    def __init__(self, config: VitPoseBackboneConfig):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.num_features = [config.hidden_size for _ in range(config.num_hidden_layers + 1)]\n+        self.embeddings = VitPoseBackboneEmbeddings(config)\n+        self.encoder = VitPoseBackboneEncoder(config)\n+\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(VITPOSE_BACKBONE_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        dataset_index: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ):\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import VitPoseBackboneConfig, VitPoseBackbone\n+        >>> import torch\n+\n+        >>> config = VitPoseBackboneConfig(out_indices=[-1])\n+        >>> model = VitPoseBackbone(config)\n+\n+        >>> pixel_values = torch.randn(1, 3, 256, 192)\n+        >>> dataset_index = torch.tensor([1])\n+        >>> outputs = model(pixel_values, dataset_index)\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        embedding_output = self.embeddings(pixel_values)\n+\n+        outputs = self.encoder(\n+            embedding_output,\n+            dataset_index=dataset_index,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=True,\n+            return_dict=return_dict,\n+        )\n+        hidden_states = outputs.hidden_states if return_dict else outputs[1]\n+\n+        feature_maps = ()\n+        for stage, hidden_state in zip(self.stage_names, hidden_states):\n+            if stage in self.out_features:\n+                hidden_state = self.layernorm(hidden_state)\n+                feature_maps += (hidden_state,)\n+\n+        if not return_dict:\n+            if output_hidden_states:\n+                output = (feature_maps,) + outputs[1:]\n+            else:\n+                output = (feature_maps,) + outputs[2:]\n+            return output\n+\n+        return BackboneOutput(\n+            feature_maps=feature_maps,\n+            hidden_states=outputs.hidden_states if output_hidden_states else None,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "76df70f19203d0956a96393b538e5cd72a590dff",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -9753,6 +9753,34 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class VitPoseForPoseEstimation(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class VitPosePreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class VitPoseBackbone(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class VitPoseBackbonePreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class VitsModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e9e87a4b3d44b7070987be5c10fb610137617438",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -695,6 +695,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class VitPoseImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class VivitImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/vitpose/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2F__init__.py?ref=8490d3159ce37087151e30211e33f59008a44233"
        },
        {
            "sha": "5edf27e6a69af78c74d169fdc86d337e0642bb4b",
            "filename": "tests/models/vitpose/test_image_processing_vitpose.py",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,229 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import VitPoseImageProcessor\n+\n+\n+class VitPoseImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_affine_transform=True,\n+        size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+    ):\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_affine_transform = do_affine_transform\n+        self.size = size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_affine_transform\": self.do_affine_transform,\n+            \"size\": self.size,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class VitPoseImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = VitPoseImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = VitPoseImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_affine_transform\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n+\n+        image_processor = self.image_processing_class.from_dict(\n+            self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+        )\n+        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    def test_call_pil(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PIL images\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, Image.Image)\n+\n+        # Test not batched input\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+        encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+        # Test batched\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+        encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(\n+            tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n+        )\n+\n+    def test_call_numpy(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, np.ndarray)\n+\n+        # Test not batched input\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+        encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+        # Test batched\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+        encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(\n+            tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n+        )\n+\n+    def test_call_pytorch(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+        for image in image_inputs:\n+            self.assertIsInstance(image, torch.Tensor)\n+\n+        # Test not batched input\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+        encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+        # Test batched\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+        encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(\n+            tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n+        )\n+\n+    def test_call_numpy_4_channels(self):\n+        # Test that can process images which have an arbitrary number of channels\n+        # Initialize image_processing\n+        image_processor = self.image_processing_class(**self.image_processor_dict)\n+\n+        # create random numpy tensors\n+        self.image_processor_tester.num_channels = 4\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+        # Test not batched input\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+        encoded_images = image_processor(\n+            image_inputs[0],\n+            boxes=boxes,\n+            return_tensors=\"pt\",\n+            input_data_format=\"channels_last\",\n+            image_mean=0,\n+            image_std=1,\n+        ).pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+        self.assertEqual(tuple(encoded_images.shape), (len(boxes[0]), *expected_output_image_shape))\n+\n+        # Test batched\n+        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+        encoded_images = image_processor(\n+            image_inputs,\n+            boxes=boxes,\n+            return_tensors=\"pt\",\n+            input_data_format=\"channels_last\",\n+            image_mean=0,\n+            image_std=1,\n+        ).pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+        self.assertEqual(\n+            tuple(encoded_images.shape),\n+            (self.image_processor_tester.batch_size * len(boxes[0]), *expected_output_image_shape),\n+        )"
        },
        {
            "sha": "1c33b6cf367131ae0a3e2df7503f8a0167380097",
            "filename": "tests/models/vitpose/test_modeling_vitpose.py",
            "status": "added",
            "additions": 332,
            "deletions": 0,
            "changes": 332,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,332 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch VitPose model.\"\"\"\n+\n+import inspect\n+import unittest\n+\n+import requests\n+\n+from transformers import VitPoseBackboneConfig, VitPoseConfig\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import VitPoseForPoseEstimation\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import VitPoseImageProcessor\n+\n+\n+class VitPoseModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=[16 * 8, 12 * 8],\n+        patch_size=[8, 8],\n+        num_channels=3,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=5,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        type_sequence_label_size=10,\n+        initializer_range=0.02,\n+        num_labels=2,\n+        scale_factor=4,\n+        out_indices=[-1],\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.scale_factor = scale_factor\n+        self.out_indices = out_indices\n+        self.scope = scope\n+\n+        # in VitPose, the seq length equals the number of patches\n+        num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])\n+        self.seq_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size[0], self.image_size[1]])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return VitPoseConfig(\n+            backbone_config=self.get_backbone_config(),\n+        )\n+\n+    def get_backbone_config(self):\n+        return VitPoseBackboneConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            num_hidden_layers=self.num_hidden_layers,\n+            hidden_size=self.hidden_size,\n+            intermediate_size=self.intermediate_size,\n+            num_attention_heads=self.num_attention_heads,\n+            hidden_act=self.hidden_act,\n+            out_indices=self.out_indices,\n+        )\n+\n+    def create_and_check_for_pose_estimation(self, config, pixel_values, labels):\n+        model = VitPoseForPoseEstimation(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        expected_height = (self.image_size[0] // self.patch_size[0]) * self.scale_factor\n+        expected_width = (self.image_size[1] // self.patch_size[1]) * self.scale_factor\n+\n+        self.parent.assertEqual(\n+            result.heatmaps.shape, (self.batch_size, self.num_labels, expected_height, expected_width)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values,\n+            labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class VitPoseModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as VitPose does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (VitPoseForPoseEstimation,) if is_torch_available() else ()\n+    fx_compatible = False\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = VitPoseModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=VitPoseConfig, has_text_modality=False, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.create_and_test_config_to_json_string()\n+        self.config_tester.create_and_test_config_to_json_file()\n+        self.config_tester.create_and_test_config_from_and_save_pretrained()\n+        self.config_tester.create_and_test_config_with_num_labels()\n+        self.config_tester.check_config_can_be_init_without_params()\n+        self.config_tester.check_config_arguments_init()\n+\n+    @unittest.skip(reason=\"VitPose does not support input and output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPose does not support input and output embeddings\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPose does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPose does not support training yet\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPose does not support training yet\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPose does not support training yet\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPose does not support training yet\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_for_pose_estimation(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_pose_estimation(*config_and_inputs)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"usyd-community/vitpose-base-simple\"\n+        model = VitPoseForPoseEstimation.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+\n+# We will verify our results on an image of people in house\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000000139.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw)\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class VitPoseModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return (\n+            VitPoseImageProcessor.from_pretrained(\"usyd-community/vitpose-base-simple\")\n+            if is_vision_available()\n+            else None\n+        )\n+\n+    @slow\n+    def test_inference_pose_estimation(self):\n+        image_processor = self.default_image_processor\n+        model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\")\n+        model.to(torch_device)\n+        model.eval()\n+\n+        image = prepare_img()\n+        boxes = [[[412.8, 157.61, 53.05, 138.01], [384.43, 172.21, 15.12, 35.74]]]\n+\n+        inputs = image_processor(images=image, boxes=boxes, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        heatmaps = outputs.heatmaps\n+\n+        assert heatmaps.shape == (2, 17, 64, 48)\n+\n+        expected_slice = torch.tensor(\n+            [\n+                [9.9330e-06, 9.9330e-06, 9.9330e-06],\n+                [9.9330e-06, 9.9330e-06, 9.9330e-06],\n+                [9.9330e-06, 9.9330e-06, 9.9330e-06],\n+            ]\n+        ).to(torch_device)\n+\n+        assert torch.allclose(heatmaps[0, 0, :3, :3], expected_slice, atol=1e-4)\n+\n+        pose_results = image_processor.post_process_pose_estimation(outputs, boxes=boxes)[0]\n+\n+        expected_bbox = torch.tensor([391.9900, 190.0800, 391.1575, 189.3034])\n+        expected_keypoints = torch.tensor(\n+            [\n+                [3.9813e02, 1.8184e02],\n+                [3.9828e02, 1.7981e02],\n+                [3.9596e02, 1.7948e02],\n+            ]\n+        )\n+        expected_scores = torch.tensor([8.7529e-01, 8.4315e-01, 9.2678e-01])\n+\n+        self.assertEqual(len(pose_results), 2)\n+        self.assertTrue(torch.allclose(pose_results[1][\"bbox\"].cpu(), expected_bbox, atol=1e-4))\n+        self.assertTrue(torch.allclose(pose_results[1][\"keypoints\"][:3].cpu(), expected_keypoints, atol=1e-2))\n+        self.assertTrue(torch.allclose(pose_results[1][\"scores\"][:3].cpu(), expected_scores, atol=1e-4))\n+\n+    @slow\n+    def test_batched_inference(self):\n+        image_processor = self.default_image_processor\n+        model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-base-simple\")\n+        model.to(torch_device)\n+        model.eval()\n+\n+        image = prepare_img()\n+        boxes = [\n+            [[412.8, 157.61, 53.05, 138.01], [384.43, 172.21, 15.12, 35.74]],\n+            [[412.8, 157.61, 53.05, 138.01], [384.43, 172.21, 15.12, 35.74]],\n+        ]\n+\n+        inputs = image_processor(images=[image, image], boxes=boxes, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        heatmaps = outputs.heatmaps\n+\n+        assert heatmaps.shape == (4, 17, 64, 48)\n+\n+        expected_slice = torch.tensor(\n+            [\n+                [9.9330e-06, 9.9330e-06, 9.9330e-06],\n+                [9.9330e-06, 9.9330e-06, 9.9330e-06],\n+                [9.9330e-06, 9.9330e-06, 9.9330e-06],\n+            ]\n+        ).to(torch_device)\n+\n+        assert torch.allclose(heatmaps[0, 0, :3, :3], expected_slice, atol=1e-4)\n+\n+        pose_results = image_processor.post_process_pose_estimation(outputs, boxes=boxes)\n+        print(pose_results)\n+\n+        expected_bbox = torch.tensor([391.9900, 190.0800, 391.1575, 189.3034])\n+        expected_keypoints = torch.tensor(\n+            [\n+                [3.9813e02, 1.8184e02],\n+                [3.9828e02, 1.7981e02],\n+                [3.9596e02, 1.7948e02],\n+            ]\n+        )\n+        expected_scores = torch.tensor([8.7529e-01, 8.4315e-01, 9.2678e-01])\n+\n+        self.assertEqual(len(pose_results), 2)\n+        self.assertEqual(len(pose_results[0]), 2)\n+        self.assertTrue(torch.allclose(pose_results[0][1][\"bbox\"].cpu(), expected_bbox, atol=1e-4))\n+        self.assertTrue(torch.allclose(pose_results[0][1][\"keypoints\"][:3].cpu(), expected_keypoints, atol=1e-2))\n+        self.assertTrue(torch.allclose(pose_results[0][1][\"scores\"][:3].cpu(), expected_scores, atol=1e-4))"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/vitpose_backbone/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose_backbone%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose_backbone%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose_backbone%2F__init__.py?ref=8490d3159ce37087151e30211e33f59008a44233"
        },
        {
            "sha": "c32a57e9f2987f1c530ed60602dd61d5a28f2c48",
            "filename": "tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -0,0 +1,199 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch VitPose backbone model.\"\"\"\n+\n+import inspect\n+import unittest\n+\n+from transformers import VitPoseBackboneConfig\n+from transformers.testing_utils import require_torch\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_backbone_common import BackboneTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    from transformers import VitPoseBackbone\n+\n+\n+if is_vision_available():\n+    pass\n+\n+\n+class VitPoseBackboneModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        image_size=[16 * 8, 12 * 8],\n+        patch_size=[8, 8],\n+        num_channels=3,\n+        is_training=True,\n+        use_labels=True,\n+        hidden_size=32,\n+        num_hidden_layers=5,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.1,\n+        attention_probs_dropout_prob=0.1,\n+        type_sequence_label_size=10,\n+        initializer_range=0.02,\n+        num_labels=2,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n+        self.type_sequence_label_size = type_sequence_label_size\n+        self.initializer_range = initializer_range\n+        self.num_labels = num_labels\n+        self.scope = scope\n+\n+        # in VitPoseBackbone, the seq length equals the number of patches\n+        num_patches = (image_size[0] // patch_size[0]) * (image_size[1] // patch_size[1])\n+        self.seq_length = num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size[0], self.image_size[1]])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return VitPoseBackboneConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            hidden_act=self.hidden_act,\n+            hidden_dropout_prob=self.hidden_dropout_prob,\n+            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n+            initializer_range=self.initializer_range,\n+            num_labels=self.num_labels,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            pixel_values,\n+            labels,\n+        ) = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class VitPoseBackboneModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as VitPoseBackbone does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (VitPoseBackbone,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = VitPoseBackboneModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=VitPoseBackboneConfig, has_text_modality=False, hidden_size=37\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support input and output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support input and output embeddings\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not output a loss\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support training yet\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support training yet\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support training yet\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"VitPoseBackbone does not support training yet\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+\n+@require_torch\n+class VitPoseBackboneTest(unittest.TestCase, BackboneTesterMixin):\n+    all_model_classes = (VitPoseBackbone,) if is_torch_available() else ()\n+    config_class = VitPoseBackboneConfig\n+\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = VitPoseBackboneModelTester(self)"
        },
        {
            "sha": "daf90b4d4a2c822d61171d001d7a6295637ddcc1",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -1092,6 +1092,7 @@ def _find_text_in_file(filename: str, start_prompt: str, end_prompt: str) -> Tup\n     \"CLIPVisionModel\",\n     \"SiglipVisionModel\",\n     \"ChineseCLIPVisionModel\",\n+    \"VitPoseBackbone\",\n ]\n \n # Template for new entries to add in the main README when we have missing models."
        },
        {
            "sha": "d20760bcf75eba87eda7fc57e4776c6db6f28962",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8490d3159ce37087151e30211e33f59008a44233/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8490d3159ce37087151e30211e33f59008a44233/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=8490d3159ce37087151e30211e33f59008a44233",
            "patch": "@@ -330,6 +330,7 @@\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n+    \"VitPoseForPoseEstimation\",\n     \"CLIPTextModel\",\n     \"MoshiForConditionalGeneration\",  # no auto class for speech-to-speech\n ]\n@@ -993,6 +994,8 @@ def find_all_documented_objects() -> List[str]:\n     \"logging\",  # External module\n     \"requires_backends\",  # Internal function\n     \"AltRobertaModel\",  # Internal module\n+    \"VitPoseBackbone\",  # Internal module\n+    \"VitPoseBackboneConfig\",  # Internal module\n ]\n \n # This list should be empty. Objects in it should get their own doc page."
        }
    ],
    "stats": {
        "total": 3350,
        "additions": 3350,
        "deletions": 0
    }
}