{
    "author": "zucchini-nlp",
    "message": "processor tests - use dummy videos (#40537)\n\n* use dummy videos\n\n* failing on main, new model merged had conflicts",
    "sha": "0b2450737992b675e11639396537539a0fd9505c",
    "files": [
        {
            "sha": "53c7a250a62e0939214090b1b121b9fdf0239016",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -400,6 +400,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):"
        },
        {
            "sha": "a4fe2f61b1fd69b31bb3a5c0a8e531a69eafdbf5",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -1631,7 +1631,9 @@ def apply_chat_template(\n \n             # Always sample frames by default unless explicitly set to `False` by users. If users do not pass `num_frames`/`video_fps`\n             # sampling should not done for BC.\n-            if \"do_sample_frames\" not in kwargs and (\"fps\" in kwargs or \"num_frames\" in kwargs):\n+            if \"do_sample_frames\" not in kwargs and (\n+                kwargs.get(\"fps\") is not None or kwargs.get(\"num_frames\") is not None\n+            ):\n                 kwargs[\"do_sample_frames\"] = True\n \n             out = self("
        },
        {
            "sha": "526284e1517ad4f3d5d996cfb61934bd0af13d48",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -141,7 +141,10 @@ def _test_apply_chat_template(\n             tokenize=True,\n             return_dict=True,\n             return_tensors=return_tensors,\n-            fps=2,  # by default no more than 2 frames per second, otherwise too slow\n+            fps=2\n+            if isinstance(input_data[0], str)\n+            else None,  # by default no more than 2 frames per second, otherwise too slow\n+            do_sample_frames=bool(isinstance(input_data[0], str)),  # don't sample frames if decoded video is used\n         )\n         input_name = getattr(self, input_name)\n         self.assertTrue(input_name in out_dict)"
        },
        {
            "sha": "38687dd29050bb99f8aefcb988813026caba3ac0",
            "filename": "tests/models/internvl/test_processing_internvl.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -17,13 +17,11 @@\n import tempfile\n import unittest\n \n-from parameterized import parameterized\n-\n from transformers import AutoProcessor, AutoTokenizer, InternVLProcessor\n from transformers.testing_utils import require_av, require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n-from ...test_processing_common import MODALITY_INPUT_DATA, ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_torch_available():\n@@ -266,15 +264,22 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 2)\n \n-    @require_av\n-    @parameterized.expand([(1, \"pt\"), (2, \"pt\"), (3, \"pt\")])\n-    def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n+    @require_torch\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if \"video_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"`video_processor` attribute not present in {self.processor_class}\")\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n         batch_messages = [\n             [\n@@ -325,8 +330,8 @@ def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n         self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n \n         # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n-        for idx, url in enumerate(MODALITY_INPUT_DATA[\"videos\"][:batch_size]):\n-            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": \"video\", \"url\": url}]\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n \n         out_dict = processor.apply_chat_template(\n             batch_messages,\n@@ -345,8 +350,8 @@ def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n         # removed hardcoded video length check video_len = 2 if batch_size == 1 else 3\n         # from experiment video_len looks like batch_size + 1\n         # TODO: update expected video_len calculation based on the internal processing logic of InternVLProcessor\n-        video_len = batch_size + 1\n-        self.assertEqual(len(out_dict[self.videos_input_name]), video_len)\n+        output_len = batch_size + 1 if modality == \"video\" else batch_size\n+        self.assertEqual(len(out_dict[self.videos_input_name]), output_len)\n         for k in out_dict:\n             self.assertIsInstance(out_dict[k], torch.Tensor)\n "
        },
        {
            "sha": "7130a623e033e339669b8f53ff91fa5295c2f83b",
            "filename": "tests/models/janus/test_processing_janus.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -21,7 +21,7 @@\n \n from transformers import AutoProcessor, AutoTokenizer, JanusProcessor\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n class JanusProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n@@ -128,7 +128,7 @@ def test_chat_template_single(self):\n \n         # Now test the ability to return dict\n         messages[0][0][\"content\"][1].update(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n         )\n         out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n         self.assertTrue(self.images_input_name in out_dict)\n@@ -269,10 +269,10 @@ def test_chat_template_batched(self):\n \n         # Verify image inputs are included in the output dict\n         batched_messages[0][0][\"content\"][1].update(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n         )\n         batched_messages[1][0][\"content\"][1].update(\n-            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+            {\"type\": \"image\", \"url\": url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\")}\n         )\n         out_dict = processor.apply_chat_template(\n             batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n@@ -419,7 +419,7 @@ def test_chat_template_accepts_processing_kwargs(self):\n         # Test 3: Image processing kwargs\n         # Add an image and test image processing parameters\n         messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n         )\n         # Process with image rescaling and verify the pixel values are negative\n         out_dict = processor.apply_chat_template("
        },
        {
            "sha": "4c6af0dd1e9e7d4017f454f2676ed50d13dd3da8",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -593,12 +593,8 @@ def test_special_mm_token_truncation(self):\n                 max_length=20,\n             )\n \n-    @unittest.skip(\"SmolVLM cannot accept image URL as video frames, because it needs to know video fps and duration\")\n-    def test_apply_chat_template_video_1(self):\n-        pass\n-\n     @unittest.skip(\n         \"SmolVLM cannot accept list of decoded video frames, because it needs to know video fps and duration\"\n     )\n-    def test_apply_chat_template_video_2(self):\n+    def test_apply_chat_template_decoded_video_0(self):\n         pass"
        },
        {
            "sha": "7845b926e06caa7a40976351c9a031e25dc4733e",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 8,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -16,6 +16,7 @@\n import inspect\n import json\n import random\n+import sys\n import tempfile\n from pathlib import Path\n from typing import Optional, Union\n@@ -33,7 +34,11 @@\n     require_torch,\n     require_vision,\n )\n-from transformers.utils import is_av_available, is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+\n+sys.path.append(\".\")\n+from utils.fetch_hub_objects_for_ci import url_to_local_path\n \n \n global_rng = random.Random()\n@@ -59,14 +64,9 @@\n     ],\n }\n \n-if is_av_available():\n-    from transformers.video_utils import load_video\n \n-    # load a video file in memory for testing\n-    video, _ = load_video(\n-        \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\"\n-    )\n-    MODALITY_INPUT_DATA[\"videos\"].append(video)\n+for modality, urls in MODALITY_INPUT_DATA.items():\n+    MODALITY_INPUT_DATA[modality] = [url_to_local_path(url) for url in urls]\n \n \n def prepare_image_inputs():\n@@ -1006,6 +1006,15 @@ def test_apply_chat_template_audio(self, batch_size: int, return_tensors: str):\n             \"audio\", batch_size, return_tensors, \"audio_input_name\", \"feature_extractor\", MODALITY_INPUT_DATA[\"audio\"]\n         )\n \n+    @require_av\n+    @parameterized.expand([(1, \"pt\")])\n+    def test_apply_chat_template_decoded_video(self, batch_size: int, return_tensors: str):\n+        dummy_preloaded_video = np.array(self.prepare_video_inputs())\n+        input_data = [dummy_preloaded_video]\n+        self._test_apply_chat_template(\n+            \"video\", batch_size, return_tensors, \"videos_input_name\", \"video_processor\", input_data\n+        )\n+\n     @require_av\n     @parameterized.expand([(1, \"pt\"), (2, \"pt\")])  # video processor supports only torchvision\n     def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):"
        },
        {
            "sha": "da9dc55e2a848d368df7c6b829d5ce4ba57b825e",
            "filename": "utils/fetch_hub_objects_for_ci.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b2450737992b675e11639396537539a0fd9505c/utils%2Ffetch_hub_objects_for_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b2450737992b675e11639396537539a0fd9505c/utils%2Ffetch_hub_objects_for_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ffetch_hub_objects_for_ci.py?ref=0b2450737992b675e11639396537539a0fd9505c",
            "patch": "@@ -1,12 +1,56 @@\n+import os\n+\n+import requests\n from huggingface_hub import hf_hub_download\n \n from transformers.testing_utils import _run_pipeline_tests\n \n \n+URLS_FOR_TESTING_DATA = [\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\",\n+    \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+]\n+\n+\n+def url_to_local_path(url, return_url_if_not_found=True):\n+    filename = url.split(\"/\")[-1]\n+\n+    if not os.path.exists(filename) and return_url_if_not_found:\n+        return url\n+\n+    return filename\n+\n+\n if __name__ == \"__main__\":\n     if _run_pipeline_tests:\n         import datasets\n \n         _ = datasets.load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n         _ = datasets.load_dataset(\"hf-internal-testing/fixtures_image_utils\", split=\"test\", revision=\"refs/pr/1\")\n         _ = hf_hub_download(repo_id=\"nateraw/video-demo\", filename=\"archery.mp4\", repo_type=\"dataset\")\n+\n+    # Download files from URLs to local directory\n+    for url in URLS_FOR_TESTING_DATA:\n+        filename = url_to_local_path(url, return_url_if_not_found=False)\n+\n+        # Skip if file already exists\n+        if os.path.exists(filename):\n+            print(f\"File already exists: {filename}\")\n+            continue\n+\n+        print(f\"Downloading {filename}...\")\n+        try:\n+            response = requests.get(url, stream=True)\n+            response.raise_for_status()\n+\n+            with open(filename, \"wb\") as f:\n+                for chunk in response.iter_content(chunk_size=8192):\n+                    f.write(chunk)\n+            print(f\"Successfully downloaded: {filename}\")\n+        except requests.exceptions.RequestException as e:\n+            print(f\"Error downloading {filename}: {e}\")"
        }
    ],
    "stats": {
        "total": 124,
        "additions": 92,
        "deletions": 32
    }
}