{
    "author": "qubvel",
    "message": "Replace image classification loss functions to `self.loss_function` (#40764)",
    "sha": "77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
    "files": [
        {
            "sha": "09c887bcd2b49d4b80398fa512d3a09b5c75054b",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -23,7 +23,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1020,26 +1020,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n+\n         if not return_dict:\n             output = (logits,) + outputs[2:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "ec778380b6baf0e2f324aca2bbcbad476c1f3919",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -22,7 +22,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import (\n@@ -744,25 +743,7 @@ def forward(\n         loss = None\n \n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "196381f33bbdf9526dba01dc45a8f409e9c0d248",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -19,7 +19,6 @@\n \n import torch\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n@@ -1220,28 +1219,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         return ImageClassifierOutput(\n             loss=loss,"
        },
        {
            "sha": "6d76852122ac2a8b05256b01800a29e271818aff",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -23,7 +23,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -935,26 +935,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n+\n         if not return_dict:\n             output = (logits,) + outputs[2:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "3d918e7f572076fe349edede381ab69c856e5994",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n from ....modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n@@ -660,26 +659,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "d463e2bc89cc52d045c28de2f0b57b4e3a07539a",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n from ....modeling_outputs import BackboneOutput\n@@ -810,26 +809,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "025234e4e71f961dc87c085e76503a4392718fde",
            "filename": "src/transformers/models/deprecated/van/modeling_van.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fmodeling_van.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n from ....modeling_outputs import (\n@@ -510,26 +509,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.config.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.config.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "2d92655cc3e4ceedcb35561e7ff89a425a678584",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ....activations import ACT2FN\n from ....modeling_layers import GradientCheckpointingLayer\n@@ -725,28 +724,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "384bdee49d35e97acf683d660fe43650a41a09be",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput\n@@ -736,26 +735,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "882fd72c508e190f29bd6c05043e69a1fd34c474",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -1014,7 +1014,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=logits, config=self.config)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "70ec3914f7de2c4e658c0adb4c4cc20425b4b4dc",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import (\n@@ -547,26 +546,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "e56ada740e222c585796963a22b6a58d6cf8a98f",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -22,7 +22,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -846,26 +845,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "1cd0e857afcdecbd536ecb2122e649a3fc0b1b6a",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -25,7 +25,6 @@\n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput, BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n@@ -465,25 +464,7 @@ def forward(\n         loss = None\n \n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "b0c46d688053d6f0d8da5067ef3798868e452f32",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -19,7 +19,6 @@\n import torch\n import torch.nn.functional as F\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...configuration_utils import PretrainedConfig\n from ...modeling_outputs import (\n@@ -588,25 +587,7 @@ def forward(\n         loss = None\n \n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "bfef87618156a83c552d421102dac97936f94280",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1320,28 +1319,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "c916a82aad0398bb56932588198b9a7df4aa5d7e",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n@@ -1001,26 +1001,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n+            loss = self.loss_function(labels, logits, self.config)\n \n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n         if not return_dict:\n             output = (logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "a72f5604825f9e6b8a84c882ca13ff9dd8acfa50",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...modeling_outputs import (\n     BaseModelOutputWithNoAttention,\n@@ -580,26 +579,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n+\n         if not return_dict:\n             output = (logits,) + outputs[2:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "58c8ea95655163d395de256f1302ba302ca25ded",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -9,7 +9,6 @@\n \n import torch\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n@@ -1356,28 +1355,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         return ImageClassifierOutput(\n             loss=loss,"
        },
        {
            "sha": "25997a46790c009bc62e5b4323d378386e3c1718",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -18,7 +18,6 @@\n \n import torch\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutputWithPoolingAndNoAttention, ImageClassifierOutputWithNoAttention\n@@ -394,26 +393,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "8f178f0480dd0673fae42923ac834aa046fcc428",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -18,7 +18,7 @@\n \n import torch\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import (\n@@ -597,26 +597,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "10fe620f7c0d6c255daf0e82d1242f607cb06a2c",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -22,7 +22,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -774,26 +774,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "291ce6136a5482238e75414c8285edd7ce2ab952",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 21,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -21,7 +21,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -720,26 +720,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "1f6b84343d00d5d3d2d87ba641aa694309aa8962",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 3,
            "deletions": 60,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -1214,26 +1214,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]\n@@ -1355,26 +1336,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]\n@@ -1497,26 +1459,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "3753eb464b040f76bbf0c4afa928ee47fd170162",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n@@ -370,26 +369,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "446a859448013e9e2d1c43481d5c558c9fdd5f2e",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -25,7 +25,6 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n@@ -576,26 +575,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "e434223a94a298de7bf7eec5b12d44250792e415",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -22,7 +22,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -524,26 +523,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "5eb65d92b8be1d8c6aeed6fa1d86d34c57510122",
            "filename": "src/transformers/models/regnet/modeling_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_regnet.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import (\n@@ -366,25 +365,7 @@ def forward(\n         loss = None\n \n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "c766a91cd277738525380f59db0ef57920e1dee1",
            "filename": "src/transformers/models/resnet/modeling_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import (\n@@ -349,25 +348,7 @@ def forward(\n         loss = None\n \n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "4aa49d86466b8f73cfda300d2977512338eafbe3",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -20,7 +20,7 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput, SemanticSegmenterOutput\n@@ -567,26 +567,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n+\n         if not return_dict:\n             output = (logits,) + outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "11cab1bf21879fdd03dd342e2faf792d5eeaa27b",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -22,7 +22,6 @@\n import numpy as np\n import torch\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n from torch.nn.init import _calculate_fan_in_and_fan_out\n \n from ...activations import ACT2FN\n@@ -1080,28 +1079,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         return ImageClassifierOutput(\n             loss=loss,"
        },
        {
            "sha": "cf2553bd6fe24218988725aeff297d21755df056",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -27,7 +27,6 @@\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n from torch.nn.init import _calculate_fan_in_and_fan_out\n \n from ...activations import ACT2FN\n@@ -1178,28 +1177,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         return ImageClassifierOutput(\n             loss=loss,"
        },
        {
            "sha": "260a82e5143e157301f4d590270504e6205ae9ea",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -17,7 +17,6 @@\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from transformers.models.siglip.configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n from transformers.models.siglip.modeling_siglip import (\n@@ -584,28 +583,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         return ImageClassifierOutput(\n             loss=loss,"
        },
        {
            "sha": "9e0c4c3147b76a48f88147c6b299dd7837448558",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -20,7 +20,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2CLS\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n@@ -509,26 +508,7 @@ def forward(\n         # calculate loss\n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "37d3413fae5d94bb697721c2b032dc41e7cbd015",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -1152,7 +1152,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=logits, config=self.config)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "4d030178ed495560f13fd38d0b5e230cf876888c",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -1227,7 +1227,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=logits, config=self.config)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "ca39fdc0f2aa7753d6771ca4e71aaaf53a1c98d5",
            "filename": "src/transformers/models/textnet/modeling_textnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -19,7 +19,6 @@\n import torch\n import torch.nn as nn\n from torch import Tensor\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from transformers import PreTrainedModel\n from transformers.activations import ACT2CLS\n@@ -335,25 +334,7 @@ def forward(\n         loss = None\n \n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "7839bf7813f2bae935c8e6ba235102b1a806aaa9",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/77aa35ee9c6366c9bb42e48084a2807dc891ab1f/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=77aa35ee9c6366c9bb42e48084a2807dc891ab1f",
            "patch": "@@ -18,7 +18,6 @@\n \n import torch\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...modeling_outputs import ImageClassifierOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -344,25 +343,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(labels, logits, self.config)\n \n         if not return_dict:\n             outputs = (loss, logits, hidden_states)"
        }
    ],
    "stats": {
        "total": 812,
        "additions": 50,
        "deletions": 762
    }
}