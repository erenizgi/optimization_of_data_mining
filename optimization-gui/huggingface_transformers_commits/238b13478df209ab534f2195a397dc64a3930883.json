{
    "author": "gante",
    "message": "Gemma2: fix config initialization (`cache_implementation`) (#33684)",
    "sha": "238b13478df209ab534f2195a397dc64a3930883",
    "files": [
        {
            "sha": "44f96efb6df3f98058c72e9805135b28df1d531b",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/238b13478df209ab534f2195a397dc64a3930883/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/238b13478df209ab534f2195a397dc64a3930883/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=238b13478df209ab534f2195a397dc64a3930883",
            "patch": "@@ -85,6 +85,7 @@ class Gemma2Config(PretrainedConfig):\n             size of the sliding window.\n         final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n+        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n \n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n@@ -98,7 +99,6 @@ class Gemma2Config(PretrainedConfig):\n \n     model_type = \"gemma2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    cache_implementation = \"hybrid\"\n \n     def __init__(\n         self,\n@@ -125,6 +125,7 @@ def __init__(\n         sliding_window=4096,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,\n+        cache_implementation=\"hybrid\",\n         **kwargs,\n     ):\n         super().__init__(\n@@ -153,3 +154,4 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n+        self.cache_implementation = cache_implementation"
        },
        {
            "sha": "6decd28a4dd57ecc6c05a6babc7e34726a24dd76",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/238b13478df209ab534f2195a397dc64a3930883/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/238b13478df209ab534f2195a397dc64a3930883/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=238b13478df209ab534f2195a397dc64a3930883",
            "patch": "@@ -117,6 +117,7 @@ class Gemma2Config(PretrainedConfig):\n             size of the sliding window.\n         final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n+        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n \n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n@@ -130,7 +131,6 @@ class Gemma2Config(PretrainedConfig):\n \n     model_type = \"gemma2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    cache_implementation = \"hybrid\"\n \n     def __init__(\n         self,\n@@ -157,6 +157,7 @@ def __init__(\n         sliding_window=4096,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,\n+        cache_implementation=\"hybrid\",\n         **kwargs,\n     ):\n         super().__init__(\n@@ -185,6 +186,7 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n+        self.cache_implementation = cache_implementation\n \n \n class Gemma2RMSNorm(GemmaRMSNorm):"
        },
        {
            "sha": "b9feccd1f9629c16ddbca681a335f4867817eba1",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/238b13478df209ab534f2195a397dc64a3930883/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/238b13478df209ab534f2195a397dc64a3930883/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=238b13478df209ab534f2195a397dc64a3930883",
            "patch": "@@ -44,7 +44,9 @@\n     \"Qwen2Config\": [\"use_sliding_window\"],\n     \"Qwen2MoeConfig\": [\"use_sliding_window\"],\n     \"Qwen2VLConfig\": [\"use_sliding_window\"],\n-    \"Gemma2Config\": [\"tie_word_embeddings\"],\n+    # `cache_implementation` should be in the default generation config, but we don't yet support per-model\n+    # generation configs (TODO joao)\n+    \"Gemma2Config\": [\"tie_word_embeddings\", \"cache_implementation\"],\n     # used to compute the property `self.chunk_length`\n     \"EncodecConfig\": [\"overlap\"],\n     # used to compute the property `self.layers_block_type`"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}