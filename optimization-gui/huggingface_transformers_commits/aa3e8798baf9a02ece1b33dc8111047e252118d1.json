{
    "author": "SunMarc",
    "message": "[v5] Remove tokenizer from Trainer (#41128)\n\n* tokenizer deprecated\n\n* style\n\n* forgot this\n\n* style",
    "sha": "aa3e8798baf9a02ece1b33dc8111047e252118d1",
    "files": [
        {
            "sha": "61adb1c6d67c78ebaf4e8b1d79d99e10e2f759bf",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa3e8798baf9a02ece1b33dc8111047e252118d1/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa3e8798baf9a02ece1b33dc8111047e252118d1/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=aa3e8798baf9a02ece1b33dc8111047e252118d1",
            "patch": "@@ -177,7 +177,6 @@\n     logging,\n     strtobool,\n )\n-from .utils.deprecation import deprecate_kwarg\n from .utils.import_utils import requires\n from .utils.quantization_config import QuantizationMethod\n \n@@ -406,7 +405,6 @@ class Trainer:\n     # Those are used as methods of the Trainer in examples.\n     from .trainer_pt_utils import _get_learning_rate, log_metrics, metrics_format, save_metrics, save_state\n \n-    @deprecate_kwarg(\"tokenizer\", new_name=\"processing_class\", version=\"5.0.0\", raise_if_both_names=True)\n     def __init__(\n         self,\n         model: Union[PreTrainedModel, nn.Module, None] = None,\n@@ -774,18 +772,6 @@ def __init__(\n             xs.set_global_mesh(xs.Mesh(np.array(range(num_devices)), (num_devices, 1), axis_names=(\"fsdp\", \"tensor\")))\n         self.is_fsdp_xla_v1_enabled = self.is_fsdp_xla_enabled and not self.is_fsdp_xla_v2_enabled\n \n-    @property\n-    def tokenizer(self) -> Optional[PreTrainedTokenizerBase]:\n-        logger.warning(\"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\")\n-        return self.processing_class\n-\n-    @tokenizer.setter\n-    def tokenizer(self, processing_class) -> None:\n-        logger.warning(\n-            \"Trainer.tokenizer is now deprecated. You should use `Trainer.processing_class = processing_class` instead.\"\n-        )\n-        self.processing_class = processing_class\n-\n     def _activate_neftune(self, model):\n         r\"\"\"\n         Activates the neftune as presented in this code: https://github.com/neelsjain/NEFTune and paper:"
        },
        {
            "sha": "ca6842bc0ff382f9701e65c0a26424e00470f051",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa3e8798baf9a02ece1b33dc8111047e252118d1/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa3e8798baf9a02ece1b33dc8111047e252118d1/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=aa3e8798baf9a02ece1b33dc8111047e252118d1",
            "patch": "@@ -27,7 +27,6 @@\n from .integrations.fsdp import is_fsdp_managed_module\n from .trainer import Trainer\n from .utils import is_datasets_available, logging\n-from .utils.deprecation import deprecate_kwarg\n \n \n if is_datasets_available():\n@@ -51,7 +50,6 @@\n \n \n class Seq2SeqTrainer(Trainer):\n-    @deprecate_kwarg(\"tokenizer\", new_name=\"processing_class\", version=\"5.0.0\", raise_if_both_names=True)\n     def __init__(\n         self,\n         model: Optional[Union[\"PreTrainedModel\", nn.Module]] = None,"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 0,
        "deletions": 16
    }
}