{
    "author": "cyr0930",
    "message": "fix multi-image case for llava-onevision (#38084)\n\n* _get_padding_size module\n\n* do not patchify images when processing multi image\n\n* modify llava onevision image processor fast\n\n* tensor to list of tensors\n\n* backward compat\n\n* reuse pad_to_square in llave & some clarification\n\n* add to doc\n\n* fix: consider no image cases (text only or video)\n\n* add integration test\n\n* style & repo_consistency",
    "sha": "101b3fa4ea1cfd66e72a73a0f505108454c24cce",
    "files": [
        {
            "sha": "14d5f6508add1768a039adc0cdc10deab3c2d07c",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -147,7 +147,7 @@ print(processor.decode(output[0], skip_special_tokens=True))\n \n ### Multi image inference\n \n-LLaVa-OneVision can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). For that you have to use checkpoints with an \"ov\" suffix. Here is how you can do it:\n+LLaVa-OneVision can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). For that you have to use checkpoints with an \"ov\" suffix. For multi-image cases, we recommend using a **nested list of images** as input. Otherwise, every image will be patchified and consume a lot of memory. Here is how you can do it:\n \n ```python\n import requests"
        },
        {
            "sha": "54a2ec9488c55a6965a06bc1ced1a728afdd4ee6",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -364,19 +364,23 @@ def _resize_for_patching(\n \n         return resized_image\n \n+    def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):\n+        original_height, original_width = original_resolution\n+        target_height, target_width = target_resolution\n+        paste_x, r_x = divmod(target_width - original_width, 2)\n+        paste_y, r_y = divmod(target_height - original_height, 2)\n+        return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n+\n     def _pad_for_patching(\n         self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n-        target_height, target_width = target_resolution\n-        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n-\n-        paste_x, r_x = divmod(target_width - new_width, 2)\n-        paste_y, r_y = divmod(target_height - new_height, 2)\n+        new_resolution = get_patch_output_size(image, target_resolution, input_data_format)\n+        padding = self._get_padding_size(new_resolution, target_resolution)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n+        padded_image = self.pad(image, padding=padding)\n \n         return padded_image\n "
        },
        {
            "sha": "5afc05e9159e9c233e925786c7e3f6179320fd3b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -748,19 +748,23 @@ def _resize_for_patching(\n \n         return resized_image\n \n+    def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):\n+        original_height, original_width = original_resolution\n+        target_height, target_width = target_resolution\n+        paste_x, r_x = divmod(target_width - original_width, 2)\n+        paste_y, r_y = divmod(target_height - original_height, 2)\n+        return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n+\n     def _pad_for_patching(\n         self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n-        target_height, target_width = target_resolution\n-        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n-\n-        paste_x, r_x = divmod(target_width - new_width, 2)\n-        paste_y, r_y = divmod(target_height - new_height, 2)\n+        new_resolution = get_patch_output_size(image, target_resolution, input_data_format)\n+        padding = self._get_padding_size(new_resolution, target_resolution)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n+        padded_image = self.pad(image, padding=padding)\n \n         return padded_image\n "
        },
        {
            "sha": "06601b45c52b0efd2d7439e0ea4a1822d38a942b",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -424,19 +424,23 @@ def _resize_for_patching(\n \n         return resized_image\n \n+    def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):\n+        original_height, original_width = original_resolution\n+        target_height, target_width = target_resolution\n+        paste_x, r_x = divmod(target_width - original_width, 2)\n+        paste_y, r_y = divmod(target_height - original_height, 2)\n+        return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n+\n     def _pad_for_patching(\n         self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n-        target_height, target_width = target_resolution\n-        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n-\n-        paste_x, r_x = divmod(target_width - new_width, 2)\n-        paste_y, r_y = divmod(target_height - new_height, 2)\n+        new_resolution = get_patch_output_size(image, target_resolution, input_data_format)\n+        padding = self._get_padding_size(new_resolution, target_resolution)\n \n-        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n+        padded_image = self.pad(image, padding=padding)\n \n         return padded_image\n "
        },
        {
            "sha": "ac90290cef480646ae3e3666e9d46bc69d9e60fe",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -141,19 +141,23 @@ def _resize_for_patching(\n \n         return resized_image\n \n+    def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):\n+        original_height, original_width = original_resolution\n+        target_height, target_width = target_resolution\n+        paste_x, r_x = divmod(target_width - original_width, 2)\n+        paste_y, r_y = divmod(target_height - original_height, 2)\n+        return [paste_x, paste_y, paste_x + r_x, paste_y + r_y]\n+\n     def _pad_for_patching(\n         self, image: \"torch.Tensor\", target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n-        target_height, target_width = target_resolution\n-        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n-\n-        paste_x, r_x = divmod(target_width - new_width, 2)\n-        paste_y, r_y = divmod(target_height - new_height, 2)\n+        new_resolution = get_patch_output_size(image, target_resolution, input_data_format)\n+        padding = self._get_padding_size(new_resolution, target_resolution)\n \n-        padded_image = F.pad(image, padding=[paste_x, paste_y, paste_x + r_x, paste_y + r_y])\n+        padded_image = F.pad(image, padding=padding)\n \n         return padded_image\n "
        },
        {
            "sha": "6a471d712a90f5e5f22eac54eb77aa95c9c1d8a8",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 129,
            "deletions": 24,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -315,20 +315,25 @@ def _resize_for_patching(\n \n         return resized_image\n \n+    # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._get_padding_size\n+    def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):\n+        original_height, original_width = original_resolution\n+        target_height, target_width = target_resolution\n+        paste_x, r_x = divmod(target_width - original_width, 2)\n+        paste_y, r_y = divmod(target_height - original_height, 2)\n+        return (paste_y, paste_y + r_y), (paste_x, paste_x + r_x)\n+\n     # Copied from transformers.models.llava_next.image_processing_llava_next.LlavaNextImageProcessor._pad_for_patching\n     def _pad_for_patching(\n         self, image: np.array, target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> np.array:\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n-        target_height, target_width = target_resolution\n-        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n+        new_resolution = get_patch_output_size(image, target_resolution, input_data_format)\n+        padding = self._get_padding_size(new_resolution, target_resolution)\n \n-        paste_x, r_x = divmod(target_width - new_width, 2)\n-        paste_y, r_y = divmod(target_height - new_height, 2)\n-\n-        padded_image = self.pad(image, padding=((paste_y, paste_y + r_y), (paste_x, paste_x + r_x)))\n+        padded_image = self.pad(image, padding=padding)\n \n         return padded_image\n \n@@ -437,6 +442,85 @@ def _pad_for_batching(\n \n         return pixel_values\n \n+    # Copied from transformers.models.llava.image_processing_llava.LlavaImageProcessor.pad_to_square\n+    def pad_to_square(\n+        self,\n+        image: np.ndarray,\n+        background_color: Union[int, Tuple[int, int, int]] = 0,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.array:\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                The image to pad.\n+            background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the output image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use same as the input image.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                    - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                    - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                If unset, will use the inferred format of the input image.\n+\n+        Returns:\n+            `np.ndarray`: The padded image.\n+        \"\"\"\n+        height, width = get_image_size(image, input_data_format)\n+        num_channels = image.shape[0] if input_data_format == ChannelDimension.FIRST else image.shape[-1]\n+\n+        if height == width:\n+            image = (\n+                to_channel_dimension_format(image, data_format, input_data_format)\n+                if data_format is not None\n+                else image\n+            )\n+            return image\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        if input_data_format == ChannelDimension.FIRST:\n+            result = np.zeros((num_channels, max_dim, max_dim), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[i, :, :] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[:, start : start + height, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, :, start : start + width] = image\n+        else:\n+            result = np.zeros((max_dim, max_dim, num_channels), dtype=image.dtype)\n+            for i, color in enumerate(background_color):\n+                result[:, :, i] = color\n+            if width > height:\n+                start = (max_dim - height) // 2\n+                result[start : start + height, :, :] = image\n+            else:\n+                start = (max_dim - width) // 2\n+                result[:, start : start + width, :] = image\n+\n+        image = (\n+            to_channel_dimension_format(result, data_format, input_data_format) if data_format is not None else result\n+        )\n+        return image\n+\n     def _preprocess(\n         self,\n         images: ImageInput,\n@@ -595,6 +679,17 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n+        if isinstance(images, (tuple, list)) and isinstance(images[0], (tuple, list)):\n+            # if the first element is a list, we assume that all elements are lists\n+            batch_num_images = [len(x) for x in images]\n+        elif isinstance(images, (tuple, list)):\n+            # treat this as a single-image case for backward compatibility\n+            batch_num_images = [1] * len(images)\n+        else:\n+            batch_num_images = [1]\n+        # only single image patching is supported\n+        need_patching = [n == 1 for n in batch_num_images for _ in range(n)]\n+\n         images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n@@ -630,25 +725,34 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n+        size_tuple = (\n+            (size[\"height\"], size[\"width\"])\n+            if \"height\" in size and \"width\" in size\n+            else (size[\"shortest_edge\"], size[\"shortest_edge\"])\n+        )\n+\n         new_images = []\n         image_sizes = [get_image_size(image, channel_dim=input_data_format) for image in images]\n-        for image in images:\n-            # convert image into a list of patches\n-            # we intentionally use the same data format as the input data format\n-            size_tuple = (\n-                (size[\"height\"], size[\"width\"])\n-                if \"height\" in size and \"width\" in size\n-                else (size[\"shortest_edge\"], size[\"shortest_edge\"])\n-            )\n-            image_patches = self.get_image_patches(\n-                image,\n-                image_grid_pinpoints,\n-                size=size_tuple,\n-                patch_size=size_tuple[0],\n-                resample=resample,\n-                data_format=input_data_format,\n-                input_data_format=input_data_format,\n-            )\n+        for i, image in enumerate(images):\n+            if need_patching[i]:\n+                # convert image into a list of patches\n+                # we intentionally use the same data format as the input data format\n+                image_patches = self.get_image_patches(\n+                    image,\n+                    image_grid_pinpoints,\n+                    size=size_tuple,\n+                    patch_size=size_tuple[0],\n+                    resample=resample,\n+                    data_format=input_data_format,\n+                    input_data_format=input_data_format,\n+                )\n+            else:\n+                padded_image = self.pad_to_square(\n+                    image=image,\n+                    background_color=tuple(int(x * 255) for x in self.image_mean),\n+                    input_data_format=input_data_format,\n+                )\n+                image_patches = [padded_image]\n \n             # preprocess patches\n             pixel_values = self._preprocess(\n@@ -671,7 +775,8 @@ def preprocess(\n             processed_images = self._pad_for_batching(new_images)\n \n         return BatchFeature(\n-            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes}, tensor_type=return_tensors\n+            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"batch_num_images\": batch_num_images},\n+            tensor_type=return_tensors,\n         )\n \n "
        },
        {
            "sha": "a29631fcb6af6da339c06db5a89160394a5c67d8",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 84,
            "deletions": 16,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -19,7 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import List, Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n \n@@ -89,6 +89,15 @@ def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]):\n \n     @auto_docstring\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]) -> BatchFeature:\n+        if isinstance(images, (tuple, list)) and isinstance(images[0], (tuple, list)):\n+            # if the first element is a list, we assume that all elements are lists\n+            batch_num_images = [len(x) for x in images]\n+        elif isinstance(images, (tuple, list)):\n+            # treat this as a single-image case for backward compatibility\n+            batch_num_images = [1] * len(images)\n+        else:\n+            batch_num_images = [1]\n+        kwargs[\"batch_num_images\"] = batch_num_images\n         return super().preprocess(images, **kwargs)\n \n     def _prepare_images_structure(\n@@ -137,19 +146,23 @@ def _resize_for_patching(\n \n         return resized_image\n \n+    def _get_padding_size(self, original_resolution: tuple, target_resolution: tuple):\n+        original_height, original_width = original_resolution\n+        target_height, target_width = target_resolution\n+        paste_x, r_x = divmod(target_width - original_width, 2)\n+        paste_y, r_y = divmod(target_height - original_height, 2)\n+        return [paste_x, paste_y, paste_x + r_x, paste_y + r_y]\n+\n     def _pad_for_patching(\n         self, image: \"torch.Tensor\", target_resolution: tuple, input_data_format: ChannelDimension\n     ) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image to a target resolution while maintaining aspect ratio.\n         \"\"\"\n-        target_height, target_width = target_resolution\n-        new_height, new_width = get_patch_output_size(image, target_resolution, input_data_format)\n-\n-        paste_x, r_x = divmod(target_width - new_width, 2)\n-        paste_y, r_y = divmod(target_height - new_height, 2)\n+        new_resolution = get_patch_output_size(image, target_resolution, input_data_format)\n+        padding = self._get_padding_size(new_resolution, target_resolution)\n \n-        padded_image = F.pad(image, padding=[paste_x, paste_y, paste_x + r_x, paste_y + r_y])\n+        padded_image = F.pad(image, padding=padding)\n \n         return padded_image\n \n@@ -234,10 +247,15 @@ def _preprocess(\n         image_mean: Optional[Union[float, List[float]]],\n         image_std: Optional[Union[float, List[float]]],\n         do_pad: bool,\n+        batch_num_images: List[int],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         processed_images = []\n         image_sizes = []\n+\n+        # only single image patching is supported\n+        need_patching = [n == 1 for n in batch_num_images for _ in range(n)]\n+\n         # Determine the size tuple\n         if size and size.height and size.width:\n             size_tuple = (size.height, size.width)\n@@ -252,14 +270,20 @@ def _preprocess(\n         else:\n             patch_size = size.shortest_edge\n \n-        for image in images:\n-            image_patches = self._get_image_patches(\n-                image,\n-                image_grid_pinpoints,\n-                size=size_tuple,\n-                patch_size=patch_size,\n-                interpolation=interpolation,\n-            )\n+        for i, image in enumerate(images):\n+            if need_patching[i]:\n+                image_patches = self._get_image_patches(\n+                    image,\n+                    image_grid_pinpoints,\n+                    size=size_tuple,\n+                    patch_size=patch_size,\n+                    interpolation=interpolation,\n+                )\n+            else:\n+                padded_image = self.pad_to_square(\n+                    images=image, background_color=tuple(int(x * 255) for x in self.image_mean)\n+                )\n+                image_patches = [padded_image]\n \n             # Group images by size for batched processing\n             processed_image_patches_grouped = {}\n@@ -289,8 +313,52 @@ def _preprocess(\n             processed_images = self._pad_for_batching(processed_images)\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(\n-            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes}, tensor_type=return_tensors\n+            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"batch_num_images\": batch_num_images},\n+            tensor_type=return_tensors,\n         )\n \n+    # Copied from transformers.models.llava.image_processing_llava_fast.LlavaImageProcessorFast.pad_to_square\n+    def pad_to_square(\n+        self,\n+        images: \"torch.Tensor\",\n+        background_color: Union[int, Tuple[int, int, int]] = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            images (`np.ndarray`):\n+                The images to pad.\n+            background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+        Returns:\n+            `torch.Tensor`: The padded images.\n+        \"\"\"\n+        height, width = get_image_size(images, ChannelDimension.FIRST)\n+\n+        if height == width:\n+            return images\n+\n+        num_channels = images.shape[1] if len(images.shape) == 4 else images.shape[0]\n+        if isinstance(background_color, int):\n+            background_color = [background_color] + [0] * (num_channels - 1)\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        max_dim = max(height, width)\n+        paste_x_left = (max_dim - width) // 2\n+        paste_y_left = (max_dim - height) // 2\n+        paste_x_right = max_dim - width - paste_x_left\n+        paste_y_right = max_dim - height - paste_y_left\n+        padded_images = F.pad(\n+            images, padding=[paste_x_left, paste_y_left, paste_x_right, paste_y_right], fill=background_color\n+        )\n+\n+        return padded_images\n+\n \n __all__ = [\"LlavaOnevisionImageProcessorFast\"]"
        },
        {
            "sha": "1a60c092ed922c9614f3bb54f78f93be25d3b4cf",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 23,
            "deletions": 14,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -419,8 +419,9 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         image_sizes: torch.Tensor,\n-        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+        batch_num_images: Optional[torch.LongTensor] = None,\n     ):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -430,34 +431,34 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`Union[int, List[int]]`, *optional*):\n+            vision_feature_layer (`Union[int, List[int]]`):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n-            vision_feature_select_strategy (`str`, *optional*):\n+            vision_feature_select_strategy (`str`):\n                 The feature selection strategy used to select the vision feature from the vision backbone.\n                 Can be one of `\"default\"` or `\"full\"`\n+            batch_num_images (`torch.LongTensor`, *optional*):\n+                Number of images in each sample.\n         Returns:\n             image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n             and are of shape `(num_patches, image_length, embed_dim)`).\n         \"\"\"\n-        vision_feature_layer = (\n-            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n-        )\n-        vision_feature_select_strategy = (\n-            vision_feature_select_strategy\n-            if vision_feature_select_strategy is not None\n-            else self.config.vision_feature_select_strategy\n-        )\n-\n         # ! infer image_num_patches from image_sizes\n+        if batch_num_images is None:\n+            # treat this as a single-image case for backward compatibility\n+            need_patching = [True] * len(image_sizes)\n+        else:\n+            need_patching = [n == 1 for n in batch_num_images for _ in range(n)]\n         image_num_patches = [\n             image_size_to_num_patches(\n                 image_size=imsize,\n                 grid_pinpoints=self.config.image_grid_pinpoints,\n                 patch_size=self.config.vision_config.image_size,\n             )\n-            for imsize in image_sizes\n+            if should_patch\n+            else 1\n+            for imsize, should_patch in zip(image_sizes, need_patching)\n         ]\n         if pixel_values.dim() == 5:\n             # stacked if input is (batch_size, num_patches, num_channels, height, width)\n@@ -500,6 +501,7 @@ def forward(\n         vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         vision_aspect_ratio: Optional[str] = None,\n+        batch_num_images: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -520,6 +522,8 @@ def forward(\n             If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        batch_num_images (`torch.LongTensor`, *optional*):\n+            Number of images in each sample.\n         \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -558,6 +562,7 @@ def forward(\n                 image_sizes,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n+                batch_num_images=batch_num_images,\n             )\n             image_features, feature_lens = self.pack_image_features(\n                 image_features,\n@@ -749,6 +754,7 @@ def forward(\n         vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         vision_aspect_ratio: Optional[str] = None,\n+        batch_num_images: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -771,6 +777,8 @@ def forward(\n             If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        batch_num_images (`torch.LongTensor`, *optional*):\n+            Number of images in each sample.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -832,6 +840,7 @@ def forward(\n             vision_aspect_ratio=vision_aspect_ratio,\n             vision_feature_layer=vision_feature_layer,\n             vision_feature_select_strategy=vision_feature_select_strategy,\n+            batch_num_images=batch_num_images,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "f838c5f703f6a52a3f7b0410d573b51cbe6d3518",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 262,
            "deletions": 2,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -28,18 +28,59 @@\n     LlavaNextVideoModelOutputWithPast,\n     LlavaNextVideoPreTrainedModel,\n     get_anyres_image_grid_shape,\n+    image_size_to_num_patches,\n     unpad_image,\n )\n \n-from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import DefaultFastImageProcessorKwargs, group_images_by_shape, reorder_images\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    get_image_size,\n+)\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    can_return_tuple,\n+    is_torchdynamo_compiling,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n \n \n logger = logging.get_logger(__name__)\n \n \n+class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    image_grid_pinpoints (`List[List[int]]`, *optional*):\n+        A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n+        based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n+        method.\n+    do_pad (`bool`, *optional*):\n+        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+    \"\"\"\n+\n+    image_grid_pinpoints: Optional[List[List[int]]]\n+    do_pad: Optional[bool]\n+\n+\n class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):\n     resample = PILImageResampling.BICUBIC\n     image_mean = OPENAI_CLIP_MEAN\n@@ -56,6 +97,147 @@ class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):\n     image_grid_pinpoints = [[384, 384], [384, 768], [384, 1152], [384, 1536], [384, 1920], [384, 2304], [768, 384], [768, 768], [768, 1152], [768, 1536], [768, 1920], [768, 2304], [1152, 384], [1152, 768], [1152, 1152], [1152, 1536], [1152, 1920], [1152, 2304], [1536, 384], [1536, 768], [1536, 1152], [1536, 1536], [1536, 1920], [1536, 2304], [1920, 384], [1920, 768], [1920, 1152], [1920, 1536], [1920, 1920], [1920, 2304], [2304, 384], [2304, 768], [2304, 1152], [2304, 1536], [2304, 1920], [2304, 2304]]  # fmt: skip\n     model_input_names = [\"pixel_values_videos\"]\n \n+    # Copied from transformers.models.llava.image_processing_llava_fast.LlavaImageProcessorFast.pad_to_square\n+    def pad_to_square(\n+        self,\n+        images: \"torch.Tensor\",\n+        background_color: Union[int, Tuple[int, int, int]] = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            images (`np.ndarray`):\n+                The images to pad.\n+            background_color (`int` or `Tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+        Returns:\n+            `torch.Tensor`: The padded images.\n+        \"\"\"\n+        height, width = get_image_size(images, ChannelDimension.FIRST)\n+\n+        if height == width:\n+            return images\n+\n+        num_channels = images.shape[1] if len(images.shape) == 4 else images.shape[0]\n+        if isinstance(background_color, int):\n+            background_color = [background_color] + [0] * (num_channels - 1)\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        max_dim = max(height, width)\n+        paste_x_left = (max_dim - width) // 2\n+        paste_y_left = (max_dim - height) // 2\n+        paste_x_right = max_dim - width - paste_x_left\n+        paste_y_right = max_dim - height - paste_y_left\n+        padded_images = F.pad(\n+            images, padding=[paste_x_left, paste_y_left, paste_x_right, paste_y_right], fill=background_color\n+        )\n+\n+        return padded_images\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]) -> BatchFeature:\n+        if isinstance(images, (tuple, list)) and isinstance(images[0], (tuple, list)):\n+            # if the first element is a list, we assume that all elements are lists\n+            batch_num_images = [len(x) for x in images]\n+        elif isinstance(images, (tuple, list)):\n+            # treat this as a single-image case for backward compatibility\n+            batch_num_images = [1] * len(images)\n+        else:\n+            batch_num_images = [1]\n+        kwargs[\"batch_num_images\"] = batch_num_images\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        image_grid_pinpoints: List[List[int]],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        do_pad: bool,\n+        batch_num_images: List[int],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        processed_images = []\n+        image_sizes = []\n+\n+        # only single image patching is supported\n+        need_patching = [n == 1 for n in batch_num_images for _ in range(n)]\n+\n+        # Determine the size tuple\n+        if size and size.height and size.width:\n+            size_tuple = (size.height, size.width)\n+        else:\n+            size_tuple = (size.shortest_edge, size.shortest_edge)\n+\n+        # Determine the patch size\n+        if crop_size and crop_size.height:\n+            patch_size = crop_size.height\n+        elif size and size.height:\n+            patch_size = size.height\n+        else:\n+            patch_size = size.shortest_edge\n+\n+        for i, image in enumerate(images):\n+            if need_patching[i]:\n+                image_patches = self._get_image_patches(\n+                    image,\n+                    image_grid_pinpoints,\n+                    size=size_tuple,\n+                    patch_size=patch_size,\n+                    interpolation=interpolation,\n+                )\n+            else:\n+                padded_image = self.pad_to_square(\n+                    images=image, background_color=tuple(int(x * 255) for x in self.image_mean)\n+                )\n+                image_patches = [padded_image]\n+\n+            # Group images by size for batched processing\n+            processed_image_patches_grouped = {}\n+            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(image_patches)\n+            for shape, stacked_image_patches in grouped_image_patches.items():\n+                if do_resize:\n+                    stacked_image_patches = self.resize(\n+                        image=stacked_image_patches,\n+                        size=size,\n+                        interpolation=interpolation,\n+                    )\n+                if do_center_crop:\n+                    stacked_image_patches = self.center_crop(stacked_image_patches, crop_size)\n+                # Fused rescale and normalize\n+                stacked_image_patches = self.rescale_and_normalize(\n+                    stacked_image_patches, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+                )\n+                processed_image_patches_grouped[shape] = stacked_image_patches\n+            processed_image_patches = reorder_images(processed_image_patches_grouped, grouped_image_patches_index)\n+            processed_image_patches = (\n+                torch.stack(processed_image_patches, dim=0) if return_tensors else processed_image_patches\n+            )\n+            processed_images.append(processed_image_patches)\n+            image_sizes.append(get_image_size(image, ChannelDimension.FIRST))\n+\n+        if do_pad:\n+            processed_images = self._pad_for_batching(processed_images)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"batch_num_images\": batch_num_images},\n+            tensor_type=return_tensors,\n+        )\n+\n \n class LlavaOnevisionModelOutputWithPast(LlavaNextVideoModelOutputWithPast):\n     pass\n@@ -154,6 +336,76 @@ def apply_pooling(self, image_features):\n         image_features = image_features.view(batch_frames, -1, dim)\n         return image_features\n \n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        image_sizes: torch.Tensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+        batch_num_images: Optional[torch.LongTensor] = None,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_patches, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+            batch_num_images (`torch.LongTensor`, *optional*):\n+                Number of images in each sample.\n+        Returns:\n+            image_features (List[`torch.Tensor`]): List of image feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_patches, image_length, embed_dim)`).\n+        \"\"\"\n+        # ! infer image_num_patches from image_sizes\n+        if batch_num_images is None:\n+            # treat this as a single-image case for backward compatibility\n+            need_patching = [True] * len(image_sizes)\n+        else:\n+            need_patching = [n == 1 for n in batch_num_images for _ in range(n)]\n+        image_num_patches = [\n+            image_size_to_num_patches(\n+                image_size=imsize,\n+                grid_pinpoints=self.config.image_grid_pinpoints,\n+                patch_size=self.config.vision_config.image_size,\n+            )\n+            if should_patch\n+            else 1\n+            for imsize, should_patch in zip(image_sizes, need_patching)\n+        ]\n+        if pixel_values.dim() == 5:\n+            # stacked if input is (batch_size, num_patches, num_channels, height, width)\n+            _pixel_values_list = [pix_val[:num_patch] for pix_val, num_patch in zip(pixel_values, image_num_patches)]\n+            pixel_values = torch.cat(_pixel_values_list, dim=0)\n+        elif pixel_values.dim() != 4:\n+            # otherwise has to be stacked from list of (num_patches, num_channels, height, width)\n+            raise ValueError(f\"pixel_values of shape {pixel_values.shape}, expect to be of 4 or 5 dimensions\")\n+\n+        image_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_image_feature = image_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [image_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_image_feature = torch.cat(hs_pool, dim=-1)\n+\n+        if vision_feature_select_strategy == \"default\":\n+            selected_image_feature = selected_image_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_image_feature = selected_image_feature\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = torch.split(image_features, image_num_patches, dim=0)\n+        return image_features\n+\n     def get_video_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -214,6 +466,7 @@ def forward(\n         vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         vision_aspect_ratio: Optional[str] = None,\n+        batch_num_images: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -234,6 +487,8 @@ def forward(\n             If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        batch_num_images (`torch.LongTensor`, *optional*):\n+            Number of images in each sample.\n         \"\"\"\n \n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -272,6 +527,7 @@ def forward(\n                 image_sizes,\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n+                batch_num_images=batch_num_images,\n             )\n             image_features, feature_lens = self.pack_image_features(\n                 image_features,\n@@ -355,6 +611,7 @@ def forward(\n         vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         vision_aspect_ratio: Optional[str] = None,\n+        batch_num_images: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -377,6 +634,8 @@ def forward(\n             If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        batch_num_images (`torch.LongTensor`, *optional*):\n+            Number of images in each sample.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n@@ -438,6 +697,7 @@ def forward(\n             vision_aspect_ratio=vision_aspect_ratio,\n             vision_feature_layer=vision_feature_layer,\n             vision_feature_select_strategy=vision_feature_select_strategy,\n+            batch_num_images=batch_num_images,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "ca45ed63f391e8718f4ebb6152e157ea67f493c3",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 19,
            "deletions": 10,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -170,12 +170,15 @@ def __call__(\n         if images is not None:\n             image_inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n \n+            batch_num_images = iter(image_inputs[\"batch_num_images\"])\n             image_sizes = iter(image_inputs[\"image_sizes\"])\n             height, width = get_image_size(\n                 to_numpy_array(image_inputs[\"pixel_values\"][0][0]),\n                 channel_dim=output_kwargs[\"images_kwargs\"].get(\"data_format\"),\n             )\n-            text, num_image_tokens = self._expand_image_tokens(text, image_sizes, height, width, self.image_token)\n+            text, num_image_tokens = self._expand_image_tokens(\n+                text, image_sizes, height, width, self.image_token, batch_num_images\n+            )\n \n         if videos is not None:\n             video_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n@@ -205,23 +208,29 @@ def _expand_image_tokens(\n         height: int,\n         width: int,\n         special_token: str,\n-        num_frames: int = 1,\n+        batch_num_images: Iterable[int],\n     ):\n         prompt_strings = []\n         max_num_vision_tokens = 0\n         for sample in text:\n+            if special_token in sample:\n+                is_multi_image = next(batch_num_images) != 1\n+            else:\n+                is_multi_image = False\n             while special_token in sample:\n-                image_size_list = next(image_sizes)\n-                original_size = image_size_list[0] if num_frames != 1 else image_size_list\n-                if not isinstance(original_size, (list, tuple)):\n-                    # cast to list to avoid numerical precision errors when calculating unpadding\n-                    original_size = original_size.tolist()\n-                orig_height, orig_width = original_size\n-                num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n+                if is_multi_image:\n+                    num_image_tokens = self.num_image_tokens + 1  # one for image_newline\n+                else:\n+                    original_size = next(image_sizes)\n+                    if not isinstance(original_size, (list, tuple)):\n+                        # cast to list to avoid numerical precision errors when calculating unpadding\n+                        original_size = original_size.tolist()\n+                    orig_height, orig_width = original_size\n+                    num_image_tokens = self._get_number_of_features(orig_height, orig_width, height, width)\n                 max_num_vision_tokens = max(max_num_vision_tokens, num_image_tokens)\n                 if self.vision_feature_select_strategy == \"default\":\n                     num_image_tokens -= 1\n-                sample = sample.replace(special_token, \"<placeholder>\" * num_image_tokens * num_frames, 1)\n+                sample = sample.replace(special_token, \"<placeholder>\" * num_image_tokens, 1)\n             prompt_strings.append(sample)\n         text = [sample.replace(\"<placeholder>\", special_token) for sample in prompt_strings]\n         return text, max_num_vision_tokens"
        },
        {
            "sha": "4aba232c9dfb65d9daecc38ae9742e885369dbe0",
            "filename": "tests/models/llava_onevision/test_image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 34,
            "deletions": 1,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -202,14 +202,47 @@ def test_nested_input(self):\n             self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n             # Test batched as a nested list of images, where each sublist is one batch\n-            image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n+            image_inputs_nested = [[image_input] for image_input in image_inputs]\n             encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n             expected_output_image_shape = (7, 1522, 3, 20, 20)\n             self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n \n             # Image processor should return same pixel values, independently of input format\n             self.assertTrue((encoded_images_nested == encoded_images).all())\n \n+    def test_multi_images(self):\n+        length = 384\n+        scale_single, scale_multi = 2, 3\n+        image_processor_dict = self.image_processor_tester.prepare_image_processor_dict()\n+        image_processor_dict[\"size\"] = {\"height\": length, \"width\": length}  # patch size\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            # Test batched as a nested list of images, where each sublist is one batch\n+            len_image_1 = length * scale_single\n+            image_inputs_1 = prepare_image_inputs(\n+                batch_size=1,\n+                min_resolution=0,  # not used\n+                max_resolution=len_image_1,\n+                num_channels=3,\n+                equal_resolution=True,\n+            )\n+            len_image_2 = length * scale_multi\n+            image_inputs_2 = prepare_image_inputs(\n+                batch_size=7,\n+                min_resolution=0,  # not used\n+                max_resolution=len_image_2,\n+                num_channels=3,\n+                equal_resolution=True,\n+            )\n+            image_inputs = [image_inputs_1, image_inputs_2]\n+\n+            # Only single image should be patchified\n+            expected_num_patches = scale_single**2 + 1  # +1 for base image patch\n+            expected_output_image_shape = (8, expected_num_patches, 3, length, length)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n     @unittest.skip(\n         reason=\"LlavaOnevisionImageProcessorFast doesn't compile (infinitely) when using class transforms\"\n     )  # FIXME yoni"
        },
        {
            "sha": "53dc267d7789f9857717168890ffd38fdc8fa0dc",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -460,6 +460,33 @@ def test_small_model_integration_test_multi_image(self):\n             EXPECTED_DECODED_TEXT,\n         )\n \n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test_multi_image_nested(self):\n+        # related to (#34585)\n+        model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n+            \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\",\n+            torch_dtype=\"float16\",\n+            device_map=torch_device,\n+        )\n+\n+        url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        prompt = (\n+            \"user\\n<image><image>\\nWhat is the difference between these images?<|im_end|>\\n<|im_start|>assistant\\n\"\n+        )\n+        images_nested = [[self.image, image]]\n+        inputs = self.processor(text=prompt, images=images_nested, return_tensors=\"pt\").to(torch_device, torch.float16)\n+\n+        # verify generation\n+        output = model.generate(**inputs, max_new_tokens=40)\n+        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe first image is a radar chart showing the performance of different models in a specific task, while the second image is a street scene with a stop sign in the foreground.\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n     @slow\n     @require_bitsandbytes\n     def test_small_model_integration_test_multi_video(self):"
        },
        {
            "sha": "12b9531b848757059d9e8ba2d7967037cbd1e4c8",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/101b3fa4ea1cfd66e72a73a0f505108454c24cce/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/101b3fa4ea1cfd66e72a73a0f505108454c24cce/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=101b3fa4ea1cfd66e72a73a0f505108454c24cce",
            "patch": "@@ -233,7 +233,7 @@ def measure_time(image_processor, image):\n             avg_time = sum(sorted(all_times[:3])) / 3.0\n             return avg_time\n \n-        dummy_images = torch.randint(0, 255, (4, 3, 224, 224), dtype=torch.uint8)\n+        dummy_images = [torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8) for _ in range(4)]\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        }
    ],
    "stats": {
        "total": 713,
        "additions": 620,
        "deletions": 93
    }
}