{
    "author": "MekkCyber",
    "message": "[core] Fix torchao  (#42289)\n\n* inital commit\n\n* up\n\n* update unexpected later on\n\n* fix\n\n* update\n\n* simplify our lives\n\n* isolate a bit more\n\n* fixup\n\n* small nits\n\n* style\n\n* nit\n\n* fix common cases\n\n* fix post merge\n\n* bnb needs missing keys\n\n* small fix\n\n* bettrer documentation\n\n* no veradict + base class\n\n* rake review comments\n\n* take all comments\n\n* fix super init\n\n* update doc to be more real\n\n* up\n\n* fix some tests\n\n* weight convertor\n\n* up\n\n* mostly correct\n\n* oups\n\n* skip non linears\n\n* only some tests to go\n\n* need quantization\n\n* fix tests\n\n* rm comment\n\n* revert\n\n* revert 2\n\n* style\n\n* up\n\n* up\n\n* remove unsafe loading path\n\n* fix\n\n* fix\n\n* fix\n\n* up\n\n* rm Dtensor import\n\n* rm replicate import\n\n* fix imports\n\n* up\n\n* minor modifications\n\n* add quantizaton_operation\n\n* delattr\n\n* fix\n\n* fix get_param_buffer\n\n* better to just set module initialized\n\n* rm tie_weights\n\n* guard imports\n\n* up\n\n* rm offloading for now\n\n* add license\n\n* don't guard torch\n\n* comment\n\n* fix\n\n* rm torch.grad\n\n* revert\n\n* fix\n\n* add guard\n\n* add second guard\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\nCo-authored-by: Marc Sun <marc@huggingface.co>",
    "sha": "5169c2364ba1ad2f555b83eac1b9065aa2955101",
    "files": [
        {
            "sha": "9559088a1039c1093488276ddc94fcf2bddb31e4",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 39,
            "deletions": 8,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -115,6 +115,8 @@ def convert(\n         source_keys: list[str],\n         target_keys: list[str],\n         full_layer_name: str,\n+        model,\n+        missing_keys,\n         config,\n         **kwargs,\n     ) -> dict[str, list[torch.Tensor]]:\n@@ -138,6 +140,8 @@ def convert(\n         source_keys: list[str],\n         target_keys: list[str],\n         full_layer_name: str,\n+        model,\n+        missing_keys,\n         config,\n     ) -> dict[str, list[torch.Tensor]]:\n         tensors = next(iter(value.values()))\n@@ -163,6 +167,8 @@ def convert(\n         source_keys: list[str],\n         target_keys: list[str],\n         full_layer_name: str,\n+        model,\n+        missing_keys,\n         config,\n     ) -> dict[str, torch.Tensor]:\n         if len(target_keys) != 1:\n@@ -191,6 +197,8 @@ def convert(\n         source_keys: list[str],\n         target_keys: list[str],\n         full_layer_name: str,\n+        model,\n+        missing_keys,\n         config,\n     ) -> dict[str, torch.Tensor]:\n         merged: dict[str, torch.Tensor] = {}\n@@ -220,6 +228,8 @@ def convert(\n         source_keys: list[str],\n         target_keys: list[str],\n         full_layer_name: str,\n+        model,\n+        missing_keys,\n         config,\n     ) -> dict[str, list[torch.Tensor]]:\n         if len(value) != len(self.sizes):\n@@ -258,6 +268,8 @@ def convert(\n         source_keys: list[str],\n         target_keys: list[str],\n         full_layer_name: str,\n+        model,\n+        missing_keys,\n         config,\n     ) -> dict[str, list[torch.Tensor]]:\n         self.config = config\n@@ -298,21 +310,28 @@ def add_tensor(self, target_key: str, source_key: str, source_pattern: str, futu\n class WeightRenaming(WeightTransform):\n     # Special case of WeightTransform that only renames keys without any conversion.\n \n-    def convert(self, layer_name: str, config=None, quantizer=None, missing_keys: Optional[MutableSet[str]] = None):\n+    def convert(\n+        self,\n+        layer_name: str,\n+        model=None,\n+        config=None,\n+        hf_quantizer=None,\n+        missing_keys: Optional[MutableSet[str]] = None,\n+    ):\n         misc = {}\n         for pattern, futures in self.collected_tensors.items():\n             self.collected_tensors[pattern] = [future.result() for future in futures]\n \n         collected_tensors = self.collected_tensors\n-        if quantizer is not None and self.quantization_operation is not None:\n+        if hf_quantizer is not None and self.quantization_operation is not None:\n             with log_to_misc(layer_name, misc, (self.collected_tensors, layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert(\n                     self.collected_tensors,\n                     source_keys=self.source_keys,\n                     target_keys=self.target_keys,\n                     full_layer_name=layer_name,\n+                    model=model,\n                     config=config,\n-                    quant_config=quantizer.quantization_config,\n                     missing_keys=missing_keys,\n                 )\n \n@@ -332,7 +351,14 @@ def __post_init__(self):\n         if not self.operations:\n             raise ValueError(\"WeightConverter requires at least one operation.\")\n \n-    def convert(self, layer_name: str, config=None, quantizer=None, missing_keys: Optional[MutableSet[str]] = None):\n+    def convert(\n+        self,\n+        layer_name: str,\n+        model=None,\n+        config=None,\n+        hf_quantizer=None,\n+        missing_keys: Optional[MutableSet[str]] = None,\n+    ):\n         misc = {}\n         for pattern, futures in self.collected_tensors.items():\n             self.collected_tensors[pattern] = [future.result() for future in futures]\n@@ -345,17 +371,19 @@ def convert(self, layer_name: str, config=None, quantizer=None, missing_keys: Op\n                     source_keys=self.source_keys,\n                     target_keys=self.target_keys,\n                     full_layer_name=layer_name,\n+                    model=model,\n                     config=config,\n+                    missing_keys=missing_keys,\n                 )\n-        if quantizer is not None and self.quantization_operation is not None:\n+        if hf_quantizer is not None and self.quantization_operation is not None:\n             with log_to_misc(layer_name, misc, (collected_tensors, layer_name), self.quantization_operation):\n                 collected_tensors = self.quantization_operation.convert(\n                     collected_tensors,\n                     source_keys=self.source_keys,\n                     target_keys=self.target_keys,\n                     full_layer_name=layer_name,\n                     config=config,\n-                    quant_config=quantizer.quantization_config,\n+                    model=model,\n                     missing_keys=missing_keys,\n                 )\n         return collected_tensors, misc\n@@ -626,7 +654,6 @@ def convert_and_load_state_dict_in_model(\n     ```\n \n     \"\"\"\n-\n     prefix = model.base_model_prefix\n     tp_plan = tp_plan or {}\n     device_map = device_map or {\"\": \"cpu\"}\n@@ -750,7 +777,11 @@ def convert_and_load_state_dict_in_model(\n             pbar.refresh()\n             try:\n                 realized_value, misc = mapping.convert(\n-                    first_param_name, config=model.config, quantizer=hf_quantizer, missing_keys=missing_keys\n+                    first_param_name,\n+                    model=model,\n+                    config=model.config,\n+                    hf_quantizer=hf_quantizer,\n+                    missing_keys=missing_keys,\n                 )\n                 for target_name, param in realized_value.items():\n                     param = param[0] if isinstance(param, list) else param"
        },
        {
            "sha": "3c1f9fd49c90423edb0e9b5337078bf3bf2b92e7",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -241,7 +241,7 @@ def all_tensors():\n         if name in tied_keys:\n             continue\n         if hf_quantizer is not None:\n-            dtype_size = hf_quantizer.param_element_size(model, name)\n+            dtype_size = hf_quantizer.param_element_size(model, name, param)\n         else:\n             dtype_size = param.element_size()\n         size = param.numel() * dtype_size"
        },
        {
            "sha": "1d08e4a7074e69a14438e093ee0a2401bfd492e2",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -36,7 +36,11 @@ def __init__(self, hf_quantizer):\n         self.hf_quantizer = hf_quantizer\n \n     def convert(\n-        self, input_dict: torch.Tensor, model: Optional[torch.nn.Module] = None, missing_keys=None, **kwargs\n+        self,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        model: Optional[torch.nn.Module] = None,\n+        missing_keys=None,\n+        **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         \"\"\"\n         we need to store some parameters to create the quantized weight. For example, bnb requires 6 values that are stored in the checkpoint to recover the quantized weight. So we store them in a dict that it stored in hf_quantizer for now as we can't save it in the op since we create an op per tensor.\n@@ -59,6 +63,7 @@ def convert(\n             # remove missing keys that were create when initializing Params4bit\n             for key in new_value.quant_state.as_dict(packed=True).keys():\n                 missing_keys.discard(f\"{full_name}.{key}\")\n+            module._is_hf_initialized = True\n             return {target_key: new_value}\n         else:\n             module_name = target_key.rsplit(\".\", 1)[0]\n@@ -77,6 +82,7 @@ def convert(\n                     device=value.device,\n                     module=module,\n                 )\n+                module._is_hf_initialized = True\n                 del self.hf_quantizer.param_quant_stats[module_name]\n                 return {target_key: new_value}\n             return {}\n@@ -87,7 +93,11 @@ def __init__(self, hf_quantizer):\n         self.hf_quantizer = hf_quantizer\n \n     def convert(\n-        self, input_dict: torch.Tensor, model: Optional[torch.nn.Module] = None, missing_keys=None, **kwargs\n+        self,\n+        input_dict: dict[str, list[torch.Tensor]],\n+        model: Optional[torch.nn.Module] = None,\n+        missing_keys=None,\n+        **kwargs,\n     ) -> dict[str, torch.Tensor]:\n         target_key, value = tuple(input_dict.items())[0]\n         value = value[0] if isinstance(value, list) else value"
        },
        {
            "sha": "3c1b99651e4ef215e97538b16190c9769679122a",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 26,
            "deletions": 21,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -20,9 +20,13 @@\n from functools import partial, reduce\n from typing import Optional\n \n-import torch\n-import torch.distributed as dist\n-from torch import nn\n+from ..utils.import_utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.distributed as dist\n+    from torch import nn\n \n from ..distributed import DistributedConfig\n from ..utils import is_torch_greater_or_equal, logging\n@@ -31,12 +35,12 @@\n \n logger = logging.get_logger(__name__)\n \n-# Cache this result has it's a C FFI call which can be pretty time-consuming\n-_torch_distributed_available = torch.distributed.is_available()\n-\n+if is_torch_available():\n+    # Cache this result has it's a C FFI call which can be pretty time-consuming\n+    _torch_distributed_available = torch.distributed.is_available()\n \n-if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-    from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n+    if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n+        from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n \n \n def initialize_tensor_parallelism(\n@@ -169,19 +173,20 @@ def _get_parameter_tp_plan(parameter_name: str, tp_plan: dict[str, str], is_weig\n     return None\n \n \n-str_to_dtype = {\n-    \"BOOL\": torch.bool,\n-    \"U8\": torch.uint8,\n-    \"I8\": torch.int8,\n-    \"I16\": torch.int16,\n-    \"F16\": torch.float16,\n-    \"BF16\": torch.bfloat16,\n-    \"I32\": torch.int32,\n-    \"F32\": torch.float32,\n-    \"F64\": torch.float64,\n-    \"I64\": torch.int64,\n-    \"F8_E4M3\": torch.float8_e4m3fn,\n-}\n+if is_torch_available():\n+    str_to_dtype = {\n+        \"BOOL\": torch.bool,\n+        \"U8\": torch.uint8,\n+        \"I8\": torch.int8,\n+        \"I16\": torch.int16,\n+        \"F16\": torch.float16,\n+        \"BF16\": torch.bfloat16,\n+        \"I32\": torch.int32,\n+        \"F32\": torch.float32,\n+        \"F64\": torch.float64,\n+        \"I64\": torch.int64,\n+        \"F8_E4M3\": torch.float8_e4m3fn,\n+    }\n \n \n def get_packed_weights(param, empty_param, device_mesh, rank, dim):"
        },
        {
            "sha": "3a1fdb0d407e0eefefb1d75a95e60520ea5aa6e6",
            "filename": "src/transformers/integrations/torchao.py",
            "status": "added",
            "additions": 274,
            "deletions": 0,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fintegrations%2Ftorchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftorchao.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -0,0 +1,274 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import importlib.metadata\n+import re\n+import types\n+from typing import Optional\n+\n+import torch\n+from packaging import version\n+\n+from transformers.utils import logging\n+from transformers.utils.import_utils import is_torch_available, is_torchao_available\n+\n+\n+if is_torch_available():\n+    from ..core_model_loading import ConversionOps\n+from ..quantizers.quantizers_utils import get_module_from_name\n+\n+\n+if is_torchao_available():\n+    TORCHAO_VERSION = version.parse(importlib.metadata.version(\"torchao\"))\n+    if version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.14.0\"):\n+        from torchao.prototype.safetensors.safetensors_support import (\n+            unflatten_tensor_state_dict,\n+        )\n+        from torchao.prototype.safetensors.safetensors_utils import is_metadata_torchao\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def fuzzy_match_size(config_name: str) -> Optional[str]:\n+    \"\"\"\n+    Extract the size digit from strings like \"4weight\", \"8weight\".\n+    Returns the digit as an integer if found, otherwise None.\n+    \"\"\"\n+    config_name = config_name.lower()\n+\n+    str_match = re.search(r\"(\\d)weight\", config_name)\n+\n+    if str_match:\n+        return str_match.group(1)\n+\n+    return None\n+\n+\n+def _quantization_type(weight):\n+    from torchao.dtypes import AffineQuantizedTensor\n+    from torchao.quantization.linear_activation_quantized_tensor import LinearActivationQuantizedTensor\n+\n+    if isinstance(weight, AffineQuantizedTensor):\n+        return f\"{weight.__class__.__name__}({weight._quantization_type()})\"\n+\n+    if isinstance(weight, LinearActivationQuantizedTensor):\n+        return f\"{weight.__class__.__name__}(activation={weight.input_quant_func}, weight={_quantization_type(weight.original_weight_tensor)})\"\n+\n+\n+def _linear_extra_repr(self):\n+    weight = _quantization_type(self.weight)\n+    if weight is None:\n+        return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight=None\"\n+    else:\n+        return f\"in_features={self.weight.shape[1]}, out_features={self.weight.shape[0]}, weight={weight}\"\n+\n+\n+class TorchAoQuantize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        full_layer_name: str | None = None,\n+        missing_keys=None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        from torchao.quantization import quantize_\n+\n+        _, value = tuple(input_dict.items())[0]\n+        value = value[0] if isinstance(value, list) else value\n+\n+        module, tensor_name = get_module_from_name(model, full_layer_name)\n+\n+        module._parameters[tensor_name] = torch.nn.Parameter(value, requires_grad=value.requires_grad).to(value.device)\n+        # if we are quantizing tied parameters, to avoid tying the quantized weights\n+        # the correct order to do it is\n+        # 1. load the weight to model\n+        # 2. run tie_weights to populate the weights\n+        # 3. quantize\n+        input_embed = model.get_input_embeddings()\n+        is_embedding_param = id(module) == id(input_embed)\n+        untie_embedding_weights = self.hf_quantizer.quantization_config.untie_embedding_weights\n+\n+        if untie_embedding_weights and is_embedding_param:\n+            setattr(model.config.get_text_config(decoder=True), \"tie_word_embeddings\", False)\n+\n+        # handle FqnToConfig, introduced in torchao 0.15.0+\n+        if self.hf_quantizer.quantization_config._get_ao_version() >= version.Version(\"0.15.0\"):\n+            from torchao.quantization import FqnToConfig\n+\n+            config = self.hf_quantizer.quantization_config.get_apply_tensor_subclass()\n+            if isinstance(config, FqnToConfig):\n+                module_fqn, top_level_param_name = full_layer_name.rsplit(\".\", 1)\n+                c = None\n+                if full_layer_name in config.fqn_to_config:\n+                    assert not module_fqn.startswith(\"re:\"), (\n+                        \"param fqn should not start with`re:`, which is used for specifying regex\"\n+                    )\n+                    c = config.module_fqn_to_config[full_layer_name]\n+                elif module_fqn in config.fqn_to_config:\n+                    assert not module_fqn.startswith(\"re:\"), (\n+                        \"module fqn should not start with`re:`, which is used for specifying regex\"\n+                    )\n+                    c = config.module_fqn_to_config[module_fqn]\n+                # regex match module and param\n+                else:\n+                    for maybe_module_fqn_pattern in config.fqn_to_config:\n+                        # if key doesn't start with re, it is an exact fqn key, so we don't regex match\n+                        if not maybe_module_fqn_pattern.startswith(\"re:\"):\n+                            continue\n+                        # see if param matches first\n+                        elif re.fullmatch(maybe_module_fqn_pattern[3:], full_layer_name):\n+                            c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n+                            break\n+                        elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n+                            # we'll apply the config for first fully matched pattern\n+                            c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n+                            break\n+                    else:\n+                        c = config.module_fqn_to_config.get(\"_default\", None)\n+\n+                if c is not None:\n+                    if top_level_param_name == \"weight\":\n+                        if is_embedding_param and untie_embedding_weights:\n+                            lm_head = module.weight.clone()\n+                        # we can apply the module config directly\n+                        quantize_(module, c, (lambda x, fqn: True))\n+                        missing_keys.discard(full_layer_name)\n+                        module._is_hf_initialized = True\n+                        return {\"lm_head.weight\": lm_head} if is_embedding_param and untie_embedding_weights else {}\n+                    else:\n+                        # need to apply to custom param name\n+                        custom_param_fqn_config = FqnToConfig({top_level_param_name: c})\n+                        quantize_(module, custom_param_fqn_config, filter_fn=None)\n+                        missing_keys.discard(full_layer_name)\n+                        module._is_hf_initialized = True\n+                        return {}\n+                return {full_layer_name: value}\n+\n+        # handle ModuleFqnToConfig, introduced in torchao 0.12.0+\n+        # TODO deprecate this when we deprecate ModuleFqnToConfig\n+        elif self.hf_quantizer.quantization_config._get_ao_version() >= version.Version(\"0.12.0\"):\n+            from torchao.quantization import ModuleFqnToConfig\n+\n+            config = self.hf_quantizer.quantization_config.get_apply_tensor_subclass()\n+            if isinstance(config, ModuleFqnToConfig):\n+                module_fqn, _ = full_layer_name.rsplit(\".\", 1)\n+                c = None\n+                if module_fqn in config.module_fqn_to_config:\n+                    assert not module_fqn.startswith(\"re:\"), (\n+                        \"module fqn should not start with`re:`, which is used for specifying regex\"\n+                    )\n+                    c = config.module_fqn_to_config[module_fqn]\n+                else:\n+                    for maybe_module_fqn_pattern in config.module_fqn_to_config:\n+                        if not maybe_module_fqn_pattern.startswith(\"re:\"):\n+                            continue\n+                        elif re.fullmatch(maybe_module_fqn_pattern[3:], module_fqn):\n+                            # we'll apply the config for first fully matched pattern\n+                            c = config.module_fqn_to_config[maybe_module_fqn_pattern]\n+                            break\n+                    else:\n+                        c = config.module_fqn_to_config.get(\"_default\", None)\n+                if c is not None:\n+                    # filter_fn: not filtering out any modules\n+                    if is_embedding_param and untie_embedding_weights:\n+                        lm_head = module.weight.clone()\n+                    quantize_(module, c, filter_fn=lambda x, fqn: True)\n+                    missing_keys.discard(full_layer_name)\n+                    module._is_hf_initialized = True\n+                    return {\"lm_head.weight\": lm_head} if is_embedding_param and untie_embedding_weights else {}\n+\n+                return {full_layer_name: value}\n+\n+        if is_embedding_param and untie_embedding_weights:\n+            lm_head = module.weight.clone()\n+        quantize_(module, self.hf_quantizer.quantization_config.get_apply_tensor_subclass())\n+        missing_keys.discard(full_layer_name)\n+        module._is_hf_initialized = True\n+        return {\"lm_head.weight\": lm_head} if is_embedding_param and untie_embedding_weights else {}\n+\n+\n+class TorchAoDeserialize(ConversionOps):\n+    def __init__(self, hf_quantizer):\n+        self.hf_quantizer = hf_quantizer\n+\n+    def convert(\n+        self,\n+        input_dict: dict[str, torch.Tensor],\n+        model: Optional[torch.nn.Module] = None,\n+        full_layer_name: str | None = None,\n+        missing_keys=None,\n+        **kwargs,\n+    ) -> dict[str, torch.Tensor]:\n+        if isinstance(self.hf_quantizer.quantization_config.quant_type, str):\n+            is_int_4 = \"int4\" in self.hf_quantizer.quantization_config.quant_type\n+        else:\n+            config_name = self.hf_quantizer.quantization_config.quant_type.__class__.__name__\n+            is_int_4 = fuzzy_match_size(config_name) == \"4\"\n+\n+        # Simple case if we gather layermsnorm weights, we can just return the value since they are not quantized\n+        if \"weight:_data\" in input_dict.keys():\n+            value = (\n+                input_dict[\"weight:_data\"][0]\n+                if isinstance(input_dict[\"weight:_data\"], list)\n+                else input_dict[\"weight:_data\"]\n+            )\n+            return {full_layer_name: value}\n+\n+        is_unsafe_serialization = \":\" not in list(input_dict.keys())[0]\n+\n+        param_data = {}\n+        if is_unsafe_serialization:\n+            if isinstance(input_dict[\"weight\"], list):\n+                weight = input_dict[\"weight\"][0]\n+            else:\n+                weight = input_dict[\"weight\"]\n+        else:\n+            if isinstance(input_dict[\"weight:qdata\"], list):\n+                param_data[f\"{full_layer_name}:qdata\"] = input_dict[\"weight:qdata\"][0]\n+            else:\n+                param_data[f\"{full_layer_name}:qdata\"] = input_dict[\"weight:qdata\"]\n+\n+            if isinstance(input_dict[\"weight:scale\"], list):\n+                param_data[f\"{full_layer_name}:scale\"] = input_dict[\"weight:scale\"][0]\n+            else:\n+                param_data[f\"{full_layer_name}:scale\"] = input_dict[\"weight:scale\"]\n+\n+            if is_int_4:\n+                if isinstance(input_dict[\"weight:zero_point\"], list):\n+                    param_data[f\"{full_layer_name}:zero_point\"] = input_dict[\"weight:zero_point\"][0]\n+                else:\n+                    param_data[f\"{full_layer_name}:zero_point\"] = input_dict[\"weight:zero_point\"]\n+\n+        # If it's a bias, no need to do anything special (except removing the \":_data\" part of the key, but was\n+        # already done) - if it's unsafe-serialized (i.e. not safetensors), not need for anything either\n+        if is_unsafe_serialization:\n+            return {full_layer_name: weight}\n+        # Sanity check for the new serialization format\n+        elif not (TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(self.hf_quantizer.metadata)):\n+            # print(\"metadata\", self.hf_quantizer.metadata)\n+            raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.14.0` installed\")\n+\n+        new_param = unflatten_tensor_state_dict(param_data, self.hf_quantizer.metadata)[full_layer_name]\n+\n+        module, _ = get_module_from_name(model, full_layer_name)\n+        # Add repr to the module\n+        if isinstance(module, torch.nn.Linear):\n+            module.extra_repr = types.MethodType(_linear_extra_repr, module)\n+\n+        return {full_layer_name: new_param}"
        },
        {
            "sha": "748d7af639af07b6979230eed18a79b107342511",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -3889,6 +3889,8 @@ def from_pretrained(\n                 weight_conversions.extend(\n                     [WeightRenaming(source_keys=k, target_keys=v) for k, v in key_mapping.items()]\n                 )\n+            if hf_quantizer is not None:\n+                weight_conversions.extend(hf_quantizer.get_weight_conversions())\n \n         if gguf_file:\n             if hf_quantizer is not None:"
        },
        {
            "sha": "b0a5873da3035ad5c55c6468d920179d1a921b7e",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -165,8 +165,9 @@ def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         \"\"\"\n         return dtype\n \n-    def param_element_size(self, model: \"PreTrainedModel\", param_name: str) -> float:\n+    def param_element_size(self, model: \"PreTrainedModel\", param_name: str, param: \"torch.Tensor\") -> float:\n         \"Return the element size (in bytes) for `param_name`.\"\n+\n         if self.param_needs_quantization(model, param_name):\n             from accelerate.utils import CustomDtype\n \n@@ -179,7 +180,7 @@ def param_element_size(self, model: \"PreTrainedModel\", param_name: str) -> float\n             # The value passed is actually not used when the method is overridden\n             if (custom_dtype := self.adjust_target_dtype(torch.float16)) in mapping:\n                 return mapping[custom_dtype]\n-        return model.get_parameter_or_buffer(param_name).element_size()\n+        return param.element_size()\n \n     def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n         \"\"\"\n@@ -406,6 +407,9 @@ def get_quantize_ops(self):\n             f\"{self.quantization_config.quant_method} is not available yet and will be supported soon.\"\n         )\n \n+    def get_weight_conversions(self):\n+        return []\n+\n \n class SequentialLlama4TextExperts(ModuleList):\n     \"\"\""
        },
        {
            "sha": "777ae193db0dd14c89cca62f92e26a353be5269b",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 53,
            "deletions": 23,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -31,6 +31,10 @@\n from ..utils import is_torch_available, is_torchao_available, logging\n \n \n+if is_torch_available():\n+    from ..core_model_loading import WeightConverter\n+\n+\n if is_torch_available():\n     import torch\n     import torch.nn as nn\n@@ -237,6 +241,8 @@ def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]\n         return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.full_ao_keys)]\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n+        if self.pre_quantized:\n+            return False\n         if self.quantization_config.quant_type == \"autoquant\":\n             return False\n \n@@ -245,30 +251,30 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n             return False\n         elif any(param_name.endswith(f\":{x}\") for x in self.full_ao_keys):\n             return True\n-        else:\n-            # we only quantize the weight of nn.Linear and nn.Embedding\n-            module, tensor_name = get_module_from_name(model, param_name)\n-            _QUANTIZABLE = [torch.nn.Linear]\n-            if self.quantization_config.include_input_output_embeddings:\n-                _QUANTIZABLE.append(torch.nn.Embedding)\n-\n-            # Handle FqnToConfig, introduced in torchao 0.15.0+\n-            if self.quantization_config._get_ao_version() >= version.parse(\"0.15.0\"):\n-                from torchao.quantization import FqnToConfig, fqn_matches_fqn_config\n-\n-                if isinstance(self.quantization_config.quant_type, FqnToConfig):\n-                    module_fqn, param_name_fqn = param_name.rsplit(\".\", 1)\n-                    if (\n-                        fqn_matches_fqn_config(module_fqn, self.quantization_config.quant_type)\n-                        or fqn_matches_fqn_config(param_name, self.quantization_config.quant_type)\n-                        or (\n-                            \"_default\" in self.quantization_config.quant_type.fqn_to_config\n-                            and isinstance(module, tuple(_QUANTIZABLE))\n-                        )\n-                    ):\n-                        return True\n \n-            return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n+        # we only quantize the weight of nn.Linear and nn.Embedding\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        _QUANTIZABLE = [torch.nn.Linear]\n+        if self.quantization_config.include_input_output_embeddings:\n+            _QUANTIZABLE.append(torch.nn.Embedding)\n+\n+        # Handle FqnToConfig, introduced in torchao 0.15.0+\n+        if self.quantization_config._get_ao_version() >= version.parse(\"0.15.0\"):\n+            from torchao.quantization import FqnToConfig, fqn_matches_fqn_config\n+\n+            if isinstance(self.quantization_config.quant_type, FqnToConfig):\n+                module_fqn, param_name_fqn = param_name.rsplit(\".\", 1)\n+                if (\n+                    fqn_matches_fqn_config(module_fqn, self.quantization_config.quant_type)\n+                    or fqn_matches_fqn_config(param_name, self.quantization_config.quant_type)\n+                    or (\n+                        \"_default\" in self.quantization_config.quant_type.fqn_to_config\n+                        and isinstance(module, tuple(_QUANTIZABLE))\n+                    )\n+                ):\n+                    return True\n+\n+        return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n \n     def create_quantized_param(\n         self,\n@@ -530,3 +536,27 @@ def set_metadata(self, checkpoint_files: list[str]):\n                     metadata.update(metadata_)\n             # Save it\n             self.metadata = metadata\n+\n+    def get_quantize_ops(self):\n+        from ..integrations.torchao import TorchAoQuantize\n+\n+        return TorchAoQuantize(self)\n+\n+    def get_weight_conversions(self):\n+        from ..integrations.torchao import TorchAoDeserialize\n+\n+        if self.pre_quantized:\n+            return [\n+                WeightConverter(\n+                    source_keys=[\"weight:qdata\", \"weight:scale\", \"weight:zero_point\"],\n+                    target_keys=\"weight\",\n+                    operations=[TorchAoDeserialize(self)],\n+                ),\n+                WeightConverter(\n+                    source_keys=[\"weight:_data\"],\n+                    target_keys=\"weight\",\n+                    operations=[TorchAoDeserialize(self)],\n+                ),\n+                # used for unsafe serialization\n+            ]\n+        return []"
        },
        {
            "sha": "b05ee61dcb38d1aa6d0731ee9418a8cc6bb57b5b",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -93,8 +93,8 @@ class FP8QuantizerTest(unittest.TestCase):\n         \"model.layers.13\": \"cpu\",\n         \"model.layers.14\": \"cpu\",\n         \"model.layers.15\": \"cpu\",\n-        \"model.rotary_emb\": \"disk\",\n-        \"model.norm\": \"disk\",\n+        \"model.rotary_emb\": \"cpu\",\n+        \"model.norm\": \"cpu\",\n         \"lm_head\": 0,\n     }\n \n@@ -138,7 +138,7 @@ def test_quantized_model_conversion(self):\n         for module in model.modules():\n             if isinstance(module, FP8Linear):\n                 nb_fp8_linear += 1\n-\n+        print(model)\n         self.assertEqual(nb_linears - 1, nb_fp8_linear)\n \n         with init_empty_weights():\n@@ -209,6 +209,7 @@ def test_quantized_model_multi_accelerator(self):\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name, device_map=\"auto\", quantization_config=quantization_config\n         )\n+        print(\"hf_device_map\", quantized_model.hf_device_map)\n         self.assertTrue(set(quantized_model.hf_device_map.values()) == {0, 1})\n \n         output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)"
        },
        {
            "sha": "06c1239ab177c47784cb38321c468a098db06c1a",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5169c2364ba1ad2f555b83eac1b9065aa2955101/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5169c2364ba1ad2f555b83eac1b9065aa2955101/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=5169c2364ba1ad2f555b83eac1b9065aa2955101",
            "patch": "@@ -217,6 +217,7 @@ def test_int8_dynamic_activation_int8_weight_quant(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n \n@@ -249,6 +250,7 @@ def test_include_input_output_embeddings(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         # making sure embedding is quantized\n         self.assertTrue(isinstance(quantized_model.model.embed_tokens.weight, AffineQuantizedTensor))\n@@ -273,6 +275,7 @@ def test_per_module_config_skip(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         # making sure `model.layers.0.self_attn.q_proj` is skipped\n         self.assertTrue(not isinstance(quantized_model.model.layers[0].self_attn.q_proj.weight, AffineQuantizedTensor))\n@@ -296,6 +299,7 @@ def test_module_fqn_to_config_regex_basic(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         # making sure `model.layers.0.self_attn.q_proj` is skipped\n         self.assertTrue(not isinstance(quantized_model.model.layers[0].self_attn.q_proj.weight, AffineQuantizedTensor))\n@@ -329,6 +333,7 @@ def test_module_fqn_to_config_regex_fullmatch(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         # highest precedence is fully specified module fqn\n         self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, Float8Tensor))\n@@ -362,6 +367,7 @@ def test_module_fqn_to_config_regex_precedence(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         # highest precedence is fully specified module fqn\n         self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, Float8Tensor))\n@@ -396,6 +402,7 @@ def test_fqn_to_config_regex_precedence(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, Float8Tensor))\n         self.assertTrue(not isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n@@ -427,6 +434,7 @@ def test_fqn_to_config_param_over_module_regex_precedence(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         self.assertTrue(not isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n         self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.k_proj.weight, AffineQuantizedTensor))\n@@ -457,6 +465,7 @@ def test_fqn_to_config_param_over_module_precedence(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         self.assertTrue(not isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, AffineQuantizedTensor))\n         self.assertTrue(isinstance(quantized_model.model.layers[3].self_attn.k_proj.weight, AffineQuantizedTensor))\n@@ -487,6 +496,7 @@ def test_fqn_to_config_exact_over_regex_precedence(self):\n             self.model_name,\n             device_map=self.device,\n             quantization_config=quant_config,\n+            torch_dtype=torch.bfloat16,\n         )\n         self.assertTrue(not isinstance(quantized_model.model.layers[3].self_attn.q_proj.weight, AffineQuantizedTensor))\n         self.assertTrue(isinstance(quantized_model.model.layers[1].self_attn.q_proj.weight, AffineQuantizedTensor))\n@@ -576,7 +586,7 @@ def test_int4wo_offload(self):\n             \"model.layers.18\": 0,\n             \"model.layers.19\": \"cpu\",\n             \"model.layers.20\": \"cpu\",\n-            \"model.layers.21\": \"disk\",\n+            \"model.layers.21\": \"cpu\",\n             \"model.norm\": 0,\n             \"model.rotary_emb\": 0,\n             \"lm_head\": 0,\n@@ -587,7 +597,7 @@ def test_int4wo_offload(self):\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            dtype=torch.bfloat16,\n+            torch_dtype=torch.bfloat16,\n             device_map=device_map_offload,\n             quantization_config=quant_config,\n         )\n@@ -599,7 +609,7 @@ def test_int4wo_offload(self):\n         EXPECTED_OUTPUTS = Expectations(\n             {\n                 (\"xpu\", 3): \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n-                (\"cuda\", 7): \"What are we having for dinner?\\n- 2. What is the temperature outside\",\n+                (\"cuda\", 7): \"What are we having for dinner?\\n- 1. What is the temperature outside\",\n             }\n         )\n         # fmt: on\n@@ -622,7 +632,7 @@ def test_int4wo_quant_multi_accelerator(self):\n         quant_config = TorchAoConfig(config)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            dtype=torch.bfloat16,\n+            torch_dtype=torch.bfloat16,\n             device_map=\"auto\",\n             quantization_config=quant_config,\n         )\n@@ -643,7 +653,7 @@ def test_autoquant(self):\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            dtype=\"auto\",\n+            torch_dtype=\"auto\",\n             device_map=self.device,\n             quantization_config=quant_config,\n         )\n@@ -712,7 +722,9 @@ def check_serialization_expected_output(self, device, expected_output, safe_seri\n         dtype = torch.bfloat16 if isinstance(self.quant_scheme, Int4WeightOnlyConfig) else \"auto\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname, safe_serialization=safe_serialization)\n-            loaded_quantized_model = AutoModelForCausalLM.from_pretrained(tmpdirname, dtype=dtype, device_map=device)\n+            loaded_quantized_model = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname, dtype=dtype, device_map=device, torch_dtype=dtype, use_safetensors=safe_serialization\n+            )\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(device)\n \n             output = loaded_quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n@@ -729,7 +741,7 @@ class TorchAoSafeSerializationTest(TorchAoSerializationTest):\n     @classmethod\n     def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n-        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n+        cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n         # placeholder\n         cls.quant_scheme = torchao.quantization.Float8WeightOnlyConfig()\n "
        }
    ],
    "stats": {
        "total": 503,
        "additions": 436,
        "deletions": 67
    }
}