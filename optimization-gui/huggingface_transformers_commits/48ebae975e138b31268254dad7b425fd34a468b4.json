{
    "author": "zucchini-nlp",
    "message": "Fix llava image processor (#40588)\n\nfix",
    "sha": "48ebae975e138b31268254dad7b425fd34a468b4",
    "files": [
        {
            "sha": "31ba2764bb2632d1250774878c32d8edad520ead",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/48ebae975e138b31268254dad7b425fd34a468b4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48ebae975e138b31268254dad7b425fd34a468b4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=48ebae975e138b31268254dad7b425fd34a468b4",
            "patch": "@@ -678,7 +678,7 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        new_images = []\n+        processed_images = []\n         image_sizes = [get_image_size(image, channel_dim=input_data_format) for image in images]\n         for image in images:\n             # convert image into a list of patches\n@@ -712,10 +712,10 @@ def preprocess(\n                 input_data_format=input_data_format,\n             )\n             pixel_values = np.array(pixel_values)\n-            new_images.append(pixel_values)\n+            processed_images.append(pixel_values)\n \n         if do_pad:\n-            processed_images = self._pad_for_batching(new_images)\n+            processed_images = self._pad_for_batching(processed_images)\n \n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes}, tensor_type=return_tensors"
        },
        {
            "sha": "9022e00e0221bcf8d4d8d2af936ef0989dda0ccd",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/48ebae975e138b31268254dad7b425fd34a468b4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48ebae975e138b31268254dad7b425fd34a468b4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=48ebae975e138b31268254dad7b425fd34a468b4",
            "patch": "@@ -734,7 +734,7 @@ def preprocess(\n             else (size[\"shortest_edge\"], size[\"shortest_edge\"])\n         )\n \n-        new_images = []\n+        processed_images = []\n         image_sizes = [get_image_size(image, channel_dim=input_data_format) for image in images]\n         for i, image in enumerate(images):\n             if need_patching[i]:\n@@ -772,10 +772,10 @@ def preprocess(\n                 input_data_format=input_data_format,\n             )\n             pixel_values = np.array(pixel_values)\n-            new_images.append(pixel_values)\n+            processed_images.append(pixel_values)\n \n         if do_pad:\n-            processed_images = self._pad_for_batching(new_images)\n+            processed_images = self._pad_for_batching(processed_images)\n \n         return BatchFeature(\n             data={\"pixel_values\": processed_images, \"image_sizes\": image_sizes, \"batch_num_images\": batch_num_images},"
        },
        {
            "sha": "f8e3feb92fa38fa3c54570a1b6173e95ca8c01c6",
            "filename": "tests/models/llava_next/test_image_processing_llava_next.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/48ebae975e138b31268254dad7b425fd34a468b4/tests%2Fmodels%2Fllava_next%2Ftest_image_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48ebae975e138b31268254dad7b425fd34a468b4/tests%2Fmodels%2Fllava_next%2Ftest_image_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_image_processing_llava_next.py?ref=48ebae975e138b31268254dad7b425fd34a468b4",
            "patch": "@@ -265,3 +265,18 @@ def test_pad_for_patching(self):\n                 encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n             )\n             self.assertEqual(encoded_image_shape, image_shape)\n+\n+    def test_call_without_padding(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], do_pad=False).pixel_values\n+            self.assertEqual(len(encoded_images), 1)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, do_pad=False).pixel_values\n+            self.assertEqual(len(encoded_images), len(image_inputs))"
        },
        {
            "sha": "a7a2979aa42c7ad82cdf85dc9dbc8c3122465f38",
            "filename": "tests/models/llava_onevision/test_image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/48ebae975e138b31268254dad7b425fd34a468b4/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48ebae975e138b31268254dad7b425fd34a468b4/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py?ref=48ebae975e138b31268254dad7b425fd34a468b4",
            "patch": "@@ -285,3 +285,18 @@ def test_pad_for_patching(self):\n                 encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n             )\n             self.assertEqual(encoded_image_shape, image_shape)\n+\n+    def test_call_without_padding(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], do_pad=False).pixel_values\n+            self.assertEqual(len(encoded_images), 1)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, do_pad=False).pixel_values\n+            self.assertEqual(len(encoded_images), len(image_inputs))"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 36,
        "deletions": 6
    }
}