{
    "author": "i3hz",
    "message": "Add fast path for bidirectional mask creation to fix regression (#41586)\n\n* fixed performance regression\n\n* also fixed the older_torch function\n\n* Update src/transformers/masking_utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* fix\n\n* more general\n\n* fix slicing\n\n* fix data dependent\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "56a727dde5c29d902d58d984d346affc223ae241",
    "files": [
        {
            "sha": "c0aa98cdce0c27ebbe3ec2808437b9e0ef150b42",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/56a727dde5c29d902d58d984d346affc223ae241/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/56a727dde5c29d902d58d984d346affc223ae241/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=56a727dde5c29d902d58d984d346affc223ae241",
            "patch": "@@ -405,6 +405,16 @@ def sdpa_mask_recent_torch(\n     if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n         return None\n \n+    # vmap can incur performance issues as reported in #41566 for bidirectional mask as we only need to expand the\n+    # padding mask. Thus, we allow early exit here if we do not detect any modification to the base mask function\n+    if mask_function is bidirectional_mask_function:\n+        if padding_mask is not None:\n+            # used for slicing without data-dependent slicing\n+            mask_indices = torch.arange(kv_length, device=cache_position.device) + kv_offset\n+            return padding_mask[:, None, None, mask_indices].expand(-1, -1, q_length, -1)\n+        else:\n+            return torch.ones(batch_size, 1, q_length, kv_length, dtype=torch.bool, device=cache_position.device)\n+\n     # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n     # but without data-dependent slicing (i.e. torch.compile friendly)\n     kv_arange = torch.arange(kv_length, device=cache_position.device)\n@@ -485,6 +495,14 @@ def sdpa_mask_older_torch(\n     if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n         return None\n \n+    # vmap can incur performance issues as reported in #41566 for bidirectional mask as we only need to expand the\n+    # padding mask. Thus, we allow early exit here if we do not detect any modification to the base mask function\n+    if mask_function is bidirectional_mask_function:\n+        if padding_mask is not None:\n+            return padding_mask[:, None, None, :].expand(-1, -1, q_length, -1)\n+        else:\n+            return torch.ones(batch_size, 1, q_length, kv_length, dtype=torch.bool, device=cache_position.device)\n+\n     # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n     # but without data-dependent slicing (i.e. torch.compile friendly)\n     kv_arange = torch.arange(kv_length, device=cache_position.device)"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 18,
        "deletions": 0
    }
}