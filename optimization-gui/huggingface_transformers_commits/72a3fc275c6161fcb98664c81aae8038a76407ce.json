{
    "author": "cyyever",
    "message": "Remove infer_device (#41088)\n\n* Remove infer_device\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix docs using accelerator\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix conflict\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "72a3fc275c6161fcb98664c81aae8038a76407ce",
    "files": [
        {
            "sha": "06c3b8202e4587226e5209286479921b7b8ddad7",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -98,9 +98,10 @@ The example below demonstrates how to create a generation loop with [`DynamicCac\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+from accelerate import Accelerator\n \n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)\n@@ -143,9 +144,10 @@ The generation loop usually takes care of the cache position, but if you're writ\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+from accelerate import Accelerator\n \n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, device_map=device)"
        },
        {
            "sha": "10c17ae67fd4241056b1365acf0eec0a00d178f1",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -32,9 +32,10 @@ Greedy search works well for tasks with relatively short outputs where creativit\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n@@ -54,9 +55,10 @@ Enable multinomial sampling with `do_sample=True` and `num_beams=1`.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n@@ -79,9 +81,10 @@ Enable beam search with the `num_beams` parameter (should be greater than 1 othe\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n inputs = tokenizer(\"Hugging Face is an open-source company\", return_tensors=\"pt\").to(device)\n@@ -166,9 +169,10 @@ Enable prompt lookup decoding with the `prompt_lookup_num_tokens` parameter.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\")\n model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-1.7B\", dtype=torch.float16).to(device)"
        },
        {
            "sha": "4b969825cbb68cf37421d72ecae12909f76b4c51",
            "filename": "docs/source/en/internal/file_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Ffile_utils.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -43,4 +43,3 @@ Most of those are only useful if you are studying the general code in the librar\n ## Other Utilities\n \n [[autodoc]] utils._LazyModule\n-[[autodoc]] pytorch_utils.infer_device"
        },
        {
            "sha": "92f316d63dff172eed211c5ff261ca0e79c0f5ab",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -124,11 +124,12 @@ The example below shows how you can fallback to an offloaded cache if you run ou\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, infer_device\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+from accelerate import Accelerator\n \n def resilient_generate(model, *args, **kwargs):\n     oom = False\n-    device = infer_device()\n+    device = Accelerator().device\n     torch_device_module = getattr(torch, device, torch.cuda)\n     try:\n         return model.generate(*args, **kwargs)"
        },
        {
            "sha": "92961d2de5efbb1ae745e7142e85ffa9dbac1a86",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -114,7 +114,8 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n Another option for using [`StaticCache`] is to pass it to a models forward pass using the same `past_key_values` argument. This allows you to write your own custom decoding function to decode the next token given the current token, position, and cache position of previously generated tokens.\n \n ```py\n-from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging, infer_device\n+from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\n+from accelerate import Accelerator\n from transformers.testing_utils import CaptureLogger\n import torch\n \n@@ -124,7 +125,7 @@ prompts = [\n ]\n \n NUM_TOKENS_TO_GENERATE = 40\n-torch_device = infer_device()\n+torch_device = Accelerator().device\n \n tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token=\"</s>\", padding_side=\"right\")\n model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"sequential\")\n@@ -208,10 +209,11 @@ Enable speculative decoding by loading an assistant model and passing it to [`~G\n <hfoption id=\"greedy search\">\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n@@ -229,10 +231,11 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n For speculative sampling decoding, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n@@ -257,10 +260,11 @@ To enable prompt lookup decoding, specify the number of tokens that should be ov\n <hfoption id=\"greedy decoding\">\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n@@ -278,10 +282,11 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n For prompt lookup decoding with sampling, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)"
        },
        {
            "sha": "88dd020ea1ee4135620e28903f477b764095f45b",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -43,10 +43,11 @@ Bark can be optimized with just a few extra lines of code, which **significantly\n You can speed up inference and reduce memory footprint by 50% simply by loading the model in half-precision.\n \n ```python\n-from transformers import BarkModel, infer_device\n+from transformers import BarkModel\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16).to(device)\n ```\n \n@@ -98,10 +99,11 @@ To put this into perspective, on an NVIDIA A100 and when generating 400 semantic\n You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 all at once.\n \n ```python\n-from transformers import BarkModel, infer_device\n+from transformers import BarkModel\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n \n # load in fp16 and use Flash Attention 2\n model = BarkModel.from_pretrained(\"suno/bark-small\", dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)"
        },
        {
            "sha": "6810ca529a04d5a31394caf22282b473913b98f0",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -107,10 +107,11 @@ import requests\n import torch\n from PIL import Image\n \n-from transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor, infer_device\n+from transformers import BitsAndBytesConfig, ColQwen2ForRetrieval, ColQwen2Processor\n+from accelerate import Accelerator\n \n model_name = \"vidore/colqwen2-v1.0-hf\"\n-device = infer_device()\n+device = Accelerator().device\n \n # 4-bit quantization configuration\n bnb_config = BitsAndBytesConfig("
        },
        {
            "sha": "f1265b701b5bccd28ce0836b632e1866b9e74dbd",
            "filename": "docs/source/en/model_doc/csm.md",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcsm.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -38,10 +38,11 @@ CSM can be used to simply generate speech from a text prompt:\n \n ```python\n import torch\n-from transformers import CsmForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n model_id = \"sesame/csm-1b\"\n-device = infer_device()\n+device = Accelerator().device\n \n # load the model and the processor\n processor = AutoProcessor.from_pretrained(model_id)\n@@ -72,11 +73,12 @@ CSM can be used to generate speech given a conversation, allowing consistency in\n \n ```python\n import torch\n-from transformers import CsmForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n from datasets import load_dataset, Audio\n \n model_id = \"sesame/csm-1b\"\n-device = infer_device()\n+device = Accelerator().device\n \n # load the model and the processor\n processor = AutoProcessor.from_pretrained(model_id)\n@@ -117,11 +119,12 @@ CSM supports batched inference!\n \n ```python\n import torch\n-from transformers import CsmForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n from datasets import load_dataset, Audio\n \n model_id = \"sesame/csm-1b\"\n-device = infer_device()\n+device = Accelerator().device\n \n # load the model and the processor\n processor = AutoProcessor.from_pretrained(model_id)\n@@ -306,11 +309,12 @@ print(\"=\"*50)\n CSM Transformers integration supports training!\n \n ```python\n-from transformers import CsmForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import CsmForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n from datasets import load_dataset, Audio\n \n model_id = \"sesame/csm-1b\"\n-device = infer_device()\n+device = Accelerator().device\n \n # load the model and the processor\n processor = AutoProcessor.from_pretrained(model_id)"
        },
        {
            "sha": "8e9da3d074d21832a201d794d05fa254dacadc62",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -46,9 +46,10 @@ The DepthPro model processes an input image by first downsampling it at multiple\n >>> import requests\n >>> from PIL import Image\n >>> import torch\n->>> from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation, infer_device\n+>>> from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\n+from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n \n >>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n >>> image = Image.open(requests.get(url, stream=True).raw)"
        },
        {
            "sha": "706a7e9c56729403f4a7570e7cf3d3dedfbcdce6",
            "filename": "docs/source/en/model_doc/dia.md",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdia.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -42,9 +42,10 @@ tokens and decodes them back into audio.\n ### Generation with Text\n \n ```python\n-from transformers import AutoProcessor, DiaForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, DiaForConditionalGeneration\n+from accelerate import Accelerator\n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \"nari-labs/Dia-1.6B-0626\"\n \n text = [\"[S1] Dia is an open weights text to dialogue model.\"]\n@@ -64,9 +65,10 @@ processor.save_audio(outputs, \"example.wav\")\n \n ```python\n from datasets import load_dataset, Audio\n-from transformers import AutoProcessor, DiaForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, DiaForConditionalGeneration\n+from accelerate import Accelerator\n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \"nari-labs/Dia-1.6B-0626\"\n \n ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n@@ -91,9 +93,10 @@ processor.save_audio(outputs, \"example_with_audio.wav\")\n \n ```python\n from datasets import load_dataset, Audio\n-from transformers import AutoProcessor, DiaForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, DiaForConditionalGeneration\n+from accelerate import Accelerator\n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \"nari-labs/Dia-1.6B-0626\"\n \n ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")"
        },
        {
            "sha": "666d5b993f6eabc98bea241f89cdafbd9f3cdea7",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -119,14 +119,15 @@ print(answer)\n \n     ```py\n     >>> import re\n-    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel, infer_device\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n+from accelerate import Accelerator\n     >>> from datasets import load_dataset\n     >>> import torch\n \n     >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n     >>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-rvlcdip\")\n \n-    >>> device = infer_device()\n+    >>> device = Accelerator().device\n     >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n     >>> # load document image\n@@ -161,14 +162,15 @@ print(answer)\n \n     ```py\n     >>> import re\n-    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel, infer_device\n+    >>> from transformers import DonutProcessor, VisionEncoderDecoderModel\n+from accelerate import Accelerator\n     >>> from datasets import load_dataset\n     >>> import torch\n \n     >>> processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n     >>> model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n \n-    >>> device = infer_device()\n+    >>> device = Accelerator().device\n     >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n     >>> # load document image"
        },
        {
            "sha": "643ffa15b7fa31290b6f3306059e33cc63e77337",
            "filename": "docs/source/en/model_doc/edgetam.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -61,12 +61,13 @@ EdgeTAM can be used for automatic mask generation to segment all objects in an i\n You can segment objects by providing a single point click on the object you want to segment:\n \n ```python\n->>> from transformers import Sam2Processor, EdgeTamModel, infer_device\n+>>> from transformers import Sam2Processor, EdgeTamModel\n+from accelerate import Accelerator\n >>> import torch\n >>> from PIL import Image\n >>> import requests\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n \n >>> model = EdgeTamModel.from_pretrained(\"yonigozlan/edgetam-1\").to(device)\n >>> processor = Sam2Processor.from_pretrained(\"yonigozlan/edgetam-1\")\n@@ -157,12 +158,13 @@ IoU scores: tensor([0.7616, 0.9465], device='cuda:0')\n Process multiple images simultaneously for improved efficiency:\n \n ```python\n->>> from transformers import Sam2Processor, EdgeTamModel, infer_device\n+>>> from transformers import Sam2Processor, EdgeTamModel\n+from accelerate import Accelerator\n >>> import torch\n >>> from PIL import Image\n >>> import requests\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n \n >>> model = EdgeTamModel.from_pretrained(\"yonigozlan/edgetam-1\").to(device)\n >>> processor = Sam2Processor.from_pretrained(\"yonigozlan/edgetam-1\")"
        },
        {
            "sha": "cd7804321253e711407b125904264b43718be8a4",
            "filename": "docs/source/en/model_doc/edgetam_video.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fedgetam_video.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -51,10 +51,11 @@ EdgeTAM Video's key strength is its ability to track objects across video frames\n #### Basic Video Tracking\n \n ```python\n->>> from transformers import EdgeTamVideoModel, Sam2VideoProcessor, infer_device\n+>>> from transformers import EdgeTamVideoModel, Sam2VideoProcessor\n+from accelerate import Accelerator\n >>> import torch\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = EdgeTamVideoModel.from_pretrained(\"yonigozlan/edgetam-video-1\").to(device, dtype=torch.bfloat16)\n >>> processor = Sam2VideoProcessor.from_pretrained(\"yonigozlan/edgetam-video-1\")\n "
        },
        {
            "sha": "d09e44c6f3af7a44b8d1d585c40eb1d8b3b9724a",
            "filename": "docs/source/en/model_doc/glm.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -60,8 +60,9 @@ Tips:\n In the following, we demonstrate how to use `glm-4-9b-chat` for the inference. Note that we have used the ChatML format for dialog, in this demo we show how to leverage `apply_chat_template` for this purpose.\n \n ```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n->>> device = infer_device() # the device to load the model onto\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n+>>> device = Accelerator().device # the device to load the model onto\n \n >>> model = AutoModelForCausalLM.from_pretrained(\"THUDM/glm-4-9b-chat\", device_map=\"auto\", trust_remote_code=True)\n >>> tokenizer = AutoTokenizer.from_pretrained(\"THUDM/glm-4-9b-chat\")"
        },
        {
            "sha": "874b71435b482b9a38384bfa069dd5c23cf170b4",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -132,10 +132,11 @@ Using GLM-4.1V with video input is similar to using it with image input.\n The model can process video data and generate text based on the content of the video.\n \n ```python\n-from transformers import AutoProcessor, Glm4vForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, Glm4vForConditionalGeneration\n+from accelerate import Accelerator\n import torch\n \n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n processor = AutoProcessor.from_pretrained(\"THUDM/GLM-4.1V-9B-Thinking\")\n model = Glm4vForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "3c4225ccaef712151d480a54b579c66c1b69f2e9",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 19,
            "deletions": 13,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -48,9 +48,10 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -73,9 +74,10 @@ The original code can be found [here](https://github.com/Ucas-HaoranWei/GOT-OCR2\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -102,9 +104,10 @@ GOT-OCR2 can also generate formatted text, such as markdown or LaTeX. Here is an\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -130,9 +133,10 @@ Here is an example of how to process multiple pages at once:\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -159,9 +163,10 @@ Here is an example of how to process cropped patches:\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", dtype=torch.bfloat16, device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -188,7 +193,7 @@ GOT supports interactive OCR, where the user can specify the region to be recogn\n >>> import torch\n >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n \n@@ -214,10 +219,11 @@ Here is an example of how to process sheet music:\n \n ```python\n >>> import torch\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n >>> import verovio\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = AutoModelForImageTextToText.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", device_map=device)\n >>> processor = AutoProcessor.from_pretrained(\"stepfun-ai/GOT-OCR-2.0-hf\", use_fast=True)\n "
        },
        {
            "sha": "d3e481d8b277e4bba6940f937140e80968088802",
            "filename": "docs/source/en/model_doc/gpt_bigcode.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_bigcode.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -64,8 +64,9 @@ To load and run a model using Flash Attention 2, refer to the snippet below:\n \n ```python\n >>> import torch\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n->>> device = infer_device() # the device to load the model onto\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n+>>> device = Accelerator().device # the device to load the model onto\n \n >>> model = AutoModelForCausalLM.from_pretrained(\"bigcode/gpt_bigcode-santacoder\", dtype=torch.float16, attn_implementation=\"flash_attention_2\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"bigcode/gpt_bigcode-santacoder\")"
        },
        {
            "sha": "ac71338b3368970e8d18c994918bce2a46eb6c41",
            "filename": "docs/source/en/model_doc/gptj.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptj.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -38,10 +38,11 @@ This model was contributed by [Stella Biderman](https://huggingface.co/stellaath\n   which could be used to further minimize the RAM usage:\n \n ```python\n->>> from transformers import GPTJForCausalLM, infer_device\n+>>> from transformers import GPTJForCausalLM\n+from accelerate import Accelerator\n >>> import torch\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = GPTJForCausalLM.from_pretrained(\n ...     \"EleutherAI/gpt-j-6B\",\n ...     revision=\"float16\",\n@@ -93,10 +94,11 @@ model.\n ...or in float16 precision:\n \n ```python\n->>> from transformers import GPTJForCausalLM, AutoTokenizer, infer_device\n+>>> from transformers import GPTJForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n >>> import torch\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", dtype=torch.float16).to(device)\n >>> tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n "
        },
        {
            "sha": "7281dd3da7040808fbc5ff7b08ad48d8fe8f87a5",
            "filename": "docs/source/en/model_doc/gptsan-japanese.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -42,10 +42,11 @@ fine-tune for translation or summarization.\n The `generate()` method can be used to generate text using GPTSAN-Japanese model.\n \n ```python\n->>> from transformers import AutoModel, AutoTokenizer, infer_device\n+>>> from transformers import AutoModel, AutoTokenizer\n+from accelerate import Accelerator\n >>> import torch\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n >>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n >>> x_tok = tokenizer(\"は、\", prefix_text=\"織田信長\", return_tensors=\"pt\")"
        },
        {
            "sha": "17b499215def5e1e8f8f6da56a35c6a3790040ec",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -34,9 +34,10 @@ Tips:\n Sample inference:\n \n ```python\n-from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration, infer_device\n+from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n model_path = \"ibm-granite/granite-vision-3.1-2b-preview\"\n processor = LlavaNextProcessor.from_pretrained(model_path)"
        },
        {
            "sha": "fd2c1d5092f160bcef930700cad71c538f58e6f9",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -50,10 +50,11 @@ Here's how to use the model for zero-shot object detection:\n \n >>> import torch\n >>> from PIL import Image\n->>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection, infer_device\n+>>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n+from accelerate import Accelerator\n \n >>> model_id = \"IDEA-Research/grounding-dino-tiny\"\n->>> device = infer_device()\n+>>> device = Accelerator().device\n \n >>> processor = AutoProcessor.from_pretrained(model_id)\n >>> model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)"
        },
        {
            "sha": "4647fa797d5a2cef3b86158e80a186c199bcab7c",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -57,10 +57,11 @@ Example of how to use the processor on chat messages:\n ```python\n import requests\n from PIL import Image\n-from transformers import Idefics2Processor, Idefics2ForConditionalGeneration, infer_device\n+from transformers import Idefics2Processor, Idefics2ForConditionalGeneration\n+from accelerate import Accelerator\n import torch\n \n-device = infer_device()\n+device = Accelerator().device\n \n url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n url_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n@@ -99,7 +100,8 @@ print(\"Generated text:\", generated_text)\n ```python\n import requests\n from PIL import Image\n-from transformers import Idefics2Processor, Idefics2ForConditionalGeneration, infer_device\n+from transformers import Idefics2Processor, Idefics2ForConditionalGeneration\n+from accelerate import Accelerator\n import torch\n \n url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n@@ -124,7 +126,7 @@ messages = [{\n     ],\n }]\n \n-device = infer_device()\n+device = Accelerator().device\n \n processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n model = Idefics2ForConditionalGeneration.from_pretrained(\"HuggingFaceM4/idefics2-8b\")"
        },
        {
            "sha": "7fce9eaf8b15b4d0339d453afc8a19bf57dcaa8c",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -45,7 +45,8 @@ import re\n import torch\n import requests\n from PIL import Image, ImageDraw\n-from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n+from accelerate import Accelerator\n \n repo = \"microsoft/kosmos-2.5\"\n device = \"cuda:0\"\n@@ -84,7 +85,8 @@ import re\n import torch\n import requests\n from PIL import Image, ImageDraw\n-from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, Kosmos2_5ForConditionalGeneration\n+from accelerate import Accelerator\n \n repo = \"microsoft/kosmos-2.5\"\n device = \"cuda:0\""
        },
        {
            "sha": "8dc3f554cfae3b7e5e233df5acd3a5ba3148efce",
            "filename": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -34,10 +34,11 @@ rendered properly in your Markdown viewer.\n ```python\n import torch\n from datasets import load_dataset, Audio\n-from transformers import infer_device, KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+from accelerate import Accelerator\n \n # 1. load the model and the processor\n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_id = \"kyutai/stt-2.6b-en-trfs\"\n \n processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n@@ -67,10 +68,11 @@ print(processor.batch_decode(output_tokens, skip_special_tokens=True))\n ```python\n import torch\n from datasets import load_dataset, Audio\n-from transformers import infer_device, KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+from accelerate import Accelerator\n \n # 1. load the model and the processor\n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_id = \"kyutai/stt-2.6b-en-trfs\"\n \n processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)"
        },
        {
            "sha": "38940d853ca45a7804d4564315d6c7027b3c26b8",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -205,7 +205,8 @@ We will work to enable running with `device_map=\"auto\"` and flex-attention witho\n tensor-parallel in the future.\n \n ```py\n-from transformers import Llama4ForConditionalGeneration, AutoTokenizer, infer_device\n+from transformers import Llama4ForConditionalGeneration, AutoTokenizer\n+from accelerate import Accelerator\n import torch\n import time\n \n@@ -228,7 +229,7 @@ messages = [\n ]\n input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\")\n \n-device = infer_device()\n+device = Accelerator().device\n torch_device_module = getattr(torch, device, torch.cuda)\n torch_device_module.synchronize()\n start = time.time()"
        },
        {
            "sha": "e07f54727161ad20f2f9d96e9b99f7c0910041a6",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -73,9 +73,10 @@ pipeline(text=messages, max_new_tokens=20, return_full_text=False)\n import torch\n import requests\n from PIL import Image\n-from transformers import AutoProcessor, LlavaNextForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, LlavaNextForConditionalGeneration\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", dtype=torch.float16).to(device)"
        },
        {
            "sha": "fe883f8dd694a11aabca37677d466c794c6776b2",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -112,10 +112,11 @@ The original code can be found [here](https://github.com/LLaVA-VL/LLaVA-NeXT/tre\n Here's how to load the model and perform inference in half-precision (`torch.float16`):\n \n ```python\n-from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration, infer_device\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+from accelerate import Accelerator\n import torch\n \n-device = f\"{infer_device}:0\"\n+device = Accelerator().device\n \n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\") \n model = LlavaOnevisionForConditionalGeneration.from_pretrained("
        },
        {
            "sha": "774fda68852edc37eb62e303cb12bd0d178409cb",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -67,9 +67,10 @@ outputs[0][\"generated_text\"]\n \n ```py\n import torch\n-from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device \n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator \n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(\n@@ -110,9 +111,10 @@ decoded_output\n \n ```py\n import torch\n-from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, dtype=torch.bfloat16)\n@@ -150,9 +152,10 @@ print(decoded_output)\n \n ```py\n import torch\n-from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, dtype=torch.bfloat16)\n@@ -193,9 +196,10 @@ messages = [\n \n ```py\n import torch\n-from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, infer_device\n+from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n+from accelerate import Accelerator\n \n-torch_device = infer_device()\n+torch_device = Accelerator().device\n model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n processor = AutoProcessor.from_pretrained(model_checkpoint)\n quantization_config = BitsAndBytesConfig(load_in_4bit=True)"
        },
        {
            "sha": "63820635e9ba5d6b5f57f5af9f5f03a57fbd78e1",
            "filename": "docs/source/en/model_doc/mm-grounding-dino.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmm-grounding-dino.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -39,13 +39,14 @@ The example below demonstrates how to generate text based on an image with the [\n \n ```py\n import torch\n-from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor, infer_device\n+from transformers import AutoModelForZeroShotObjectDetection, AutoProcessor\n+from accelerate import Accelerator\n from transformers.image_utils import load_image\n \n \n # Prepare processor and model\n model_id = \"openmmlab-community/mm_grounding_dino_tiny_o365v1_goldg_v3det\"\n-device = infer_device()\n+device = Accelerator().device\n processor = AutoProcessor.from_pretrained(model_id)\n model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n "
        },
        {
            "sha": "912d4e729cdbc02a0a604bb53bb56bf6445293d7",
            "filename": "docs/source/en/model_doc/moshi.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmoshi.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -115,13 +115,14 @@ To follow the example of the following image, `\"Hello, I'm Moshi\"` could be tran\n ```python\n >>> from datasets import load_dataset, Audio\n >>> import torch, math\n->>> from transformers import MoshiForConditionalGeneration, AutoFeatureExtractor, AutoTokenizer, infer_device\n+>>> from transformers import MoshiForConditionalGeneration, AutoFeatureExtractor, AutoTokenizer\n+from accelerate import Accelerator\n \n \n >>> librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n >>> feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/moshiko-pytorch-bf16\")\n >>> tokenizer = AutoTokenizer.from_pretrained(\"kyutai/moshiko-pytorch-bf16\")\n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> dtype = torch.bfloat16\n \n >>> # prepare user input audio "
        },
        {
            "sha": "c31aafd62676c6a15c5e0e8890b5e805d6fcda93",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -57,13 +57,14 @@ The following code provides an example of how to load the Minitron-4B model and\n \n ```python\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, infer_device\n+from transformers import AutoTokenizer, AutoModelForCausalLM\n+from accelerate import Accelerator\n \n # Load the tokenizer and model\n model_path = 'nvidia/Minitron-4B-Base'\n tokenizer  = AutoTokenizer.from_pretrained(model_path)\n \n-device = infer_device()\n+device = Accelerator().device\n dtype  = torch.bfloat16\n model  = AutoModelForCausalLM.from_pretrained(model_path, dtype=dtype, device_map=device)\n "
        },
        {
            "sha": "4025fac002c333c27d1370cb1926495342166e2c",
            "filename": "docs/source/en/model_doc/nougat.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnougat.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -62,14 +62,15 @@ into a single instance to both extract the input features and decode the predict\n >>> import re\n >>> from PIL import Image\n \n->>> from transformers import NougatProcessor, VisionEncoderDecoderModel, infer_device\n+>>> from transformers import NougatProcessor, VisionEncoderDecoderModel\n+from accelerate import Accelerator\n >>> from datasets import load_dataset\n >>> import torch\n \n >>> processor = NougatProcessor.from_pretrained(\"facebook/nougat-base\")\n >>> model = VisionEncoderDecoderModel.from_pretrained(\"facebook/nougat-base\")\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model.to(device)  # doctest: +IGNORE_RESULT\n \n >>> # prepare PDF image for the model"
        },
        {
            "sha": "c933d3a02bec12a6a98ace99dae56cfa7cad02c3",
            "filename": "docs/source/en/model_doc/olmoe.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmoe.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -59,9 +59,10 @@ print(result)\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n model = AutoModelForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", attn_implementation=\"sdpa\", dtype=\"auto\", device_map=\"auto\").to(device)\n tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n@@ -79,9 +80,10 @@ The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quan\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,"
        },
        {
            "sha": "7391ecff97a327582a919bbae3e72de3f5e52724",
            "filename": "docs/source/en/model_doc/opt.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fopt.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -81,9 +81,10 @@ The example below uses [bitsandbytes](..quantization/bitsandbytes) to quantize t\n \n ```py\n import torch\n-from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM, infer_device\n+from transformers import BitsAndBytesConfig, AutoTokenizer, AutoModelForCausalLM\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-13b\", dtype=torch.float16, attn_implementation=\"sdpa\", quantization_config=bnb_config).to(device)"
        },
        {
            "sha": "2785d43550bad3df7cb73cc28a16938f881dbcaf",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -39,9 +39,10 @@ import torch\n from torchvision import io\n from typing import Dict\n from transformers.image_utils import load_images, load_video\n-from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor, infer_device\n+from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n+from accelerate import Accelerator\n \n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n model = AutoModelForVision2Seq.from_pretrained(\n     \"thisisiron/Ovis2-2B-hf\","
        },
        {
            "sha": "c7bc70086df6e34c4c27dd493ecd68b18b4076ca",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -47,10 +47,11 @@ print(result[0]['generated_text'])\n \n ```python\n import torch\n-from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, infer_device\n+from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n+from accelerate import Accelerator\n \n model_path = \"microsoft/Phi-4-multimodal-instruct\"\n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n processor = AutoProcessor.from_pretrained(model_path)\n model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, dtype=torch.float16)\n@@ -97,10 +98,11 @@ The example below demonstrates inference with an audio and text input.\n \n ```py\n import torch\n-from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig, infer_device\n+from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n+from accelerate import Accelerator\n \n model_path = \"microsoft/Phi-4-multimodal-instruct\"\n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n processor = AutoProcessor.from_pretrained(model_path)\n model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device,  dtype=torch.float16)"
        },
        {
            "sha": "4dfb7c10bcf953c9d8420938bb3e007078521646",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -103,8 +103,8 @@ tensors. After setting up the tensor quantizers, one can use the following examp\n ...         module.enable_quant()\n \n >>> # If running on accelerator, it needs to call `.to(xx)` again because new tensors will be created by calibration process\n->>> from transformers import infer_device\n->>> device = infer_device()\n+>>> from accelerate import Accelerator\n+>>> device = Accelerator().device\n >>> model.to(device)\n \n >>> # Keep running the quantized model"
        },
        {
            "sha": "8361144e262ff7c4528a01b4628c11098d6909c0",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -50,9 +50,10 @@ Below is an example on how to run mask generation given an image and a 2D point:\n import torch\n from PIL import Image\n import requests\n-from transformers import SamModel, SamProcessor, infer_device\n+from transformers import SamModel, SamProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n \n@@ -76,9 +77,10 @@ You can also process your own masks alongside the input images in the processor\n import torch\n from PIL import Image\n import requests\n-from transformers import SamModel, SamProcessor, infer_device\n+from transformers import SamModel, SamProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n model = SamModel.from_pretrained(\"facebook/sam-vit-huge\").to(device)\n processor = SamProcessor.from_pretrained(\"facebook/sam-vit-huge\")\n "
        },
        {
            "sha": "f0363616867a7b764fa4f606fd91020d0a3bcdf3",
            "filename": "docs/source/en/model_doc/sam2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -69,12 +69,13 @@ SAM2 can be used for automatic mask generation to segment all objects in an imag\n You can segment objects by providing a single point click on the object you want to segment:\n \n ```python\n->>> from transformers import Sam2Processor, Sam2Model, infer_device\n+>>> from transformers import Sam2Processor, Sam2Model\n+from accelerate import Accelerator\n >>> import torch\n >>> from PIL import Image\n >>> import requests\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n \n >>> model = Sam2Model.from_pretrained(\"facebook/sam2.1-hiera-large\").to(device)\n >>> processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-large\")\n@@ -157,12 +158,13 @@ Generated masks for 2 objects\n Process multiple images simultaneously for improved efficiency:\n \n ```python\n->>> from transformers import Sam2Processor, Sam2Model, infer_device\n+>>> from transformers import Sam2Processor, Sam2Model\n+from accelerate import Accelerator\n >>> import torch\n >>> from PIL import Image\n >>> import requests\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n \n >>> model = Sam2Model.from_pretrained(\"facebook/sam2.1-hiera-large\").to(device)\n >>> processor = Sam2Processor.from_pretrained(\"facebook/sam2.1-hiera-large\")"
        },
        {
            "sha": "b55daf14125053ec74259a82de759f63cc5cea6b",
            "filename": "docs/source/en/model_doc/sam2_video.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam2_video.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -54,10 +54,11 @@ SAM2's key strength is its ability to track objects across video frames. Here's\n #### Basic Video Tracking\n \n ```python\n->>> from transformers import Sam2VideoModel, Sam2VideoProcessor, infer_device\n+>>> from transformers import Sam2VideoModel, Sam2VideoProcessor\n+from accelerate import Accelerator\n >>> import torch\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model = Sam2VideoModel.from_pretrained(\"facebook/sam2.1-hiera-tiny\").to(device, dtype=torch.bfloat16)\n >>> processor = Sam2VideoProcessor.from_pretrained(\"facebook/sam2.1-hiera-tiny\")\n "
        },
        {
            "sha": "394beb9b3e4344cb8612665641ee433e751889ba",
            "filename": "docs/source/en/model_doc/sam_hq.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -56,9 +56,10 @@ Below is an example on how to run mask generation given an image and a 2D point:\n import torch\n from PIL import Image\n import requests\n-from transformers import infer_device, SamHQModel, SamHQProcessor\n+from transformers import SamHQModel, SamHQProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n model = SamHQModel.from_pretrained(\"syscv-community/sam-hq-vit-base\").to(device)\n processor = SamHQProcessor.from_pretrained(\"syscv-community/sam-hq-vit-base\")\n \n@@ -82,9 +83,10 @@ You can also process your own masks alongside the input images in the processor\n import torch\n from PIL import Image\n import requests\n-from transformers import infer_device, SamHQModel, SamHQProcessor\n+from transformers import SamHQModel, SamHQProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n model = SamHQModel.from_pretrained(\"syscv-community/sam-hq-vit-base\").to(device)\n processor = SamHQProcessor.from_pretrained(\"syscv-community/sam-hq-vit-base\")\n "
        },
        {
            "sha": "0ed660ebae075ed5950c81cadbb0c529df9ca0e0",
            "filename": "docs/source/en/model_doc/stablelm.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstablelm.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -44,8 +44,9 @@ We also provide StableLM Zephyr 3B, an instruction fine-tuned version of the mod\n The following code snippet demonstrates how to use `StableLM 3B 4E1T` for inference:\n \n ```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device, set_seed\n->>> device = infer_device() # the device to load the model onto\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator, set_seed\n+>>> device = Accelerator().device # the device to load the model onto\n \n >>> set_seed(0)\n \n@@ -75,8 +76,9 @@ Now, to run the model with Flash Attention 2, refer to the snippet below:\n \n ```python\n >>> import torch\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device, set_seed\n->>> device = infer_device() # the device to load the model onto\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator, set_seed\n+>>> device = Accelerator().device # the device to load the model onto\n \n >>> set_seed(0)\n "
        },
        {
            "sha": "3cddb6bffc781c1624d726b9599f6ec5a37016c9",
            "filename": "docs/source/en/model_doc/swin.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -67,7 +67,7 @@ model = AutoModelForImageClassification.from_pretrained(\n     device_map=\"auto\"\n )\n \n-device = infer_device()\n+device = Accelerator().device\n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = Image.open(requests.get(url, stream=True).raw)\n inputs = image_processor(image, return_tensors=\"pt\").to(model.device)"
        },
        {
            "sha": "34571e350f36b46ea45b83f44f00adaab7ba86be",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -44,9 +44,10 @@ The example below demonstrates how to reconstruct the missing pixels with the [`\n import torch\n import requests\n from PIL import Image\n-from transformers import infer_device, ViTImageProcessor, ViTMAEForPreTraining\n+from transformers import ViTImageProcessor, ViTMAEForPreTraining\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n image = Image.open(requests.get(url, stream=True).raw)"
        },
        {
            "sha": "209af974cd74ec65590016b97d18ea5d9057b9e4",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -36,9 +36,10 @@ import requests\n import numpy as np\n import supervision as sv\n from PIL import Image\n-from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation, infer_device\n+from transformers import AutoProcessor, RTDetrForObjectDetection, VitPoseForPoseEstimation\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n url = \"https://www.fcbarcelona.com/fcbarcelona/photo/2021/01/31/3c55a19f-dfc1-4451-885e-afd14e890a11/mini_2021-01-31-BARCELONA-ATHLETIC-BILBAOI-30.JPG\"\n image = Image.open(requests.get(url, stream=True).raw)\n@@ -162,9 +163,10 @@ image_pose_result = pose_results[0]\n - ViTPose++ has 6 different MoE expert heads (COCO validation `0`, AiC `1`, MPII `2`, AP-10K `3`, APT-36K `4`, COCO-WholeBody `5`) which supports 6 different datasets. Pass a specific value corresponding to the dataset to the `dataset_index` to indicate which expert to use.\n \n     ```py\n-    from transformers import AutoProcessor, VitPoseForPoseEstimation, infer_device\n+    from transformers import AutoProcessor, VitPoseForPoseEstimation\n+from accelerate import Accelerator\n \n-    device = infer_device()\n+    device = Accelerator().device\n \n     image_processor = AutoProcessor.from_pretrained(\"usyd-community/vitpose-plus-base\")\n     model = VitPoseForPoseEstimation.from_pretrained(\"usyd-community/vitpose-plus-base\", device=device)"
        },
        {
            "sha": "14c6bf0fd5e266365912778b818bf1952b49c5cb",
            "filename": "docs/source/en/model_doc/vjepa2.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -74,9 +74,10 @@ import torch\n import numpy as np\n \n from torchcodec.decoders import VideoDecoder\n-from transformers import AutoVideoProcessor, AutoModelForVideoClassification, infer_device\n+from transformers import AutoVideoProcessor, AutoModelForVideoClassification\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n # Load model and video preprocessor\n hf_repo = \"facebook/vjepa2-vitl-fpc16-256-ssv2\""
        },
        {
            "sha": "f83f5ea91c0ac46f2a126ca3b1ae7493c1d34048",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 21,
            "deletions": 14,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -47,9 +47,10 @@ The model supports audio-text instructions, including multi-turn and multi-audio\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -84,9 +85,10 @@ print(\"=\" * 80)\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -125,9 +127,10 @@ print(\"=\" * 80)\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -180,9 +183,10 @@ print(\"=\" * 80)\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -216,9 +220,10 @@ print(\"=\" * 80)\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -252,9 +257,10 @@ print(\"=\" * 80)\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device()\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)\n@@ -313,9 +319,10 @@ Use the model to transcribe audio (supports English, Spanish, French, Portuguese\n \n ```python\n import torch\n-from transformers import VoxtralForConditionalGeneration, AutoProcessor, infer_device\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n \n processor = AutoProcessor.from_pretrained(repo_id)"
        },
        {
            "sha": "16de1b9b9b665e49b638bd0deff1ead06f8191ed",
            "filename": "docs/source/en/model_doc/wav2vec2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -151,12 +151,13 @@ Otherwise, [`~Wav2Vec2ProcessorWithLM.batch_decode`] performance will be slower\n ```python\n >>> # Let's see how to use a user-managed pool for batch decoding multiple audios\n >>> from multiprocessing import get_context\n->>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC, infer_device\n+>>> from transformers import AutoTokenizer, AutoProcessor, AutoModelForCTC\n+from accelerate import Accelerator\n >>> from datasets import load_dataset\n >>> import datasets\n >>> import torch\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> # import model, feature extractor, tokenizer\n >>> model = AutoModelForCTC.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\").to(device)\n >>> processor = AutoProcessor.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")\n@@ -176,7 +177,7 @@ Otherwise, [`~Wav2Vec2ProcessorWithLM.batch_decode`] performance will be slower\n \n \n >>> def map_to_pred(batch, pool):\n-...     device = infer_device()\n+...     device = Accelerator().device\n ...     inputs = processor(batch[\"speech\"], sampling_rate=16_000, padding=True, return_tensors=\"pt\")\n ...     inputs = {k: v.to(device) for k, v in inputs.items()}\n "
        },
        {
            "sha": "1f87fdc64466a2c72c90f7fbd45692ea3e6eef5c",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -61,9 +61,10 @@ detector(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.pn\n import torch\n from PIL import Image\n import requests\n-from transformers import AutoImageProcessor, AutoModelForObjectDetection, infer_device\n+from transformers import AutoImageProcessor, AutoModelForObjectDetection\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n processor = AutoImageProcessor.from_pretrained(\"hustvl/yolos-base\")\n model = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-base\", dtype=torch.float16, attn_implementation=\"sdpa\").to(device)"
        },
        {
            "sha": "bfc3836fd27adc71e7ab871add774dd3675c402c",
            "filename": "docs/source/en/perplexity.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fperplexity.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fperplexity.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperplexity.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -72,9 +72,10 @@ predictions at each step.\n Let's demonstrate this process with GPT-2.\n \n ```python\n-from transformers import GPT2LMHeadModel, GPT2TokenizerFast, infer_device\n+from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n model_id = \"openai-community/gpt2-large\"\n model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
        },
        {
            "sha": "c137401afb363c3bef194cf0888f37d99267ab5d",
            "filename": "docs/source/en/pipeline_tutorial.md",
            "status": "modified",
            "additions": 12,
            "deletions": 8,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpipeline_tutorial.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -39,9 +39,10 @@ pipeline(\"the secret to baking a really good cake is \")\n When you have more than one input, pass them as a list.\n \n ```py\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=device)\n pipeline([\"the secret to baking a really good cake is \", \"a baguette is \"])\n@@ -173,9 +174,10 @@ pipeline(\"the secret to baking a really good cake is \")\n In the example below, when there are 4 inputs and `batch_size` is set to 2, [`Pipeline`] passes a batch of 2 inputs to the model at a time.\n \n ```py\n-from transformers import pipeline, infer_device()\n+from transformers import pipeline\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-2-2b\", device=device, batch_size=2)\n pipeline([\"the secret to baking a really good cake is\", \"a baguette is\", \"paris is the\", \"hotdogs are\"])\n@@ -188,11 +190,12 @@ pipeline([\"the secret to baking a really good cake is\", \"a baguette is\", \"paris\n Another good use case for batch inference is for streaming data in [`Pipeline`].\n \n ```py\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n from transformers.pipelines.pt_utils import KeyDataset\n import datasets\n \n-device = infer_device()\n+device = Accelerator().device\n \n # KeyDataset is a utility that returns the item in the dict returned by the dataset\n dataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\n@@ -302,10 +305,11 @@ For inference with large datasets, you can iterate directly over the dataset its\n \n ```py\n from transformers.pipelines.pt_utils import KeyDataset\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n from datasets import load_dataset\n \n-device = infer_device()\n+device = Accelerator().device\n \n dataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\n pipeline = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\", device=device)"
        },
        {
            "sha": "f533f6bf337c643a6dfb8b9d50f0918b425737ab",
            "filename": "docs/source/en/quantization/awq.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fawq.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -55,10 +55,11 @@ Load the AWQ-quantized model with [`~PreTrainedModel.from_pretrained`]. This aut\n If the model is loaded on the CPU, use the `device_map` parameter to move it to an accelerator.\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from accelerate import Accelerator\n import torch\n \n-device = f\"{infer_device()}:0\"\n+device = Accelerator().device\n \n model = AutoModelForCausalLM.from_pretrained(\n   \"TheBloke/zephyr-7B-alpha-AWQ\","
        },
        {
            "sha": "85ae7e2954efc809d837e0604a8b991ea1c99c17",
            "filename": "docs/source/en/quicktour.md",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquicktour.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -124,12 +124,13 @@ Create a [`Pipeline`] object and select a task. By default, [`Pipeline`] downloa\n <hfoptions id=\"pipeline-tasks\">\n <hfoption id=\"text generation\">\n \n-Use [`infer_device`] to automatically detect an available accelerator for inference.\n+Use [`Accelerator`] to automatically detect an available accelerator for inference.\n \n ```py\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-hf\", device=device)\n ```\n@@ -144,12 +145,13 @@ pipeline(\"The secret to baking a good cake is \", max_length=50)\n </hfoption>\n <hfoption id=\"image segmentation\">\n \n-Use [`infer_device`] to automatically detect an available accelerator for inference.\n+Use [`Accelerator`] to automatically detect an available accelerator for inference.\n \n ```py\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n pipeline = pipeline(\"image-segmentation\", model=\"facebook/detr-resnet-50-panoptic\", device=device)\n ```\n@@ -171,12 +173,13 @@ segments[1][\"label\"]\n </hfoption>\n <hfoption id=\"automatic speech recognition\">\n \n-Use [`infer_device`] to automatically detect an available accelerator for inference.\n+Use [`Accelerator`] to automatically detect an available accelerator for inference.\n \n ```py\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\", device=device)\n ```"
        },
        {
            "sha": "fb7e658e288d73bfdc3c03617af7ffd0889a24be",
            "filename": "docs/source/en/tasks/image_captioning.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -247,9 +247,9 @@ image\n Prepare image for the model.\n \n ```python\n-from transformers import infer_device\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n inputs = processor(images=image, return_tensors=\"pt\").to(device)\n pixel_values = inputs.pixel_values\n ```"
        },
        {
            "sha": "bf4cf3560d67a035a29b819fbf384c1f9839a703",
            "filename": "docs/source/en/tasks/image_feature_extraction.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -42,9 +42,10 @@ Let's see the pipeline in action. First, initialize the pipeline. If you don't p\n \n ```python\n import torch\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n-DEVICE = infer_device()\n+device = Accelerator().device\n pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n ```\n "
        },
        {
            "sha": "3730823d2f2f67512abb2763260b29b6fdc36846",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -39,10 +39,11 @@ pip install -q transformers accelerate flash_attn\n Let's initialize the model and the processor.\n \n ```python\n-from transformers import AutoProcessor, AutoModelForImageTextToText, infer_device\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+from accelerate import Accelerator\n import torch\n \n-device = torch.device(infer_device())\n+device = Accelerator().device\n model = AutoModelForImageTextToText.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n     dtype=torch.bfloat16,"
        },
        {
            "sha": "42c6737656dfc90a27ec4d0a9e1c3cb83022e184",
            "filename": "docs/source/en/tasks/image_to_image.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -36,10 +36,11 @@ pip install transformers\n We can now initialize the pipeline with a [Swin2SR model](https://huggingface.co/caidas/swin2SR-lightweight-x2-64). We can then infer with the pipeline by calling it with an image. As of now, only [Swin2SR models](https://huggingface.co/models?sort=trending&search=swin2sr) are supported in this pipeline.\n \n ```python\n-from transformers import pipeline, infer_device\n+from transformers import pipeline\n+from accelerate import Accelerator\n import torch\n # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n-device = infer_device()\n+device = Accelerator().device\n pipe = pipeline(task=\"image-to-image\", model=\"caidas/swin2SR-lightweight-x2-64\", device=device)\n ```\n "
        },
        {
            "sha": "5919e7ec0dd30ca430a5f2d53ae79fdd9ab44c92",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -53,7 +53,8 @@ processed_datasets = dataset.map(process, batched=True)\n Essentially, we want the student model (a randomly initialized MobileNet) to mimic the teacher model (fine-tuned vision transformer). To achieve this, we first get the logits output from the teacher and the student. Then, we divide each of them by the parameter `temperature` which controls the importance of each soft target. A parameter called `lambda` weighs the importance of the distillation loss. In this example, we will use `temperature=5` and `lambda=0.5`. We will use the Kullback-Leibler Divergence loss to compute the divergence between the student and teacher. Given two data P and Q, KL Divergence explains how much extra information we need to represent P using Q. If two are identical, their KL divergence is zero, as there's no other information needed to explain P from Q. Thus, in the context of knowledge distillation, KL divergence is useful.\n \n ```python\n-from transformers import TrainingArguments, Trainer, infer_device\n+from transformers import TrainingArguments, Trainer\n+from accelerate import Accelerator\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n@@ -64,7 +65,7 @@ class ImageDistilTrainer(Trainer):\n         self.teacher = teacher_model\n         self.student = student_model\n         self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n-        device = infer_device()\n+        device = Accelerator().device\n         self.teacher.to(device)\n         self.teacher.eval()\n         self.temperature = temperature"
        },
        {
            "sha": "6b78843236388caa05cfb991caf4b3e03b4f02b8",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -124,9 +124,10 @@ You can also use the model without the pipeline. To do so, initialize the model\n the processor.\n \n ```python\n-from transformers import SamModel, SamProcessor, infer_device\n+from transformers import SamModel, SamProcessor\n+from accelerate import Accelerator\n import torch\n-device = infer_device()\n+device = Accelerator().device\n model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n ```"
        },
        {
            "sha": "a4dd410090ff97b18545ca1e0f0494888c8bb4d4",
            "filename": "docs/source/en/tasks/monocular_depth_estimation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -51,9 +51,10 @@ The simplest way to try out inference with a model supporting depth estimation i\n Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads):\n \n ```py\n->>> from transformers import pipeline, infer_device\n+>>> from transformers import pipeline\n+from accelerate import Accelerator\n >>> import torch\n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n >>> pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n ```"
        },
        {
            "sha": "ee1352ea87144928b703255f883619a23f31fbb8",
            "filename": "docs/source/en/tasks/object_detection.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -1494,9 +1494,9 @@ Now that you have finetuned a model, evaluated it, and uploaded it to the Huggin\n Load model and image processor from the Hugging Face Hub (skip to use already trained in this session):\n \n ```py\n->>> from transformers import infer_device\n+>>> from accelerate import Accelerator\n \n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n \n >>> image_processor = AutoImageProcessor.from_pretrained(model_repo)"
        },
        {
            "sha": "bdc2db3e952e73fa69a936312acd35c3de9f94bb",
            "filename": "docs/source/en/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -481,8 +481,8 @@ Reload the dataset and load an image for inference.\n We will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel_values` on a GPU:\n \n ```py\n->>> from transformers import infer_device\n->>> device = infer_device()\n+>>> from accelerate import Accelerator\n+>>> device = Accelerator().device\n >>> encoding = image_processor(image, return_tensors=\"pt\")\n >>> pixel_values = encoding.pixel_values.to(device)\n ```"
        },
        {
            "sha": "e06d25e1b74d1b7383c5202e66881cbf55a4db59",
            "filename": "docs/source/en/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -282,10 +282,10 @@ containing the corresponding speaker embedding.\n >>> import os\n >>> import torch\n >>> from speechbrain.inference.classifiers import EncoderClassifier\n->>> from transformers import infer_device\n+>>> from accelerate import Accelerator\n \n >>> spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> speaker_model = EncoderClassifier.from_hparams(\n ...     source=spk_model_name,\n ...     run_opts={\"device\": device},"
        },
        {
            "sha": "320fe410b0afe21f3a1fa47f9a1bc5b558be97b4",
            "filename": "docs/source/en/tasks/video_classification.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvideo_classification.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -463,7 +463,8 @@ Load a video for inference:\n The simplest way to try out your fine-tuned model for inference is to use it in a [`pipeline`](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.VideoClassificationPipeline). Instantiate a `pipeline` for video classification with your model, and pass your video to it:\n \n ```py\n->>> from transformers import pipeline, infer_device\n+>>> from transformers import pipeline\n+from accelerate import Accelerator\n \n >>> video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n >>> video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")\n@@ -487,7 +488,7 @@ You can also manually replicate the results of the `pipeline` if you'd like.\n ...         ),  # this can be skipped if you don't have labels available.\n ...     }\n \n-...     device = torch.device(infer_device())\n+...     device = Accelerator().device\n ...     inputs = {k: v.to(device) for k, v in inputs.items()}\n ...     model = model.to(device)\n "
        },
        {
            "sha": "f584762e0f48d066f034100104f5e5e8eb5ee5f7",
            "filename": "docs/source/en/tasks/visual_document_retrieval.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -52,9 +52,10 @@ Let's load the model and the tokenizer.\n \n ```python\n import torch\n-from transformers import ColPaliForRetrieval, ColPaliProcessor, infer_device\n+from transformers import ColPaliForRetrieval, ColPaliProcessor\n+from accelerate import Accelerator\n \n-device = infer_device()\n+device = Accelerator().device\n \n model_name = \"vidore/colpali-v1.2-hf\"\n "
        },
        {
            "sha": "306e7a6e34e3d3725ba3606f5271079fd1e1aad8",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -364,12 +364,13 @@ Let's illustrate how you can use this model for VQA. First, let's load the model\n GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically:\n \n ```py\n->>> from transformers import AutoProcessor, Blip2ForConditionalGeneration, infer_device\n+>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n+from accelerate import Accelerator\n >>> import torch\n \n >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n >>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", dtype=torch.float16)\n->>> device = infer_device()\n+>>> device = Accelerator().device\n >>> model.to(device)\n ```\n "
        },
        {
            "sha": "4cf88b34087584de18bc245c9bc2d2e28d135966",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -460,11 +460,7 @@\n         \"get_wsd_schedule\",\n         \"get_reduce_on_plateau_schedule\",\n     ]\n-    _import_structure[\"pytorch_utils\"] = [\n-        \"Conv1D\",\n-        \"apply_chunking_to_forward\",\n-        \"infer_device\",\n-    ]\n+    _import_structure[\"pytorch_utils\"] = [\"Conv1D\", \"apply_chunking_to_forward\"]\n     _import_structure[\"time_series_utils\"] = []\n     _import_structure[\"trainer\"] = [\"Trainer\"]\n     _import_structure[\"trainer_pt_utils\"] = [\"torch_distributed_zero_first\"]"
        },
        {
            "sha": "8086fb1e2e98c3a20854c60e754a541611f3137a",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/72a3fc275c6161fcb98664c81aae8038a76407ce/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/72a3fc275c6161fcb98664c81aae8038a76407ce/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=72a3fc275c6161fcb98664c81aae8038a76407ce",
            "patch": "@@ -24,7 +24,6 @@\n from .utils import (\n     is_torch_greater_or_equal,\n     is_torch_xla_available,\n-    is_torch_xpu_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n@@ -283,16 +282,3 @@ def wrapper(*args, **kwargs):\n         return wrapper\n \n     return decorator\n-\n-\n-def infer_device():\n-    \"\"\"\n-    Infers available device.\n-    \"\"\"\n-    torch_device = \"cpu\"\n-    if torch.cuda.is_available():\n-        torch_device = \"cuda\"\n-    elif is_torch_xpu_available():\n-        torch_device = \"xpu\"\n-\n-    return torch_device"
        }
    ],
    "stats": {
        "total": 574,
        "additions": 330,
        "deletions": 244
    }
}