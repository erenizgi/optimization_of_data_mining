{
    "author": "efsotr",
    "message": "Support passing flash_attn_kwargs when gradient_checkpointing is enabled (#37037)\n\n* support passing flash_attn_kwargs when gradient_checkpointing is enabled\n\n* make modeling_deepspeek_v3.py consistent with modular_deepseek_v3.py",
    "sha": "2b4734bd4907d54a14f992c42d079af8dfffe6b0",
    "files": [
        {
            "sha": "98a72a3e65971d70eaaad78d31c0fde63fa36a4e",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_dummy.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -544,7 +545,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "91d226d12b88ec6b8af0c31a14c1354c8d05a098",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_multimodal1.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -544,7 +545,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "d87b8ec0c5f594289fb81ad7c9c9be6425679940",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -19,6 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n from dataclasses import dataclass\n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n from ...activations import ACT2FN\n@@ -963,7 +964,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "60adcf89afe45db70642cfddb5d08c42a6cc82ab",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -27,6 +27,7 @@\n # This file is based on the LLama model definition file in transformers\n \n \n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -613,7 +614,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "be51a992a8598537fafad5db6cdd4ee3c700a2d3",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -634,7 +635,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings,\n                     causal_mask,"
        },
        {
            "sha": "ce092545f15c7857449868a4c948025fb8c2edb1",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -13,6 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -533,7 +534,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings,\n                     causal_mask,"
        },
        {
            "sha": "24870d2f69f7eba92e7893cc5e3792aa80e310a1",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -5,6 +5,7 @@\n #                          modular_deepseek_v3.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import math\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -759,7 +760,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "c86fffad7aabd40f0ff9f72d2f51882910190084",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -22,6 +22,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n+from functools import partial\n from typing import Optional, Tuple, Union\n \n import torch\n@@ -852,7 +853,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "43996b4132808edb4ab04fe6d3add815e4a4a4c2",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -21,7 +21,7 @@\n # limitations under the License.\n \n import math\n-from functools import cached_property\n+from functools import cached_property, partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -1439,7 +1439,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "6b23f26208b4526c2990295bfcf0960bb344149b",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -645,7 +646,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings,\n                     causal_mask,"
        },
        {
            "sha": "06f09fab104c50ca161a6cf84ed64d2000ee38e9",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -13,6 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -491,7 +492,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings,\n                     causal_mask,"
        },
        {
            "sha": "f5700f060d8532ac28eb33e05bd4cf16f0e212e3",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -22,6 +22,7 @@\n import copy\n from collections.abc import Callable\n from dataclasses import dataclass\n+from functools import partial\n from typing import List, Optional, Tuple, Union\n \n import torch\n@@ -732,7 +733,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings_global,\n                     position_embeddings_local,"
        },
        {
            "sha": "f869a0653059546c9ddb29d4a2b9d577a33e14fc",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -16,6 +16,7 @@\n import copy\n from collections.abc import Callable\n from dataclasses import dataclass\n+from functools import partial\n from typing import Any, Dict, List, Optional, Tuple, Union\n \n import torch\n@@ -662,7 +663,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings_global,\n                     position_embeddings_local,"
        },
        {
            "sha": "716c97de3f9f8732245bdc9a8be958ea5e3302d3",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -594,7 +595,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "f25cbe0dac148ef4e3c8cbbba545a5d2925dc9ea",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -593,7 +594,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "3781ea47adb020f05ba3a554e24e1a1d00626fa2",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -13,6 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import List, Optional, Tuple, Union\n \n import torch\n@@ -185,7 +186,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "be55e4ebf9ac5df3e5525912f0fa067bfb9818fc",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -20,6 +20,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -581,7 +582,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "78cf7a930a082fc1f2967ed2a8f42fa952976460",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -17,6 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -583,7 +584,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "c7b9a4523dd8b8114fce0fef8bef0888427d39ba",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_mistral.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -548,7 +549,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "13e14a755dbf7bdfa75b1cd87e0327df883992d1",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -24,6 +24,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -672,7 +673,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "c7fa30376b8db35e02fc5449de2a874e64b8473f",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -19,6 +19,7 @@\n # limitations under the License.\n \"\"\"PyTorch Mixtral model.\"\"\"\n \n+from functools import partial\n from typing import List, Optional, Tuple, Union\n \n import torch\n@@ -400,7 +401,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "78438151b842ed32fe093bbf8b946edbd9291d7f",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -18,6 +18,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n@@ -936,7 +937,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     encoder_hidden_states,"
        },
        {
            "sha": "f1fdd7c58d909b895fb8c88f879dce697b5c73d9",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -832,7 +833,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     encoder_hidden_states,"
        },
        {
            "sha": "23acd45eb29890855fc1c10d79839b8de3ca07a4",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -559,7 +560,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "9af94ae0aa682f8b9a059581baf57cd314b9f98d",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -560,7 +561,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "a5a008a6f1e20db80124cb614cebbe56c51347cb",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_phi.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -553,7 +554,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "4dcf74d74144d5188857032ae6e0b4aae42ec6ed",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -1,3 +1,4 @@\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -243,7 +244,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "bd781216da94c165cdb90a65230da7b84d168131",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -20,6 +20,7 @@\n # limitations under the License.\n \n \n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -623,7 +624,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "e009b6f69307526a609af9288bde6f25b8c0c3b1",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b4734bd4907d54a14f992c42d079af8dfffe6b0/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=2b4734bd4907d54a14f992c42d079af8dfffe6b0",
            "patch": "@@ -4,6 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_qwen2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -561,7 +562,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 58,
        "deletions": 30
    }
}