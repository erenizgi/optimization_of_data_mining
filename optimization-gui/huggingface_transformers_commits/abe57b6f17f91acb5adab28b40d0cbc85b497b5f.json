{
    "author": "sbucaille",
    "message": "Add SuperGlue model (#29886)\n\n* Initial commit with template code generated by transformers-cli\r\n\r\n* Multiple additions to SuperGlue implementation :\r\n\r\n- Added the SuperGlueConfig\r\n- Added the SuperGlueModel and its implementation\r\n- Added basic weight conversion script\r\n- Added new ImageMatchingOutput dataclass\r\n\r\n* Few changes for SuperGlue\r\n\r\n* Multiple changes :\r\n- Added keypoint detection config to SuperGlueConfig\r\n- Completed convert_superglue_to_pytorch and succesfully run inference\r\n\r\n* Reverted unintentional change\r\n\r\n* Multiple changes :\r\n - Added SuperGlue to a bunch of places\r\n - Divided SuperGlue into SuperGlueForImageMatching and SuperGlueModel\r\n - Added testing images\r\n\r\n* Moved things in init files\r\n\r\n* Added docs (to be finished depending on the final implementation)\r\n\r\n* Added necessary imports and some doc\r\n\r\n* Removed unnecessary import\r\n\r\n* Fixed make fix-copies bug and ran it\r\n\r\n* Deleted SuperGlueModel\r\nFixed convert script\r\n\r\n* Added SuperGlueImageProcessor\r\n\r\n* Changed SuperGlue to support batching pairs of images and modified ImageMatchingOutput in consequences\r\n\r\n* Changed convert_superglue_to_hf.py script to experiment different ways of reading an image and seeing its impact on performances\r\n\r\n* Added initial tests for SuperGlueImageProcessor\r\n\r\n* Added AutoModelForImageMatching in missing places and tests\r\n\r\n* Fixed keypoint_detector_output instructions\r\n\r\n* Fix style\r\n\r\n* Adapted to latest main changes\r\n\r\n* Added integration test\r\n\r\n* Fixed bugs to pass tests\r\n\r\n* Added keypoints returned by keypoint detector in the output of SuperGlue\r\n\r\n* Added doc to SuperGlue\r\n\r\n* SuperGlue returning all attention and hidden states for a fixed number of keypoints\r\n\r\n* Make style\r\n\r\n* Changed SuperGlueImageProcessor tests\r\n\r\n* Revert \"SuperGlue returning all attention and hidden states for a fixed number of keypoints\"\r\nChanged tests accordingly\r\n\r\nThis reverts commit 5b3b669c\r\n\r\n* Added back hidden_states and attentions masked outputs with tests\r\n\r\n* Renamed ImageMatching occurences into KeypointMatching\r\n\r\n* Changed SuperGlueImageProcessor to raise error when batch_size is not even\r\n\r\n* Added docs and clarity to hidden state and attention grouping function\r\n\r\n* Fixed some code and done refactoring\r\n\r\n* Fixed typo in SuperPoint output doc\r\n\r\n* Fixed some of the formatting and variable naming problems\r\n\r\n* Removed useless function call\r\n\r\n* Removed AutoModelForKeypointMatching\r\n\r\n* Fixed SuperGlueImageProcessor to only accept paris of images\r\n\r\n* Added more fixes to SuperGlueImageProcessor\r\n\r\n* Simplified the batching of attention and hidden states\r\n\r\n* Simplified stack functions\r\n\r\n* Moved attention instructions into class\r\n\r\n* Removed unused do_batch_norm argument\r\n\r\n* Moved weight initialization to the proper place\r\n\r\n* Replaced deepcopy for instantiation\r\n\r\n* Fixed small bug\r\n\r\n* Changed from stevenbucaille to magic-leap repo\r\n\r\n* Renamed London Bridge images to Tower Bridge\r\n\r\n* Fixed formatting\r\n\r\n* Renamed remaining \"london\" to \"tower\"\r\n\r\n* Apply suggestions from code review\r\n\r\nSmall changes in the docs\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Added AutoModelForKeypointMatching\r\n\r\n* Changed images used in example\r\n\r\n* Several changes to image_processing_superglue and style\r\n\r\n* Fixed resample type hint\r\n\r\n* Changed SuperGlueImageProcessor and added test case for list of 2 images\r\n\r\n* Changed list_of_tuples implementation\r\n\r\n* Fix in dummy objects\r\n\r\n* Added normalize_keypoint, log_sinkhorn_iterations and log_optimal_transport docstring\r\n\r\n* Added missing docstring\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Moved forward block at bottom\r\n\r\n* Added docstring to forward method\r\n\r\n* Added docstring to match_image_pair method\r\n\r\n* Changed test_model_common_attributes to test_model_get_set_embeddings test method signature\r\n\r\n* Removed AutoModelForKeypointMatching\r\n\r\n* Removed image fixtures and added load_dataset\r\n\r\n* Added padding of images in SuperGlueImageProcessor\r\n\r\n* Cleaned up convert_superglue_to_hf script\r\n\r\n* Added missing docs and fixed unused argument\r\n\r\n* Fixed SuperGlueImageProcessor tests\r\n\r\n* Transposed all hidden states from SuperGlue to reflect the standard (..., seq_len, feature_dim) shape\r\n\r\n* Added SuperGlueForKeypointMatching back to modeling_auto\r\n\r\n* Fixed image processor padding test\r\n\r\n* Changed SuperGlue docs\r\n\r\n* changes:\r\n - Abstraction to batch, concat and stack of inconsistent tensors\r\n - Changed conv1d's to linears to match standard attention implementations\r\n - Renamed all tensors to be tensor0 and not tensor_0 and be consistent\r\n - Changed match image pair to run keypoint detection on all image first, create batching tensors and then filling these tensors matches after matches\r\n - Various changes in docs, etc\r\n\r\n* Changes to SuperGlueImageProcessor:\r\n- Reworked the input image pairs checking function and added tests accordingly\r\n- Added Copied from statements\r\n- Added do_grayscale tag (also for SuperPointImageProcessor)\r\n- Misc changes for better code\r\n\r\n* Formatting changes\r\n\r\n* Reverted conv1d to linear conversion because of numerical differences\r\n\r\n* fix: changed some code to be more straightforward (e.g. filtering keypoints) and converted plot from opencv to matplotlib\r\n\r\n* fix: removed unnecessary test\r\n\r\n* chore: removed commented code and added back hidden states transpositions\r\n\r\n* chore: changed from \"inconsistent\" to \"ragged\" function names as suggested\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* docs: applied suggestions\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* docs: updated to display matched output\r\n\r\n* chore: applied suggestion for check_image_pairs_input function\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* chore: changed check_image_pairs_input function name to validate_and_format_image_pairs and used validate_preprocess_arguments function\r\n\r\n* tests: simplified tests for image input format and shapes\r\n\r\n* feat: converted SuperGlue's use of Conv1d with kernel_size of 1 with Linear layers. Changed tests and conversion script accordingly\r\n\r\n* feat: several changes to address comments\r\n\r\nConversion script:\r\n- Reverted fuse batchnorm to linear conversion\r\n- Changed all 'nn.Module' to respective SuperGlue models\r\n- Changed conversion script to use regex mapping and match other recent scripts\r\n\r\nModeling SuperGlue:\r\n- Added batching with mask and padding to attention\r\n- Removed unnecessary concat, stack and batch ragged pairs functions\r\n- Reverted batchnorm layer\r\n- Renamed query, key, value and merge layers into q, k, v, out proj\r\n- Removed Union of different Module into nn.Module in _init_weights method typehint\r\n- Changed several method's signature to combine image0 and image1 inputs with appropriate doc changes\r\n- Updated SuperGlue's doc with torch.no_grad()\r\n\r\nUpdated test to reflect changes in SuperGlue model\r\n\r\n* refactor: changed validate_and_format_image_pairs function with clarity\r\n\r\n* refactor: changed from one SuperGlueMLP class to a list of SuperGlueMLP class\r\n\r\n* fix: fixed forgotten init weight change from last commit\r\n\r\n* fix: fixed rebase mistake\r\n\r\n* fix: removed leftover commented code\r\n\r\n* fix: added typehint and changed some of arguments default values\r\n\r\n* fix: fixed attribute default values for SuperGlueConfig\r\n\r\n* feat: added SuperGlueImageProcessor post process keypoint matching method with tests\r\n\r\n* fix: fixed SuperGlue attention and hidden state tuples aggregation\r\n\r\n* chore: fixed mask optionality and reordered tensor reshapes to be cleaner\r\n\r\n* chore: fixed docs and error message returned in validate_and_format_image_pairs function\r\n\r\n* fix: fixed returned keypoints to be the ones that SuperPoint returns\r\n\r\n* fix: fixed check on number of image sizes for post process compared to the pairs in outputs of SuperGlue\r\n\r\n* fix: fixed check on number of image sizes for post process compared to the pairs in outputs of SuperGlue (bis)\r\n\r\n* fix: Changed SuperGlueMultiLayerPerceptron instantiation to avoid if statement\r\n\r\n* fix: Changed convert_superglue_to_hf script to reflect latest SuperGlue changes and got rid of nn.Modules\r\n\r\n* WIP: implement Attention from an existing class (like BERT)\r\n\r\n* docs: Changed docs to include more appealing matching plot\r\n\r\n* WIP: Implement Attention\r\n\r\n* chore: minor typehint change\r\n\r\n* chore: changed convert superglue script by removing all classes and apply conv to linear conversion in state dict + rearrange keys to comply with changes in model's layers organisation\r\n\r\n* Revert \"Fixed typo in SuperPoint output doc\"\r\n\r\nThis reverts commit 2120390e827f94fcd631c8e5728d9a4980f4a503.\r\n\r\n* chore: added comments in SuperGlueImageProcessor\r\n\r\n* chore: changed SuperGlue organization HF repo to magic-leap-community\r\n\r\n* [run-slow] refactor: small change in layer instantiation\r\n\r\n* [run-slow] chore: replaced remaining stevenbucaille org to magic-leap-community\r\n\r\n* [run-slow] chore: make style\r\n\r\n* chore: update image matching fixture dataset HF repository\r\n\r\n* [run-slow] superglue\r\n\r\n* tests: overwriting test_batching_equivalence\r\n\r\n* [run-slow] superglue\r\n\r\n* tests: changed test to cope with value changing depending on cuda version\r\n\r\n* [run-slow] superglue\r\n\r\n* tests: changed matching_threshold value\r\n\r\n* [run-slow] superglue\r\n\r\n* [run-slow] superglue\r\n\r\n* tests: changed tests for integration\r\n\r\n* [run-slow] superglue\r\n\r\n* fix: Changed tensor view and permutations to match original implementation results\r\n\r\n* fix: updated convert script and integration test to include last change in model\r\n\r\n* fix: increase tolerance for CUDA variances\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* [run-slow] superglue\r\n\r\n* chore: removed blank whitespaces\r\n\r\n* [run-slow] superglue\r\n\r\n* Revert SuperPoint image processor accident changes\r\n\r\n* [run-slow] superglue\r\n\r\n* refactor: reverted copy from BERT class\r\n\r\n* tests: lower the tolerance in integration tests for SuperGlue\r\n\r\n* [run-slow] superglue\r\n\r\n* chore: set do_grayscale to False in SuperPoint and SuperGlue image processors\r\n\r\n* [run-slow] superglue\r\n\r\n* fix: fixed imports in SuperGlue files\r\n\r\n* chore: changed do_grayscale SuperGlueImageProcessing default value to True\r\n\r\n* docs: added typehint to post_process_keypoint_matching method in SuperGlueImageProcessor\r\n\r\n* fix: set matching_threshold default value to 0.0 instead of 0.2\r\n\r\n* feat: added matching_threshold to post_process_keypoint_matching method\r\n\r\n* docs: update superglue.md to include matching_threshold parameter\r\n\r\n* docs: updated SuperGlueConfig docstring for matching_threshold default value\r\n\r\n* refactor: removed unnecessary parameters in SuperGlueConfig\r\n\r\n* fix: changed from matching_threshold to threshold\r\n\r\n* fix: re-revert changes to make SuperGlue attention classes copies of BERT\r\n\r\n* [run-slow] superglue\r\n\r\n* fix: added missing device argument in post_processing method\r\n\r\n* [run-slow] superglue\r\n\r\n* fix: add matches different from -1 to compute valid matches in post_process_keypoint_matching (and docstring)\r\n\r\n* fix: add device to image_sizes tensor instantiation\r\n\r\n* tests: added checks on do_grayscale test\r\n\r\n* chore: reordered and added Optional typehint to KeypointMatchingOutput\r\n\r\n* LightGluePR suggestions:\r\n- use `post_process_keypoint_matching` as default docs example\r\n- add `post_process_keypoint_matching` in autodoc\r\n- add `SuperPointConfig` import under TYPE_CHECKING condition\r\n- format SuperGlueConfig docstring\r\n- add device in convert_superglue_to_hf\r\n- Fix typo\r\n- Fix KeypointMatchingOutput docstring\r\n- Removed unnecessary line\r\n- Added missing SuperGlueConfig in __init__ methods\r\n\r\n* LightGluePR suggestions:\r\n- use batching to get keypoint detection\r\n\r\n* refactor: processing images done in 1 for loop instead of 4\r\n\r\n* fix: use @ instead of torch.einsum for scores computation\r\n\r\n* style: added #fmt skip to long tensor values\r\n\r\n* refactor: rollbacked validate_and_format_image_pairs valid and invalid case to more simple ones\r\n\r\n* refactor: prepare_imgs\r\n\r\n* refactor: simplified `validate_and_format_image_pairs`\r\n\r\n* docs: fixed doc\r\n\r\n---------\r\n\r\nCo-authored-by: steven <steven.bucaillle@gmail.com>\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\nCo-authored-by: Steven Bucaille <steven.bucaille@buawei.com>\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
    "files": [
        {
            "sha": "63ec4de8a4564e3217875ddaaeabb1febdd4adc7",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -713,6 +713,8 @@\n         title: SegFormer\n       - local: model_doc/seggpt\n         title: SegGpt\n+      - local: model_doc/superglue\n+        title: SuperGlue\n       - local: model_doc/superpoint\n         title: SuperPoint\n       - local: model_doc/swiftformer"
        },
        {
            "sha": "ace8f76f7d01cd7de9cf9dd7029f865952cc7bb7",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -318,6 +318,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                   [SqueezeBERT](model_doc/squeezebert)                   |       ✅        |         ❌         |      ❌      |\n |                      [StableLm](model_doc/stablelm)                      |       ✅        |         ❌         |      ❌      |\n |                    [Starcoder2](model_doc/starcoder2)                    |       ✅        |         ❌         |      ❌      |\n+|                     [SuperGlue](model_doc/superglue)                     |       ✅        |         ❌         |      ❌      |\n |                    [SuperPoint](model_doc/superpoint)                    |       ✅        |         ❌         |      ❌      |\n |                   [SwiftFormer](model_doc/swiftformer)                   |       ✅        |         ✅         |      ❌      |\n |                    [Swin Transformer](model_doc/swin)                    |       ✅        |         ✅         |      ❌      |"
        },
        {
            "sha": "08a4575dddc22d74e8c06cf7f5d617cee3c9e623",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "added",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,138 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the MIT License; you may not use this file except in compliance with\n+the License.\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+\n+-->\n+\n+# SuperGlue\n+\n+## Overview\n+\n+The SuperGlue model was proposed in [SuperGlue: Learning Feature Matching with Graph Neural Networks](https://arxiv.org/abs/1911.11763) by Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n+\n+This model consists of matching two sets of interest points detected in an image. Paired with the \n+[SuperPoint model](https://huggingface.co/magic-leap-community/superpoint), it can be used to match two images and \n+estimate the pose between them. This model is useful for tasks such as image matching, homography estimation, etc.\n+\n+The abstract from the paper is the following:\n+\n+*This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences \n+and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs \n+are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling \n+SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, \n+our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image \n+pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in \n+challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and \n+can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at this [URL](https://github.com/magicleap/SuperGluePretrainedNetwork).*\n+\n+## How to use\n+\n+Here is a quick example of using the model. Since this model is an image matching model, it requires pairs of images to be matched. \n+The raw outputs contain the list of keypoints detected by the keypoint detector as well as the list of matches with their corresponding \n+matching scores.\n+```python\n+from transformers import AutoImageProcessor, AutoModel\n+import torch\n+from PIL import Image\n+import requests\n+\n+url_image1 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_98169888_3347710852.jpg\"\n+image1 = Image.open(requests.get(url_image1, stream=True).raw)\n+url_image2 = \"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/refs/heads/master/assets/phototourism_sample_images/united_states_capitol_26757027_6717084061.jpg\"\n+image_2 = Image.open(requests.get(url_image2, stream=True).raw)\n+\n+images = [image1, image2]\n+\n+processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+\n+inputs = processor(images, return_tensors=\"pt\")\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+```\n+\n+You can use the `post_process_keypoint_matching` method from the `SuperGlueImageProcessor` to get the keypoints and matches in a more readable format:\n+\n+```python\n+image_sizes = [[(image.height, image.width) for image in images]]\n+outputs = processor.post_process_keypoint_matching(outputs, image_sizes, threshold=0.2)\n+for i, output in enumerate(outputs):\n+    print(\"For the image pair\", i)\n+    for keypoint0, keypoint1, matching_score in zip(\n+            output[\"keypoints0\"], output[\"keypoints1\"], output[\"matching_scores\"]\n+    ):\n+        print(\n+            f\"Keypoint at coordinate {keypoint0.numpy()} in the first image matches with keypoint at coordinate {keypoint1.numpy()} in the second image with a score of {matching_score}.\"\n+        )\n+\n+```\n+\n+From the outputs, you can visualize the matches between the two images using the following code:\n+```python\n+import matplotlib.pyplot as plt\n+import numpy as np\n+\n+# Create side by side image\n+merged_image = np.zeros((max(image1.height, image2.height), image1.width + image2.width, 3))\n+merged_image[: image1.height, : image1.width] = np.array(image1) / 255.0\n+merged_image[: image2.height, image1.width :] = np.array(image2) / 255.0\n+plt.imshow(merged_image)\n+plt.axis(\"off\")\n+\n+# Retrieve the keypoints and matches\n+output = outputs[0]\n+keypoints0 = output[\"keypoints0\"]\n+keypoints1 = output[\"keypoints1\"]\n+matching_scores = output[\"matching_scores\"]\n+keypoints0_x, keypoints0_y = keypoints0[:, 0].numpy(), keypoints0[:, 1].numpy()\n+keypoints1_x, keypoints1_y = keypoints1[:, 0].numpy(), keypoints1[:, 1].numpy()\n+\n+# Plot the matches\n+for keypoint0_x, keypoint0_y, keypoint1_x, keypoint1_y, matching_score in zip(\n+        keypoints0_x, keypoints0_y, keypoints1_x, keypoints1_y, matching_scores\n+):\n+    plt.plot(\n+        [keypoint0_x, keypoint1_x + image1.width],\n+        [keypoint0_y, keypoint1_y],\n+        color=plt.get_cmap(\"RdYlGn\")(matching_score.item()),\n+        alpha=0.9,\n+        linewidth=0.5,\n+    )\n+    plt.scatter(keypoint0_x, keypoint0_y, c=\"black\", s=2)\n+    plt.scatter(keypoint1_x + image1.width, keypoint1_y, c=\"black\", s=2)\n+\n+# Save the plot\n+plt.savefig(\"matched_image.png\", dpi=300, bbox_inches='tight')\n+plt.close()\n+```\n+\n+![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/01ZYaLB1NL5XdA8u7yCo4.png)\n+\n+This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n+The original code can be found [here](https://github.com/magicleap/SuperGluePretrainedNetwork).\n+\n+## SuperGlueConfig\n+\n+[[autodoc]] SuperGlueConfig\n+\n+## SuperGlueImageProcessor\n+\n+[[autodoc]] SuperGlueImageProcessor\n+\n+- preprocess\n+\n+## SuperGlueForKeypointMatching\n+\n+[[autodoc]] SuperGlueForKeypointMatching\n+\n+- forward\n+- post_process_keypoint_matching\n\\ No newline at end of file"
        },
        {
            "sha": "86708c937fec9415941265a17b802a00d5c8f956",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -785,6 +785,7 @@\n     ],\n     \"models.stablelm\": [\"StableLmConfig\"],\n     \"models.starcoder2\": [\"Starcoder2Config\"],\n+    \"models.superglue\": [\"SuperGlueConfig\"],\n     \"models.superpoint\": [\"SuperPointConfig\"],\n     \"models.swiftformer\": [\"SwiftFormerConfig\"],\n     \"models.swin\": [\"SwinConfig\"],\n@@ -1268,6 +1269,7 @@\n     _import_structure[\"models.segformer\"].extend([\"SegformerFeatureExtractor\", \"SegformerImageProcessor\"])\n     _import_structure[\"models.seggpt\"].extend([\"SegGptImageProcessor\"])\n     _import_structure[\"models.siglip\"].append(\"SiglipImageProcessor\")\n+    _import_structure[\"models.superglue\"].extend([\"SuperGlueImageProcessor\"])\n     _import_structure[\"models.superpoint\"].extend([\"SuperPointImageProcessor\"])\n     _import_structure[\"models.swin2sr\"].append(\"Swin2SRImageProcessor\")\n     _import_structure[\"models.textnet\"].extend([\"TextNetImageProcessor\"])\n@@ -3545,6 +3547,12 @@\n             \"Starcoder2PreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.superglue\"].extend(\n+        [\n+            \"SuperGlueForKeypointMatching\",\n+            \"SuperGluePreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.superpoint\"].extend(\n         [\n             \"SuperPointForKeypointDetection\",\n@@ -5861,6 +5869,7 @@\n     )\n     from .models.stablelm import StableLmConfig\n     from .models.starcoder2 import Starcoder2Config\n+    from .models.superglue import SuperGlueConfig\n     from .models.superpoint import SuperPointConfig\n     from .models.swiftformer import (\n         SwiftFormerConfig,\n@@ -6361,6 +6370,7 @@\n         from .models.segformer import SegformerFeatureExtractor, SegformerImageProcessor\n         from .models.seggpt import SegGptImageProcessor\n         from .models.siglip import SiglipImageProcessor\n+        from .models.superglue import SuperGlueImageProcessor\n         from .models.superpoint import SuperPointImageProcessor\n         from .models.swin2sr import Swin2SRImageProcessor\n         from .models.textnet import TextNetImageProcessor\n@@ -8186,6 +8196,10 @@\n             Starcoder2Model,\n             Starcoder2PreTrainedModel,\n         )\n+        from .models.superglue import (\n+            SuperGlueForKeypointMatching,\n+            SuperGluePreTrainedModel,\n+        )\n         from .models.superpoint import (\n             SuperPointForKeypointDetection,\n             SuperPointPreTrainedModel,"
        },
        {
            "sha": "69c3c26cfa2a98fd060178be210caba0310766e8",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -246,6 +246,7 @@\n     squeezebert,\n     stablelm,\n     starcoder2,\n+    superglue,\n     superpoint,\n     swiftformer,\n     swin,"
        },
        {
            "sha": "e93c201bb0cc63afd7caf3e732b0c202bbbabd4f",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -273,6 +273,7 @@\n         (\"squeezebert\", \"SqueezeBertConfig\"),\n         (\"stablelm\", \"StableLmConfig\"),\n         (\"starcoder2\", \"Starcoder2Config\"),\n+        (\"superglue\", \"SuperGlueConfig\"),\n         (\"superpoint\", \"SuperPointConfig\"),\n         (\"swiftformer\", \"SwiftFormerConfig\"),\n         (\"swin\", \"SwinConfig\"),\n@@ -608,6 +609,7 @@\n         (\"squeezebert\", \"SqueezeBERT\"),\n         (\"stablelm\", \"StableLm\"),\n         (\"starcoder2\", \"Starcoder2\"),\n+        (\"superglue\", \"SuperGlue\"),\n         (\"superpoint\", \"SuperPoint\"),\n         (\"swiftformer\", \"SwiftFormer\"),\n         (\"swin\", \"Swin Transformer\"),"
        },
        {
            "sha": "d03d50705d9b35506bcee7217f09f998742eb97a",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -133,6 +133,7 @@\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),\n             (\"siglip\", (\"SiglipImageProcessor\",)),\n+            (\"superglue\", \"SuperGlueImageProcessor\"),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin2sr\", (\"Swin2SRImageProcessor\",)),"
        },
        {
            "sha": "fc24fd4dcfa365fb31e612da9575507194523229",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -251,6 +251,7 @@\n         (\"squeezebert\", \"SqueezeBertModel\"),\n         (\"stablelm\", \"StableLmModel\"),\n         (\"starcoder2\", \"Starcoder2Model\"),\n+        (\"superglue\", \"SuperGlueForKeypointMatching\"),\n         (\"swiftformer\", \"SwiftFormerModel\"),\n         (\"swin\", \"SwinModel\"),\n         (\"swin2sr\", \"Swin2SRModel\"),"
        },
        {
            "sha": "666f44026eff6e9c5d908370f7d13f1adfd1d1cf",
            "filename": "src/transformers/models/superglue/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2F__init__.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_superglue import *\n+    from .image_processing_superglue import *\n+    from .modeling_superglue import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "fe301442d632bc935017d85cbed95c574b15b62c",
            "filename": "src/transformers/models/superglue/configuration_superglue.py",
            "status": "added",
            "additions": 120,
            "deletions": 0,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,120 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING, List\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING\n+\n+\n+if TYPE_CHECKING:\n+    from ..superpoint import SuperPointConfig\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SuperGlueConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SuperGlueModel`]. It is used to instantiate a\n+    SuperGlue model according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the SuperGlue\n+    [magic-leap-community/superglue_indoor](https://huggingface.co/magic-leap-community/superglue_indoor) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        keypoint_detector_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `SuperPointConfig`):\n+            The config object or dictionary of the keypoint detector.\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            The dimension of the descriptors.\n+        keypoint_encoder_sizes (`List[int]`, *optional*, defaults to `[32, 64, 128, 256]`):\n+            The sizes of the keypoint encoder layers.\n+        gnn_layers_types (`List[str]`, *optional*, defaults to `['self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross', 'self', 'cross']`):\n+            The types of the GNN layers. Must be either 'self' or 'cross'.\n+        num_attention_heads (`int`, *optional*, defaults to 4):\n+            The number of heads in the GNN layers.\n+        sinkhorn_iterations (`int`, *optional*, defaults to 100):\n+            The number of Sinkhorn iterations.\n+        matching_threshold (`float`, *optional*, defaults to 0.0):\n+            The matching threshold.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+\n+    Examples:\n+        ```python\n+        >>> from transformers import SuperGlueConfig, SuperGlueModel\n+\n+        >>> # Initializing a SuperGlue superglue style configuration\n+        >>> configuration = SuperGlueConfig()\n+\n+        >>> # Initializing a model from the superglue style configuration\n+        >>> model = SuperGlueModel(configuration)\n+\n+        >>> # Accessing the model configuration\n+        >>> configuration = model.config\n+        ```\n+    \"\"\"\n+\n+    model_type = \"superglue\"\n+\n+    def __init__(\n+        self,\n+        keypoint_detector_config: \"SuperPointConfig\" = None,\n+        hidden_size: int = 256,\n+        keypoint_encoder_sizes: List[int] = None,\n+        gnn_layers_types: List[str] = None,\n+        num_attention_heads: int = 4,\n+        sinkhorn_iterations: int = 100,\n+        matching_threshold: float = 0.0,\n+        initializer_range: float = 0.02,\n+        **kwargs,\n+    ):\n+        self.gnn_layers_types = gnn_layers_types if gnn_layers_types is not None else [\"self\", \"cross\"] * 9\n+        # Check whether all gnn_layers_types are either 'self' or 'cross'\n+        if not all(layer_type in [\"self\", \"cross\"] for layer_type in self.gnn_layers_types):\n+            raise ValueError(\"All gnn_layers_types must be either 'self' or 'cross'\")\n+\n+        if hidden_size % num_attention_heads != 0:\n+            raise ValueError(\"hidden_size % num_attention_heads is different from zero\")\n+\n+        self.keypoint_encoder_sizes = (\n+            keypoint_encoder_sizes if keypoint_encoder_sizes is not None else [32, 64, 128, 256]\n+        )\n+        self.hidden_size = hidden_size\n+        self.keypoint_encoder_sizes = keypoint_encoder_sizes\n+        self.gnn_layers_types = gnn_layers_types\n+        self.num_attention_heads = num_attention_heads\n+        self.sinkhorn_iterations = sinkhorn_iterations\n+        self.matching_threshold = matching_threshold\n+\n+        if isinstance(keypoint_detector_config, dict):\n+            keypoint_detector_config[\"model_type\"] = (\n+                keypoint_detector_config[\"model_type\"] if \"model_type\" in keypoint_detector_config else \"superpoint\"\n+            )\n+            keypoint_detector_config = CONFIG_MAPPING[keypoint_detector_config[\"model_type\"]](\n+                **keypoint_detector_config\n+            )\n+        if keypoint_detector_config is None:\n+            keypoint_detector_config = CONFIG_MAPPING[\"superpoint\"]()\n+\n+        self.keypoint_detector_config = keypoint_detector_config\n+        self.initializer_range = initializer_range\n+        self.attention_probs_dropout_prob = 0\n+        self.is_decoder = False\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"SuperGlueConfig\"]"
        },
        {
            "sha": "cfff39acdfd8ee457e72e35bed98c6e7bcebafea",
            "filename": "src/transformers/models/superglue/convert_superglue_to_hf.py",
            "status": "added",
            "additions": 342,
            "deletions": 0,
            "changes": 342,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconvert_superglue_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconvert_superglue_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconvert_superglue_to_hf.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,342 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import os\n+import re\n+from typing import List\n+\n+import torch\n+from datasets import load_dataset\n+\n+from transformers import (\n+    AutoModelForKeypointDetection,\n+    SuperGlueConfig,\n+    SuperGlueForKeypointMatching,\n+    SuperGlueImageProcessor,\n+)\n+\n+\n+def prepare_imgs():\n+    dataset = load_dataset(\"hf-internal-testing/image-matching-test-dataset\", split=\"train\")\n+    image1 = dataset[0][\"image\"]\n+    image2 = dataset[1][\"image\"]\n+    image3 = dataset[2][\"image\"]\n+    return [[image1, image2], [image3, image2]]\n+\n+\n+def verify_model_outputs(model, model_name, device):\n+    images = prepare_imgs()\n+    preprocessor = SuperGlueImageProcessor()\n+    inputs = preprocessor(images=images, return_tensors=\"pt\").to(device)\n+    model.to(device)\n+    with torch.no_grad():\n+        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+    predicted_matches_values = outputs.matches[0, 0, :10]\n+    predicted_matching_scores_values = outputs.matching_scores[0, 0, :10]\n+\n+    predicted_number_of_matches = torch.sum(outputs.matches[0][0] != -1).item()\n+\n+    if \"outdoor\" in model_name:\n+        expected_max_number_keypoints = 865\n+        expected_matches_shape = torch.Size((len(images), 2, expected_max_number_keypoints))\n+        expected_matching_scores_shape = torch.Size((len(images), 2, expected_max_number_keypoints))\n+\n+        expected_matches_values = torch.tensor(\n+            [125, 630, 137, 138, 136, 143, 135, -1, -1, 153], dtype=torch.int64, device=device\n+        )\n+        expected_matching_scores_values = torch.tensor(\n+            [0.9899, 0.0033, 0.9897, 0.9889, 0.9879, 0.7464, 0.7109, 0, 0, 0.9841], device=device\n+        )\n+\n+        expected_number_of_matches = 281\n+    elif \"indoor\" in model_name:\n+        expected_max_number_keypoints = 865\n+        expected_matches_shape = torch.Size((len(images), 2, expected_max_number_keypoints))\n+        expected_matching_scores_shape = torch.Size((len(images), 2, expected_max_number_keypoints))\n+\n+        expected_matches_values = torch.tensor(\n+            [125, 144, 137, 138, 136, 155, 135, -1, -1, 153], dtype=torch.int64, device=device\n+        )\n+        expected_matching_scores_values = torch.tensor(\n+            [0.9694, 0.0010, 0.9006, 0.8753, 0.8521, 0.5688, 0.6321, 0.0, 0.0, 0.7235], device=device\n+        )\n+\n+        expected_number_of_matches = 282\n+\n+    assert outputs.matches.shape == expected_matches_shape\n+    assert outputs.matching_scores.shape == expected_matching_scores_shape\n+\n+    assert torch.allclose(predicted_matches_values, expected_matches_values, atol=1e-4)\n+    assert torch.allclose(predicted_matching_scores_values, expected_matching_scores_values, atol=1e-4)\n+\n+    assert predicted_number_of_matches == expected_number_of_matches\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"kenc.encoder.(\\d+)\": r\"keypoint_encoder.encoder.\\1.old\",\n+    r\"gnn.layers.(\\d+).attn.proj.0\": r\"gnn.layers.\\1.attention.self.query\",\n+    r\"gnn.layers.(\\d+).attn.proj.1\": r\"gnn.layers.\\1.attention.self.key\",\n+    r\"gnn.layers.(\\d+).attn.proj.2\": r\"gnn.layers.\\1.attention.self.value\",\n+    r\"gnn.layers.(\\d+).attn.merge\": r\"gnn.layers.\\1.attention.output.dense\",\n+    r\"gnn.layers.(\\d+).mlp.0\": r\"gnn.layers.\\1.mlp.0.linear\",\n+    r\"gnn.layers.(\\d+).mlp.1\": r\"gnn.layers.\\1.mlp.0.batch_norm\",\n+    r\"gnn.layers.(\\d+).mlp.3\": r\"gnn.layers.\\1.mlp.1\",\n+    r\"final_proj\": r\"final_projection.final_proj\",\n+}\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: List[str], conversion_mapping=ORIGINAL_TO_CONVERTED_KEY_MAPPING):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in conversion_mapping.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def replace_state_dict_keys(all_keys, new_keys, original_state_dict):\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        state_dict[new_key] = original_state_dict.pop(key).contiguous().clone()\n+    return state_dict\n+\n+\n+def convert_state_dict(state_dict, config):\n+    converted_to_final_key_mapping = {}\n+\n+    def convert_conv_to_linear(keys):\n+        for key in keys:\n+            state_dict[key] = state_dict[key].squeeze(-1)\n+\n+    def qkv_permute_weights_and_biases(keys, num_heads=4):\n+        for key in keys:\n+            tensor = state_dict[key]\n+            shape = tensor.shape\n+            dim_out = shape[0]\n+            if len(shape) == 2:\n+                dim_in = shape[1]\n+                tensor = (\n+                    tensor.reshape(dim_out // num_heads, num_heads, dim_in).permute(1, 0, 2).reshape(dim_out, dim_in)\n+                )\n+            if len(shape) == 1:\n+                tensor = tensor.reshape(dim_out // num_heads, num_heads).permute(1, 0).reshape(dim_out)\n+            state_dict[key] = tensor\n+\n+    def output_permute_weights(keys, num_heads=4):\n+        for key in keys:\n+            tensor = state_dict[key]\n+            dim_in = tensor.shape[1]\n+            dim_out = tensor.shape[0]\n+            tensor = tensor.reshape(dim_out, dim_in // num_heads, num_heads).permute(0, 2, 1).reshape(dim_out, dim_in)\n+            state_dict[key] = tensor\n+\n+    conv_keys = []\n+    qkv_permute_keys = []\n+    output_permute_keys = []\n+    # Keypoint Encoder\n+    keypoint_encoder_key = \"keypoint_encoder.encoder\"\n+    for i in range(1, len(config.keypoint_encoder_sizes) + 2):\n+        old_conv_key = f\"{keypoint_encoder_key}.{(i - 1) * 3}.old\"\n+        new_index = i - 1\n+        new_conv_key = f\"{keypoint_encoder_key}.{new_index}.\"\n+        if i < len(config.keypoint_encoder_sizes) + 1:\n+            new_conv_key = f\"{new_conv_key}linear.\"\n+        converted_to_final_key_mapping[rf\"{old_conv_key}\\.\"] = new_conv_key\n+        if i < len(config.keypoint_encoder_sizes) + 1:\n+            old_batch_norm_key = f\"{keypoint_encoder_key}.{(i - 1) * 3 + 1}.old\"\n+            new_batch_norm_key = f\"{keypoint_encoder_key}.{new_index}.batch_norm.\"\n+            converted_to_final_key_mapping[rf\"{old_batch_norm_key}\\.\"] = new_batch_norm_key\n+\n+        conv_keys.append(f\"{old_conv_key}.weight\")\n+\n+    # Attentional GNN\n+    for i in range(len(config.gnn_layers_types)):\n+        gnn_layer_key = f\"gnn.layers.{i}\"\n+        ## Attention\n+        attention_key = f\"{gnn_layer_key}.attention\"\n+        conv_keys.extend(\n+            [\n+                f\"{attention_key}.self.query.weight\",\n+                f\"{attention_key}.self.key.weight\",\n+                f\"{attention_key}.self.value.weight\",\n+                f\"{attention_key}.output.dense.weight\",\n+            ]\n+        )\n+        qkv_permute_keys.extend(\n+            [\n+                f\"{attention_key}.self.query.weight\",\n+                f\"{attention_key}.self.key.weight\",\n+                f\"{attention_key}.self.value.weight\",\n+                f\"{attention_key}.self.query.bias\",\n+                f\"{attention_key}.self.key.bias\",\n+                f\"{attention_key}.self.value.bias\",\n+            ]\n+        )\n+        output_permute_keys.append(f\"{attention_key}.output.dense.weight\")\n+\n+        ## MLP\n+        conv_keys.extend([f\"{gnn_layer_key}.mlp.0.linear.weight\", f\"{gnn_layer_key}.mlp.1.weight\"])\n+\n+    # Final Projection\n+    conv_keys.append(\"final_projection.final_proj.weight\")\n+\n+    convert_conv_to_linear(conv_keys)\n+    qkv_permute_weights_and_biases(qkv_permute_keys)\n+    output_permute_weights(output_permute_keys)\n+    all_keys = list(state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys, converted_to_final_key_mapping)\n+    state_dict = replace_state_dict_keys(all_keys, new_keys, state_dict)\n+    return state_dict\n+\n+\n+def add_keypoint_detector_state_dict(superglue_state_dict):\n+    keypoint_detector = AutoModelForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n+    keypoint_detector_state_dict = keypoint_detector.state_dict()\n+    for k, v in keypoint_detector_state_dict.items():\n+        superglue_state_dict[f\"keypoint_detector.{k}\"] = v\n+    return superglue_state_dict\n+\n+\n+@torch.no_grad()\n+def write_model(\n+    model_path,\n+    checkpoint_url,\n+    safe_serialization=True,\n+    push_to_hub=False,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+\n+    # ------------------------------------------------------------\n+    # SuperGlue config\n+    # ------------------------------------------------------------\n+\n+    config = SuperGlueConfig(\n+        hidden_size=256,\n+        keypoint_encoder_sizes=[32, 64, 128, 256],\n+        gnn_layers_types=[\"self\", \"cross\"] * 9,\n+        sinkhorn_iterations=100,\n+        matching_threshold=0.0,\n+    )\n+    config.architectures = [\"SuperGlueForKeypointMatching\"]\n+    config.save_pretrained(model_path, push_to_hub=push_to_hub)\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    print(f\"Fetching all parameters from the checkpoint at {checkpoint_url}...\")\n+    original_state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n+\n+    print(\"Converting model...\")\n+    all_keys = list(original_state_dict.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    state_dict = replace_state_dict_keys(all_keys, new_keys, original_state_dict)\n+    state_dict = convert_state_dict(state_dict, config)\n+\n+    del original_state_dict\n+    gc.collect()\n+    state_dict = add_keypoint_detector_state_dict(state_dict)\n+\n+    print(\"Loading the checkpoint in a SuperGlue model...\")\n+    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+    with torch.device(device):\n+        model = SuperGlueForKeypointMatching(config)\n+    model.load_state_dict(state_dict, strict=True)\n+    print(\"Checkpoint loaded successfully...\")\n+    del model.config._name_or_path\n+\n+    print(\"Saving the model...\")\n+    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = SuperGlueForKeypointMatching.from_pretrained(model_path)\n+    print(\"Model reloaded successfully.\")\n+\n+    model_name = \"superglue\"\n+    if \"superglue_outdoor.pth\" in checkpoint_url:\n+        model_name += \"_outdoor\"\n+    elif \"superglue_indoor.pth\" in checkpoint_url:\n+        model_name += \"_indoor\"\n+\n+    print(\"Checking the model outputs...\")\n+    verify_model_outputs(model, model_name, device)\n+    print(\"Model outputs verified successfully.\")\n+\n+    organization = \"magic-leap-community\"\n+    if push_to_hub:\n+        print(\"Pushing model to the hub...\")\n+        model.push_to_hub(\n+            repo_id=f\"{organization}/{model_name}\",\n+            commit_message=\"Add model\",\n+        )\n+\n+    write_image_processor(model_path, model_name, organization, push_to_hub=push_to_hub)\n+\n+\n+def write_image_processor(save_dir, model_name, organization, push_to_hub=False):\n+    image_processor = SuperGlueImageProcessor()\n+    image_processor.save_pretrained(save_dir)\n+\n+    if push_to_hub:\n+        print(\"Pushing image processor to the hub...\")\n+        image_processor.push_to_hub(\n+            repo_id=f\"{organization}/{model_name}\",\n+            commit_message=\"Add image processor\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--checkpoint_url\",\n+        default=\"https://raw.githubusercontent.com/magicleap/SuperGluePretrainedNetwork/master/models/weights/superglue_indoor.pth\",\n+        type=str,\n+        help=\"URL of the original SuperGlue checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=None,\n+        type=str,\n+        required=True,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\"--save_model\", action=\"store_true\", help=\"Save model to local\")\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Push model and image preprocessor to the hub\",\n+    )\n+\n+    args = parser.parse_args()\n+    write_model(\n+        args.pytorch_dump_folder_path, args.checkpoint_url, safe_serialization=True, push_to_hub=args.push_to_hub\n+    )"
        },
        {
            "sha": "567e55580701654e37b66327871389c43580564e",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "added",
            "additions": 407,
            "deletions": 0,
            "changes": 407,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,407 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for SuperPoint.\"\"\"\n+\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ... import is_torch_available, is_vision_available\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import resize, to_channel_dimension_format\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    is_pil_image,\n+    is_scaled_image,\n+    is_valid_image,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, logging, requires_backends\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if TYPE_CHECKING:\n+    from .modeling_superglue import KeypointMatchingOutput\n+\n+if is_vision_available():\n+    import PIL\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Copied from transformers.models.superpoint.image_processing_superpoint.is_grayscale\n+def is_grayscale(\n+    image: ImageInput,\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+):\n+    if input_data_format == ChannelDimension.FIRST:\n+        if image.shape[0] == 1:\n+            return True\n+        return np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...])\n+    elif input_data_format == ChannelDimension.LAST:\n+        if image.shape[-1] == 1:\n+            return True\n+        return np.all(image[..., 0] == image[..., 1]) and np.all(image[..., 1] == image[..., 2])\n+\n+\n+# Copied from transformers.models.superpoint.image_processing_superpoint.convert_to_grayscale\n+def convert_to_grayscale(\n+    image: ImageInput,\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+) -> ImageInput:\n+    \"\"\"\n+    Converts an image to grayscale format using the NTSC formula. Only support numpy and PIL Image. TODO support torch\n+    and tensorflow grayscale conversion\n+\n+    This function is supposed to return a 1-channel image, but it returns a 3-channel image with the same value in each\n+    channel, because of an issue that is discussed in :\n+    https://github.com/huggingface/transformers/pull/25786#issuecomment-1730176446\n+\n+    Args:\n+        image (Image):\n+            The image to convert.\n+        input_data_format (`ChannelDimension` or `str`, *optional*):\n+            The channel dimension format for the input image.\n+    \"\"\"\n+    requires_backends(convert_to_grayscale, [\"vision\"])\n+\n+    if isinstance(image, np.ndarray):\n+        if is_grayscale(image, input_data_format=input_data_format):\n+            return image\n+        if input_data_format == ChannelDimension.FIRST:\n+            gray_image = image[0, ...] * 0.2989 + image[1, ...] * 0.5870 + image[2, ...] * 0.1140\n+            gray_image = np.stack([gray_image] * 3, axis=0)\n+        elif input_data_format == ChannelDimension.LAST:\n+            gray_image = image[..., 0] * 0.2989 + image[..., 1] * 0.5870 + image[..., 2] * 0.1140\n+            gray_image = np.stack([gray_image] * 3, axis=-1)\n+        return gray_image\n+\n+    if not isinstance(image, PIL.Image.Image):\n+        return image\n+\n+    image = image.convert(\"L\")\n+    return image\n+\n+\n+def validate_and_format_image_pairs(images: ImageInput):\n+    error_message = (\n+        \"Input images must be a one of the following :\",\n+        \" - A pair of PIL images.\",\n+        \" - A pair of 3D arrays.\",\n+        \" - A list of pairs of PIL images.\",\n+        \" - A list of pairs of 3D arrays.\",\n+    )\n+\n+    def _is_valid_image(image):\n+        \"\"\"images is a PIL Image or a 3D array.\"\"\"\n+        return is_pil_image(image) or (\n+            is_valid_image(image) and get_image_type(image) != ImageType.PIL and len(image.shape) == 3\n+        )\n+\n+    if isinstance(images, list):\n+        if len(images) == 2 and all((_is_valid_image(image)) for image in images):\n+            return images\n+        if all(\n+            isinstance(image_pair, list)\n+            and len(image_pair) == 2\n+            and all(_is_valid_image(image) for image in image_pair)\n+            for image_pair in images\n+        ):\n+            return [image for image_pair in images for image in image_pair]\n+    raise ValueError(error_message)\n+\n+\n+class SuperGlueImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a SuperGlue image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Controls whether to resize the image's (height, width) dimensions to the specified `size`. Can be overriden\n+            by `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"height\": 480, \"width\": 640}`):\n+            Resolution of the output image after `resize` is applied. Only has an effect if `do_resize` is set to\n+            `True`. Can be overriden by `size` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overriden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overriden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_grayscale (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to grayscale. Can be overriden by `do_grayscale` in the `preprocess` method.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255,\n+        do_grayscale: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 480, \"width\": 640}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_grayscale = do_grayscale\n+\n+    # Copied from transformers.models.superpoint.image_processing_superpoint.SuperPointImageProcessor.resize\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Resize an image.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dictionary of the form `{\"height\": int, \"width\": int}`, specifying the size of the output image.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the output image. If not provided, it will be inferred from the input\n+                image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        return resize(\n+            image,\n+            size=(size[\"height\"], size[\"width\"]),\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    def preprocess(\n+        self,\n+        images,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_grayscale: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image pairs to preprocess. Expects either a list of 2 images or a list of list of 2 images list with\n+                pixel values ranging from 0 to 255. If passing in images with pixel values between 0 and 1, set\n+                `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the output image after `resize` has been applied. If `size[\"shortest_edge\"]` >= 384, the image\n+                is resized to `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Otherwise, the smaller edge of the\n+                image will be matched to `int(size[\"shortest_edge\"]/ crop_pct)`, after which the image is cropped to\n+                `(size[\"shortest_edge\"], size[\"shortest_edge\"])`. Only has an effect if `do_resize` is set to `True`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of `PILImageResampling`, filters. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_grayscale (`bool`, *optional*, defaults to `self.do_grayscale`):\n+                Whether to convert the image to grayscale.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_grayscale = do_grayscale if do_grayscale is not None else self.do_grayscale\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        # Validate and convert the input images into a flattened list of images for all subsequent processing steps.\n+        images = validate_and_format_image_pairs(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+        )\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_grayscale:\n+                image = convert_to_grayscale(image, input_data_format=input_data_format)\n+\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            all_images.append(image)\n+\n+        # Convert back the flattened list of images into a list of pairs of images.\n+        image_pairs = [all_images[i : i + 2] for i in range(0, len(all_images), 2)]\n+\n+        data = {\"pixel_values\": image_pairs}\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, List[Tuple]],\n+        threshold: float = 0.0,\n+    ) -> List[Dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.mask.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, List):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for mask_pair, keypoints_pair, matches, scores in zip(\n+            outputs.mask, keypoints, outputs.matches[:, 0], outputs.matching_scores[:, 0]\n+        ):\n+            mask0 = mask_pair[0] > 0\n+            mask1 = mask_pair[1] > 0\n+            keypoints0 = keypoints_pair[0][mask0]\n+            keypoints1 = keypoints_pair[1][mask1]\n+            matches0 = matches[mask0]\n+            scores0 = scores[mask0]\n+\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores0 > threshold, matches0 > -1)\n+\n+            matched_keypoints0 = keypoints0[valid_matches]\n+            matched_keypoints1 = keypoints1[matches0[valid_matches]]\n+            matching_scores = scores0[valid_matches]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n+\n+\n+__all__ = [\"SuperGlueImageProcessor\"]"
        },
        {
            "sha": "049eb91b8451f3d6e59974cd8daef76aa78c486a",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "added",
            "additions": 866,
            "deletions": 0,
            "changes": 866,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,866 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch SuperGlue model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers import PreTrainedModel, add_start_docstrings\n+from transformers.models.superglue.configuration_superglue import SuperGlueConfig\n+\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...utils import ModelOutput, add_start_docstrings_to_model_forward, logging\n+from ..auto import AutoModelForKeypointDetection\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC_ = \"SuperGlueConfig\"\n+_CHECKPOINT_FOR_DOC_ = \"magic-leap-community/superglue_indoor\"\n+\n+\n+def concat_pairs(tensor_tuple0: Tuple[torch.Tensor], tensor_tuple1: Tuple[torch.Tensor]) -> Tuple[torch.Tensor]:\n+    \"\"\"\n+    Concatenate two tuples of tensors pairwise\n+\n+    Args:\n+        tensor_tuple0 (`Tuple[torch.Tensor]`):\n+            Tuple of tensors.\n+        tensor_tuple1 (`Tuple[torch.Tensor]`):\n+            Tuple of tensors.\n+\n+    Returns:\n+        (`Tuple[torch.Tensor]`): Tuple of concatenated tensors.\n+    \"\"\"\n+    return tuple([torch.cat([tensor0, tensor1]) for tensor0, tensor1 in zip(tensor_tuple0, tensor_tuple1)])\n+\n+\n+def normalize_keypoints(keypoints: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+    \"\"\"\n+    Normalize keypoints locations based on image image_shape\n+\n+    Args:\n+        keypoints (`torch.Tensor` of shape `(batch_size, num_keypoints, 2)`):\n+            Keypoints locations in (x, y) format.\n+        height (`int`):\n+            Image height.\n+        width (`int`):\n+            Image width.\n+\n+    Returns:\n+        Normalized keypoints locations of shape (`torch.Tensor` of shape `(batch_size, num_keypoints, 2)`).\n+    \"\"\"\n+    size = torch.tensor([width, height], device=keypoints.device, dtype=keypoints.dtype)[None]\n+    center = size / 2\n+    scaling = size.max(1, keepdim=True).values * 0.7\n+    return (keypoints - center[:, None, :]) / scaling[:, None, :]\n+\n+\n+def log_sinkhorn_iterations(\n+    log_cost_matrix: torch.Tensor,\n+    log_source_distribution: torch.Tensor,\n+    log_target_distribution: torch.Tensor,\n+    num_iterations: int,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Perform Sinkhorn Normalization in Log-space for stability\n+\n+    Args:\n+        log_cost_matrix (`torch.Tensor` of shape `(batch_size, num_rows, num_columns)`):\n+            Logarithm of the cost matrix.\n+        log_source_distribution (`torch.Tensor` of shape `(batch_size, num_rows)`):\n+            Logarithm of the source distribution.\n+        log_target_distribution (`torch.Tensor` of shape `(batch_size, num_columns)`):\n+            Logarithm of the target distribution.\n+\n+    Returns:\n+        log_cost_matrix (`torch.Tensor` of shape `(batch_size, num_rows, num_columns)`): Logarithm of the optimal\n+        transport matrix.\n+    \"\"\"\n+    log_u_scaling = torch.zeros_like(log_source_distribution)\n+    log_v_scaling = torch.zeros_like(log_target_distribution)\n+    for _ in range(num_iterations):\n+        log_u_scaling = log_source_distribution - torch.logsumexp(log_cost_matrix + log_v_scaling.unsqueeze(1), dim=2)\n+        log_v_scaling = log_target_distribution - torch.logsumexp(log_cost_matrix + log_u_scaling.unsqueeze(2), dim=1)\n+    return log_cost_matrix + log_u_scaling.unsqueeze(2) + log_v_scaling.unsqueeze(1)\n+\n+\n+def log_optimal_transport(scores: torch.Tensor, reg_param: torch.Tensor, iterations: int) -> torch.Tensor:\n+    \"\"\"\n+    Perform Differentiable Optimal Transport in Log-space for stability\n+\n+    Args:\n+        scores: (`torch.Tensor` of shape `(batch_size, num_rows, num_columns)`):\n+            Cost matrix.\n+        reg_param: (`torch.Tensor` of shape `(batch_size, 1, 1)`):\n+            Regularization parameter.\n+        iterations: (`int`):\n+            Number of Sinkhorn iterations.\n+\n+    Returns:\n+        log_optimal_transport_matrix: (`torch.Tensor` of shape `(batch_size, num_rows, num_columns)`): Logarithm of the\n+        optimal transport matrix.\n+    \"\"\"\n+    batch_size, num_rows, num_columns = scores.shape\n+    one_tensor = scores.new_tensor(1)\n+    num_rows_tensor, num_columns_tensor = (num_rows * one_tensor).to(scores), (num_columns * one_tensor).to(scores)\n+\n+    source_reg_param = reg_param.expand(batch_size, num_rows, 1)\n+    target_reg_param = reg_param.expand(batch_size, 1, num_columns)\n+    reg_param = reg_param.expand(batch_size, 1, 1)\n+\n+    couplings = torch.cat([torch.cat([scores, source_reg_param], -1), torch.cat([target_reg_param, reg_param], -1)], 1)\n+\n+    log_normalization = -(num_rows_tensor + num_columns_tensor).log()\n+    log_source_distribution = torch.cat(\n+        [log_normalization.expand(num_rows), num_columns_tensor.log()[None] + log_normalization]\n+    )\n+    log_target_distribution = torch.cat(\n+        [log_normalization.expand(num_columns), num_rows_tensor.log()[None] + log_normalization]\n+    )\n+    log_source_distribution, log_target_distribution = (\n+        log_source_distribution[None].expand(batch_size, -1),\n+        log_target_distribution[None].expand(batch_size, -1),\n+    )\n+\n+    log_optimal_transport_matrix = log_sinkhorn_iterations(\n+        couplings, log_source_distribution, log_target_distribution, num_iterations=iterations\n+    )\n+    log_optimal_transport_matrix = log_optimal_transport_matrix - log_normalization  # multiply probabilities by M+N\n+    return log_optimal_transport_matrix\n+\n+\n+def arange_like(x, dim: int) -> torch.Tensor:\n+    return x.new_ones(x.shape[dim]).cumsum(0) - 1\n+\n+\n+@dataclass\n+class KeypointMatchingOutput(ModelOutput):\n+    \"\"\"\n+    Base class for outputs of keypoint matching models. Due to the nature of keypoint detection and matching, the number\n+    of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the batch of\n+    images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask tensor is\n+    used to indicate which values in the keypoints, matches and matching_scores tensors are keypoint matching\n+    information.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n+            Loss computed during training.\n+        mask (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n+            Mask indicating which values in matches and matching_scores are keypoint matching information.\n+        matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+            Index of keypoint matched in the other image.\n+        matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+            Scores of predicted matches.\n+        keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n+            Absolute (x, y) coordinates of predicted keypoints in a given image.\n+        hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n+            num_keypoints)`, returned when `output_hidden_states=True` is passed or when\n+            `config.output_hidden_states=True`)\n+        attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n+            num_keypoints)`, returned when `output_attentions=True` is passed or when `config.output_attentions=True`)\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    matches: Optional[torch.FloatTensor] = None\n+    matching_scores: Optional[torch.FloatTensor] = None\n+    keypoints: Optional[torch.FloatTensor] = None\n+    mask: Optional[torch.IntTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+\n+\n+class SuperGlueMultiLayerPerceptron(nn.Module):\n+    def __init__(self, config: SuperGlueConfig, in_channels: int, out_channels: int) -> None:\n+        super().__init__()\n+        self.linear = nn.Linear(in_channels, out_channels)\n+        self.batch_norm = nn.BatchNorm1d(out_channels)\n+        self.activation = nn.ReLU()\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.linear(hidden_state)\n+        hidden_state = hidden_state.transpose(-1, -2)\n+        hidden_state = self.batch_norm(hidden_state)\n+        hidden_state = hidden_state.transpose(-1, -2)\n+        hidden_state = self.activation(hidden_state)\n+        return hidden_state\n+\n+\n+class SuperGlueKeypointEncoder(nn.Module):\n+    def __init__(self, config: SuperGlueConfig) -> None:\n+        super().__init__()\n+        layer_sizes = config.keypoint_encoder_sizes\n+        hidden_size = config.hidden_size\n+        # 3 here consists of 2 for the (x, y) coordinates and 1 for the score of the keypoint\n+        encoder_channels = [3] + layer_sizes + [hidden_size]\n+\n+        layers = [\n+            SuperGlueMultiLayerPerceptron(config, encoder_channels[i - 1], encoder_channels[i])\n+            for i in range(1, len(encoder_channels) - 1)\n+        ]\n+        layers.append(nn.Linear(encoder_channels[-2], encoder_channels[-1]))\n+        self.encoder = nn.ModuleList(layers)\n+\n+    def forward(\n+        self,\n+        keypoints: torch.Tensor,\n+        scores: torch.Tensor,\n+        output_hidden_states: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]]]:\n+        scores = scores.unsqueeze(2)\n+        hidden_state = torch.cat([keypoints, scores], dim=2)\n+        all_hidden_states = () if output_hidden_states else None\n+        for layer in self.encoder:\n+            hidden_state = layer(hidden_state)\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_state,)\n+        return hidden_state, all_hidden_states\n+\n+\n+# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->SuperGlue\n+class SuperGlueSelfAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n+\n+        self.is_decoder = config.is_decoder\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        # If this is instantiated as a cross-attention module, the keys\n+        # and values come from an encoder; the attention mask needs to be\n+        # such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        if is_cross_attention and past_key_value is not None:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value[0]\n+            value_layer = past_key_value[1]\n+            attention_mask = encoder_attention_mask\n+        elif is_cross_attention:\n+            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n+            attention_mask = encoder_attention_mask\n+        elif past_key_value is not None:\n+            key_layer = self.transpose_for_scores(self.key(hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(hidden_states))\n+            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(hidden_states))\n+\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        use_cache = past_key_value is not None\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n+            if use_cache:\n+                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n+                    -1, 1\n+                )\n+            else:\n+                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n+            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n+            distance = position_ids_l - position_ids_r\n+\n+            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n+            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n+\n+            if self.position_embedding_type == \"relative_key\":\n+                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores\n+            elif self.position_embedding_type == \"relative_key_query\":\n+                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+        if attention_mask is not None:\n+            # Apply the attention mask is (precomputed for all layers in SuperGlueModel forward() function)\n+            attention_scores = attention_scores + attention_mask\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n+class SuperGlueSelfOutput(nn.Module):\n+    def __init__(self, config: SuperGlueConfig):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor, *args) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        return hidden_states\n+\n+\n+SUPERGLUE_SELF_ATTENTION_CLASSES = {\n+    \"eager\": SuperGlueSelfAttention,\n+}\n+\n+\n+# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->SuperGlue,BERT->SUPERGLUE\n+class SuperGlueAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__()\n+        self.self = SUPERGLUE_SELF_ATTENTION_CLASSES[config._attn_implementation](\n+            config, position_embedding_type=position_embedding_type\n+        )\n+        self.output = SuperGlueSelfOutput(config)\n+        self.pruned_heads = set()\n+\n+    def prune_heads(self, heads):\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.self.query = prune_linear_layer(self.self.query, index)\n+        self.self.key = prune_linear_layer(self.self.key, index)\n+        self.self.value = prune_linear_layer(self.self.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n+        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        self_outputs = self.self(\n+            hidden_states,\n+            attention_mask,\n+            head_mask,\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            past_key_value,\n+            output_attentions,\n+        )\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+class SuperGlueAttentionalPropagation(nn.Module):\n+    def __init__(self, config: SuperGlueConfig) -> None:\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        self.attention = SuperGlueAttention(config)\n+        mlp_channels = [hidden_size * 2, hidden_size * 2, hidden_size]\n+        layers = [\n+            SuperGlueMultiLayerPerceptron(config, mlp_channels[i - 1], mlp_channels[i])\n+            for i in range(1, len(mlp_channels) - 1)\n+        ]\n+        layers.append(nn.Linear(mlp_channels[-2], mlp_channels[-1]))\n+        self.mlp = nn.ModuleList(layers)\n+\n+    def forward(\n+        self,\n+        descriptors: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[Tuple[torch.Tensor]], Optional[Tuple[torch.Tensor]]]:\n+        attention_outputs = self.attention(\n+            descriptors,\n+            attention_mask=attention_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        output = attention_outputs[0]\n+        attention = attention_outputs[1:]\n+\n+        hidden_state = torch.cat([descriptors, output], dim=2)\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        for layer in self.mlp:\n+            hidden_state = layer(hidden_state)\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_state,)\n+\n+        return hidden_state, all_hidden_states, attention\n+\n+\n+class SuperGlueAttentionalGNN(nn.Module):\n+    def __init__(self, config: SuperGlueConfig) -> None:\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.layers_types = config.gnn_layers_types\n+        self.layers = nn.ModuleList([SuperGlueAttentionalPropagation(config) for _ in range(len(self.layers_types))])\n+\n+    def forward(\n+        self,\n+        descriptors: torch.Tensor,\n+        mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[Tuple], Optional[Tuple]]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        batch_size, num_keypoints, _ = descriptors.shape\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (descriptors,)\n+\n+        for gnn_layer, layer_type in zip(self.layers, self.layers_types):\n+            encoder_hidden_states = None\n+            encoder_attention_mask = None\n+            if layer_type == \"cross\":\n+                encoder_hidden_states = (\n+                    descriptors.reshape(-1, 2, num_keypoints, self.hidden_size)\n+                    .flip(1)\n+                    .reshape(batch_size, num_keypoints, self.hidden_size)\n+                )\n+                encoder_attention_mask = (\n+                    mask.reshape(-1, 2, 1, 1, num_keypoints).flip(1).reshape(batch_size, 1, 1, num_keypoints)\n+                    if mask is not None\n+                    else None\n+                )\n+\n+            gnn_outputs = gnn_layer(\n+                descriptors,\n+                attention_mask=mask,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_hidden_states=output_hidden_states,\n+                output_attentions=output_attentions,\n+            )\n+            delta = gnn_outputs[0]\n+\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + gnn_outputs[1]\n+            if output_attentions:\n+                all_attentions = all_attentions + gnn_outputs[2]\n+\n+            descriptors = descriptors + delta\n+        return descriptors, all_hidden_states, all_attentions\n+\n+\n+class SuperGlueFinalProjection(nn.Module):\n+    def __init__(self, config: SuperGlueConfig) -> None:\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        self.final_proj = nn.Linear(hidden_size, hidden_size, bias=True)\n+\n+    def forward(self, descriptors: torch.Tensor) -> torch.Tensor:\n+        return self.final_proj(descriptors)\n+\n+\n+class SuperGluePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = SuperGlueConfig\n+    base_model_prefix = \"superglue\"\n+    main_input_name = \"pixel_values\"\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv1d)):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, SuperGlueMultiLayerPerceptron):\n+            nn.init.constant_(module.linear.bias, 0.0)\n+\n+\n+SUPERGLUE_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`SuperGlueConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+    \"\"\"\n+\n+SUPERGLUE_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`SuperGlueImageProcessor`]. See\n+            [`SuperGlueImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors. See `attentions` under returned tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"SuperGlue model taking images as inputs and outputting the matching of them.\",\n+    SUPERGLUE_START_DOCSTRING,\n+)\n+class SuperGlueForKeypointMatching(SuperGluePreTrainedModel):\n+    \"\"\"SuperGlue feature matching middle-end\n+\n+    Given two sets of keypoints and locations, we determine the\n+    correspondences by:\n+      1. Keypoint Encoding (normalization + visual feature and location fusion)\n+      2. Graph Neural Network with multiple self and cross-attention layers\n+      3. Final projection layer\n+      4. Optimal Transport Layer (a differentiable Hungarian matching algorithm)\n+      5. Thresholding matrix based on mutual exclusivity and a match_threshold\n+\n+    The correspondence ids use -1 to indicate non-matching points.\n+\n+    Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew\n+    Rabinovich. SuperGlue: Learning Feature Matching with Graph Neural\n+    Networks. In CVPR, 2020. https://arxiv.org/abs/1911.11763\n+    \"\"\"\n+\n+    def __init__(self, config: SuperGlueConfig) -> None:\n+        super().__init__(config)\n+\n+        self.keypoint_detector = AutoModelForKeypointDetection.from_config(config.keypoint_detector_config)\n+\n+        self.keypoint_encoder = SuperGlueKeypointEncoder(config)\n+        self.gnn = SuperGlueAttentionalGNN(config)\n+        self.final_projection = SuperGlueFinalProjection(config)\n+\n+        bin_score = torch.nn.Parameter(torch.tensor(1.0))\n+        self.register_parameter(\"bin_score\", bin_score)\n+\n+        self.post_init()\n+\n+    def _match_image_pair(\n+        self,\n+        keypoints: torch.Tensor,\n+        descriptors: torch.Tensor,\n+        scores: torch.Tensor,\n+        height: int,\n+        width: int,\n+        mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, Tuple, Tuple]:\n+        \"\"\"\n+        Perform keypoint matching between two images.\n+\n+        Args:\n+            keypoints (`torch.Tensor` of shape `(batch_size, 2, num_keypoints, 2)`):\n+                Keypoints detected in the pair of image.\n+            descriptors (`torch.Tensor` of shape `(batch_size, 2, descriptor_dim, num_keypoints)`):\n+                Descriptors of the keypoints detected in the image pair.\n+            scores (`torch.Tensor` of shape `(batch_size, 2, num_keypoints)`):\n+                Confidence scores of the keypoints detected in the image pair.\n+            height (`int`): Image height.\n+            width (`int`): Image width.\n+            mask (`torch.Tensor` of shape `(batch_size, 2, num_keypoints)`, *optional*):\n+                Mask indicating which values in the keypoints, matches and matching_scores tensors are keypoint matching\n+                information.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors. Default to `config.output_attentions`.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. Default to `config.output_hidden_states`.\n+\n+        Returns:\n+            matches (`torch.Tensor` of shape `(batch_size, 2, num_keypoints)`):\n+                For each image pair, for each keypoint in image0, the index of the keypoint in image1 that was matched\n+                with. And for each keypoint in image1, the index of the keypoint in image0 that was matched with.\n+            matching_scores (`torch.Tensor` of shape `(batch_size, 2, num_keypoints)`):\n+                Scores of predicted matches for each image pair\n+            all_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+                Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(1, 2, num_keypoints,\n+                num_channels)`.\n+            all_attentions (`tuple(torch.FloatTensor)`, *optional*):\n+                Tuple of `torch.FloatTensor` (one for each layer) of shape `(1, 2, num_heads, num_keypoints,\n+                num_keypoints)`.\n+        \"\"\"\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        if keypoints.shape[2] == 0:  # no keypoints\n+            shape = keypoints.shape[:-1]\n+            return (\n+                keypoints.new_full(shape, -1, dtype=torch.int),\n+                keypoints.new_zeros(shape),\n+                all_hidden_states,\n+                all_attentions,\n+            )\n+\n+        batch_size, _, num_keypoints, _ = keypoints.shape\n+        # (batch_size, 2, num_keypoints, 2) -> (batch_size * 2, num_keypoints, 2)\n+        keypoints = keypoints.reshape(batch_size * 2, num_keypoints, 2)\n+        descriptors = descriptors.reshape(batch_size * 2, num_keypoints, self.config.hidden_size)\n+        scores = scores.reshape(batch_size * 2, num_keypoints)\n+        mask = mask.reshape(batch_size * 2, num_keypoints) if mask is not None else None\n+\n+        # Keypoint normalization\n+        keypoints = normalize_keypoints(keypoints, height, width)\n+\n+        encoded_keypoints = self.keypoint_encoder(keypoints, scores, output_hidden_states=output_hidden_states)\n+\n+        last_hidden_state = encoded_keypoints[0]\n+\n+        # Keypoint MLP encoder.\n+        descriptors = descriptors + last_hidden_state\n+\n+        if mask is not None:\n+            input_shape = descriptors.size()\n+            extended_attention_mask = self.get_extended_attention_mask(mask, input_shape)\n+        else:\n+            extended_attention_mask = torch.ones((batch_size, num_keypoints), device=keypoints.device)\n+\n+        # Multi-layer Transformer network.\n+        gnn_outputs = self.gnn(\n+            descriptors,\n+            mask=extended_attention_mask,\n+            output_hidden_states=output_hidden_states,\n+            output_attentions=output_attentions,\n+        )\n+        descriptors = gnn_outputs[0]\n+\n+        # Final MLP projection.\n+        projected_descriptors = self.final_projection(descriptors)\n+\n+        # (batch_size * 2, num_keypoints, descriptor_dim) -> (batch_size, 2, num_keypoints, descriptor_dim)\n+        final_descriptors = projected_descriptors.reshape(batch_size, 2, num_keypoints, self.config.hidden_size)\n+        final_descriptors0 = final_descriptors[:, 0]\n+        final_descriptors1 = final_descriptors[:, 1]\n+\n+        # Compute matching descriptor distance.\n+        scores = final_descriptors0 @ final_descriptors1.transpose(1, 2)\n+        scores = scores / self.config.hidden_size**0.5\n+\n+        if mask is not None:\n+            mask = mask.reshape(batch_size, 2, num_keypoints)\n+            mask0 = mask[:, 0].unsqueeze(-1).expand(-1, -1, num_keypoints)\n+            scores = scores.masked_fill(mask0 == 0, -1e9)\n+\n+        # Run the optimal transport.\n+        scores = log_optimal_transport(scores, self.bin_score, iterations=self.config.sinkhorn_iterations)\n+\n+        # Get the matches with score above \"match_threshold\".\n+        max0 = scores[:, :-1, :-1].max(2)\n+        max1 = scores[:, :-1, :-1].max(1)\n+        indices0 = max0.indices\n+        indices1 = max1.indices\n+        mutual0 = arange_like(indices0, 1)[None] == indices1.gather(1, indices0)\n+        mutual1 = arange_like(indices1, 1)[None] == indices0.gather(1, indices1)\n+        zero = scores.new_tensor(0)\n+        matching_scores0 = torch.where(mutual0, max0.values.exp(), zero)\n+        matching_scores0 = torch.where(matching_scores0 > self.config.matching_threshold, matching_scores0, zero)\n+        matching_scores1 = torch.where(mutual1, matching_scores0.gather(1, indices1), zero)\n+        valid0 = mutual0 & (matching_scores0 > zero)\n+        valid1 = mutual1 & valid0.gather(1, indices1)\n+        matches0 = torch.where(valid0, indices0, indices0.new_tensor(-1))\n+        matches1 = torch.where(valid1, indices1, indices1.new_tensor(-1))\n+\n+        matches = torch.cat([matches0, matches1]).reshape(batch_size, 2, -1)\n+        matching_scores = torch.cat([matching_scores0, matching_scores1]).reshape(batch_size, 2, -1)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + encoded_keypoints[1]\n+            all_hidden_states = all_hidden_states + gnn_outputs[1]\n+            all_hidden_states = all_hidden_states + (projected_descriptors,)\n+            all_hidden_states = tuple(\n+                x.reshape(batch_size, 2, num_keypoints, -1).transpose(-1, -2) for x in all_hidden_states\n+            )\n+        if output_attentions:\n+            all_attentions = all_attentions + gnn_outputs[2]\n+            all_attentions = tuple(x.reshape(batch_size, 2, -1, num_keypoints, num_keypoints) for x in all_attentions)\n+\n+        return (\n+            matches,\n+            matching_scores,\n+            all_hidden_states,\n+            all_attentions,\n+        )\n+\n+    @add_start_docstrings_to_model_forward(SUPERGLUE_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, KeypointMatchingOutput]:\n+        \"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, AutoModel\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_78916675_4568141288.jpg?raw=true\"\n+        >>> image1 = Image.open(requests.get(url, stream=True).raw)\n+        >>> url = \"https://github.com/magicleap/SuperGluePretrainedNetwork/blob/master/assets/phototourism_sample_images/london_bridge_19481797_2295892421.jpg?raw=true\"\n+        >>> image2 = Image.open(requests.get(url, stream=True).raw)\n+        >>> images = [image1, image2]\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+        >>> model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+\n+        >>> with torch.no_grad():\n+        >>>     inputs = processor(images, return_tensors=\"pt\")\n+        >>>     outputs = model(**inputs)\n+        ```\"\"\"\n+        loss = None\n+        if labels is not None:\n+            raise ValueError(\"SuperGlue is not trainable, no labels should be provided.\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values.ndim != 5 or pixel_values.size(1) != 2:\n+            raise ValueError(\"Input must be a 5D tensor of shape (batch_size, 2, num_channels, height, width)\")\n+\n+        batch_size, _, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * 2, channels, height, width)\n+        keypoint_detections = self.keypoint_detector(pixel_values)\n+\n+        keypoints, scores, descriptors, mask = keypoint_detections[:4]\n+        keypoints = keypoints.reshape(batch_size, 2, -1, 2).to(pixel_values)\n+        scores = scores.reshape(batch_size, 2, -1).to(pixel_values)\n+        descriptors = descriptors.reshape(batch_size, 2, -1, self.config.hidden_size).to(pixel_values)\n+        mask = mask.reshape(batch_size, 2, -1)\n+\n+        absolute_keypoints = keypoints.clone()\n+        absolute_keypoints[:, :, :, 0] = absolute_keypoints[:, :, :, 0] * width\n+        absolute_keypoints[:, :, :, 1] = absolute_keypoints[:, :, :, 1] * height\n+\n+        matches, matching_scores, hidden_states, attentions = self._match_image_pair(\n+            absolute_keypoints,\n+            descriptors,\n+            scores,\n+            height,\n+            width,\n+            mask=mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [loss, matches, matching_scores, keypoints, mask, hidden_states, attentions]\n+                if v is not None\n+            )\n+\n+        return KeypointMatchingOutput(\n+            loss=loss,\n+            matches=matches,\n+            matching_scores=matching_scores,\n+            keypoints=keypoints,\n+            mask=mask,\n+            hidden_states=hidden_states,\n+            attentions=attentions,\n+        )\n+\n+\n+__all__ = [\"SuperGluePreTrainedModel\", \"SuperGlueForKeypointMatching\"]"
        },
        {
            "sha": "007966a0557a3f49048efb29c87c8897c50ea350",
            "filename": "src/transformers/models/superpoint/convert_superpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fconvert_superpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fconvert_superpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fconvert_superpoint_to_pytorch.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -144,7 +144,7 @@ def convert_superpoint_checkpoint(checkpoint_url, pytorch_dump_folder_path, save\n         model.save_pretrained(pytorch_dump_folder_path)\n         preprocessor.save_pretrained(pytorch_dump_folder_path)\n \n-        model_name = \"superpoint\"\n+        model_name = \"magic-leap-community/superpoint\"\n         if push_to_hub:\n             print(f\"Pushing {model_name} to the hub...\")\n         model.push_to_hub(model_name)"
        },
        {
            "sha": "cfae68f90204329809f69b36f938e1b6e1d7a447",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 17,
            "deletions": 5,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -49,8 +49,12 @@ def is_grayscale(\n     input_data_format: Optional[Union[str, ChannelDimension]] = None,\n ):\n     if input_data_format == ChannelDimension.FIRST:\n+        if image.shape[0] == 1:\n+            return True\n         return np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...])\n     elif input_data_format == ChannelDimension.LAST:\n+        if image.shape[-1] == 1:\n+            return True\n         return np.all(image[..., 0] == image[..., 1]) and np.all(image[..., 1] == image[..., 2])\n \n \n@@ -75,6 +79,8 @@ def convert_to_grayscale(\n     requires_backends(convert_to_grayscale, [\"vision\"])\n \n     if isinstance(image, np.ndarray):\n+        if is_grayscale(image, input_data_format=input_data_format):\n+            return image\n         if input_data_format == ChannelDimension.FIRST:\n             gray_image = image[0, ...] * 0.2989 + image[1, ...] * 0.5870 + image[2, ...] * 0.1140\n             gray_image = np.stack([gray_image] * 3, axis=0)\n@@ -107,6 +113,8 @@ class SuperPointImageProcessor(BaseImageProcessor):\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n             Scale factor to use if rescaling the image. Can be overriden by `rescale_factor` in the `preprocess`\n             method.\n+        do_grayscale (`bool`, *optional*, defaults to `False`):\n+            Whether to convert the image to grayscale. Can be overriden by `do_grayscale` in the `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -117,6 +125,7 @@ def __init__(\n         size: Dict[str, int] = None,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255,\n+        do_grayscale: bool = False,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -127,6 +136,7 @@ def __init__(\n         self.size = size\n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n+        self.do_grayscale = do_grayscale\n \n     def resize(\n         self,\n@@ -174,6 +184,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n+        do_grayscale: bool = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -197,6 +208,8 @@ def preprocess(\n                 Whether to rescale the image values between [0 - 1].\n             rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_grayscale (`bool`, *optional*, defaults to `self.do_grayscale`):\n+                Whether to convert the image to grayscale.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -220,6 +233,7 @@ def preprocess(\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n         rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_grayscale = do_grayscale if do_grayscale is not None else self.do_grayscale\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n@@ -264,10 +278,8 @@ def preprocess(\n             # We assume that all images have the same channel dimension format.\n             input_data_format = infer_channel_dimension_format(images[0])\n \n-        # Checking if image is RGB or grayscale\n-        for i in range(len(images)):\n-            if not is_grayscale(images[i], input_data_format):\n-                images[i] = convert_to_grayscale(images[i], input_data_format=input_data_format)\n+        if do_grayscale:\n+            images = [convert_to_grayscale(image, input_data_format=input_data_format) for image in images]\n \n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n@@ -299,7 +311,7 @@ def post_process_keypoint_detection(\n             raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n \n         if isinstance(target_sizes, List):\n-            image_sizes = torch.tensor(target_sizes)\n+            image_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n         else:\n             if target_sizes.shape[1] != 2:\n                 raise ValueError("
        },
        {
            "sha": "519bfd7795fe95109b38782523c4988a5481c354",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -8961,6 +8961,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class SuperGlueForKeypointMatching(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class SuperGluePreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class SuperPointForKeypointDetection(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "8c221a004bb7f27e71ebc8974834655d3f56fad2",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -611,6 +611,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class SuperGlueImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class SuperPointImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/superglue/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperglue%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperglue%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2F__init__.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f"
        },
        {
            "sha": "b98d34888cfc1b5c92d8497865a6156bb6a3dd18",
            "filename": "tests/models/superglue/test_image_processing_superglue.py",
            "status": "added",
            "additions": 384,
            "deletions": 0,
            "changes": 384,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,384 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import (\n+    ImageProcessingTestMixin,\n+    prepare_image_inputs,\n+)\n+\n+\n+if is_torch_available():\n+    import numpy as np\n+    import torch\n+\n+    from transformers.models.superglue.modeling_superglue import KeypointMatchingOutput\n+\n+if is_vision_available():\n+    from transformers import SuperGlueImageProcessor\n+\n+\n+def random_array(size):\n+    return np.random.randint(255, size=size)\n+\n+\n+def random_tensor(size):\n+    return torch.rand(size)\n+\n+\n+class SuperGlueImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=6,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_grayscale=True,\n+    ):\n+        size = size if size is not None else {\"height\": 480, \"width\": 640}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_grayscale = do_grayscale\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_grayscale\": self.do_grayscale,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return 2, self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False, pairs=True, batch_size=None):\n+        batch_size = batch_size if batch_size is not None else self.batch_size\n+        image_inputs = prepare_image_inputs(\n+            batch_size=batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+        if pairs:\n+            image_inputs = [image_inputs[i : i + 2] for i in range(0, len(image_inputs), 2)]\n+        return image_inputs\n+\n+    def prepare_keypoint_matching_output(self, pixel_values):\n+        max_number_keypoints = 50\n+        batch_size = len(pixel_values)\n+        mask = torch.zeros((batch_size, 2, max_number_keypoints), dtype=torch.int)\n+        keypoints = torch.zeros((batch_size, 2, max_number_keypoints, 2))\n+        matches = torch.full((batch_size, 2, max_number_keypoints), -1, dtype=torch.int)\n+        scores = torch.zeros((batch_size, 2, max_number_keypoints))\n+        for i in range(batch_size):\n+            random_number_keypoints0 = np.random.randint(10, max_number_keypoints)\n+            random_number_keypoints1 = np.random.randint(10, max_number_keypoints)\n+            random_number_matches = np.random.randint(5, min(random_number_keypoints0, random_number_keypoints1))\n+            mask[i, 0, :random_number_keypoints0] = 1\n+            mask[i, 1, :random_number_keypoints1] = 1\n+            keypoints[i, 0, :random_number_keypoints0] = torch.rand((random_number_keypoints0, 2))\n+            keypoints[i, 1, :random_number_keypoints1] = torch.rand((random_number_keypoints1, 2))\n+            random_matches_indices0 = torch.randperm(random_number_keypoints1, dtype=torch.int)[:random_number_matches]\n+            random_matches_indices1 = torch.randperm(random_number_keypoints0, dtype=torch.int)[:random_number_matches]\n+            matches[i, 0, random_matches_indices1] = random_matches_indices0\n+            matches[i, 1, random_matches_indices0] = random_matches_indices1\n+            scores[i, 0, random_matches_indices1] = torch.rand((random_number_matches,))\n+            scores[i, 1, random_matches_indices0] = torch.rand((random_number_matches,))\n+        return KeypointMatchingOutput(mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores)\n+\n+\n+@require_torch\n+@require_vision\n+class SuperGlueImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = SuperGlueImageProcessor if is_vision_available() else None\n+\n+    def setUp(self) -> None:\n+        super().setUp()\n+        self.image_processor_tester = SuperGlueImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processing(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+        self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"height\": 480, \"width\": 640})\n+\n+        image_processor = self.image_processing_class.from_dict(\n+            self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+        )\n+        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    @unittest.skip(reason=\"SuperPointImageProcessor is always supposed to return a grayscaled image\")\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_number_and_format_of_images_in_input(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+\n+        # Cases where the number of images and the format of lists in the input is correct\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=2)\n+        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=2)\n+        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=4)\n+        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual((2, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=6)\n+        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual((3, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+        # Cases where the number of images or the format of lists in the input is incorrect\n+        ## List of 4 images\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=4)\n+        with self.assertRaises(ValueError) as cm:\n+            image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual(ValueError, cm.exception.__class__)\n+\n+        ## List of 3 images\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=3)\n+        with self.assertRaises(ValueError) as cm:\n+            image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual(ValueError, cm.exception.__class__)\n+\n+        ## List of 2 pairs and 1 image\n+        image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=3)\n+        with self.assertRaises(ValueError) as cm:\n+            image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual(ValueError, cm.exception.__class__)\n+\n+    @parameterized.expand(\n+        [\n+            ([random_array((3, 100, 200)), random_array((3, 100, 200))], (1, 2, 3, 480, 640)),\n+            ([[random_array((3, 100, 200)), random_array((3, 100, 200))]], (1, 2, 3, 480, 640)),\n+            ([random_tensor((3, 100, 200)), random_tensor((3, 100, 200))], (1, 2, 3, 480, 640)),\n+            ([random_tensor((3, 100, 200)), random_tensor((3, 100, 200))], (1, 2, 3, 480, 640)),\n+        ],\n+    )\n+    def test_valid_image_shape_in_input(self, image_input, output):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual(output, tuple(image_processed[\"pixel_values\"].shape))\n+\n+    @parameterized.expand(\n+        [\n+            (random_array((3, 100, 200)),),\n+            ([random_array((3, 100, 200))],),\n+            (random_array((1, 3, 100, 200)),),\n+            ([[random_array((3, 100, 200))]],),\n+            ([[random_array((3, 100, 200))], [random_array((3, 100, 200))]],),\n+            ([random_array((1, 3, 100, 200)), random_array((1, 3, 100, 200))],),\n+            (random_array((1, 1, 3, 100, 200)),),\n+        ],\n+    )\n+    def test_invalid_image_shape_in_input(self, image_input):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        with self.assertRaises(ValueError) as cm:\n+            image_processor.preprocess(image_input, return_tensors=\"pt\")\n+        self.assertEqual(ValueError, cm.exception.__class__)\n+\n+    def test_input_images_properly_paired(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs()\n+        pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"np\")\n+        self.assertEqual(len(pre_processed_images[\"pixel_values\"].shape), 5)\n+        self.assertEqual(pre_processed_images[\"pixel_values\"].shape[1], 2)\n+\n+    def test_input_not_paired_images_raises_error(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(pairs=False)\n+        with self.assertRaises(ValueError):\n+            image_processor.preprocess(image_inputs[0])\n+\n+    def test_input_image_properly_converted_to_grayscale(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs()\n+        pre_processed_images = image_processor.preprocess(image_inputs)\n+        for image_pair in pre_processed_images[\"pixel_values\"]:\n+            for image in image_pair:\n+                self.assertTrue(np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...]))\n+\n+    def test_call_numpy(self):\n+        # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n+\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+        for image_pair in image_pairs:\n+            self.assertEqual(len(image_pair), 2)\n+\n+        expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+        # Test with 2 images\n+        encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+        # Test with list of pairs\n+        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+        # Test without paired images\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, pairs=False\n+        )\n+        with self.assertRaises(ValueError):\n+            image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+\n+    def test_call_pil(self):\n+        # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n+\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PIL images\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+        for image_pair in image_pairs:\n+            self.assertEqual(len(image_pair), 2)\n+\n+        expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+        # Test with 2 images\n+        encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+        # Test with list of pairs\n+        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+        # Test without paired images\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, pairs=False)\n+        with self.assertRaises(ValueError):\n+            image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+\n+    def test_call_pytorch(self):\n+        # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n+\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        for image_pair in image_pairs:\n+            self.assertEqual(len(image_pair), 2)\n+\n+        expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+        # Test with 2 images\n+        encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+        self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+        # Test with list of pairs\n+        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+        # Test without paired images\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, torchify=True, pairs=False\n+        )\n+        with self.assertRaises(ValueError):\n+            image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+\n+    def test_image_processor_with_list_of_two_images(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+\n+        image_pairs = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+        )\n+        self.assertEqual(len(image_pairs), 2)\n+        self.assertTrue(isinstance(image_pairs[0], np.ndarray))\n+        self.assertTrue(isinstance(image_pairs[1], np.ndarray))\n+\n+        expected_batch_size = 1\n+        encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+        self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+    @require_torch\n+    def test_post_processing_keypoint_matching(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs()\n+        pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n+        outputs = self.image_processor_tester.prepare_keypoint_matching_output(**pre_processed_images)\n+\n+        def check_post_processed_output(post_processed_output, image_pair_size):\n+            for post_processed_output, (image_size0, image_size1) in zip(post_processed_output, image_pair_size):\n+                self.assertTrue(\"keypoints0\" in post_processed_output)\n+                self.assertTrue(\"keypoints1\" in post_processed_output)\n+                self.assertTrue(\"matching_scores\" in post_processed_output)\n+                keypoints0 = post_processed_output[\"keypoints0\"]\n+                keypoints1 = post_processed_output[\"keypoints1\"]\n+                all_below_image_size0 = torch.all(keypoints0[:, 0] <= image_size0[1]) and torch.all(\n+                    keypoints0[:, 1] <= image_size0[0]\n+                )\n+                all_below_image_size1 = torch.all(keypoints1[:, 0] <= image_size1[1]) and torch.all(\n+                    keypoints1[:, 1] <= image_size1[0]\n+                )\n+                all_above_zero0 = torch.all(keypoints0[:, 0] >= 0) and torch.all(keypoints0[:, 1] >= 0)\n+                all_above_zero1 = torch.all(keypoints0[:, 0] >= 0) and torch.all(keypoints0[:, 1] >= 0)\n+                self.assertTrue(all_below_image_size0)\n+                self.assertTrue(all_below_image_size1)\n+                self.assertTrue(all_above_zero0)\n+                self.assertTrue(all_above_zero1)\n+                all_scores_different_from_minus_one = torch.all(post_processed_output[\"matching_scores\"] != -1)\n+                self.assertTrue(all_scores_different_from_minus_one)\n+\n+        tuple_image_sizes = [\n+            ((image_pair[0].size[0], image_pair[0].size[1]), (image_pair[1].size[0], image_pair[1].size[1]))\n+            for image_pair in image_inputs\n+        ]\n+        tuple_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tuple_image_sizes)\n+\n+        check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n+\n+        tensor_image_sizes = torch.tensor(\n+            [(image_pair[0].size, image_pair[1].size) for image_pair in image_inputs]\n+        ).flip(2)\n+        tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n+\n+        check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)"
        },
        {
            "sha": "0dda82ed8ad90f63617c106c090f50eb6e9bf70b",
            "filename": "tests/models/superglue/test_modeling_superglue.py",
            "status": "added",
            "additions": 427,
            "deletions": 0,
            "changes": 427,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -0,0 +1,427 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import inspect\n+import unittest\n+from typing import List\n+\n+from datasets import load_dataset\n+\n+from transformers.models.superglue.configuration_superglue import SuperGlueConfig\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.utils import cached_property, is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import SuperGlueForKeypointMatching\n+\n+if is_vision_available():\n+    from transformers import AutoImageProcessor\n+\n+\n+class SuperGlueModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        image_width=80,\n+        image_height=60,\n+        keypoint_detector_config=None,\n+        hidden_size: int = 64,\n+        keypoint_encoder_sizes: List[int] = [32, 64],\n+        gnn_layers_types: List[str] = [\"self\", \"cross\"] * 2,\n+        num_attention_heads: int = 4,\n+        sinkhorn_iterations: int = 100,\n+        matching_threshold: float = 0.2,\n+    ):\n+        if keypoint_detector_config is None:\n+            keypoint_detector_config = {\n+                \"encoder_hidden_sizes\": [32, 64],\n+                \"decoder_hidden_size\": 64,\n+                \"keypoint_decoder_dim\": 65,\n+                \"descriptor_decoder_dim\": 64,\n+                \"keypoint_threshold\": 0.005,\n+                \"max_keypoints\": 256,\n+                \"nms_radius\": 4,\n+                \"border_removal_distance\": 4,\n+            }\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_width = image_width\n+        self.image_height = image_height\n+\n+        self.keypoint_detector_config = keypoint_detector_config\n+        self.hidden_size = hidden_size\n+        self.keypoint_encoder_sizes = keypoint_encoder_sizes\n+        self.gnn_layers_types = gnn_layers_types\n+        self.num_attention_heads = num_attention_heads\n+        self.sinkhorn_iterations = sinkhorn_iterations\n+        self.matching_threshold = matching_threshold\n+\n+    def prepare_config_and_inputs(self):\n+        # SuperGlue expects a grayscale image as input\n+        pixel_values = floats_tensor([self.batch_size, 2, 3, self.image_height, self.image_width])\n+        config = self.get_config()\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return SuperGlueConfig(\n+            keypoint_detector_config=self.keypoint_detector_config,\n+            hidden_size=self.hidden_size,\n+            keypoint_encoder_sizes=self.keypoint_encoder_sizes,\n+            gnn_layers_types=self.gnn_layers_types,\n+            num_attention_heads=self.num_attention_heads,\n+            sinkhorn_iterations=self.sinkhorn_iterations,\n+            matching_threshold=self.matching_threshold,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = SuperGlueForKeypointMatching(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        maximum_num_matches = result.mask.shape[-1]\n+        self.parent.assertEqual(\n+            result.keypoints.shape,\n+            (self.batch_size, 2, maximum_num_matches, 2),\n+        )\n+        self.parent.assertEqual(\n+            result.matches.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+        self.parent.assertEqual(\n+            result.matching_scores.shape,\n+            (self.batch_size, 2, maximum_num_matches),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class SuperGlueModelTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (SuperGlueForKeypointMatching,) if is_torch_available() else ()\n+    all_generative_model_classes = () if is_torch_available() else ()\n+\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    has_attentions = True\n+\n+    def setUp(self):\n+        self.model_tester = SuperGlueModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=SuperGlueConfig, has_text_modality=False, hidden_size=64)\n+\n+    def test_config(self):\n+        self.config_tester.create_and_test_config_to_json_string()\n+        self.config_tester.create_and_test_config_to_json_file()\n+        self.config_tester.create_and_test_config_from_and_save_pretrained()\n+        self.config_tester.create_and_test_config_with_num_labels()\n+        self.config_tester.check_config_can_be_init_without_params()\n+        self.config_tester.check_config_arguments_init()\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching does not use feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching is not trainable\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlueForKeypointMatching is not trainable\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SuperGlue does not output any loss term in the forward pass\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.hidden_states\n+            maximum_num_matches = outputs.mask.shape[-1]\n+\n+            hidden_states_sizes = (\n+                self.model_tester.keypoint_encoder_sizes\n+                + [self.model_tester.hidden_size]\n+                + [self.model_tester.hidden_size, self.model_tester.hidden_size * 2]\n+                * len(self.model_tester.gnn_layers_types)\n+                + [self.model_tester.hidden_size] * 2\n+            )\n+\n+            for i, hidden_states_size in enumerate(hidden_states_sizes):\n+                self.assertListEqual(\n+                    list(hidden_states[i].shape[-2:]),\n+                    [hidden_states_size, maximum_num_matches],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_attention_outputs(self):\n+        def check_attention_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.attentions\n+            maximum_num_matches = outputs.mask.shape[-1]\n+\n+            expected_attention_shape = [\n+                self.model_tester.num_attention_heads,\n+                maximum_num_matches,\n+                maximum_num_matches,\n+            ]\n+\n+            for i, attention in enumerate(attentions):\n+                self.assertListEqual(\n+                    list(attention.shape[-3:]),\n+                    expected_attention_shape,\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            check_attention_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+\n+            check_attention_output(inputs_dict, config, model_class)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        from_pretrained_ids = [\"magic-leap-community/superglue_indoor\", \"magic-leap-community/superglue_outdoor\"]\n+        for model_name in from_pretrained_ids:\n+            model = SuperGlueForKeypointMatching.from_pretrained(model_name)\n+            self.assertIsNotNone(model)\n+\n+    def test_forward_labels_should_be_none(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                model_inputs = self._prepare_for_class(inputs_dict, model_class)\n+                # Provide an arbitrary sized Tensor as labels to model inputs\n+                model_inputs[\"labels\"] = torch.rand((128, 128))\n+\n+                with self.assertRaises(ValueError) as cm:\n+                    model(**model_inputs)\n+                self.assertEqual(ValueError, cm.exception.__class__)\n+\n+    def test_batching_equivalence(self):\n+        \"\"\"\n+        Overwriting ModelTesterMixin.test_batching_equivalence since SuperGlue returns `matching_scores` tensors full of\n+        zeros which causes the test to fail, because cosine_similarity of two zero tensors is 0.\n+        Discussed here : https://github.com/huggingface/transformers/pull/29886#issuecomment-2481539481\n+        \"\"\"\n+\n+        def recursive_check(batched_object, single_row_object, model_name, key):\n+            if isinstance(batched_object, (list, tuple)):\n+                for batched_object_value, single_row_object_value in zip(batched_object, single_row_object):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            elif isinstance(batched_object, dict):\n+                for batched_object_value, single_row_object_value in zip(\n+                    batched_object.values(), single_row_object.values()\n+                ):\n+                    recursive_check(batched_object_value, single_row_object_value, model_name, key)\n+            # do not compare returned loss (0-dim tensor) / codebook ids (int) / caching objects\n+            elif batched_object is None or not isinstance(batched_object, torch.Tensor):\n+                return\n+            elif batched_object.dim() == 0:\n+                return\n+            else:\n+                # indexing the first element does not always work\n+                # e.g. models that output similarity scores of size (N, M) would need to index [0, 0]\n+                slice_ids = [slice(0, index) for index in single_row_object.shape]\n+                batched_row = batched_object[slice_ids]\n+                self.assertFalse(\n+                    torch.isnan(batched_row).any(), f\"Batched output has `nan` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isinf(batched_row).any(), f\"Batched output has `inf` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isnan(single_row_object).any(), f\"Single row output has `nan` in {model_name} for key={key}\"\n+                )\n+                self.assertFalse(\n+                    torch.isinf(single_row_object).any(), f\"Single row output has `inf` in {model_name} for key={key}\"\n+                )\n+                self.assertTrue(\n+                    (equivalence(batched_row, single_row_object)) <= 1e-03,\n+                    msg=(\n+                        f\"Batched and Single row outputs are not equal in {model_name} for key={key}. \"\n+                        f\"Difference={equivalence(batched_row, single_row_object)}.\"\n+                    ),\n+                )\n+\n+        def equivalence(tensor1, tensor2):\n+            return torch.max(torch.abs(tensor1 - tensor2))\n+\n+        config, batched_input = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            config.output_hidden_states = True\n+\n+            model_name = model_class.__name__\n+            batched_input_prepared = self._prepare_for_class(batched_input, model_class)\n+            model = model_class(config).to(torch_device).eval()\n+\n+            batch_size = self.model_tester.batch_size\n+            single_row_input = {}\n+            for key, value in batched_input_prepared.items():\n+                if isinstance(value, torch.Tensor) and value.shape[0] % batch_size == 0:\n+                    # e.g. musicgen has inputs of size (bs*codebooks). in most cases value.shape[0] == batch_size\n+                    single_batch_shape = value.shape[0] // batch_size\n+                    single_row_input[key] = value[:single_batch_shape]\n+                else:\n+                    single_row_input[key] = value\n+\n+            with torch.no_grad():\n+                model_batched_output = model(**batched_input_prepared)\n+                model_row_output = model(**single_row_input)\n+\n+            if isinstance(model_batched_output, torch.Tensor):\n+                model_batched_output = {\"model_output\": model_batched_output}\n+                model_row_output = {\"model_output\": model_row_output}\n+\n+            for key in model_batched_output:\n+                recursive_check(model_batched_output[key], model_row_output[key], model_name, key)\n+\n+\n+def prepare_imgs():\n+    dataset = load_dataset(\"hf-internal-testing/image-matching-test-dataset\", split=\"train\")\n+    image1 = dataset[0][\"image\"]\n+    image2 = dataset[1][\"image\"]\n+    image3 = dataset[2][\"image\"]\n+    return [[image1, image2], [image3, image2]]\n+\n+\n+@require_torch\n+@require_vision\n+class SuperGlueModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return (\n+            AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n+            if is_vision_available()\n+            else None\n+        )\n+\n+    @slow\n+    def test_inference(self):\n+        model = SuperGlueForKeypointMatching.from_pretrained(\"magic-leap-community/superglue_outdoor\").to(torch_device)\n+        preprocessor = self.default_image_processor\n+        images = prepare_imgs()\n+        inputs = preprocessor(images=images, return_tensors=\"pt\").to(torch_device)\n+        with torch.no_grad():\n+            outputs = model(**inputs, output_hidden_states=True, output_attentions=True)\n+\n+        predicted_number_of_matches = torch.sum(outputs.matches[0][0] != -1).item()\n+        predicted_matches_values = outputs.matches[0, 0, :30]\n+        predicted_matching_scores_values = outputs.matching_scores[0, 0, :20]\n+\n+        expected_number_of_matches = 282\n+        expected_matches_values = torch.tensor([125,630,137,138,136,143,135,-1,-1,153,\n+                                                154,156,117,160,-1,149,147,152,168,-1,\n+                                                165,182,-1,190,187,188,189,112,-1,193],\n+                                                device=predicted_matches_values.device)  # fmt:skip\n+        expected_matching_scores_values = torch.tensor([0.9899,0.0033,0.9897,0.9889,0.9879,0.7464,0.7109,0.0,0.0,0.9841,\n+                                                        0.9889,0.9639,0.0114,0.9559,0.0,0.9735,0.8018,0.5190,0.9157,0.0],\n+                                                        device=predicted_matches_values.device)  # fmt:skip\n+\n+        \"\"\"\n+        Because of inconsistencies introduced between CUDA versions, the checks here are less strict. SuperGlue relies\n+        on SuperPoint, which may, depending on CUDA version, return different number of keypoints (866 or 867 in this\n+        specific test example). The consequence of having different number of keypoints is that the number of matches\n+        will also be different. In the 20 first matches being checked, having one keypoint less will result in 1 less\n+        match. The matching scores will also be different, as the keypoints are different. The checks here are less\n+        strict to account for these inconsistencies.\n+        Therefore, the test checks that the predicted number of matches, matches and matching scores are close to the\n+        expected values, individually. Here, the tolerance of the number of values changing is set to 2.\n+\n+        This was discussed [here](https://github.com/huggingface/transformers/pull/29886#issuecomment-2482752787)\n+        Such CUDA inconsistencies can be found\n+        [here](https://github.com/huggingface/transformers/pull/33200/files#r1785980300)\n+        \"\"\"\n+\n+        self.assertTrue(abs(predicted_number_of_matches - expected_number_of_matches) < 4)\n+        self.assertTrue(\n+            torch.sum(~torch.isclose(predicted_matching_scores_values, expected_matching_scores_values, atol=1e-2)) < 4\n+        )\n+        self.assertTrue(torch.sum(predicted_matches_values != expected_matches_values) < 4)"
        },
        {
            "sha": "38a90ca63a0152a453045e3ff5394324ee537948",
            "filename": "tests/models/superpoint/test_image_processing_superpoint.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/abe57b6f17f91acb5adab28b40d0cbc85b497b5f/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py?ref=abe57b6f17f91acb5adab28b40d0cbc85b497b5f",
            "patch": "@@ -44,6 +44,7 @@ def __init__(\n         max_resolution=400,\n         do_resize=True,\n         size=None,\n+        do_grayscale=True,\n     ):\n         size = size if size is not None else {\"height\": 480, \"width\": 640}\n         self.parent = parent\n@@ -54,11 +55,13 @@ def __init__(\n         self.max_resolution = max_resolution\n         self.do_resize = do_resize\n         self.size = size\n+        self.do_grayscale = do_grayscale\n \n     def prepare_image_processor_dict(self):\n         return {\n             \"do_resize\": self.do_resize,\n             \"size\": self.size,\n+            \"do_grayscale\": self.do_grayscale,\n         }\n \n     def expected_output_image_shape(self, images):\n@@ -112,6 +115,7 @@ def test_image_processing(self):\n         self.assertTrue(hasattr(image_processing, \"size\"))\n         self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n         self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+        self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n         image_processor = self.image_processing_class.from_dict(self.image_processor_dict)"
        }
    ],
    "stats": {
        "total": 2783,
        "additions": 2777,
        "deletions": 6
    }
}