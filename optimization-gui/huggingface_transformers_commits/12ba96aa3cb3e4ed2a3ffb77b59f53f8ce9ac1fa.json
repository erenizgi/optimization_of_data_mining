{
    "author": "NielsRogge",
    "message": "[Dinov2 with Registers] Some fixes (#35411)\n\n* First draft\r\n\r\n* Thanks claude\r\n\r\n* Remove print statement\r\n\r\n* Use torch_int\r\n\r\n* Address comments\r\n\r\n* Address comment",
    "sha": "12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa",
    "files": [
        {
            "sha": "a0295899fcd72f6481aa4e12cbef8630652f8149",
            "filename": "src/transformers/models/dinov2_with_registers/configuration_dinov2_with_registers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fconfiguration_dinov2_with_registers.py?ref=12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n from ...configuration_utils import PretrainedConfig\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n@@ -69,10 +70,6 @@ class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n             Whether to use the SwiGLU feedforward neural network.\n         num_register_tokens (`int`, *optional*, defaults to 4):\n             Number of register tokens to use.\n-        interpolate_antialias (`bool`, *optional*, defaults to `True`):\n-            Whether to use antialiasing when interpolating the image patches.\n-        interpolate_offset (`float`, *optional*, defaults to 0.0):\n-            Offset to use when interpolating the image patches.\n         out_features (`List[str]`, *optional*):\n             If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n             (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n@@ -105,7 +102,7 @@ class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    model_type = \"dinov2-with-registers-base\"\n+    model_type = \"dinov2_with_registers\"\n \n     def __init__(\n         self,\n@@ -126,8 +123,6 @@ def __init__(\n         drop_path_rate=0.0,\n         use_swiglu_ffn=False,\n         num_register_tokens=4,\n-        interpolate_antialias=True,\n-        interpolate_offset=0.0,\n         out_features=None,\n         out_indices=None,\n         apply_layernorm=True,\n@@ -153,8 +148,6 @@ def __init__(\n         self.drop_path_rate = drop_path_rate\n         self.use_swiglu_ffn = use_swiglu_ffn\n         self.num_register_tokens = num_register_tokens\n-        self.interpolate_antialias = interpolate_antialias\n-        self.interpolate_offset = interpolate_offset\n         self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, num_hidden_layers + 1)]\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names"
        },
        {
            "sha": "bd9d181cdf35c19e40858b866f48a2280ab7d59c",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 35,
            "deletions": 15,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa",
            "patch": "@@ -19,6 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n import collections.abc\n import math\n from typing import Dict, List, Optional, Set, Tuple, Union\n@@ -37,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_dinov2_with_registers import Dinov2WithRegistersConfig\n@@ -99,43 +101,61 @@ def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        resolution images. This implementation supports torch.jit tracing while maintaining backwards compatibility\n+        with the original implementation.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/main/vision_transformer.py\n+        - https://github.com/facebookresearch/dinov2/blob/main/dinov2/models/vision_transformer.py\n         \"\"\"\n-\n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # Skip interpolation for matching dimensions (unless tracing)\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n+\n+        # Handle class token and patch embeddings separately\n         class_pos_embed = self.position_embeddings[:, 0]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n         dim = embeddings.shape[-1]\n+\n+        # Calculate new dimensions\n         height = height // self.config.patch_size\n         width = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        height, width = height + self.config.interpolate_offset, width + self.config.interpolate_offset\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        # Reshape for interpolation\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        # Store original dtype for restoration after interpolation\n         target_dtype = patch_pos_embed.dtype\n+\n+        # Interpolate at float32 precision\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed.to(dtype=torch.float32),\n-            scale_factor=(float(height / math.sqrt(num_positions)), float(width / math.sqrt(num_positions))),\n+            size=(torch_int(height), torch_int(width)),  # Explicit size instead of scale_factor\n             mode=\"bicubic\",\n             align_corners=False,\n-            antialias=self.config.interpolate_antialias,\n-        )\n-        patch_pos_embed = patch_pos_embed.to(dtype=target_dtype)\n-        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+            antialias=True,\n+        ).to(dtype=target_dtype)\n+\n+        # Validate output dimensions if not tracing\n+        if not torch.jit.is_tracing():\n+            if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n+                raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+\n+        # Reshape back to original format\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        # Combine class and patch embeddings\n         return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:"
        },
        {
            "sha": "cbd316c421b0369d86202e15c2f6c6c9e6175de7",
            "filename": "src/transformers/models/dinov2_with_registers/modular_dinov2_with_registers.py",
            "status": "modified",
            "additions": 36,
            "deletions": 26,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodular_dinov2_with_registers.py?ref=12ba96aa3cb3e4ed2a3ffb77b59f53f8ce9ac1fa",
            "patch": "@@ -13,7 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import math\n+\n from typing import Optional\n \n import torch\n@@ -30,7 +30,7 @@\n )\n from ...configuration_utils import PretrainedConfig\n from ...modeling_outputs import BackboneOutput\n-from ...utils import logging\n+from ...utils import logging, torch_int\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n@@ -83,10 +83,6 @@ class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n             Whether to use the SwiGLU feedforward neural network.\n         num_register_tokens (`int`, *optional*, defaults to 4):\n             Number of register tokens to use.\n-        interpolate_antialias (`bool`, *optional*, defaults to `True`):\n-            Whether to use antialiasing when interpolating the image patches.\n-        interpolate_offset (`float`, *optional*, defaults to 0.0):\n-            Offset to use when interpolating the image patches.\n         out_features (`List[str]`, *optional*):\n             If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n             (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n@@ -119,7 +115,7 @@ class Dinov2WithRegistersConfig(BackboneConfigMixin, PretrainedConfig):\n     >>> configuration = model.config\n     ```\"\"\"\n \n-    model_type = \"dinov2-with-registers-base\"\n+    model_type = \"dinov2_with_registers\"\n \n     def __init__(\n         self,\n@@ -140,8 +136,6 @@ def __init__(\n         drop_path_rate=0.0,\n         use_swiglu_ffn=False,\n         num_register_tokens=4,\n-        interpolate_antialias=True,\n-        interpolate_offset=0.0,\n         out_features=None,\n         out_indices=None,\n         apply_layernorm=True,\n@@ -167,8 +161,6 @@ def __init__(\n         self.drop_path_rate = drop_path_rate\n         self.use_swiglu_ffn = use_swiglu_ffn\n         self.num_register_tokens = num_register_tokens\n-        self.interpolate_antialias = interpolate_antialias\n-        self.interpolate_offset = interpolate_offset\n         self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, num_hidden_layers + 1)]\n         self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n             out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n@@ -196,43 +188,61 @@ def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n         This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        resolution images. This implementation supports torch.jit tracing while maintaining backwards compatibility\n+        with the original implementation.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/main/vision_transformer.py\n+        - https://github.com/facebookresearch/dinov2/blob/main/dinov2/models/vision_transformer.py\n         \"\"\"\n-\n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # Skip interpolation for matching dimensions (unless tracing)\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n+\n+        # Handle class token and patch embeddings separately\n         class_pos_embed = self.position_embeddings[:, 0]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n         dim = embeddings.shape[-1]\n+\n+        # Calculate new dimensions\n         height = height // self.config.patch_size\n         width = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        height, width = height + self.config.interpolate_offset, width + self.config.interpolate_offset\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        # Reshape for interpolation\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        # Store original dtype for restoration after interpolation\n         target_dtype = patch_pos_embed.dtype\n+\n+        # Interpolate at float32 precision\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed.to(dtype=torch.float32),\n-            scale_factor=(float(height / math.sqrt(num_positions)), float(width / math.sqrt(num_positions))),\n+            size=(torch_int(height), torch_int(width)),  # Explicit size instead of scale_factor\n             mode=\"bicubic\",\n             align_corners=False,\n-            antialias=self.config.interpolate_antialias,\n-        )\n-        patch_pos_embed = patch_pos_embed.to(dtype=target_dtype)\n-        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+            antialias=True,\n+        ).to(dtype=target_dtype)\n+\n+        # Validate output dimensions if not tracing\n+        if not torch.jit.is_tracing():\n+            if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n+                raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+\n+        # Reshape back to original format\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        # Combine class and patch embeddings\n         return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:"
        }
    ],
    "stats": {
        "total": 123,
        "additions": 73,
        "deletions": 50
    }
}