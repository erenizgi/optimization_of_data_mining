{
    "author": "zhuhanqing",
    "message": " Optim: APOLLO optimizer integration (#36062)\n\n* Added APOLLO optimizer integration\r\n\r\n* fix comment\r\n\r\n* Remove redundancy: Modularize low-rank optimizer construction\r\n\r\n* Remove redundancy: Remove useless comment\r\n\r\n* Fix comment: Add typing\r\n\r\n* Fix comment: Rewrite apollo desc",
    "sha": "08c4959a23a07126978eae47976db10feaee889b",
    "files": [
        {
            "sha": "92bb4367139f1f623d951f68c37740359c82e602",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 91,
            "deletions": 0,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -443,6 +443,97 @@ trainer.train()\n \n Note layerwise optimization is a bit experimental and does not support DDP (Distributed Data Parallel), thus you can run the training script only on a single GPU. Please see [this appropriate section](https://github.com/jiaweizzhao/GaLore?tab=readme-ov-file#train-7b-model-with-a-single-gpu-with-24gb-memory) for more details. Other features such as gradient clipping, DeepSpeed, etc might not be supported out of the box. Please [raise an issue on GitHub](https://github.com/huggingface/transformers/issues) if you encounter such issue.\n \n+### APOLLO\n+\n+Approximated Gradient Scaling for Memory Efficient LLM Optimization (APOLLO) is a memory-efficient training strategy that allows full-parameter learning for both pre-training and fine-tuning, while maintaining AdamW-level performance with SGD-like memory efficiency.\n+\n+* **Ultra-low rank efficiency** → Requires much lower rank than GaLore—even rank 1 (APOLLO-Mini) suffices.\n+* **No expensive SVD computations** → Unlike GaLore, APOLLO leverages random projection, avoiding training stalls.\n+\n+You can read more about the method in the [original repository](https://github.com/zhuhanqing/APOLLO) or the [APOLLO: SGD-like Memory, AdamW-level Performance](https://arxiv.org/abs/2412.05270).\n+\n+First, make sure to install APOLLO from its official repository:\n+\n+```bash\n+pip install apollo-torch\n+```\n+\n+Then, APOLLO optimizers can be used simply by setting `optim=\"apollo_adamw\"` and specifying `optim_target_modules`.\n+`optim_target_modules` can be a list of strings, regex or full path corresponding to the target module names you want to adapt. \n+Currently, only Linear layers are considered to use the APOLLO optimizers, i.e., included in `optim_target_modules,` while the remaining models are still using AdamW. \n+\n+\n+You can also enable layer-wise APOLLO by appending \"layerwise\" to the optimizer name (optim=\"apollo_adamw_layerwise\"), the same as layer-wise GaLore. This saves additional memory for gradient by performing weight updates layer by layer.\n+\n+Below is an end-to-end example script (make sure to `pip install trl datasets`):\n+\n+```python\n+import torch\n+import datasets\n+import trl\n+\n+from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n+\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-apollo\",\n+    max_steps=100,\n+    per_device_train_batch_size=2,\n+    optim=\"apollo_adamw\",\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"]\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=model,\n+    args=args,\n+    train_dataset=train_dataset,\n+    dataset_text_field='text',\n+    max_seq_length=512,\n+)\n+\n+trainer.train()\n+```\n+\n+\n+You can further customize APOLLO’s behavior by passing hyperparameters using `optim_args`.\n+\n+| Parameter         | Description |\n+|------------------|-------------|\n+| `rank` | Rank of the auxiliary sub-space used for gradient scaling. <br> **APOLLO (default=256)** → Works well for 1B and 7B models. <br> **APOLLO-Mini (default=1)** |\n+| `scale_type` | How scaling factors are applied. <br> **`channel`** → Per-channel scaling (used in APOLLO). <br> **`tensor`** → Per-tensor scaling (used in APOLLO-Mini). |\n+| `scale` | Adjusts gradient updates to stabilize training. <br> **APOLLO (default=1.0)** <br> **APOLLO-Mini (default=128)** |\n+| `update_proj_gap` | Steps before updating projection matrices. Default: **200**. |\n+| `proj` | Type of projection. Default: **`random`**. |\n+\n+\n+<Tip>\n+\n+The `scale` parameter can be set to `n/r`, where `n` is the original space dimension and `r` is the low-rank space dimension.\n+Alternatively, you can achieve a similar effect by adjusting the learning rate, while keeping scale at its default value.\n+\n+</Tip>\n+\n+For example, you can enable APOLLO-Mini (rank=1 for extreme memory efficiency) by passing `optim_args`:\n+\n+```python\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-galore\",\n+    max_steps=100,\n+    per_device_train_batch_size=2,\n+    optim=\"apollo_adamw\",\n+    optim_target_modules=[r\".*.attn.*\", r\".*.mlp.*\"],\n+    optim_args=\"proj=random,rank=1,scale=128.0,scale_type=tensor,update_proj_gap=200\",\n+\n+)\n+```\n+\n ### LOMO optimizer\n \n The LOMO optimizers have been introduced in [Full Parameter Fine-Tuning for Large Language Models with Limited Resources](https://hf.co/papers/2306.09782) and [AdaLomo: Low-memory Optimization with Adaptive Learning Rate](https://hf.co/papers/2310.10195)."
        },
        {
            "sha": "831ff48fa9bfa98c5c1d3886672c101c9374be7f",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -62,6 +62,7 @@\n     GGUF_MIN_VERSION,\n     is_accelerate_available,\n     is_apex_available,\n+    is_apollo_torch_available,\n     is_aqlm_available,\n     is_auto_awq_available,\n     is_auto_gptq_available,\n@@ -404,6 +405,14 @@ def require_galore_torch(test_case):\n     return unittest.skipUnless(is_galore_torch_available(), \"test requires GaLore\")(test_case)\n \n \n+def require_apollo_torch(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires GaLore. These tests are skipped when APOLLO isn't installed.\n+    https://github.com/zhuhanqing/APOLLO\n+    \"\"\"\n+    return unittest.skipUnless(is_apollo_torch_available(), \"test requires APOLLO\")(test_case)\n+\n+\n def require_lomo(test_case):\n     \"\"\"\n     Decorator marking a test that requires LOMO. These tests are skipped when LOMO-optim isn't installed."
        },
        {
            "sha": "ca09fc71fe6369c457f809f04675f0886d5aeaea",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 128,
            "deletions": 93,
            "changes": 221,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -151,6 +151,7 @@\n     find_labels,\n     is_accelerate_available,\n     is_apex_available,\n+    is_apollo_torch_available,\n     is_bitsandbytes_available,\n     is_datasets_available,\n     is_galore_torch_available,\n@@ -1315,6 +1316,103 @@ def get_optimizer_cls_and_kwargs(\n             \"betas\": (args.adam_beta1, args.adam_beta2),\n             \"eps\": args.adam_epsilon,\n         }\n+\n+        def setup_low_rank_optimizer(\n+            optimizer_name: str,\n+            optimizer_mapping: Dict[str, Any],\n+            optim_kwargs: Dict[str, Any],\n+            is_layerwise_supported: bool = True,\n+        ) -> Tuple[Any, Any]:\n+            \"\"\"\n+            Helper function to set up low-rank optimizers like GaLore and Apollo.\n+\n+            Args:\n+                optimizer_name (str): Name of the optimizer.\n+                optimizer_mapping (dict): Mapping of optimizer names to their classes.\n+                optim_kwargs (dict): Keyword arguments for the optimizer.\n+                is_layerwise_supported (bool): Whether layerwise optimization is supported.\n+\n+            Returns:\n+                Tuple[Any, Any]: Optimizer class and updated optimizer kwargs.\n+            \"\"\"\n+            is_layerwise = optimizer_name.lower().endswith(\"layerwise\")\n+            if is_layerwise and args.parallel_mode == ParallelMode.DISTRIBUTED and is_layerwise_supported:\n+                raise NotImplementedError(f\"Layer-wise {optimizer_name} does not support DDP at this time\")\n+\n+            optimizer_cls = optimizer_mapping[optimizer_name]\n+\n+            if args.optim_target_modules is None:\n+                raise ValueError(f\"You need to define `optim_target_modules` to use {optimizer_name} optimizers\")\n+\n+            if not isinstance(args.optim_target_modules, (list, str)):\n+                raise ValueError(\n+                    f\"`optim_target_modules` must be a list of strings, a regex string, or 'all-linear'. Got: {args.optim_target_modules}\"\n+                )\n+\n+            if model is None:\n+                raise ValueError(f\"You need to pass a model to initialize {optimizer_name} optimizer.\")\n+\n+            all_linear = (\n+                isinstance(args.optim_target_modules, str)\n+                and args.optim_target_modules.replace(\"_\", \"-\") == \"all-linear\"\n+            )\n+\n+            target_params = []\n+            target_params_names = []\n+            for module_name, module in model.named_modules():\n+                target_module_exists, is_regex = check_target_module_exists(\n+                    args.optim_target_modules, module_name, return_is_regex=True\n+                )\n+\n+                if not isinstance(module, nn.Linear):\n+                    if target_module_exists and not is_regex:\n+                        logger.warning(\n+                            f\"{module_name} matched but ignored. {optimizer_name} only supports linear layers.\"\n+                        )\n+                    continue\n+\n+                if not target_module_exists and not all_linear:\n+                    continue\n+\n+                target_params.append(module.weight)\n+                target_params_names.append(module_name + \".weight\")\n+\n+            if len(target_params) == 0:\n+                raise ValueError(f\"No target modules found for {optimizer_name} ({args.optim_target_modules}).\")\n+\n+            non_target_params = [p for n, p in model.named_parameters() if n not in target_params_names]\n+            optim_kwargs.update(optim_args)\n+\n+            param_groups = [\n+                {\"params\": non_target_params},\n+                {\"params\": target_params, **optim_kwargs},\n+            ]\n+\n+            if is_layerwise:\n+                if args.gradient_accumulation_steps != 1:\n+                    raise ValueError(f\"Layerwise {optimizer_name} does not support gradient accumulation!\")\n+\n+                optimizer_dict = {}\n+                for param in non_target_params:\n+                    optimizer_dict[param] = optimizer_cls([{\"params\": [param]}], **optimizer_kwargs)\n+                for param in target_params:\n+                    optimizer_dict[param] = optimizer_cls([{\"params\": [param], **optim_kwargs}], **optimizer_kwargs)\n+\n+                def optimizer_hook(param):\n+                    if param.grad is not None:\n+                        optimizer_dict[param].step()\n+                        optimizer_dict[param].zero_grad()\n+\n+                for param in model.parameters():\n+                    if param.requires_grad:\n+                        param.register_post_accumulate_grad_hook(optimizer_hook)\n+\n+                optimizer_cls = LayerWiseDummyOptimizer\n+                optimizer_kwargs.update({\"optimizer_dict\": optimizer_dict})\n+\n+            optimizer_kwargs.update({\"params\": param_groups})\n+            return optimizer_cls, optimizer_kwargs\n+\n         if args.optim == OptimizerNames.ADAFACTOR:\n             optimizer_cls = Adafactor\n             optimizer_kwargs.update({\"scale_parameter\": False, \"relative_step\": False})\n@@ -1476,10 +1574,6 @@ def get_optimizer_cls_and_kwargs(\n                 )\n             from galore_torch import GaLoreAdafactor, GaLoreAdamW, GaLoreAdamW8bit\n \n-            is_layerwise = args.optim.lower().endswith(\"layerwise\")\n-            if is_layerwise and args.parallel_mode == ParallelMode.DISTRIBUTED:\n-                raise NotImplementedError(\"Layer-wise GaLore does not support DDP at this time\")\n-\n             optimizer_mapping = {\n                 OptimizerNames.GALORE_ADAMW: GaLoreAdamW,\n                 OptimizerNames.GALORE_ADAMW_8BIT: GaLoreAdamW8bit,\n@@ -1489,105 +1583,46 @@ def get_optimizer_cls_and_kwargs(\n                 OptimizerNames.GALORE_ADAFACTOR_LAYERWISE: GaLoreAdafactor,\n             }\n \n-            optimizer_cls = optimizer_mapping[args.optim]\n-\n-            if args.optim_target_modules is None:\n-                raise ValueError(\n-                    \"You need to define a `optim_target_modules` in order to properly use GaLore optimizers\"\n-                )\n-\n-            if not isinstance(args.optim_target_modules, (list, str)):\n-                raise ValueError(\n-                    f\"`optim_target_modules` has to be a list of strings, a string corresponding to a regex, or a specific module or 'all-linear', you passed {args.optim_target_modules}\"\n-                )\n-\n-            if model is None:\n-                raise ValueError(\"You need to pass a model in order to correctly initialize a GaLore optimizer.\")\n-\n-            logger.warning(\n-                \"Activated GaLoRE fine-tuning, depending on your model size and hardware, the training might take a while before starting. Please be patient !\"\n-            )\n-\n-            all_linear = (\n-                isinstance(args.optim_target_modules, str)\n-                and args.optim_target_modules.replace(\"_\", \"-\") == \"all-linear\"\n-            )\n-\n-            galore_params = []\n-            galore_params_names = []\n-            for module_name, module in model.named_modules():\n-                target_module_exists, is_regex = check_target_module_exists(\n-                    args.optim_target_modules, module_name, return_is_regex=True\n-                )\n-\n-                if not isinstance(module, nn.Linear):\n-                    # Warn in case we match but it's not a linear layer\n-                    if target_module_exists and not is_regex:\n-                        logger.warning(\n-                            f\"{module_name} has been matched but ignored as GaLore only supports linear layers. Please double check your `optim_target_modules`!\"\n-                        )\n-\n-                    continue\n-\n-                if not target_module_exists and not all_linear:\n-                    continue\n-\n-                galore_params.append(module.weight)\n-                galore_params_names.append(module_name + \".weight\")\n-\n-            if len(galore_params) == 0:\n-                raise ValueError(\n-                    f\"None of the target modules were found! ({args.optim_target_modules}). Please make sure to pass a valid `target_modules`.\"\n-                )\n-\n-            non_galore_params = [p for n, p in model.named_parameters() if n not in galore_params_names]\n-\n             galore_optim_kwargs = {\n                 \"rank\": int(optim_args.pop(\"rank\", 128)),\n                 \"update_proj_gap\": int(optim_args.pop(\"update_proj_gap\", 200)),\n                 \"scale\": float(optim_args.pop(\"scale\", 0.25)),\n                 \"proj_type\": optim_args.pop(\"proj_type\", \"std\"),\n             }\n \n-            # The default args are from the official repository: https://github.com/jiaweizzhao/GaLore\n-            param_groups = [\n-                {\"params\": non_galore_params},\n-                {\"params\": galore_params, **galore_optim_kwargs},\n-            ]\n-\n-            if is_layerwise:\n-                # For layer-wise optimizers, the optimization step is done through post accumulation\n-                # gradient hooks. The trick is to first attach these hooks to the model parameters then\n-                # create a dummy optimizer that will perform no-ops in the Trainer.\n-                # See the original implementation or the nice implementation from @hiyouga\n-                # here: https://github.com/hiyouga/LLaMA-Factory/commit/8664262cde3919e10eaecbd66e8c5d356856362e#diff-ebe08ab14496dfb9e06075f0fdd36799ef6d1535cc4dd4715b74c4e3e06fe3ba\n-                if args.gradient_accumulation_steps != 1:\n-                    raise ValueError(\"Layerwise GaLoRE optimizer do not support gradient accumulation !\")\n-\n-                optimizer_dict = {}\n-                for param in non_galore_params:\n-                    param_groups = [{\"params\": [param]}]\n-                    optimizer_dict[param] = optimizer_cls(param_groups, **optimizer_kwargs)\n-                for param in galore_params:\n-                    param_groups = [{\"params\": [param], **galore_optim_kwargs}]\n-                    optimizer_dict[param] = optimizer_cls(param_groups, **optimizer_kwargs)\n-\n-                def optimizer_hook(param):\n-                    if param.grad is not None:\n-                        optimizer_dict[param].step()\n-                        optimizer_dict[param].zero_grad()\n-\n-                for param in model.parameters():\n-                    if param.requires_grad:\n-                        param.register_post_accumulate_grad_hook(optimizer_hook)\n+            optimizer_cls, optimizer_kwargs = setup_low_rank_optimizer(\n+                args.optim, optimizer_mapping, galore_optim_kwargs\n+            )\n+            if args.optim == OptimizerNames.GALORE_ADAFACTOR:\n+                optimizer_kwargs.update({\"scale_parameter\": False, \"relative_step\": False})\n+        elif args.optim in [\n+            OptimizerNames.APOLLO_ADAMW,\n+            OptimizerNames.APOLLO_ADAMW_LAYERWISE,\n+        ]:\n+            if not is_apollo_torch_available():\n+                raise ImportError(\n+                    \"You need to install `apollo_torch` in order to use APOLLO optimizers\"\n+                    \" install it with `pip install git+https://github.com/zhuhanqing/APOLLO`\"\n+                )\n+            from apollo_torch import APOLLOAdamW\n \n-                optimizer_cls = LayerWiseDummyOptimizer\n-                optimizer_kwargs.update({\"optimizer_dict\": optimizer_dict})\n+            optimizer_mapping = {\n+                OptimizerNames.APOLLO_ADAMW: APOLLOAdamW,\n+                OptimizerNames.APOLLO_ADAMW_LAYERWISE: APOLLOAdamW,\n+            }\n \n-            optimizer_kwargs.update({\"params\": param_groups})\n+            apollo_optim_kwargs = {\n+                \"rank\": int(optim_args.pop(\"rank\", 128)),\n+                \"proj\": optim_args.pop(\"proj\", \"random\"),\n+                \"scale_type\": optim_args.pop(\"scale_type\", \"channel\"),\n+                \"update_proj_gap\": int(optim_args.pop(\"update_proj_gap\", 200)),\n+                \"scale\": float(optim_args.pop(\"scale\", 1.0)),\n+                \"proj_type\": optim_args.pop(\"proj_type\", \"std\"),\n+            }\n \n-            if args.optim == OptimizerNames.GALORE_ADAFACTOR:\n-                optimizer_kwargs.update({\"scale_parameter\": False, \"relative_step\": False})\n+            optimizer_cls, optimizer_kwargs = setup_low_rank_optimizer(\n+                args.optim, optimizer_mapping, apollo_optim_kwargs\n+            )\n         elif args.optim in [OptimizerNames.LOMO, OptimizerNames.ADALOMO]:\n             if not is_lomo_available():\n                 raise ImportError("
        },
        {
            "sha": "94ada9938b9eee3a9fd6f5a1bce0e2802905f4ce",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -185,6 +185,8 @@ class OptimizerNames(ExplicitEnum):\n     SCHEDULE_FREE_RADAM = \"schedule_free_radam\"\n     SCHEDULE_FREE_ADAMW = \"schedule_free_adamw\"\n     SCHEDULE_FREE_SGD = \"schedule_free_sgd\"\n+    APOLLO_ADAMW = \"apollo_adamw\"\n+    APOLLO_ADAMW_LAYERWISE = \"apollo_adamw_layerwise\"\n \n \n # Sometimes users will pass in a `str` repr of a dict in the CLI\n@@ -790,11 +792,10 @@ class TrainingArguments:\n             [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n             `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n         optim_target_modules (`Union[str, List[str]]`, *optional*):\n-            The target modules to optimize, i.e. the module names that you would like to train, right now this is used only for GaLore algorithm\n-            https://arxiv.org/abs/2403.03507\n-            See: https://github.com/jiaweizzhao/GaLore for more details. You need to make sure to pass a valid GaloRe\n-            optimizer, e.g. one of: \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules\n-            only.\n+            The target modules to optimize, i.e. the module names that you would like to train.\n+            Currently used for the GaLore algorithm (https://arxiv.org/abs/2403.03507) and APOLLO algorithm (https://arxiv.org/abs/2412.05270).\n+            See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n+            You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n \n         batch_eval_metrics (`Optional[bool]`, defaults to `False`):\n             If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics"
        },
        {
            "sha": "bf56a584469f1112aa330fae1b727dab0dc8bdaa",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -117,6 +117,7 @@\n     get_torch_version,\n     is_accelerate_available,\n     is_apex_available,\n+    is_apollo_torch_available,\n     is_aqlm_available,\n     is_auto_awq_available,\n     is_auto_gptq_available,"
        },
        {
            "sha": "4aca0431565960aab79c8fbaeda31139b9cea67e",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -99,6 +99,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n \n _accelerate_available, _accelerate_version = _is_package_available(\"accelerate\", return_version=True)\n _apex_available = _is_package_available(\"apex\")\n+_apollo_torch_available = _is_package_available(\"apollo_torch\")\n _aqlm_available = _is_package_available(\"aqlm\")\n _vptq_available, _vptq_version = _is_package_available(\"vptq\", return_version=True)\n _av_available = importlib.util.find_spec(\"av\") is not None\n@@ -403,6 +404,10 @@ def is_galore_torch_available():\n     return _galore_torch_available\n \n \n+def is_apollo_torch_available():\n+    return _apollo_torch_available\n+\n+\n def is_lomo_available():\n     return _lomo_available\n "
        },
        {
            "sha": "3ac2566041d1cfa60b914948da33f5be0e8d9674",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 163,
            "deletions": 0,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/08c4959a23a07126978eae47976db10feaee889b/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08c4959a23a07126978eae47976db10feaee889b/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=08c4959a23a07126978eae47976db10feaee889b",
            "patch": "@@ -66,6 +66,7 @@\n     get_tests_dir,\n     is_staging_test,\n     require_accelerate,\n+    require_apollo_torch,\n     require_bitsandbytes,\n     require_deepspeed,\n     require_galore_torch,\n@@ -2259,6 +2260,168 @@ def test_galore_lr_display_with_scheduler(self):\n         # warm up steps << total steps\n         self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n \n+    @require_apollo_torch\n+    @require_torch_gpu\n+    def test_apollo(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-9,\n+            logging_steps=5,\n+            optim=\"apollo_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # Check this works\n+        _ = trainer.train()\n+\n+    @require_apollo_torch\n+    @require_torch_gpu\n+    def test_apollo_extra_args(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-9,\n+            logging_steps=5,\n+            optim=\"apollo_adamw\",\n+            optim_args=\"proj=random,scale_type=tensor,rank=1,update_proj_gap=100,scale=128.0\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # Check this works\n+        _ = trainer.train()\n+\n+    @require_apollo_torch\n+    @require_torch_gpu\n+    def test_apollo_layerwise(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-9,\n+            logging_steps=5,\n+            optim=\"apollo_adamw_layerwise\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # Check this works\n+        _ = trainer.train()\n+\n+    @require_apollo_torch\n+    @require_torch_gpu\n+    def test_apollo_layerwise_with_scheduler(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=1e-9,\n+            logging_steps=5,\n+            optim=\"apollo_adamw_layerwise\",\n+            lr_scheduler_type=\"cosine\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # Check this works\n+        _ = trainer.train()\n+\n+    @require_apollo_torch\n+    @require_torch_gpu\n+    def test_apollo_lr_display_without_scheduler(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        learning_rate = 1e-9\n+        num_steps = 10\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            learning_rate=learning_rate,\n+            logging_steps=5,\n+            optim=\"apollo_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+        trainer.create_optimizer_and_scheduler(num_training_steps=num_steps)\n+\n+        # reflects displayed lr in trainer\n+        self.assertEqual(trainer.get_learning_rates(), [learning_rate, learning_rate])\n+\n+    @require_apollo_torch\n+    @require_torch_gpu\n+    def test_apollo_lr_display_with_scheduler(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        learning_rate = 2e-4\n+        num_train_epochs = 10\n+        num_warmup_steps = 5\n+\n+        # Trainer without inf/nan filter\n+        args = TrainingArguments(\n+            self.get_auto_remove_tmp_dir(),\n+            num_train_epochs=num_train_epochs,\n+            learning_rate=learning_rate,\n+            warmup_steps=num_warmup_steps,\n+            lr_scheduler_type=\"cosine\",\n+            logging_steps=1,\n+            optim=\"apollo_adamw\",\n+            optim_target_modules=[r\".*attn.*\", r\".*mlp.*\"],\n+        )\n+        trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+        # creating log history of trainer, results don't matter\n+        trainer.train()\n+        logs = trainer.state.log_history[1:][:-1]\n+\n+        # reach given learning rate peak and end with 0 lr\n+        self.assertTrue(logs[num_warmup_steps - 2][\"learning_rate\"] == learning_rate)\n+        self.assertTrue(logs[-1][\"learning_rate\"] == 0)\n+\n+        # increasing and decreasing pattern of lrs\n+        increasing_lrs = [\n+            logs[i][\"learning_rate\"] < logs[i + 1][\"learning_rate\"]\n+            for i in range(len(logs))\n+            if i < num_warmup_steps - 2\n+        ]\n+        decreasing_lrs = [\n+            logs[i][\"learning_rate\"] > logs[i + 1][\"learning_rate\"]\n+            for i in range(len(logs) - 1)\n+            if i >= num_warmup_steps - 2\n+        ]\n+\n+        self.assertTrue(all(increasing_lrs))\n+        self.assertTrue(all(decreasing_lrs))\n+\n+        # warm up steps << total steps\n+        self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n+\n     @require_torch_multi_accelerator\n     def test_data_is_not_parallelized_when_model_is_parallel(self):\n         model = RegressionModel()"
        }
    ],
    "stats": {
        "total": 501,
        "additions": 403,
        "deletions": 98
    }
}