{
    "author": "unknown",
    "message": "Only cast logits to float when computing loss (#34147)\n\n* Only cast logits to float when computing loss\r\n\r\nSome misses from #31292 and #33902\r\n\r\n* Move logits.float() into existing if labels is not None branch",
    "sha": "816f4424964c1a1631e303b663fc3d68f731e923",
    "files": [
        {
            "sha": "797908277930cf0cdf84840ecbefc052ed5f08e3",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -1602,14 +1602,15 @@ def forward(\n \n         hidden_states = outputs[0]\n         logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n \n         # Disallow image tokens which does not include special begin-image and end-image tokens\n         image_tokens = self.model.vocabulary_mapping.image_tokens\n         logits[:, :, image_tokens] = torch.finfo(logits.dtype).min\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()"
        },
        {
            "sha": "50c5b538af306c25fc2baa3d96da0abfd72ed790",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -1101,10 +1101,11 @@ def forward(\n         hidden_states = outputs[0]\n         logits = self.lm_head(hidden_states)\n         logits = logits / self.config.logits_scaling\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()"
        },
        {
            "sha": "07b42822621a3ef1d0c53eb1a63c4117c1649503",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -1345,10 +1345,11 @@ def forward(\n         hidden_states = outputs[0]\n         logits = self.lm_head(hidden_states)\n         logits = logits / self.config.logits_scaling\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()"
        },
        {
            "sha": "748eda8c0263775c0aa62b6fecbe43839859c4c1",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -1210,10 +1210,11 @@ def forward(\n \n         hidden_states = outputs[0]\n         logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             labels = labels.to(logits.device)\n             # Shift so that tokens < n predict n\n             if attention_mask is not None:"
        },
        {
            "sha": "1607261eaac67314964e452dfb4bee02e5998cad",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -526,9 +526,10 @@ def forward(\n         )\n \n         logits = outputs.logits\n-        logits = logits.float()\n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             shift_logits = logits[..., :-1, :]\n             shift_labels = labels[..., 1:]\n             if attention_mask is not None:"
        },
        {
            "sha": "e96eae799cda886021e27fb3b5eef5e19a43b381",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -38,7 +38,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1463,13 +1462,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        },
        {
            "sha": "f4cb84a2444eb6b19ccfda240dd414cba763acd3",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -1760,10 +1760,11 @@ def forward(\n \n         hidden_states = outputs[0]\n         logits = self.lm_head(hidden_states)\n-        logits = logits.float()\n \n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()"
        },
        {
            "sha": "d3164b17fe130c50f961bbec4619a0d674875cb8",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -870,9 +870,10 @@ def forward(\n         cap = self.config.logits_soft_cap\n         logits = nn.functional.tanh(logits / cap) * cap\n \n-        logits = logits.float()\n         loss = None\n         if labels is not None:\n+            # Upcast to float if we need to compute the loss to avoid potential precision issues\n+            logits = logits.float()\n             # Shift so that tokens < n predict n\n             shift_logits = logits[..., :-1, :].contiguous()\n             shift_labels = labels[..., 1:].contiguous()"
        },
        {
            "sha": "921d07f287dca5993fe874df716efb14e31fc19e",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/816f4424964c1a1631e303b663fc3d68f731e923/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=816f4424964c1a1631e303b663fc3d68f731e923",
            "patch": "@@ -51,7 +51,6 @@\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_mamba_ssm_available,\n-    is_torchdynamo_compiling,\n )\n from .configuration_zamba import ZambaConfig\n \n@@ -1473,13 +1472,8 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n-        # TODO: remove the float() operation in v4.46\n-        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :]).float()\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n \n         loss = None\n         if labels is not None:"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 16,
        "deletions": 21
    }
}