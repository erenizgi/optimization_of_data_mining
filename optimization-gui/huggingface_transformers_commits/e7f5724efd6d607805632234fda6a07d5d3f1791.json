{
    "author": "rootonchair",
    "message": "Add Fast Image Processor for Perceiver (#37176)\n\n* add test and fast image processor\n\n* make style\n\n* Update src/transformers/models/perceiver/image_processing_perceiver_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* make style\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "e7f5724efd6d607805632234fda6a07d5d3f1791",
    "files": [
        {
            "sha": "629f1859531fb699bd5491092abab293590a66ee",
            "filename": "docs/source/en/model_doc/perceiver.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7f5724efd6d607805632234fda6a07d5d3f1791/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7f5724efd6d607805632234fda6a07d5d3f1791/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fperceiver.md?ref=e7f5724efd6d607805632234fda6a07d5d3f1791",
            "patch": "@@ -132,6 +132,11 @@ audio classification, video classification, etc.\n [[autodoc]] PerceiverImageProcessor\n     - preprocess\n \n+## PerceiverImageProcessorFast\n+\n+[[autodoc]] PerceiverImageProcessorFast\n+    - preprocess\n+\n ## PerceiverTextPreprocessor\n \n [[autodoc]] models.perceiver.modeling_perceiver.PerceiverTextPreprocessor"
        },
        {
            "sha": "0eccd9ae123fd40473088e8b090e52f7f0b7a70b",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7f5724efd6d607805632234fda6a07d5d3f1791/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7f5724efd6d607805632234fda6a07d5d3f1791/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e7f5724efd6d607805632234fda6a07d5d3f1791",
            "patch": "@@ -125,7 +125,7 @@\n             (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n             (\"owlvit\", (\"OwlViTImageProcessor\",)),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n-            (\"perceiver\", (\"PerceiverImageProcessor\",)),\n+            (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n             (\"phi4_multimodal\", \"Phi4MultimodalImageProcessorFast\"),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),"
        },
        {
            "sha": "da8d30a5a9b54b27afb74fba377024c2c95436f6",
            "filename": "src/transformers/models/perceiver/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7f5724efd6d607805632234fda6a07d5d3f1791/src%2Ftransformers%2Fmodels%2Fperceiver%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7f5724efd6d607805632234fda6a07d5d3f1791/src%2Ftransformers%2Fmodels%2Fperceiver%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2F__init__.py?ref=e7f5724efd6d607805632234fda6a07d5d3f1791",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_perceiver import *\n     from .feature_extraction_perceiver import *\n     from .image_processing_perceiver import *\n+    from .image_processing_perceiver_fast import *\n     from .modeling_perceiver import *\n     from .tokenization_perceiver import *\n else:"
        },
        {
            "sha": "87b24e768465b9fb7db0f06f0d8bb5c15cf038c4",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "added",
            "additions": 133,
            "deletions": 0,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7f5724efd6d607805632234fda6a07d5d3f1791/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7f5724efd6d607805632234fda6a07d5d3f1791/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=e7f5724efd6d607805632234fda6a07d5d3f1791",
            "patch": "@@ -0,0 +1,133 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Perceiver.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING, BaseImageProcessorFast, BatchFeature\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, PILImageResampling, SizeDict\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Perceiver image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class PerceiverImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 224, \"width\": 224}\n+    crop_size = {\"height\": 256, \"width\": 256}\n+    do_resize = True\n+    do_center_crop = True\n+    do_rescale = True\n+    do_normalize = True\n+\n+    def center_crop(\n+        self,\n+        image: \"torch.Tensor\",\n+        crop_size: dict[str, int],\n+        size: dict[str, int],\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Center crop an image to `(size[\"height\"] / crop_size[\"height\"] * min_dim, size[\"width\"] / crop_size[\"width\"] *\n+        min_dim)`. Where `min_dim = min(size[\"height\"], size[\"width\"])`.\n+\n+        If the input size is smaller than `crop_size` along any edge, the image will be padded with zeros and then\n+        center cropped.\n+\n+        Args:\n+            image (`\"torch.Tensor\"`):\n+                Image to center crop.\n+            crop_size (`Dict[str, int]`):\n+                Desired output size after applying the center crop.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+\n+        Returns:\n+            `torch.Tensor`: The center cropped image.\n+        \"\"\"\n+        if size.height is None or size.width is None:\n+            raise ValueError(f\"The size dictionary must have keys 'height' and 'width'. Got {size.keys()}\")\n+        height, width = image.shape[-2:]\n+        min_dim = min(height, width)\n+        cropped_height = int((size.height / crop_size.height) * min_dim)\n+        cropped_width = int((size.width / crop_size.width) * min_dim)\n+        return F.center_crop(image, (cropped_height, cropped_width))\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, size=size, crop_size=crop_size)\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"PerceiverImageProcessorFast\"]"
        },
        {
            "sha": "4fd7aa28ac509623641cee30c5a663ee7a7852d3",
            "filename": "tests/models/perceiver/test_image_processing_perceiver.py",
            "status": "added",
            "additions": 227,
            "deletions": 0,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7f5724efd6d607805632234fda6a07d5d3f1791/tests%2Fmodels%2Fperceiver%2Ftest_image_processing_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7f5724efd6d607805632234fda6a07d5d3f1791/tests%2Fmodels%2Fperceiver%2Ftest_image_processing_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_image_processing_perceiver.py?ref=e7f5724efd6d607805632234fda6a07d5d3f1791",
            "patch": "@@ -0,0 +1,227 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.image_utils import PILImageResampling\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import PerceiverImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import PerceiverImageProcessorFast\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class PerceiverImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        num_images=1,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=40,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_resize=True,\n+        size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        resample=PILImageResampling.BICUBIC,\n+    ):\n+        self.crop_size = crop_size if crop_size is not None else {\"height\": 256, \"width\": 256}\n+        self.size = size if size is not None else {\"height\": 224, \"width\": 224}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.num_images = num_images\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_center_crop = do_center_crop\n+        self.do_resize = do_resize\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"resample\": self.resample,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class PerceiverImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = PerceiverImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = PerceiverImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = PerceiverImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"crop_size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for sample_images in image_inputs:\n+                for image in sample_images:\n+                    self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_numpy_4_channels(self):\n+        # Idefics3 always processes images as RGB, so it always returns images with 3 channels\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processor_dict = self.image_processor_dict\n+            image_processing = image_processing_class(**image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+\n+            for sample_images in image_inputs:\n+                for image in sample_images:\n+                    self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size, *expected_output_image_shape)\n+            )\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for images in image_inputs:\n+                for image in images:\n+                    self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test batched\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size, *expected_output_image_shape),\n+            )"
        }
    ],
    "stats": {
        "total": 368,
        "additions": 367,
        "deletions": 1
    }
}