{
    "author": "keetrap",
    "message": "Add Distill Any Depth (#36614)\n\n* Added conversion Script\n\n* Update src/transformers/models/depth_anything/convert_distill_any_depth_to_hf.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Updated Conversion Script\n\n* Update src/transformers/models/depth_anything/convert_distill_any_depth_to_hf.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "7e813f9cf0d66a8340c19ea56be964e5f1b440a8",
    "files": [
        {
            "sha": "3dc8db2e97bbb48e359d7f4e4f14747b12cac281",
            "filename": "src/transformers/models/depth_anything/convert_distill_any_depth_to_hf.py",
            "status": "added",
            "additions": 246,
            "deletions": 0,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e813f9cf0d66a8340c19ea56be964e5f1b440a8/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconvert_distill_any_depth_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e813f9cf0d66a8340c19ea56be964e5f1b440a8/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconvert_distill_any_depth_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconvert_distill_any_depth_to_hf.py?ref=7e813f9cf0d66a8340c19ea56be964e5f1b440a8",
            "patch": "@@ -0,0 +1,246 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert Distill Any Depth checkpoints from the original repository. URL:\n+https://github.com/Westlake-AGI-Lab/Distill-Any-Depth\"\"\"\n+\n+import argparse\n+import re\n+from pathlib import Path\n+\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+from safetensors.torch import load_file\n+\n+from transformers import DepthAnythingConfig, DepthAnythingForDepthEstimation, Dinov2Config, DPTImageProcessor\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"(backbone|pretrained)\\.cls_token\": r\"backbone.embeddings.cls_token\",\n+    r\"(backbone|pretrained)\\.mask_token\": r\"backbone.embeddings.mask_token\",\n+    r\"(backbone|pretrained)\\.pos_embed\": r\"backbone.embeddings.position_embeddings\",\n+    r\"(backbone|pretrained)\\.patch_embed\\.proj\\.(weight|bias)\": r\"backbone.embeddings.patch_embeddings.projection.\\2\",\n+    r\"(backbone|pretrained)\\.norm\\.(weight|bias)\": r\"backbone.layernorm.\\2\",\n+    r\"(backbone|pretrained)(\\.blocks(\\.\\d+)?)?\\.(\\d+)\\.attn\\.proj\\.(weight|bias)\": r\"backbone.encoder.layer.\\4.attention.output.dense.\\5\",\n+    r\"(backbone|pretrained)(\\.blocks(\\.\\d+)?)?\\.(\\d+)\\.ls(1|2)\\.gamma\": r\"backbone.encoder.layer.\\4.layer_scale\\5.lambda1\",\n+    r\"(backbone|pretrained)(\\.blocks(\\.\\d+)?)?\\.(\\d+)\\.mlp\\.fc(1|2)\\.(weight|bias)\": r\"backbone.encoder.layer.\\4.mlp.fc\\5.\\6\",\n+    r\"(backbone|pretrained)(\\.blocks(\\.\\d+)?)?\\.(\\d+)\\.norm(1|2)\\.(weight|bias)\": r\"backbone.encoder.layer.\\4.norm\\5.\\6\",\n+    r\"depth_head\\.projects\\.(\\d+)\\.(weight|bias)\": r\"neck.reassemble_stage.layers.\\1.projection.\\2\",\n+    r\"depth_head\\.resize_layers\\.(?!2)(\\d+)\\.(weight|bias)\": r\"neck.reassemble_stage.layers.\\1.resize.\\2\",\n+    r\"depth_head\\.scratch\\.layer(\\d+)_rn\\.weight\": lambda m: f\"neck.convs.{int(m[1])-1}.weight\",\n+    r\"depth_head\\.scratch\\.output_conv(\\d+)(?:\\.(\\d+))?\\.(weight|bias)\": lambda m: (\n+        f\"head.conv{int(m[1]) + (int(m[2])//2 if m[2] else 0)}.{m[3]}\" if m[1] == \"2\" else f\"head.conv{m[1]}.{m[3]}\"\n+    ),\n+    r\"depth_head\\.scratch\\.refinenet(\\d+)\\.out_conv\\.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{3 - (int(m[1])-1)}.projection.{m[2]}\",\n+    r\"depth_head\\.scratch\\.refinenet(\\d+)\\.resConfUnit(\\d+)\\.conv(\\d+)\\.(weight|bias)\": lambda m: f\"neck.fusion_stage.layers.{3 - (int(m[1])-1)}.residual_layer{m[2]}.convolution{m[3]}.{m[4]}\",\n+}\n+\n+\n+def get_dpt_config(model_name):\n+    if \"small\" in model_name:\n+        out_indices = [3, 6, 9, 12]\n+        backbone_config = Dinov2Config.from_pretrained(\n+            \"facebook/dinov2-small\", out_indices=out_indices, apply_layernorm=True, reshape_hidden_states=False\n+        )\n+        fusion_hidden_size = 64\n+        neck_hidden_sizes = [48, 96, 192, 384]\n+    elif \"base\" in model_name:\n+        out_indices = [3, 6, 9, 12]\n+        backbone_config = Dinov2Config.from_pretrained(\n+            \"facebook/dinov2-base\", out_indices=out_indices, apply_layernorm=True, reshape_hidden_states=False\n+        )\n+        fusion_hidden_size = 128\n+        neck_hidden_sizes = [96, 192, 384, 768]\n+    elif \"large\" in model_name:\n+        out_indices = [5, 12, 18, 24]\n+        backbone_config = Dinov2Config.from_pretrained(\n+            \"facebook/dinov2-large\", out_indices=out_indices, apply_layernorm=True, reshape_hidden_states=False\n+        )\n+        fusion_hidden_size = 256\n+        neck_hidden_sizes = [256, 512, 1024, 1024]\n+    else:\n+        raise NotImplementedError(f\"Model not supported: {model_name}\")\n+\n+    depth_estimation_type = \"relative\"\n+    max_depth = None\n+\n+    config = DepthAnythingConfig(\n+        reassemble_hidden_size=backbone_config.hidden_size,\n+        patch_size=backbone_config.patch_size,\n+        backbone_config=backbone_config,\n+        fusion_hidden_size=fusion_hidden_size,\n+        neck_hidden_sizes=neck_hidden_sizes,\n+        depth_estimation_type=depth_estimation_type,\n+        max_depth=max_depth,\n+    )\n+\n+    return config\n+\n+\n+def convert_key_pattern(key, mapping):\n+    for pattern, replacement in mapping.items():\n+        match = re.fullmatch(pattern, key)\n+        if match:\n+            if callable(replacement):\n+                return replacement(match)\n+            return re.sub(pattern, replacement, key)\n+    return None\n+\n+\n+def convert_keys(state_dict, config):\n+    new_state_dict = {}\n+    qkv_pattern = r\"(backbone|pretrained)(\\.blocks(\\.\\d+)?)?\\.(\\d+)\\.attn\\.qkv\\.(weight|bias)\"\n+    qkv_keys = [k for k in list(state_dict.keys()) if re.match(qkv_pattern, k)]\n+    for old_key in qkv_keys:\n+        value = state_dict.pop(old_key)\n+        match = re.match(qkv_pattern, old_key)\n+        _, _, _, layer, attr = match.groups()\n+        hidden_size = config.backbone_config.hidden_size\n+        q = value[:hidden_size]\n+        k = value[hidden_size : hidden_size * 2]\n+        v = value[-hidden_size:]\n+\n+        for proj, tensor in zip([\"query\", \"key\", \"value\"], [q, k, v]):\n+            new_key = f\"backbone.encoder.layer.{layer}.attention.attention.{proj}.{attr}\"\n+            new_state_dict[new_key] = tensor\n+\n+    for old_key in list(state_dict.keys()):\n+        value = state_dict.pop(old_key)\n+        new_key = convert_key_pattern(old_key, ORIGINAL_TO_CONVERTED_KEY_MAPPING)\n+\n+        new_state_dict[new_key] = value\n+\n+    return new_state_dict\n+\n+\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    return Image.open(requests.get(url, stream=True).raw)\n+\n+\n+name_to_checkpoint = {\n+    \"distill-any-depth-small\": \"small/model.safetensors\",\n+    \"distill-any-depth-base\": \"base/model.safetensors\",\n+    \"distill-any-depth-large\": \"large/model.safetensors\",\n+}\n+\n+\n+@torch.no_grad()\n+def convert_dpt_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub, verify_logits):\n+    config = get_dpt_config(model_name)\n+\n+    repo_id = \"xingyang1/Distill-Any-Depth\"\n+    filepath = hf_hub_download(repo_id=repo_id, filename=name_to_checkpoint[model_name])\n+    state_dict = load_file(filepath)\n+\n+    converted_state_dict = convert_keys(state_dict, config)\n+\n+    model = DepthAnythingForDepthEstimation(config)\n+    model.load_state_dict(converted_state_dict)\n+    model.eval()\n+\n+    processor = DPTImageProcessor(\n+        do_resize=True,\n+        size={\"height\": 518, \"width\": 518},\n+        ensure_multiple_of=14,\n+        keep_aspect_ratio=True,\n+        do_rescale=True,\n+        do_normalize=True,\n+        image_mean=[0.485, 0.456, 0.406],\n+        image_std=[0.229, 0.224, 0.225],\n+    )\n+\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw)\n+\n+    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n+\n+    with torch.no_grad():\n+        outputs = model(pixel_values)\n+        predicted_depth = outputs.predicted_depth\n+\n+    print(\"Shape of predicted depth:\", predicted_depth.shape)\n+    print(\"First values:\", predicted_depth[0, :3, :3])\n+\n+    if verify_logits:\n+        print(\"Verifying logits...\")\n+        expected_shape = torch.Size([1, 518, 686])\n+\n+        if model_name == \"distill-any-depth-small\":\n+            expected_slice = torch.tensor(\n+                [[2.5653, 2.5249, 2.5570], [2.4897, 2.5235, 2.5355], [2.5255, 2.5261, 2.5422]]\n+            )\n+        elif model_name == \"distill-any-depth-base\":\n+            expected_slice = torch.tensor(\n+                [[4.8976, 4.9075, 4.9403], [4.8872, 4.8906, 4.9448], [4.8712, 4.8898, 4.8838]]\n+            )\n+        elif model_name == \"distill-any-depth-large\":\n+            expected_slice = torch.tensor(\n+                [[55.1067, 51.1828, 51.6803], [51.9098, 50.7529, 51.4494], [50.1745, 50.5491, 50.8818]]\n+            )\n+        else:\n+            raise ValueError(\"Not supported\")\n+\n+        assert predicted_depth.shape == torch.Size(expected_shape)\n+        assert torch.allclose(predicted_depth[0, :3, :3], expected_slice, atol=1e-4)\n+        print(\"Looks ok!\")\n+\n+    if pytorch_dump_folder_path is not None:\n+        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+        print(f\"Saving model and processor to {pytorch_dump_folder_path}\")\n+        model.save_pretrained(pytorch_dump_folder_path)\n+        processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    if push_to_hub:\n+        print(\"Pushing model and processor to hub...\")\n+        model.push_to_hub(repo_id=f\"{model_name.title()}-hf\")\n+        processor.push_to_hub(repo_id=f\"{model_name.title()}-hf\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"distill-any-depth-small\",\n+        type=str,\n+        choices=name_to_checkpoint.keys(),\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=None,\n+        type=str,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether to push the model to the hub after conversion.\",\n+    )\n+    parser.add_argument(\n+        \"--verify_logits\",\n+        action=\"store_true\",\n+        required=False,\n+        help=\"Whether to verify the logits after conversion.\",\n+    )\n+\n+    args = parser.parse_args()\n+    convert_dpt_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.push_to_hub, args.verify_logits)"
        }
    ],
    "stats": {
        "total": 246,
        "additions": 246,
        "deletions": 0
    }
}