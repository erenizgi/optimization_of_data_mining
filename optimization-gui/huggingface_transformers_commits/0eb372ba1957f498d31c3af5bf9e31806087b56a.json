{
    "author": "MekkCyber",
    "message": "[quantization] fix torchao tests after 0.14.0 release (#41777)\n\n* initial commit\n\n* clean int4_weight_only\n\n* make style\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "0eb372ba1957f498d31c3af5bf9e31806087b56a",
    "files": [
        {
            "sha": "6e7da559a4c80d5c21b5d309ef964ecd88acd93e",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 40,
            "deletions": 27,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/0eb372ba1957f498d31c3af5bf9e31806087b56a/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0eb372ba1957f498d31c3af5bf9e31806087b56a/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=0eb372ba1957f498d31c3af5bf9e31806087b56a",
            "patch": "@@ -48,6 +48,8 @@\n     from torchao.quantization import (\n         Float8Tensor,\n         Float8WeightOnlyConfig,\n+        Int4WeightOnlyConfig,\n+        Int8DynamicActivationInt8WeightConfig,\n         Int8WeightOnlyConfig,\n         IntxWeightOnlyConfig,\n         MappingType,\n@@ -118,17 +120,18 @@ def test_repr(self):\n         \"\"\"\n         Check that there is no error in the repr\n         \"\"\"\n-        quantization_config = TorchAoConfig(\"int4_weight_only\", modules_to_not_convert=[\"conv\"], group_size=8)\n+        config = Int4WeightOnlyConfig(group_size=8, layout=TensorCoreTiledLayout())\n+        quantization_config = TorchAoConfig(config, modules_to_not_convert=[\"conv\"])\n         repr(quantization_config)\n \n     def test_json_serializable(self):\n         \"\"\"\n         Check that the config dict can be JSON serialized.\n         \"\"\"\n-        quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=32, layout=TensorCoreTiledLayout())\n+        config = Int4WeightOnlyConfig(group_size=32, layout=TensorCoreTiledLayout())\n+        quantization_config = TorchAoConfig(config)\n         d = quantization_config.to_dict()\n-        self.assertIsInstance(d[\"quant_type_kwargs\"][\"layout\"], list)\n-        self.assertTrue(\"inner_k_tiles\" in d[\"quant_type_kwargs\"][\"layout\"][1])\n+        self.assertTrue(\"inner_k_tiles\" in d[\"quant_type\"][\"default\"][\"_data\"][\"layout\"][\"_data\"])\n         quantization_config.to_json_string(use_diff=False)\n \n \n@@ -159,7 +162,8 @@ def test_int4wo_quant(self):\n         \"\"\"\n         Simple LLM model testing int4 weight only quantization\n         \"\"\"\n-        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n+        config = Int4WeightOnlyConfig(**self.quant_scheme_kwargs)\n+        quant_config = TorchAoConfig(config)\n \n         # Note: we quantize the bfloat16 model on the fly to int4\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n@@ -181,7 +185,8 @@ def test_int4wo_quant_bfloat16_conversion(self):\n         \"\"\"\n         Testing the dtype of model will be modified to be bfloat16 for int4 weight only quantization\n         \"\"\"\n-        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n+        config = Int4WeightOnlyConfig(**self.quant_scheme_kwargs)\n+        quant_config = TorchAoConfig(config)\n \n         # Note: we quantize the bfloat16 model on the fly to int4\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n@@ -203,7 +208,8 @@ def test_int8_dynamic_activation_int8_weight_quant(self):\n         \"\"\"\n         Simple LLM model testing int8_dynamic_activation_int8_weight\n         \"\"\"\n-        quant_config = TorchAoConfig(\"int8_dynamic_activation_int8_weight\")\n+        config = Int8DynamicActivationInt8WeightConfig()\n+        quant_config = TorchAoConfig(config)\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n@@ -426,7 +432,8 @@ def test_int4wo_offload(self):\n             \"lm_head\": 0,\n         }\n \n-        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n+        config = Int4WeightOnlyConfig(**self.quant_scheme_kwargs)\n+        quant_config = TorchAoConfig(config)\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n@@ -461,7 +468,8 @@ def test_int4wo_quant_multi_accelerator(self):\n         set ZE_AFFINITY_MASK=0,1 if you have more than 2 Intel XPUs\n         \"\"\"\n \n-        quant_config = TorchAoConfig(\"int4_weight_only\", **self.quant_scheme_kwargs)\n+        config = Int4WeightOnlyConfig(**self.quant_scheme_kwargs)\n+        quant_config = TorchAoConfig(config)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             dtype=torch.bfloat16,\n@@ -505,29 +513,30 @@ def test_autoquant(self):\n         self.assertEqual(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUT)\n \n \n-@require_torchao\n-@require_torchao_version_greater_or_equal(\"0.8.0\")\n+@require_torchao_version_greater_or_equal(\"0.11.0\")\n class TorchAoSerializationTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 10\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n-    quant_scheme = \"int4_weight_only\"\n-    quant_scheme_kwargs = (\n-        {\"group_size\": 32, \"layout\": Int4CPULayout(), \"version\": 1}\n-        if is_torchao_available() and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n-        else {\"group_size\": 32}\n-    )\n+\n     device = \"cpu\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n+        cls.quant_scheme_kwargs = (\n+            {\"group_size\": 32, \"layout\": Int4CPULayout(), \"version\": 1}\n+            if is_torchao_available()\n+            and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+            else {\"group_size\": 32}\n+        )\n+        cls.quant_scheme = Int4WeightOnlyConfig(**cls.quant_scheme_kwargs)\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n \n     def setUp(self):\n-        self.quant_config = TorchAoConfig(self.quant_scheme, **self.quant_scheme_kwargs)\n-        dtype = torch.bfloat16 if self.quant_scheme == \"int4_weight_only\" else \"auto\"\n+        self.quant_config = TorchAoConfig(self.quant_scheme)\n+        dtype = torch.bfloat16 if isinstance(self.quant_scheme, Int4WeightOnlyConfig) else \"auto\"\n         self.quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n             dtype=dtype,\n@@ -550,7 +559,7 @@ def check_serialization_expected_output(self, device, expected_output, safe_seri\n         \"\"\"\n         Test if we can serialize and load/infer the model again on the same device\n         \"\"\"\n-        dtype = torch.bfloat16 if self.quant_scheme == \"int4_weight_only\" else \"auto\"\n+        dtype = torch.bfloat16 if isinstance(self.quant_scheme, Int4WeightOnlyConfig) else \"auto\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname, safe_serialization=safe_serialization)\n             loaded_quantized_model = AutoModelForCausalLM.from_pretrained(tmpdirname, dtype=dtype, device_map=device)\n@@ -606,12 +615,12 @@ def test_serialization_expected_output(self, config, expected_output):\n \n \n class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n-    quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n-\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n+        cls.quant_scheme = Int8DynamicActivationInt8WeightConfig()\n+        cls.quant_scheme_kwargs = {}\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n     @require_torch_accelerator\n@@ -623,12 +632,12 @@ def test_serialization_expected_output_on_accelerator(self):\n \n \n class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n-    quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n-\n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n+        cls.quant_scheme = Int8WeightOnlyConfig()\n+        cls.quant_scheme_kwargs = {}\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n     @require_torch_accelerator\n@@ -640,15 +649,17 @@ def test_serialization_expected_output_on_accelerator(self):\n \n \n @require_torch_accelerator\n+@require_torchao\n class TorchAoSerializationAcceleratorTest(TorchAoSerializationTest):\n-    quant_scheme, quant_scheme_kwargs = \"int4_weight_only\", {\"group_size\": 32, \"version\": 1}\n     device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n         # fmt: off\n+        cls.quant_scheme = Int4WeightOnlyConfig(**{\"group_size\": 32, \"version\": 1})\n+        cls.quant_scheme_kwargs = {}\n         EXPECTED_OUTPUTS = Expectations(\n             {\n                 (\"xpu\", 3): \"What are we having for dinner?\\n\\nJessica: (smiling)\",\n@@ -661,25 +672,27 @@ def setUpClass(cls):\n \n @require_torch_accelerator\n class TorchAoSerializationW8A8AcceleratorTest(TorchAoSerializationTest):\n-    quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n     device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n+        cls.quant_scheme = Int8DynamicActivationInt8WeightConfig()\n+        cls.quant_scheme_kwargs = {}\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n \n @require_torch_accelerator\n class TorchAoSerializationW8AcceleratorTest(TorchAoSerializationTest):\n-    quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n     device = f\"{torch_device}:0\"\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         super().setUpClass()\n+        cls.quant_scheme = Int8WeightOnlyConfig()\n+        cls.quant_scheme_kwargs = {}\n         cls.EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n "
        }
    ],
    "stats": {
        "total": 67,
        "additions": 40,
        "deletions": 27
    }
}