{
    "author": "Liangliang-Ma",
    "message": "Add XPU type for work-around -inf mask causing sdpa NaN issue in modeling files (#35647)\n\n* add xpu for unmask\r\n\r\n* change modular for generated matching\r\n\r\n* add lastest modeling for helium",
    "sha": "315a9f494e0e00d8652722ce950be590852a4727",
    "files": [
        {
            "sha": "1b0ad5ad92fe18385413e8edf57295d7a0b8a5e3",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -639,7 +639,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ec54af22186e4d93b6068b75ff2bb4431a0d7538",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -639,7 +639,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "86669310c4f884ff8970ab0cefe473007fb0d50e",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -644,7 +644,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "454860458636283679d0724bbaf5489a85989a1c",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -561,7 +561,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ee692c9616f99263d4bc6a5d698cf0a67d2991c7",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1056,7 +1056,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "c393fc877d02965e1431b4fef66e77517ecddc27",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1360,7 +1360,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4b694b37729ba69a3e246d142f0c1a5d1ff82b09",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1101,7 +1101,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "19ca679ad0dfddb119d0eaa73e987b3d56170ff8",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -785,7 +785,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "1e088fcaba00ab61077093771c0cf5360d9cb349",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1431,7 +1431,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "a0c70f58cf9768e47ee1ec1023cb213e04d7ccf4",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -628,7 +628,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "90b4e6dc63c14567f15c4ae7c808acdbdc622dc6",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -706,7 +706,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "3d82b1829226251ee10ac65eb3643412f43716d0",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1159,7 +1159,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "fcc34d05dc47897edff533b05b1137f55598ad47",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -945,7 +945,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "38b285be7373c36beee3dd4766d11c67195beaf4",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1524,7 +1524,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "a914b9ccf0623ab04a61e220559b39a1e6dbfa4a",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1088,7 +1088,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "14639c1ec76581a9888a73a67792a4f234d3bbff",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -677,7 +677,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "534172cc7372ed3025ac18488386fff3c0457b0e",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -687,7 +687,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "3c4ddf7d0c20518ec72dfb8600e7a755241b9966",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -927,7 +927,7 @@ def forward(\n                     # [batch_size, target_length, 1, source_length], not compatible with SDPA, hence this transpose.\n                     self_attention_mask = self_attention_mask.transpose(1, 2)\n \n-                if query_length > 1 and attention_mask is not None and attention_mask.device.type == \"cuda\":\n+                if query_length > 1 and attention_mask is not None and attention_mask.device.type in [\"cuda\", \"xpu\"]:\n                     # From PyTorch 2.1 onwards, F.scaled_dot_product_attention with the memory-efficient attention backend\n                     # produces nans if sequences are completely unattended in the attention mask. Details: https://github.com/pytorch/pytorch/issues/110213\n                     self_attention_mask = AttentionMaskConverter._unmask_unattended("
        },
        {
            "sha": "c30f178d75768025d76588e1618dcd89ac11b385",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -837,7 +837,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d5cd5445772f5c6a4bc2c2bc779bc8483340f2aa",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -681,7 +681,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "738be443603022ff1c48ce4cf4cf86905851e3d3",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -708,7 +708,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "1deda16314058c616114cd193d221e706a832bc4",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -936,7 +936,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "52cdc96e6435121bc45c69e6be953444474b486c",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -690,7 +690,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d4433b42967e648df2a860239726981ec327f88d",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1177,7 +1177,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "fffae951884f3092d269246093056fd6b0feb990",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -674,7 +674,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4dbe4ad4c7f9a7294a51db634207fdae1e8f03f4",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1408,7 +1408,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d1075d2cfdcd7c061085bb33cdda52123569d0b4",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1381,7 +1381,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path."
        },
        {
            "sha": "9738195f3e9d13fb223289e12143962a2c5b486b",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1170,7 +1170,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4fca0602727f8e963406c77692d73911bd515347",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -676,7 +676,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "64984b9c8f4e0accbcadb1f798f635df5be0eb53",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1645,7 +1645,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "4bae713813970e79edf2c4a0a3e88c606c39dd4f",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1133,7 +1133,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "65547967553abdd406c403501032d2c277be25e7",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -667,7 +667,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "fb4d788ff857ede65c9d06f05efd011610a92cc2",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -185,7 +185,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "391797a0a93a59e202776921ad4702e041fbf7cc",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -801,7 +801,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "1a3278367228cef54a047a8d4be5daee1b428259",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1123,7 +1123,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "fdcb1600d37f48586f28d91e35efb5fa34fda648",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1040,7 +1040,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "01d2ff1940fc44a5ca39c3df647f5a7833ee78dd",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1363,7 +1363,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n@@ -1675,7 +1675,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "7be31c35c3f5545e75ca988d336cfe09b82c2643",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1237,7 +1237,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "3c891bf96cb0e949b105b1d9bf068415f1614891",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -924,7 +924,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "ef3e10582f59e9979d0f056fa95cd2c7ea2108de",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -652,7 +652,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "561b7fdf089e8977c02937c37bbd0b50ba795dd8",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -653,7 +653,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "9b0336a32b1ca33cada46b744e2ab9fa2c590df3",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1084,7 +1084,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d6806b89e5fd7adbdea0f9cb9eedaecd1d398183",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -725,7 +725,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "47f67599b869f31946502d55497c7a13e50c05f5",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -650,7 +650,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "0673ece97f30cd591dbda2ecf07f612af9a775fc",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -737,7 +737,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "48d34f137f5afb1de6c0eae70907e8d229ec6c9b",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1247,7 +1247,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "6f22682481fffba98a10a8b37315d047438233e1",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1632,7 +1632,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "43aba63c8a413fca3fed75bea30f25f065de00b3",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1045,7 +1045,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "1f7715c13dcf98032e6855da07e85561d3357e34",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -661,7 +661,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d67942517845fff656e04a48a953c5a7e0562910",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1135,7 +1135,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "d94daa39a7290809f8ca032c64754cdb21abfbd3",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1202,7 +1202,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "37a7666cf4050cfa417e59b456749c5c93cc13aa",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -765,7 +765,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n                 padding_mask = causal_mask[..., :mask_length].eq(0.0) * attention_mask[:, None, None, :].eq(0.0)\n                 causal_mask[..., :mask_length] = causal_mask[..., :mask_length].masked_fill(padding_mask, min_dtype)\n \n-        if attention_mask is not None and attention_mask.device.type == \"cuda\":\n+        if attention_mask is not None and attention_mask.device.type in [\"cuda\", \"xpu\"]:\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213"
        },
        {
            "sha": "66f599d67492705f260dead6991d8e67edd6fdc2",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -980,7 +980,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "9ad40dae53d65ca25b9b5b8b96f88fb6359df63e",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -663,7 +663,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "cd6dfdb52f7325af7a07242380c5885495422647",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1181,7 +1181,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "a1e33404f4b6dfccfa600139006fa60e539ba7aa",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1250,7 +1250,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "af54dcdd87dc1131f7ae30ad8264f8820a419490",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1583,7 +1583,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "00bee69bf907dbc7ade5ffd0adec8dbad9e3396e",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -894,7 +894,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "13d6f05f43709cca0cd4820c5366f32264600e62",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1420,7 +1420,7 @@ def _update_causal_mask(\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n             and not output_attentions\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when"
        },
        {
            "sha": "8088ccd8166ce650fb60b6895a64ef5499ba6def",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315a9f494e0e00d8652722ce950be590852a4727/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=315a9f494e0e00d8652722ce950be590852a4727",
            "patch": "@@ -1168,7 +1168,7 @@ def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n         if (\n             self.config._attn_implementation == \"sdpa\"\n             and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n+            and attention_mask.device.type in [\"cuda\", \"xpu\"]\n         ):\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path."
        }
    ],
    "stats": {
        "total": 122,
        "additions": 61,
        "deletions": 61
    }
}