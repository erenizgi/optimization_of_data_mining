{
    "author": "taemincode",
    "message": "ğŸŒ [i18n-KO] Translated `gpt2.md` to Korean (#39808)\n\n* docs: ko: bamba.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* docs: ko: gpt2.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* Remove bamba.md from docs/source/ko/model_doc/\n\n* Update _toctree.yml",
    "sha": "ac52c77a66ac407b2c207b65e3580bec6051be93",
    "files": [
        {
            "sha": "c2b800ab50138f85eaf9f4572c164997a18f2013",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac52c77a66ac407b2c207b65e3580bec6051be93/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac52c77a66ac407b2c207b65e3580bec6051be93/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=ac52c77a66ac407b2c207b65e3580bec6051be93",
            "patch": "@@ -573,8 +573,8 @@\n         title: GPT NeoX Japanese\n       - local: in_translation\n         title: GPT-J\n-      - local: in_translation\n-        title: GPT2\n+      - local: model_doc/gpt2\n+        title: GPT-2\n       - local: in_translation\n         title: GPTBigCode\n       - local: in_translation"
        },
        {
            "sha": "cdd4dbc615f3bdef6dc813f18b81b1a5225e6110",
            "filename": "docs/source/ko/model_doc/gpt2.md",
            "status": "added",
            "additions": 219,
            "deletions": 0,
            "changes": 219,
            "blob_url": "https://github.com/huggingface/transformers/blob/ac52c77a66ac407b2c207b65e3580bec6051be93/docs%2Fsource%2Fko%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ac52c77a66ac407b2c207b65e3580bec6051be93/docs%2Fsource%2Fko%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgpt2.md?ref=ac52c77a66ac407b2c207b65e3580bec6051be93",
            "patch": "@@ -0,0 +1,219 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+  <div class=\"flex flex-wrap space-x-1\">\n+    <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+    <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+  </div>\n+</div>\n+\n+\n+# GPT-2[[gpt-2]]\n+\n+[GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)ëŠ” GPTì˜ í™•ì¥ ë²„ì „ìœ¼ë¡œ, ì¸ê³¼ì  íŠ¸ëœìŠ¤í¬ë¨¸ ì–¸ì–´ ëª¨ë¸ì´ë©°, 10ë°° ë” ë§ì€ ë§¤ê°œë³€ìˆ˜ì™€ í•™ìŠµ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ì´ì „ì˜ ëª¨ë“  ë‹¨ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ì–´ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ 40GB ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì‚¬ì „ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ì´ ëª¨ë¸ì€ ì œë¡œìƒ· ì„¤ì •ì—ì„œ ë§ì€ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ëª¨ë¸ ì•„í‚¤í…ì²˜ëŠ” ê° í† í°ì´ ì´ì „ í† í°ì—ë§Œ ì£¼ì˜ë¥¼ ê¸°ìš¸ì¼ ìˆ˜ ìˆëŠ” ë‹¨ë°©í–¥(ì¸ê³¼ì ) ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•˜ë¯€ë¡œ, í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ì— íŠ¹íˆ íš¨ê³¼ì ì…ë‹ˆë‹¤.\n+\n+ëª¨ë“  ì›ë³¸ GPT-2 ì²´í¬í¬ì¸íŠ¸ëŠ” [OpenAI community](https://huggingface.co/openai-community?search_models=gpt) ì¡°ì§ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì˜ GPT-2 ëª¨ë¸ì„ í´ë¦­í•˜ì—¬ GPT-2ë¥¼ ë‹¤ì–‘í•œ ì–¸ì–´ ì‘ì—…ì— ì ìš©í•˜ëŠ” ë” ë§ì€ ì˜ˆì‹œë¥¼ í™•ì¸í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” [`Pipeline`] ë˜ëŠ” [`AutoModel`], ê·¸ë¦¬ê³  ëª…ë ¹ì¤„ì—ì„œ GPT-2ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+# í…ìŠ¤íŠ¸ ìƒì„±ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ìƒì„±\n+pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\", torch_dtype=torch.float16, device=0)\n+pipeline(\"Hello, I'm a language model\")\n+```\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+# ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ\n+model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+\n+# ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  GPUë¡œ ì´ë™\n+input_ids = tokenizer(\"Hello, I'm a language model\", return_tensors=\"pt\").to(\"cuda\")\n+\n+# í…ìŠ¤íŠ¸ ìƒì„±\n+output = model.generate(**input_ids, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Hello, I'm a language model\" | transformers run --task text-generation --model openai-community/gpt2 --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+`transformers backend`ë¥¼ ì‚¬ìš©í•˜ì—¬ vLLMìœ¼ë¡œ ëª¨ë¸ì„ ì„œë¹™í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```\n+vllm serve openai-community/gpt2 --model-imp transformers\n+```\n+\n+ì–‘ìí™”ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë” ë‚®ì€ ì •ë°€ë„ë¡œ í‘œí˜„í•˜ì—¬ ëŒ€í˜• ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì…ë‹ˆë‹¤. ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë” ë§ì€ ì–‘ìí™” ë°±ì—”ë“œì— ëŒ€í•´ì„œëŠ” [Quantization](../quantization/overview) ê°œìš”ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” [bitsandbytes](../quantization/bitsandbytes)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë§Œ 4ë¹„íŠ¸ë¡œ ì–‘ìí™”í•©ë‹ˆë‹¤.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n+\n+# ì–‘ìí™” ì„¤ì • êµ¬ì„±\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=\"float16\",\n+    bnb_4bit_use_double_quant=True\n+)\n+\n+# ì–‘ìí™”ëœ ëª¨ë¸ ë¡œë“œ\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"openai-community/gpt2-xl\",\n+    quantization_config=quantization_config,\n+    device_map=\"auto\"\n+)\n+\n+# í† í¬ë‚˜ì´ì € ë¡œë“œ ë° í…ìŠ¤íŠ¸ ìƒì„±\n+tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-xl\")\n+inputs = tokenizer(\"Once upon a time, there was a magical forest\", return_tensors=\"pt\").to(\"cuda\")\n+outputs = model.generate(**inputs, max_new_tokens=100)\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n+```\n+\n+## ì°¸ê³ ì‚¬í•­[[notes]]\n+\n+- GPT-2ëŠ” ì ˆëŒ€ ìœ„ì¹˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì…ë ¥ì„ ì˜¤ë¥¸ìª½ì— íŒ¨ë”©í•˜ì„¸ìš”.\n+- GPT-2ëŠ” ì´ì „ì— ê³„ì‚°ëœ í‚¤-ê°’ ì–´í…ì…˜ ìŒì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [`GPT2Model.forward`]ì˜ [past_key_values](https://huggingface.co/docs/transformers//en/model_doc/gpt2#transformers.GPT2Model.forward.past_key_values) ë§¤ê°œë³€ìˆ˜ë¡œ ì´ ê¸°ëŠ¥ì— ì ‘ê·¼í•˜ì„¸ìš”.\n+- [Mistral](./mistral)ì˜ í•™ìŠµ ì•ˆì •ì„± ê°œì„  ì‚¬í•­ì„ ì ìš©í•˜ë ¤ë©´ [scale_attn_by_inverse_layer_idx](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config.scale_attn_by_inverse_layer_idx)ì™€ [reorder_and_upcast_attn](https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config.reorder_and_upcast_attn) ë§¤ê°œë³€ìˆ˜ë¥¼ í™œì„±í™”í•˜ì„¸ìš”.\n+\n+## GPT2Config\n+\n+[[autodoc]] GPT2Config\n+\n+## GPT2Tokenizer\n+\n+[[autodoc]] GPT2Tokenizer\n+    - save_vocabulary\n+\n+## GPT2TokenizerFast\n+\n+[[autodoc]] GPT2TokenizerFast\n+\n+## GPT2 íŠ¹ì • ì¶œë ¥[[gpt2-specific-outputs]]\n+\n+[[autodoc]] models.gpt2.modeling_gpt2.GPT2DoubleHeadsModelOutput\n+\n+[[autodoc]] models.gpt2.modeling_tf_gpt2.TFGPT2DoubleHeadsModelOutput\n+\n+<frameworkcontent>\n+<pt>\n+\n+## GPT2Model\n+\n+[[autodoc]] GPT2Model\n+    - forward\n+\n+## GPT2LMHeadModel\n+\n+[[autodoc]] GPT2LMHeadModel\n+    - forward\n+\n+## GPT2DoubleHeadsModel\n+\n+[[autodoc]] GPT2DoubleHeadsModel\n+    - forward\n+\n+## GPT2ForQuestionAnswering\n+\n+[[autodoc]] GPT2ForQuestionAnswering\n+    - forward\n+\n+## GPT2ForSequenceClassification\n+\n+[[autodoc]] GPT2ForSequenceClassification\n+    - forward\n+\n+## GPT2ForTokenClassification\n+\n+[[autodoc]] GPT2ForTokenClassification\n+    - forward\n+\n+</pt>\n+<tf>\n+\n+## TFGPT2Model\n+\n+[[autodoc]] TFGPT2Model\n+    - call\n+\n+## TFGPT2LMHeadModel\n+\n+[[autodoc]] TFGPT2LMHeadModel\n+    - call\n+\n+## TFGPT2DoubleHeadsModel\n+\n+[[autodoc]] TFGPT2DoubleHeadsModel\n+    - call\n+\n+## TFGPT2ForSequenceClassification\n+\n+[[autodoc]] TFGPT2ForSequenceClassification\n+    - call\n+\n+## TFSequenceClassifierOutputWithPast\n+\n+[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutputWithPast\n+\n+## TFGPT2Tokenizer\n+\n+[[autodoc]] TFGPT2Tokenizer\n+\n+</tf>\n+<jax>\n+\n+## FlaxGPT2Model\n+\n+[[autodoc]] FlaxGPT2Model\n+    - __call__\n+\n+## FlaxGPT2LMHeadModel\n+\n+[[autodoc]] FlaxGPT2LMHeadModel\n+    - __call__\n+\n+</jax>\n+</frameworkcontent>\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 223,
        "additions": 221,
        "deletions": 2
    }
}