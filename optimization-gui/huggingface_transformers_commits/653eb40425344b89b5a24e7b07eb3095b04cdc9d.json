{
    "author": "OmarManzoor",
    "message": "Add sdpa for BioGpt (#33592)\n\n* Add sdpa for BioGpt\r\n\r\n* Updates\r\n\r\n* Add the docs\r\n\r\n* [run_slow] biogpt\r\n\r\n* Use the copy mechanism to ensure consistency\r\n\r\n* [run_slow] biogpt",
    "sha": "653eb40425344b89b5a24e7b07eb3095b04cdc9d",
    "files": [
        {
            "sha": "7d0943d5393df66c363af16f25cf78fde55995bf",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/653eb40425344b89b5a24e7b07eb3095b04cdc9d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/653eb40425344b89b5a24e7b07eb3095b04cdc9d/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=653eb40425344b89b5a24e7b07eb3095b04cdc9d",
            "patch": "@@ -32,6 +32,51 @@ This model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The\n - BioGPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows BioGPT to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n - The model can take the `past_key_values` (for PyTorch) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the BioGptForCausalLM.forward() method for more information on its usage.\n \n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```\n+from transformers import BioGptForCausalLM\n+model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+```\n+\n+On a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and `microsoft/biogpt` model with a CausalLM head,\n+we saw the following speedups during training.\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+| num_training_steps | batch_size | seq_len | is cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | sdpa peak mem (MB) | Mem saving (%) |\n+|--------------------|------------|---------|---------|----------------------------|---------------------------|-------------|---------------------|--------------------|----------------|\n+| 100                | 1          | 128     | False   | 0.038                      | 0.031                     | 21.301      | 1601.862            | 1601.497           | 0.023          |\n+| 100                | 1          | 256     | False   | 0.039                      | 0.034                     | 15.084      | 1624.944            | 1625.296           | -0.022         |\n+| 100                | 2          | 128     | False   | 0.039                      | 0.033                     | 16.820      | 1624.567            | 1625.296           | -0.045         |\n+| 100                | 2          | 256     | False   | 0.065                      | 0.059                     | 10.255      | 1672.164            | 1672.164           | 0.000          |\n+| 100                | 4          | 128     | False   | 0.062                      | 0.058                     | 6.998       | 1671.435            | 1672.164           | -0.044         |\n+| 100                | 4          | 256     | False   | 0.113                      | 0.100                     | 13.316      | 2350.179            | 1848.435           | 27.144         |\n+| 100                | 8          | 128     | False   | 0.107                      | 0.098                     | 9.883       | 2098.521            | 1848.435           | 13.530         |\n+| 100                | 8          | 256     | False   | 0.222                      | 0.196                     | 13.413      | 3989.980            | 2986.492           | 33.601         |\n+\n+On a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and `microsoft/biogpt` model with a simple AutoModel head,\n+we saw the following speedups during inference.\n+\n+| num_batches | batch_size | seq_len | is cuda | is half | use mask | Per token latency eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem eager (MB) | Mem BT (MB) | Mem saved (%) |\n+|-------------|------------|---------|---------|---------|----------|------------------------------|-----------------------------|-------------|----------------|--------------|---------------|\n+| 50          | 1          | 64      | True    | True    | True     | 0.115                        | 0.098                       | 17.392      | 716.998        | 716.998      | 0.000         |\n+| 50          | 1          | 128     | True    | True    | True     | 0.115                        | 0.093                       | 24.640      | 730.916        | 730.916      | 0.000         |\n+| 50          | 2          | 64      | True    | True    | True     | 0.114                        | 0.096                       | 19.204      | 730.900        | 730.900      | 0.000         |\n+| 50          | 2          | 128     | True    | True    | True     | 0.117                        | 0.095                       | 23.529      | 759.262        | 759.262      | 0.000         |\n+| 50          | 4          | 64      | True    | True    | True     | 0.113                        | 0.096                       | 18.325      | 759.229        | 759.229      | 0.000         |\n+| 50          | 4          | 128     | True    | True    | True     | 0.186                        | 0.178                       | 4.289       | 816.478        | 816.478      | 0.000         |\n+\n+\n ## Resources\n \n - [Causal language modeling task guide](../tasks/language_modeling)"
        },
        {
            "sha": "73ae4d5c0c921cfc5152f26c18d5c8e01380a3dd",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/653eb40425344b89b5a24e7b07eb3095b04cdc9d/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/653eb40425344b89b5a24e7b07eb3095b04cdc9d/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=653eb40425344b89b5a24e7b07eb3095b04cdc9d",
            "patch": "@@ -208,6 +208,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Audio Spectrogram Transformer](https://huggingface.co/docs/transformers/model_doc/audio-spectrogram-transformer#transformers.ASTModel)\n * [Bart](https://huggingface.co/docs/transformers/model_doc/bart#transformers.BartModel)\n * [Bert](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel)\n+* [BioGpt](https://huggingface.co/docs/transformers/model_doc/biogpt#transformers.BioGptModel)\n * [CamemBERT](https://huggingface.co/docs/transformers/model_doc/camembert#transformers.CamembertModel)\n * [Chameleon](https://huggingface.co/docs/transformers/model_doc/chameleon#transformers.Chameleon)\n * [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)"
        },
        {
            "sha": "16f7aab5c3dfa5a88abf67489fc38665546113cc",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 128,
            "deletions": 5,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/653eb40425344b89b5a24e7b07eb3095b04cdc9d/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/653eb40425344b89b5a24e7b07eb3095b04cdc9d/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=653eb40425344b89b5a24e7b07eb3095b04cdc9d",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -244,16 +244,130 @@ def forward(\n         return attn_output, attn_weights_reshaped, past_key_value\n \n \n+# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->BioGpt\n+class BioGptSdpaAttention(BioGptAttention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        if output_attentions or layer_head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"BioGptModel is using BioGptSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                key_value_states=key_value_states,\n+                past_key_value=past_key_value,\n+                attention_mask=attention_mask,\n+                layer_head_mask=layer_head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # get query proj\n+        query_states = self.q_proj(hidden_states)\n+        # get key, value proj\n+        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # the provided `key_value_states` to support prefix tuning\n+        if (\n+            is_cross_attention\n+            and past_key_value is not None\n+            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+        ):\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value[0]\n+            value_states = past_key_value[1]\n+        elif is_cross_attention:\n+            # cross_attentions\n+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+        elif past_key_value is not None:\n+            # reuse k, v, self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        else:\n+            # self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_states, value_states)\n+\n+        query_states = self._shape(query_states, tgt_len, bsz)\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n+        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+\n+        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n+        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+\n+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n+        # partitioned across GPUs when using tensor-parallelism.\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n+BIOGPT_ATTENTION_CLASSES = {\n+    \"eager\": BioGptAttention,\n+    \"sdpa\": BioGptSdpaAttention,\n+}\n+\n+\n class BioGptDecoderLayer(nn.Module):\n     def __init__(self, config: BioGptConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n \n-        self.self_attn = BioGptAttention(\n+        self.self_attn = BIOGPT_ATTENTION_CLASSES[config._attn_implementation](\n             embed_dim=self.embed_dim,\n             num_heads=config.num_attention_heads,\n             dropout=config.attention_probs_dropout_prob,\n             is_decoder=True,\n+            is_causal=True,\n         )\n         self.dropout = config.hidden_dropout_prob\n         self.activation_fn = ACT2FN[config.hidden_act]\n@@ -337,6 +451,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     config_class = BioGptConfig\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -444,6 +559,7 @@ def __init__(self, config: BioGptConfig):\n         self.layer_norm = nn.LayerNorm(self.embed_dim)\n \n         self.gradient_checkpointing = False\n+        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -511,9 +627,16 @@ def forward(\n         # embed positions\n         positions = self.embed_positions(attention_mask, past_key_values_length)\n \n-        attention_mask = _prepare_4d_causal_attention_mask(\n-            attention_mask, input_shape, inputs_embeds, past_key_values_length\n-        )\n+        if self._use_sdpa and not output_attentions and head_mask is None:\n+            # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n+        else:\n+            attention_mask = _prepare_4d_causal_attention_mask(\n+                attention_mask, input_shape, inputs_embeds, past_key_values_length\n+            )\n \n         hidden_states = inputs_embeds + positions\n "
        }
    ],
    "stats": {
        "total": 179,
        "additions": 174,
        "deletions": 5
    }
}