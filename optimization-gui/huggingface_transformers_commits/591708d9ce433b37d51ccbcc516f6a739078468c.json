{
    "author": "ydshieh",
    "message": "Add pytest marker: `torch_compile_test` and `torch_export_test` (#39950)\n\n* new marker\n\n* trigger CI\n\n* update\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "591708d9ce433b37d51ccbcc516f6a739078468c",
    "files": [
        {
            "sha": "7a8344ea5056d05fea40eb9fb47a9abe7976794b",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -83,6 +83,8 @@ def pytest_configure(config):\n     config.addinivalue_line(\"markers\", \"is_staging_test: mark test to run only in the staging environment\")\n     config.addinivalue_line(\"markers\", \"accelerate_tests: mark test that require accelerate\")\n     config.addinivalue_line(\"markers\", \"not_device_test: mark the tests always running on cpu\")\n+    config.addinivalue_line(\"markers\", \"torch_compile_test: mark test which tests torch compile functionality\")\n+    config.addinivalue_line(\"markers\", \"torch_export_test: mark test which tests torch export functionality\")\n \n \n def pytest_collection_modifyitems(items):"
        },
        {
            "sha": "76161928f6ba490a33d0af1cd2165ae390ae022d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -2048,6 +2048,7 @@ def test_generate_with_quant_cache(self):\n                 model.generate(**generation_kwargs, **inputs_dict)\n \n     @pytest.mark.generate\n+    @pytest.mark.torch_compile_test\n     @require_torch_greater_or_equal(\"2.6\")  # Uses torch.compiler.set_stance\n     def test_generate_compile_model_forward(self):\n         \"\"\"\n@@ -2744,6 +2745,7 @@ def test_speculative_sampling_target_distribution(self):\n         self.assertTrue(last_token_counts[1] > last_token_counts[3] > last_token_counts[7] > 0)\n         self.assertTrue(last_token_counts[8] > last_token_counts[3])\n \n+    @pytest.mark.torch_export_test\n     def test_cache_dependant_input_preparation_exporting(self):\n         self.assertFalse(\n             is_torchdynamo_exporting()\n@@ -4342,6 +4344,7 @@ def test_prepare_inputs_for_generation_encoder_decoder_llm(self):\n         self.assertTrue(model_inputs[\"encoder_outputs\"] == \"foo\")\n         # See the decoder-only test for more corner cases. The code is the same, so we don't repeat it here.\n \n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_fullgraph_tiny(self):\n         \"\"\"\n         Tests that we can call end-to-end generation with a tiny model (i.e. doesn't crash)\n@@ -4931,6 +4934,7 @@ def test_cache_device_map_with_vision_layer_device_map(self):\n         _ = model.generate(**inputs, max_new_tokens=2, do_sample=False)\n \n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     def test_cpu_offload_doesnt_compile(self):\n         \"\"\"Test that CPU offload doesn't trigger compilation\"\"\"\n         tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")"
        },
        {
            "sha": "eb857f3383c9f62f6a7a45cfd2df96d5470e5436",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from packaging import version\n \n from transformers import AlbertConfig, AutoTokenizer, is_torch_available\n@@ -337,6 +338,7 @@ def test_inference_no_head_absolute_embedding(self):\n         torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n+    @pytest.mark.torch_export_test\n     def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")"
        },
        {
            "sha": "c5829fb97ffb28d76a09c1b9ca4f7e97db458f7f",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -211,6 +212,7 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "f6f33d39ea78167baee8bc57a1a3733e24ce23b3",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -267,6 +267,7 @@ def test_initialization(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "ff696f8cf60709dd2fb94df0b79390f30e885da2",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from datasets import load_dataset\n \n from transformers import BeitConfig\n@@ -285,6 +286,7 @@ def test_feed_forward_chunking(self):\n         pass\n \n     @unittest.skip(reason=\"BEiT can't compile dynamic\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "9cc0a8be24374eac2c0a1eb5d37fe2f0ed8c5c20",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n import unittest\n \n+import pytest\n from packaging import version\n \n from transformers import AutoTokenizer, BertConfig, is_torch_available\n@@ -722,6 +723,7 @@ def test_sdpa_ignored_mask(self):\n             )\n \n     @slow\n+    @pytest.mark.torch_export_test\n     def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")"
        },
        {
            "sha": "b506a442b9eb9ef5ca5b1ebdfc37fe9c587d8ead",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -19,6 +19,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n import requests\n from parameterized import parameterized\n from pytest import mark\n@@ -708,6 +709,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n         self.skipTest(reason=\"CLIP text tower has two attention masks: `causal_attention_mask` and `attention_mask`\")\n \n     @require_torch_sdpa\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"CLIP model can't be compiled dynamic, error in clip_loss`\")\n "
        },
        {
            "sha": "1cf2a8424b54bfee38575b8458cbc151ce1f7493",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -225,6 +225,7 @@ def test_model_flash_attn(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n+    @pytest.mark.torch_export_test\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.5.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.5 to run.\")"
        },
        {
            "sha": "62f87af46007e283b12676493e6d1cf8bfe08292",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -19,6 +19,7 @@\n import unittest\n from typing import ClassVar\n \n+import pytest\n import torch\n from datasets import load_dataset\n \n@@ -287,6 +288,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n     @unittest.skip(reason=\"Pass because ColPali requires `attention_mask is not None`\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "3bf5b8a6c4965b7f3093edf845b0ea04b44fa1cd",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -17,6 +17,7 @@\n import unittest\n from typing import ClassVar\n \n+import pytest\n import torch\n from datasets import load_dataset\n \n@@ -277,6 +278,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n     @unittest.skip(reason=\"Pass because ColQwen2 requires `attention_mask is not None`\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "709436ce1419dc8f510eb9854d7e70b6e055cb6e",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import Data2VecVisionConfig\n from transformers.testing_utils import (\n     require_torch,\n@@ -214,6 +216,7 @@ def test_config(self):\n     @unittest.skip(\n         reason=\"Will fix only if requested by the community: it fails with `torch._dynamo.exc.InternalTorchDynamoError: IndexError: list index out of range`. Without compile, the test pass.\"\n     )\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "f1c6cf6786f1abaa132772cc057c3901842f9ce6",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import BitsAndBytesConfig, Cache, DeepseekV2Config, is_torch_available\n from transformers.testing_utils import require_read_token, require_torch, require_torch_accelerator, slow, torch_device\n \n@@ -173,10 +175,12 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n                 self.assertEqual(layer.values.shape, expected_value_shape)\n \n     @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_compilation_all_outputs(self):\n         pass\n \n     @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n \n@@ -185,10 +189,12 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n     @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_with_static_cache(self):\n         pass\n \n     @unittest.skip(\"Dynamic control flow in MoE\")\n+    @pytest.mark.torch_compile_test\n     def test_torch_compile_for_training(self):\n         pass\n "
        },
        {
            "sha": "3e1dc11998c41d6d2cf364466acd30fdd748be87",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from packaging import version\n from parameterized import parameterized\n \n@@ -311,6 +312,7 @@ def test_generate_compilation_all_outputs(self):\n         pass\n \n     @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n \n@@ -533,6 +535,7 @@ def tearDown(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2"
        },
        {
            "sha": "f0c638a76f225ca9b8d128fb9f94ce33047e8029",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import DepthAnythingConfig, Dinov2Config\n from transformers.file_utils import is_torch_available, is_vision_available\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n@@ -286,6 +288,7 @@ def test_inference(self):\n \n         torch.testing.assert_close(predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n+    @pytest.mark.torch_export_test\n     def test_export(self):\n         for strict in [False, True]:\n             with self.subTest(strict=strict):"
        },
        {
            "sha": "0e644c7c1892e5c5dc60fba4a40dbebdb921dce7",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import DepthProConfig\n from transformers.file_utils import is_torch_available, is_vision_available\n from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n@@ -221,6 +223,7 @@ def test_config(self):\n         self.config_tester.run_common_tests()\n \n     @unittest.skip(reason=\"Inductor error: name 'OpaqueUnaryFn_log2' is not defined\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "10675b1681df3bf87d4c92b5e9ec3768d3317077",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -570,6 +570,7 @@ def tearDown(self):\n     @slow\n     @require_torch_accelerator\n     @require_read_token\n+    @pytest.mark.torch_compile_test\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2\n         # work as intended. See https://github.com/pytorch/pytorch/issues/121943"
        },
        {
            "sha": "871b32b3af70dc72b8a4f5d2a8377f1259a70b2e",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -399,6 +399,7 @@ def test_inference_no_head_absolute_embedding(self):\n \n         torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:"
        },
        {
            "sha": "9426ff7d300a6fb5e2bb910767239f9af72deaeb",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -96,6 +96,7 @@ def test_generate_compilation_all_outputs(self):\n         pass\n \n     @unittest.skip(\"dots.llm1's moe is not compatible `token_indices, weight_indices = torch.where(mask)`\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n "
        },
        {
            "sha": "1d693e7f408c7306f5407a37986e88667d8a980f",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import DPTConfig\n from transformers.file_utils import is_torch_available, is_vision_available\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n@@ -255,6 +257,7 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n     @unittest.skip(reason=\"Inductor error for dynamic shape\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n@@ -420,6 +423,7 @@ def test_post_processing_depth_estimation(self):\n         self.assertTrue(output_enlarged.shape == expected_shape)\n         torch.testing.assert_close(predicted_depth_l, output_enlarged, atol=1e-3, rtol=1e-3)\n \n+    @pytest.mark.torch_export_test\n     def test_export(self):\n         for strict in [True, False]:\n             with self.subTest(strict=strict):"
        },
        {
            "sha": "7bd98b1850fc1611978754dd8f120347701bb088",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -354,6 +354,7 @@ def test_model_generation_beyond_sliding_window(self):\n         del model\n         cleanup(torch_device, gc_collect=True)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "eafebbcb5365f781e06541848d10b9d2c2f03762",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -17,6 +17,8 @@\n import unittest\n from unittest.util import safe_repr\n \n+import pytest\n+\n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, FalconMambaConfig, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n@@ -487,6 +489,7 @@ def test_generation_4bit(self):\n             \"Hello today Iava,\\n\\nI'm sorry to hear that you're having trouble with the \",\n         )\n \n+    @pytest.mark.torch_compile_test\n     def test_generation_torch_compile(self):\n         model = AutoModelForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.float16).to(torch_device)\n         model = torch.compile(model)"
        },
        {
            "sha": "4b5d939359ac17ac834c5bc8c5bebe445d1a8a1b",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -356,6 +356,7 @@ def test_model_7b_4bit(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2\n@@ -394,6 +395,7 @@ def test_compile_static_cache(self):\n         static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     @require_read_token\n     def test_export_static_cache(self):"
        },
        {
            "sha": "b8bfcaad43c134fed2ca5eea219b6fe216c8d4a9",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -306,6 +306,7 @@ def test_model_9b_flash_attn(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     @require_read_token\n     def test_export_static_cache(self):\n@@ -379,6 +380,7 @@ def test_export_static_cache(self):\n     @slow\n     @require_read_token\n     @require_large_cpu_ram\n+    @pytest.mark.torch_export_test\n     def test_export_hybrid_cache(self):\n         from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n         from transformers.pytorch_utils import is_torch_greater_or_equal"
        },
        {
            "sha": "eb87743b2a8a4da3441087ef3fa01fca17587d9f",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -819,6 +819,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n \n+    @pytest.mark.torch_export_test\n     def test_export_text_only_with_hybrid_cache(self):\n         if not is_torch_greater_or_equal(\"2.6.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")"
        },
        {
            "sha": "3d3582cb24354d794289db1b345dff9199266439",
            "filename": "tests/models/glm4_moe/test_modeling_glm4_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n import torch\n from packaging import version\n \n@@ -93,6 +94,7 @@ def tearDown(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     @require_read_token\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2"
        },
        {
            "sha": "49d940f6fb0a9075a175b8d4b3b3387cc58488d4",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -195,6 +195,7 @@ def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported in idefics3 models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n@@ -379,6 +380,7 @@ def test_eager_matches_sdpa_generate(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported in Idefics3 models end-to-end\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "a4dd976fa8ab05096cc885b4b9bea4b7f3069236",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -17,6 +17,7 @@\n import unittest\n from io import BytesIO\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -216,6 +217,7 @@ def test_initialization(self):\n                     )\n \n     @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "9c334b609175df8d7bf0320f53b28eda9d4ef396",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -20,6 +20,7 @@\n from functools import reduce\n \n import numpy as np\n+import pytest\n import requests\n \n from transformers import (\n@@ -294,6 +295,7 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                             pass\n \n     @unittest.skip(\"There are recompilations in Janus\")  # TODO (joao, raushan): fix me\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n "
        },
        {
            "sha": "9c2a3eee735d5412510dcdf0b45e08b24ba2289b",
            "filename": "tests/models/layoutlmv2/test_image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -14,6 +14,7 @@\n \n import unittest\n \n+import pytest\n import requests\n from packaging import version\n \n@@ -202,6 +203,7 @@ def test_slow_fast_equivalence_batched(self):\n     @slow\n     @require_torch_accelerator\n     @require_vision\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):\n         if self.fast_image_processing_class is None:\n             self.skipTest(\"Skipping compilation test as fast image processor is not defined\")"
        },
        {
            "sha": "4603f54dc7f78c2e3084104ecfe2c11af67c8b8d",
            "filename": "tests/models/lfm2/test_modeling_lfm2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2%2Ftest_modeling_lfm2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     require_read_token,\n@@ -88,6 +90,7 @@ def test_contrastive_generate_low_memory(self):\n     @unittest.skip(\n         \"Lfm2 has a special cache format which is not compatible with compile as it has static address for conv cache\"\n     )\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "5be6e9803e05197e9d4f8e4015417a50ab903012",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from packaging import version\n \n from transformers import AutoTokenizer, StaticCache, is_torch_available\n@@ -256,6 +257,7 @@ def test_model_7b_dola_generation(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2\n         # work as intended. See https://github.com/pytorch/pytorch/issues/121943\n@@ -296,6 +298,7 @@ def test_compile_static_cache(self):\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_text)\n \n     @slow\n+    @pytest.mark.torch_export_test\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")"
        },
        {
            "sha": "29f20cc7124e1aa6ac62851b0b033d5b20753cb5",
            "filename": "tests/models/llava_onevision/test_image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_image_processing_llava_onevision.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n \n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension\n from transformers.testing_utils import require_torch, require_vision\n@@ -246,6 +247,7 @@ def test_multi_images(self):\n     @unittest.skip(\n         reason=\"LlavaOnevisionImageProcessorFast doesn't compile (infinitely) when using class transforms\"\n     )  # FIXME yoni\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):\n         pass\n "
        },
        {
            "sha": "2fbe6ef81b37bc1e8bc4ff612e45ce249e78ee77",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -17,6 +17,7 @@\n import unittest\n from unittest.util import safe_repr\n \n+import pytest\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, MambaConfig, is_torch_available\n@@ -518,6 +519,7 @@ def test_simple_generate_cuda_kernels_big(self, device):\n         self.assertEqual(output_sentence, expected_output)\n \n     @slow\n+    @pytest.mark.torch_compile_test\n     def test_compile_mamba_cache(self):\n         expected_output = \"Hello my name is John and I am a\\n\\nI am a single father of a beautiful daughter. I am a\"\n "
        },
        {
            "sha": "de0efc0410d9da902c47754f92af2396c6c46d99",
            "filename": "tests/models/mask2former/test_modeling_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n \n from tests.test_modeling_common import floats_tensor\n from transformers import AutoModelForImageClassification, Mask2FormerConfig, is_torch_available, is_vision_available\n@@ -576,6 +577,7 @@ def test_with_segmentation_maps_and_loss(self):\n \n         self.assertTrue(outputs.loss is not None)\n \n+    @pytest.mark.torch_export_test\n     def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")"
        },
        {
            "sha": "1ca50b33560dc71cfe04297d0ac80fcf87fd86d4",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -19,6 +19,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n from datasets import Audio, load_dataset\n from pytest import mark\n \n@@ -446,6 +447,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n         pass\n \n     @unittest.skip(reason=\"The MimiModel does not have support dynamic compile yet\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "dce2f756119eff9e20911b816ce5bf8051f286df",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -278,6 +278,7 @@ def test_speculative_generation(self):\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n+    @pytest.mark.torch_compile_test\n     @slow\n     def test_compile_static_cache(self):\n         # `torch==2.2` will throw an error on this test (as in other compilation tests), but torch==2.1.2 and torch>2.2"
        },
        {
            "sha": "99b2394037fc3862448401b90b2a0de720f1db0f",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n import accelerate\n+import pytest\n \n from transformers import (\n     AutoProcessor,\n@@ -207,6 +208,7 @@ def test_initialization(self):\n                     )\n \n     @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "0c623d5fa3969577b4dd094af51f54c128d41918",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -352,6 +352,7 @@ def test_generate_with_quant_cache(self):\n         pass\n \n     @unittest.skip(\"For some unknown reasons the tests fails in CrossAttention layer when doing torch.sdpa(). \")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "72bc842ec9c8bc836c2fb533c9f2fe5698a19a4a",
            "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from packaging import version\n \n from transformers import AutoTokenizer, MobileBertConfig, MobileBertForMaskedLM, is_torch_available\n@@ -386,6 +387,7 @@ def test_inference_no_head(self):\n \n         self.assertTrue(lower_bound and upper_bound)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "2a9c630898190520a32814f2f09f6990260bcb74",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -390,6 +390,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n     def test_flash_attn_2_conversion(self):\n         self.skipTest(reason=\"ModernBert doesn't use the ModernBertFlashAttention2 class method.\")\n \n+    @pytest.mark.torch_compile_test\n     def test_saved_config_excludes_reference_compile(self):\n         config = ModernBertConfig(reference_compile=True)\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -501,6 +502,7 @@ def test_inference_sequence_classification(self):\n         expected = torch.tensor([[1.6466, 4.5662]])\n         torch.testing.assert_close(output, expected, rtol=1e-4, atol=1e-4)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "286f93e84805595217a060b768256e35e9d063cf",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -178,6 +178,7 @@ def setUp(self):\n         )\n \n     @unittest.skip(reason=\"The MoshiModel does not have support dynamic compile yet\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n@@ -636,6 +637,7 @@ def test_eager_matches_sdpa_inference(\n         pass\n \n     @unittest.skip(reason=\"The Moshi model does not have support dynamic compile yet\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "4098c45ba21863e8ea4381f70db9fc09885c0ae9",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -20,6 +20,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n from pytest import mark\n \n from transformers import (\n@@ -1235,6 +1236,7 @@ def test_generation_tester_mixin_inheritance(self):\n         pass\n \n     @unittest.skip(reason=(\"MusicGen has a set of composite models which might not have SDPA themselves, e.g. T5.\"))\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "180436e6268c1d99dd1fe7e31dcd376f7f003a86",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -20,6 +20,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n from pytest import mark\n \n from transformers import (\n@@ -1236,6 +1237,7 @@ def test_generation_tester_mixin_inheritance(self):\n         pass\n \n     @unittest.skip(reason=(\"MusicGen has a set of composite models which might not have SDPA themselves, e.g. T5.\"))\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "38395fbbbaa338e98b0839940fa4a2f82d1a5659",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from packaging import version\n from parameterized import parameterized\n \n@@ -327,6 +328,7 @@ def test_simple_encode_decode(self):\n \n         self.assertEqual(rust_tokenizer.encode(\" Hello\"), [24387])\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "b980d10a0f0e1f169e074e2da54f94533f856bc8",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n from packaging import version\n from parameterized import parameterized\n \n@@ -327,6 +328,7 @@ def test_simple_encode_decode(self):\n \n         self.assertEqual(rust_tokenizer.encode(\" Hello\"), [22691])\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "17735b89c6e939e9d93d1cea2aba1b7900834eae",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -318,6 +318,7 @@ def test_generate_with_static_cache(self):\n         pass\n \n     @pytest.mark.generate\n+    @pytest.mark.torch_compile_test\n     @is_flaky\n     def test_generate_compile_model_forward(self):\n         super().test_generate_compile_model_forward()"
        },
        {
            "sha": "71b99e6786c3e986c7eaef45aa2190417df5d331",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import Phi3Config, StaticCache, is_torch_available\n from transformers.models.auto.configuration_auto import AutoConfig\n from transformers.testing_utils import (\n@@ -342,6 +344,7 @@ def test_phi3_mini_4k_sliding_window(self):\n \n         self.assertListEqual(output_text, EXPECTED_OUTPUT)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4"
        },
        {
            "sha": "25a5ef9f3c9387a5ce9183743be6f278eb4f9eb8",
            "filename": "tests/models/phi4_multimodal/test_image_processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fphi4_multimodal%2Ftest_image_processing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fphi4_multimodal%2Ftest_image_processing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_image_processing_phi4_multimodal.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -20,6 +20,7 @@\n import warnings\n \n import numpy as np\n+import pytest\n from packaging import version\n \n from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n@@ -288,6 +289,7 @@ def test_image_processor_preprocess_arguments(self):\n             self.skipTest(reason=\"No validation found for `preprocess` method\")\n \n     @slow\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):\n         if self.fast_image_processing_class is None:\n             self.skipTest(\"Skipping compilation test as fast image processor is not defined\")"
        },
        {
            "sha": "497d6ae08cfa9f308a7f4d2e35a74e1cfb4e8acf",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -14,6 +14,7 @@\n \n import unittest\n \n+import pytest\n import requests\n from parameterized import parameterized\n \n@@ -253,6 +254,7 @@ def test_generate_compilation_all_outputs(self):\n     @unittest.skip(\n         reason=\"Supported only for text-only inputs (otherwise dynamic control flows for multimodal inputs)\"\n     )\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n "
        },
        {
            "sha": "b2763a348a9dac16a74ce6ef4bce16204448375f",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n import requests\n from packaging import version\n \n@@ -263,6 +264,7 @@ def test_slow_fast_equivalence_batched(self):\n     @slow\n     @require_torch_gpu\n     @require_vision\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):\n         if self.fast_image_processing_class is None:\n             self.skipTest(\"Skipping compilation test as fast image processor is not defined\")"
        },
        {
            "sha": "e0aad3d5d9ef359099cbcd5daaf82ed99459a81b",
            "filename": "tests/models/prompt_depth_anything/test_modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fprompt_depth_anything%2Ftest_modeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fprompt_depth_anything%2Ftest_modeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprompt_depth_anything%2Ftest_modeling_prompt_depth_anything.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n \n import unittest\n \n+import pytest\n import requests\n \n from transformers import Dinov2Config, PromptDepthAnythingConfig\n@@ -284,6 +285,7 @@ def test_inference(self):\n \n         self.assertTrue(torch.allclose(predicted_depth[0, :3, :3], expected_slice, atol=1e-3))\n \n+    @pytest.mark.torch_export_test\n     def test_export(self):\n         for strict in [False, True]:\n             if strict and get_torch_major_and_minor_version() == \"2.7\":"
        },
        {
            "sha": "d520b593f638a06d8e382cd12e0801933dcab9f7",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -239,6 +239,7 @@ def test_speculative_generation(self):\n         backend_empty_cache(torch_device)\n         gc.collect()\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "b930aef695bb75a3d039aa0e7f527b98072210b4",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -21,6 +21,7 @@\n from urllib.request import urlopen\n \n import librosa\n+import pytest\n import requests\n \n from transformers import (\n@@ -281,6 +282,7 @@ def test_correct_missing_keys(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported because in QwenOmniThinker models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n@@ -444,6 +446,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     # TODO (joao, raushan): there are multiple standardization issues in this model that prevent this test from\n     # passing, fix me\n     @unittest.skip(\"Cannot handle 4D attention mask\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n "
        },
        {
            "sha": "1c4aa2c09387cfdf3855794b3c077e62efa1b30a",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -19,6 +19,7 @@\n from urllib.request import urlopen\n \n import librosa\n+import pytest\n \n from transformers import (\n     AutoProcessor,\n@@ -148,6 +149,7 @@ def setUp(self):\n         self.config_tester = ConfigTester(self, config_class=Qwen2AudioConfig, has_text_modality=False)\n \n     @unittest.skip(reason=\"Compile not yet supported because in Qwen2Audio models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "223112f24a0f1d5ecac997d454b941333bb93340",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -231,6 +231,7 @@ def test_speculative_generation(self):\n \n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "5001438e4c6eb3d4b6709b7aaf50cd72fa743473",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,8 @@\n \n import unittest\n \n+import pytest\n+\n from transformers import AutoTokenizer, RobertaConfig, is_torch_available\n from transformers.testing_utils import TestCasePlus, require_torch, slow, torch_device\n \n@@ -575,6 +577,7 @@ def test_inference_classification_head(self):\n \n         torch.testing.assert_close(output, expected_tensor, rtol=1e-4, atol=1e-4)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:"
        },
        {
            "sha": "4b6f1df1451a020c9bedbc92b5f7c7510a01a9e2",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,7 @@\n import tempfile\n import unittest\n \n+import pytest\n import requests\n \n from transformers import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig, pipeline\n@@ -257,6 +258,7 @@ def test_hidden_states_output(self):\n         pass\n \n     @require_torch_sdpa\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n \n@@ -658,6 +660,7 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @require_torch_sdpa\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n "
        },
        {
            "sha": "98a7f5a452568d3d898179c872744bd682246fe1",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -17,6 +17,7 @@\n import tempfile\n import unittest\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -265,6 +266,7 @@ def test_hidden_states_output(self):\n         pass\n \n     @require_torch_sdpa\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n \n@@ -706,6 +708,7 @@ def test_model_from_pretrained(self):\n         self.assertIsNotNone(model)\n \n     @require_torch_sdpa\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         self.skipTest(reason=\"SamHQModel can't be compiled dynamic yet\")\n "
        },
        {
            "sha": "cb58cca5d49b4734f2d7b01023abbab47b70d082",
            "filename": "tests/models/smollm3/test_modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmollm3%2Ftest_modeling_smollm3.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -172,6 +172,7 @@ def test_model_3b_long_prompt(self):\n         backend_empty_cache(torch_device)\n         gc.collect()\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):"
        },
        {
            "sha": "495c0d346aa5d5925dbd58d5382389cc97bc552d",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -186,6 +186,7 @@ def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported in SmolVLM models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n@@ -387,6 +388,7 @@ def test_generate_with_static_cache(self):\n         pass\n \n     @unittest.skip(reason=\"Compile not yet supported in SmolVLM models\")\n+    @pytest.mark.torch_compile_test\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n "
        },
        {
            "sha": "535008a1a02f24b0fb13bbd7e995c44f90d38dd7",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -19,6 +19,8 @@\n import tempfile\n import unittest\n \n+import pytest\n+\n from transformers import T5Config, is_torch_available\n from transformers.models.auto.modeling_auto import MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES\n from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n@@ -1610,6 +1612,7 @@ def test_contrastive_search_t5(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     def test_compile_static_cache(self):\n         NUM_TOKENS_TO_GENERATE = 40\n         EXPECTED_TEXT_COMPLETION = [\n@@ -1650,6 +1653,7 @@ def test_compile_static_cache(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     def test_compile_static_cache_encoder(self):\n         prompts = [\n             \"summarize: Simply put, the theory of relativity states that 1) the speed of light is constant in all inertial \"\n@@ -1668,6 +1672,7 @@ def test_compile_static_cache_encoder(self):\n         logits_compiled = model(**inputs)\n         torch.testing.assert_close(logits[0][:, -3:, -3], logits_compiled[0][:, -3:, -3], rtol=1e-5, atol=1e-5)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_encoder(self):\n         \"\"\"Test exporting T5EncoderModel to torch export format.\"\"\"\n@@ -1704,6 +1709,7 @@ def test_export_encoder(self):\n         # Verify outputs are close enough\n         self.assertTrue(torch.allclose(original_output, exported_output, atol=1e-5))\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_decoder(self):\n         \"\"\"Test exporting T5 decoder with static cache to torch export format.\"\"\"\n@@ -1765,6 +1771,7 @@ def test_export_decoder(self):\n             # Verify cache buffers are 3D\n             self.assertEqual(buffer.shape[2], max_cache_len)\n \n+    @pytest.mark.torch_export_test\n     @slow\n     def test_export_t5_summarization(self):\n         \"\"\"Test composing exported T5 encoder and decoder for summarization.\"\"\""
        },
        {
            "sha": "a76953920897d8b09968c0f33301fa10649eb1fa",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -18,6 +18,7 @@\n import warnings\n \n import numpy as np\n+import pytest\n import requests\n from packaging import version\n \n@@ -340,6 +341,7 @@ def test_slow_fast_equivalence_batched(self):\n     @slow\n     @require_torch_accelerator\n     @require_vision\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):\n         # override as trimaps are needed for the image processor\n         if self.fast_image_processing_class is None:"
        },
        {
            "sha": "5a35795a7495c7a39dfa05875698bf2f8d4cce6d",
            "filename": "tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,8 @@\n import inspect\n import unittest\n \n+import pytest\n+\n from transformers import VitPoseBackboneConfig\n from transformers.testing_utils import require_torch, torch_device\n from transformers.utils import is_torch_available\n@@ -193,6 +195,7 @@ def test_forward_signature(self):\n             expected_arg_names = [\"pixel_values\"]\n             self.assertListEqual(arg_names[:1], expected_arg_names)\n \n+    @pytest.mark.torch_export_test\n     def test_torch_export(self):\n         # Dense architecture\n         super().test_torch_export()"
        },
        {
            "sha": "b1a57bcc564fcc489f895928716e4760d0e83cb1",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -1420,6 +1420,7 @@ def test_labels_sequence_max_length_error_after_changing_config(self):\n \n     # TODO (joao, eustache): fix me :) The model is not returning a `Cache` by default\n     @unittest.skip(reason=\"Whisper's custom generate is not consistent regarding the cache return types\")\n+    @pytest.mark.torch_compile_test\n     def test_generate_compile_model_forward(self):\n         pass\n "
        },
        {
            "sha": "9d935e9f76232fa9ae3ce30463071e8ded13ef70",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -18,6 +18,7 @@\n import unittest\n from unittest import skip\n \n+import pytest\n from packaging import version\n \n from transformers import AqlmConfig, AutoConfig, AutoModelForCausalLM, AutoTokenizer, OPTForCausalLM, StaticCache\n@@ -198,6 +199,7 @@ def test_quantized_model_multi_gpu(self):\n         is_aqlm_available() and version.parse(importlib.metadata.version(\"aqlm\")) >= version.parse(\"1.0.3\"),\n         \"test requires `aqlm>=1.0.3`\",\n     )\n+    @pytest.mark.torch_compile_test\n     def test_quantized_model_compile(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly"
        },
        {
            "sha": "b49fd43f1793f99ee1e33dd7b372b1ba51d8cea5",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,7 @@\n import tempfile\n import unittest\n \n+import pytest\n from packaging import version\n \n from transformers import (\n@@ -849,6 +850,7 @@ def setUp(self):\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n         self.model_4bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_4bit=True)\n \n+    @pytest.mark.torch_compile_test\n     def test_generate_compile(self):\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n "
        },
        {
            "sha": "8a3bcb84af334a89395ea811cb7f5b9b0e95f306",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,7 @@\n import tempfile\n import unittest\n \n+import pytest\n from packaging import version\n \n from transformers import (\n@@ -996,6 +997,7 @@ def setUp(self):\n         self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n         self.model_8bit = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n \n+    @pytest.mark.torch_compile_test\n     def test_generate_compile(self):\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n "
        },
        {
            "sha": "973ecd6e7db48a4845571abfb8a5f0b5bd80d9e0",
            "filename": "tests/quantization/spqr_integration/test_spqr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,8 @@\n import tempfile\n import unittest\n \n+import pytest\n+\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, SpQRConfig, StaticCache\n from transformers.testing_utils import (\n     backend_empty_cache,\n@@ -179,6 +181,7 @@ def test_quantized_model_multi_gpu(self):\n \n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n+    @pytest.mark.torch_compile_test\n     def test_quantized_model_compile(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly"
        },
        {
            "sha": "635d6a35dc859fdefe1dda21fb04c1de97fe8be9",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -22,6 +22,7 @@\n from copy import deepcopy\n \n import numpy as np\n+import pytest\n import requests\n from packaging import version\n \n@@ -614,6 +615,7 @@ def test_override_instance_attributes_does_not_affect_other_instances(self):\n     @slow\n     @require_torch_accelerator\n     @require_vision\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_image_processor(self):\n         if self.fast_image_processing_class is None:\n             self.skipTest(\"Skipping compilation test as fast image processor is not defined\")"
        },
        {
            "sha": "50d7b2724d5e54bcceaf29bd23b562aa0b14d048",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -27,6 +27,7 @@\n from contextlib import contextmanager\n \n import numpy as np\n+import pytest\n from packaging import version\n from parameterized import parameterized\n from pytest import mark\n@@ -3866,6 +3867,7 @@ def test_sdpa_can_dispatch_on_flash(self):\n \n     @require_torch_sdpa\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     @slow\n     def test_sdpa_can_compile_dynamic(self):\n         if not self.has_attentions:\n@@ -4114,6 +4116,7 @@ def test_flash_attn_2_fp32_ln(self):\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test\n+    @pytest.mark.torch_compile_test\n     @slow\n     def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(self):\n         if version.parse(torch.__version__) < version.parse(\"2.3\"):\n@@ -4581,6 +4584,7 @@ def test_custom_4d_attention_mask(self):\n \n     @slow\n     @require_torch_accelerator\n+    @pytest.mark.torch_compile_test\n     def test_torch_compile_for_training(self):\n         if version.parse(torch.__version__) < version.parse(\"2.3\"):\n             self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n@@ -4653,6 +4657,7 @@ def test_forward_with_logits_to_keep(self):\n \n     @slow\n     @require_torch_greater_or_equal(\"2.5\")\n+    @pytest.mark.torch_export_test\n     def test_torch_export(self, config=None, inputs_dict=None, tolerance=1e-4):\n         \"\"\"\n         Test if model can be exported with torch.export.export()"
        },
        {
            "sha": "5f8f378c12cc978260ee5a72754f7bde57ae50d7",
            "filename": "tests/test_video_processing_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftest_video_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftest_video_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_video_processing_common.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -21,6 +21,7 @@\n from copy import deepcopy\n \n import numpy as np\n+import pytest\n from packaging import version\n \n from transformers import AutoVideoProcessor\n@@ -168,6 +169,7 @@ def test_init_without_params(self):\n     @slow\n     @require_torch_accelerator\n     @require_vision\n+    @pytest.mark.torch_compile_test\n     def test_can_compile_fast_video_processor(self):\n         if self.fast_video_processing_class is None:\n             self.skipTest(\"Skipping compilation test as fast video processor is not defined\")"
        },
        {
            "sha": "4c2b64f0d07f0c18b84bab87c6924dcc2677e4ea",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -31,6 +31,7 @@\n from unittest.mock import Mock, patch\n \n import numpy as np\n+import pytest\n from huggingface_hub import HfFolder, ModelCard, create_branch, list_repo_commits, list_repo_files\n from packaging import version\n from parameterized import parameterized\n@@ -1358,6 +1359,7 @@ def test_number_of_steps_in_training(self):\n         train_output = trainer.train()\n         self.assertEqual(train_output.global_step, 10)\n \n+    @pytest.mark.torch_compile_test\n     def test_torch_compile_loss_func_compatibility(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -1377,6 +1379,7 @@ def test_torch_compile_loss_func_compatibility(self):\n \n     @require_peft\n     @require_bitsandbytes\n+    @pytest.mark.torch_compile_test\n     def test_bnb_compile(self):\n         from peft import LoraConfig, get_peft_model\n "
        },
        {
            "sha": "54a7dc24cf63c769db6e382df6aedbf4bf89021f",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n import copy\n import unittest\n \n+import pytest\n from packaging import version\n from parameterized import parameterized\n \n@@ -594,6 +595,7 @@ def test_cache_gptj_model(self, cache_implementation):\n class CacheExportIntegrationTest(unittest.TestCase):\n     \"\"\"Cache tests that rely on `torch.export()` and model loading\"\"\"\n \n+    @pytest.mark.torch_export_test\n     def test_dynamic_cache_exportability(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n         model = model.eval()\n@@ -635,6 +637,7 @@ def test_dynamic_cache_exportability(self):\n             self.assertTrue(torch.allclose(l1.keys, l2.keys, atol=1e-5))\n             self.assertTrue(torch.allclose(l1.values, l2.values, atol=1e-5))\n \n+    @pytest.mark.torch_export_test\n     def test_dynamic_cache_exportability_multiple_run(self):\n         # When exporting with DynamicCache, you should export two graphs:\n         #   1. A graph without cache\n@@ -730,6 +733,7 @@ def test_dynamic_cache_exportability_multiple_run(self):\n             self.assertTrue(torch.allclose(l1.values, l2.values, atol=1e-5))\n \n     @unittest.skip(\"Runs on my machine locally, passed, no idea why it does not online\")\n+    @pytest.mark.torch_export_test\n     def test_static_cache_exportability(self):\n         \"\"\"\n         Tests that static cache works with `torch.export()`\n@@ -808,6 +812,7 @@ def test_static_cache_exportability(self):\n             strict=strict,\n         )\n \n+    @pytest.mark.torch_export_test\n     def test_hybrid_cache_exportability(self):\n         \"\"\"\n         Tests that static cache works with `torch.export()`"
        },
        {
            "sha": "f09c42101941621d49878c6616ecc1c3ca6ef2ea",
            "filename": "tests/utils/test_deprecation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_deprecation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_deprecation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_deprecation.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -15,6 +15,7 @@\n import unittest\n import warnings\n \n+import pytest\n from parameterized import parameterized\n \n from transformers import __version__, is_torch_available\n@@ -174,6 +175,7 @@ def dummy_function(new_name=None, **kwargs):\n             result = dummy_function(deprecated_name=\"old_value\", new_name=\"new_value\")\n         self.assertEqual(result, \"new_value\")\n \n+    @pytest.mark.torch_compile_test\n     @require_torch_accelerator\n     def test_compile_safe(self):\n         @deprecate_kwarg(\"deprecated_factor\", new_name=\"new_factor\", version=INFINITE_VERSION)"
        },
        {
            "sha": "77e7cdba7c2cd98031f6d63e3681ebc4c7edeae3",
            "filename": "tests/utils/test_generic.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_generic.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_generic.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_generic.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -16,6 +16,7 @@\n import warnings\n \n import numpy as np\n+import pytest\n \n from transformers.configuration_utils import PretrainedConfig\n from transformers.modeling_outputs import BaseModelOutput\n@@ -261,6 +262,7 @@ def test_decorator_eager(self):\n                 message = f\"output should be a {expected_type.__name__} when config.use_return_dict={config_return_dict} and return_dict={return_dict}\"\n                 self.assertIsInstance(output, expected_type, message)\n \n+    @pytest.mark.torch_compile_test\n     def test_decorator_compiled(self):\n         \"\"\"Test that the can_return_tuple decorator works with compiled mode.\"\"\"\n         config = PretrainedConfig()\n@@ -277,6 +279,7 @@ def test_decorator_compiled(self):\n         output = compiled_model(torch.tensor(10), return_dict=False)\n         self.assertIsInstance(output, tuple)\n \n+    @pytest.mark.torch_export_test\n     def test_decorator_torch_export(self):\n         \"\"\"Test that the can_return_tuple decorator works with torch.export.\"\"\"\n         config = PretrainedConfig()"
        },
        {
            "sha": "eef5feb014f51d72f34ec745461b062634ddbcb6",
            "filename": "tests/utils/test_model_output.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_model_output.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_model_output.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_model_output.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -17,6 +17,8 @@\n from dataclasses import dataclass\n from typing import Optional\n \n+import pytest\n+\n from transformers import AlbertForMaskedLM\n from transformers.testing_utils import require_torch\n from transformers.utils import ModelOutput, is_torch_available\n@@ -160,6 +162,7 @@ def test_torch_pytree(self):\n     # TODO: @ydshieh\n     @unittest.skip(reason=\"CPU OOM\")\n     @require_torch\n+    @pytest.mark.torch_export_test\n     def test_export_serialization(self):\n         if not is_torch_greater_or_equal_than_2_2:\n             self.skipTest(reason=\"Export serialization requires torch >= 2.2.0\")"
        },
        {
            "sha": "a7837d5d2336d3a750444e4c3e835c5084c215c3",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/591708d9ce433b37d51ccbcc516f6a739078468c/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=591708d9ce433b37d51ccbcc516f6a739078468c",
            "patch": "@@ -27,6 +27,7 @@\n import warnings\n from pathlib import Path\n \n+import pytest\n import requests\n from huggingface_hub import HfApi, HfFolder\n from parameterized import parameterized\n@@ -2541,6 +2542,7 @@ def test_causal_mask_sliding(self):\n         # non auto-regressive case\n         self.check_to_causal(mask_converter, q_len=7, kv_len=7)\n \n+    @pytest.mark.torch_compile_test\n     def test_torch_compile_fullgraph(self):\n         model = Prepare4dCausalAttentionMaskModel()\n "
        }
    ],
    "stats": {
        "total": 178,
        "additions": 178,
        "deletions": 0
    }
}