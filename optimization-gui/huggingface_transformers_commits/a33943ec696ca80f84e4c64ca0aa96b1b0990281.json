{
    "author": "zucchini-nlp",
    "message": "Fix an edge case for `get_encoder()` (#42295)\n\n* fix and edge case\n\n* ci must turn green\n\n* ci must turn green - second try",
    "sha": "a33943ec696ca80f84e4c64ca0aa96b1b0990281",
    "files": [
        {
            "sha": "5172d74cc397e758a2ba34a313990af2c67e6bf2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a33943ec696ca80f84e4c64ca0aa96b1b0990281/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a33943ec696ca80f84e4c64ca0aa96b1b0990281/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a33943ec696ca80f84e4c64ca0aa96b1b0990281",
            "patch": "@@ -2030,7 +2030,11 @@ def get_encoder(self, modality: Optional[str] = None):\n                 return getattr(self, name)\n \n         if self.base_model is not self and hasattr(self.base_model, \"get_encoder\"):\n-            return self.base_model.get_encoder(modality=modality)\n+            base_encoder = self.base_model.get_encoder(modality=modality)\n+            # Base model will always have attr `get_encoder` if inherited from `PreTrainedModel`\n+            # But it doesn't mean that the model has an encoder module, and we need to return `self`\n+            if base_encoder != self.base_model:\n+                return base_encoder\n \n         # If this is a base transformer model (no encoder/model attributes), return self\n         return self"
        },
        {
            "sha": "b4bafd1d2b2e3cc3f540c7a10619d50ce7e39bd2",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a33943ec696ca80f84e4c64ca0aa96b1b0990281/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a33943ec696ca80f84e4c64ca0aa96b1b0990281/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=a33943ec696ca80f84e4c64ca0aa96b1b0990281",
            "patch": "@@ -3233,7 +3233,7 @@ def test_decoder_only_model_returns_self(self):\n             num_hidden_layers=2,\n             num_attention_heads=4,\n         )\n-        model = MistralModel(cfg)\n+        model = MistralForCausalLM(cfg)\n         encoder = model.get_encoder()\n \n         assert encoder is model, f\"Base model get_encoder() should return self, got {type(encoder)}\""
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}