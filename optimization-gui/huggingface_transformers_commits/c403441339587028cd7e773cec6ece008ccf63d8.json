{
    "author": "faaany",
    "message": "[docs] add the missing huggingface hub username (#33431)\n\n* add username\r\n\r\n* update username\r\n\r\n* add username",
    "sha": "c403441339587028cd7e773cec6ece008ccf63d8",
    "files": [
        {
            "sha": "fc63c35425db25f1b5e38cd182eb8b4eedb67f2e",
            "filename": "docs/source/en/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c403441339587028cd7e773cec6ece008ccf63d8/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c403441339587028cd7e773cec6ece008ccf63d8/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md?ref=c403441339587028cd7e773cec6ece008ccf63d8",
            "patch": "@@ -399,7 +399,7 @@ Tokenize each prompt and candidate answer pair and return PyTorch tensors. You s\n ```py\n >>> from transformers import AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n >>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n >>> labels = torch.tensor(0).unsqueeze(0)\n ```\n@@ -409,7 +409,7 @@ Pass your inputs and labels to the model and return the `logits`:\n ```py\n >>> from transformers import AutoModelForMultipleChoice\n \n->>> model = AutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n+>>> model = AutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n >>> outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n >>> logits = outputs.logits\n ```\n@@ -428,7 +428,7 @@ Tokenize each prompt and candidate answer pair and return TensorFlow tensors:\n ```py\n >>> from transformers import AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n >>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n ```\n \n@@ -437,7 +437,7 @@ Pass your inputs to the model and return the `logits`:\n ```py\n >>> from transformers import TFAutoModelForMultipleChoice\n \n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n+>>> model = TFAutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n >>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n >>> outputs = model(inputs)\n >>> logits = outputs.logits"
        },
        {
            "sha": "76e750ed3bf499cef2c6b9d9876087193eed4b68",
            "filename": "docs/source/en/tasks/summarization.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c403441339587028cd7e773cec6ece008ccf63d8/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c403441339587028cd7e773cec6ece008ccf63d8/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsummarization.md?ref=c403441339587028cd7e773cec6ece008ccf63d8",
            "patch": "@@ -336,7 +336,7 @@ The simplest way to try out your finetuned model for inference is to use it in a\n ```py\n >>> from transformers import pipeline\n \n->>> summarizer = pipeline(\"summarization\", model=\"stevhliu/my_awesome_billsum_model\")\n+>>> summarizer = pipeline(\"summarization\", model=\"username/my_awesome_billsum_model\")\n >>> summarizer(text)\n [{\"summary_text\": \"The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country.\"}]\n ```\n@@ -351,7 +351,7 @@ Tokenize the text and return the `input_ids` as PyTorch tensors:\n ```py\n >>> from transformers import AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n >>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n ```\n \n@@ -360,7 +360,7 @@ Use the [`~generation.GenerationMixin.generate`] method to create the summarizat\n ```py\n >>> from transformers import AutoModelForSeq2SeqLM\n \n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n+>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n >>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n ```\n \n@@ -377,7 +377,7 @@ Tokenize the text and return the `input_ids` as TensorFlow tensors:\n ```py\n >>> from transformers import AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n >>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n ```\n \n@@ -386,7 +386,7 @@ Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method\n ```py\n >>> from transformers import TFAutoModelForSeq2SeqLM\n \n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n+>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n >>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n ```\n "
        },
        {
            "sha": "1028e4c6cf153bae9f3a5acf379580062a90aa06",
            "filename": "docs/source/en/tasks/translation.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c403441339587028cd7e773cec6ece008ccf63d8/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c403441339587028cd7e773cec6ece008ccf63d8/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftranslation.md?ref=c403441339587028cd7e773cec6ece008ccf63d8",
            "patch": "@@ -346,7 +346,7 @@ The simplest way to try out your finetuned model for inference is to use it in a\n # Change `xx` to the language of the input and `yy` to the language of the desired output.\n # Examples: \"en\" for English, \"fr\" for French, \"de\" for German, \"es\" for Spanish, \"zh\" for Chinese, etc; translation_en_to_fr translates English to French\n # You can view all the lists of languages here - https://huggingface.co/languages\n->>> translator = pipeline(\"translation_xx_to_yy\", model=\"my_awesome_opus_books_model\")\n+>>> translator = pipeline(\"translation_xx_to_yy\", model=\"username/my_awesome_opus_books_model\")\n >>> translator(text)\n [{'translation_text': 'Legumes partagent des ressources avec des bactÃ©ries azotantes.'}]\n ```\n@@ -360,7 +360,7 @@ Tokenize the text and return the `input_ids` as PyTorch tensors:\n ```py\n >>> from transformers import AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n >>> inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n ```\n \n@@ -369,7 +369,7 @@ Use the [`~generation.GenerationMixin.generate`] method to create the translatio\n ```py\n >>> from transformers import AutoModelForSeq2SeqLM\n \n->>> model = AutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n+>>> model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n >>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n ```\n \n@@ -386,7 +386,7 @@ Tokenize the text and return the `input_ids` as TensorFlow tensors:\n ```py\n >>> from transformers import AutoTokenizer\n \n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n >>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n ```\n \n@@ -395,7 +395,7 @@ Use the [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] method\n ```py\n >>> from transformers import TFAutoModelForSeq2SeqLM\n \n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n+>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n >>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n ```\n "
        }
    ],
    "stats": {
        "total": 28,
        "additions": 14,
        "deletions": 14
    }
}