{
    "author": "yonigozlan",
    "message": "Add fast image processor Janus, Deepseek VL, Deepseek VL hybrid (#39739)\n\n* add fast image processor Janus, deepseek_vl, deepseek_vl_hybrid\n\n* fix after review",
    "sha": "7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
    "files": [
        {
            "sha": "b01ef7064a733492c68a7f556265f82876981fa0",
            "filename": "docs/source/en/model_doc/deepseek_vl.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -209,6 +209,10 @@ model = DeepseekVLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] DeepseekVLImageProcessor\n \n+## DeepseekVLImageProcessorFast\n+\n+[[autodoc]] DeepseekVLImageProcessorFast\n+\n ## DeepseekVLModel\n \n [[autodoc]] DeepseekVLModel"
        },
        {
            "sha": "e713782748c9f3d67ebcadbae35c3bfc1fcc9c55",
            "filename": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -208,6 +208,10 @@ model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n \n [[autodoc]] DeepseekVLHybridImageProcessor\n \n+## DeepseekVLHybridImageProcessorFast\n+\n+[[autodoc]] DeepseekVLHybridImageProcessorFast\n+\n ## DeepseekVLHybridModel\n \n [[autodoc]] DeepseekVLHybridModel"
        },
        {
            "sha": "f2825cbc9738495de1cd7e379f813f1a4ce71afd",
            "filename": "docs/source/en/model_doc/janus.md",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -44,11 +44,11 @@ Here is the example of visual understanding with a single image.\n > Note that the model has been trained with a specific prompt format for chatting. Use `processor.apply_chat_template(my_conversation_dict)` to correctly format your prompts.\n \n ```python\n-import torch  \n-from PIL import Image  \n-import requests  \n+import torch\n+from PIL import Image\n+import requests\n \n-from transformers import JanusForConditionalGeneration, JanusProcessor  \n+from transformers import JanusForConditionalGeneration, JanusProcessor\n \n model_id = \"deepseek-community/Janus-Pro-1B\"\n # Prepare Input for generation.\n@@ -64,7 +64,7 @@ messages = [\n \n # Set generation mode to `text` to perform text generation.\n processor = JanusProcessor.from_pretrained(model_id)\n-model = JanusForConditionalGeneration.from_pretrained(model_id,     \n+model = JanusForConditionalGeneration.from_pretrained(model_id,\n         torch_dtype=torch.bfloat16,\n         device_map=\"auto\")\n \n@@ -209,6 +209,10 @@ for i, image in enumerate(images['pixel_values']):\n \n [[autodoc]] JanusImageProcessor\n \n+## JanusImageProcessorFast\n+\n+[[autodoc]] JanusImageProcessorFast\n+\n ## JanusVisionModel\n \n [[autodoc]] JanusVisionModel"
        },
        {
            "sha": "d0ad2238c962958747a82256c0890491db387b7d",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -78,8 +78,8 @@\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"cvt\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"data2vec-vision\", (\"BeitImageProcessor\", \"BeitImageProcessorFast\")),\n-            (\"deepseek_vl\", (\"DeepseekVLImageProcessor\")),\n-            (\"deepseek_vl_hybrid\", (\"DeepseekVLHybridImageProcessor\")),\n+            (\"deepseek_vl\", (\"DeepseekVLImageProcessor\", \"DeepseekVLImageProcessorFast\")),\n+            (\"deepseek_vl_hybrid\", (\"DeepseekVLHybridImageProcessor\", \"DeepseekVLHybridImageProcessorFast\")),\n             (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n             (\"depth_anything\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n@@ -113,7 +113,7 @@\n             (\"imagegpt\", (\"ImageGPTImageProcessor\",)),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\",)),\n-            (\"janus\", (\"JanusImageProcessor\")),\n+            (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\", \"LayoutLMv3ImageProcessorFast\")),"
        },
        {
            "sha": "cfe0086350908af0476906e98369111d93ef95b6",
            "filename": "src/transformers/models/deepseek_vl/configuration_deepseek_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconfiguration_deepseek_vl.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -20,7 +20,9 @@\n \n \n from ...configuration_utils import PretrainedConfig\n-from ...utils import logging\n+from ...utils import (\n+    logging,\n+)\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n "
        },
        {
            "sha": "8df016a80eeb8bf174ba48d39b82d899356b74cd",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -131,6 +131,7 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n+        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -142,6 +143,10 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n+            size (`dict[str, int]` or `int`):\n+                The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -160,6 +165,7 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n+        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -191,7 +197,7 @@ def resize(\n         # Expand and pad the images to obtain a square image of dimensions `size x size`\n         image = self.pad_to_square(\n             image=image,\n-            background_color=self.background_color,\n+            background_color=background_color,\n             input_data_format=input_data_format,\n         )\n         return image\n@@ -406,9 +412,5 @@ def pad_to_square(\n \n         return result\n \n-    def postprocess(self):\n-        \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n-        raise AttributeError(\"Not needed for DeepseekVL\")\n-\n \n __all__ = [\"DeepseekVLImageProcessor\"]"
        },
        {
            "sha": "5bebf43c9b6c31f54a72784a910dce6f25aab4a6",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -0,0 +1,199 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/deepseek_vl/modular_deepseek_vl.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import torch.nn.functional as F\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, PILImageResampling, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class DeepseekVLFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+    \"\"\"\n+\n+    min_size: int\n+\n+\n+@auto_docstring\n+class DeepseekVLImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    min_size = 14\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    valid_kwargs = DeepseekVLFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[DeepseekVLFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+        if kwargs.get(\"image_mean\", None) is None:\n+            background_color = (127, 127, 127)\n+        else:\n+            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+        self.background_color = tuple(background_color)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        min_size: int,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        if size.height is None or size.width is None or size.height != size.width:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size.height\n+\n+        height, width = image.shape[-2:]\n+        max_size = max(height, width)\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = SizeDict(\n+            height=max(int(height * delta), min_size),\n+            width=max(int(width * delta), min_size),\n+        )\n+\n+        return super().resize(image, size=output_size_nonpadded, interpolation=interpolation, antialias=antialias)\n+\n+    def pad_to_square(\n+        self,\n+        images: \"torch.Tensor\",\n+        background_color: Union[int, tuple[int, int, int]] = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                The images to pad.\n+            background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+\n+        Returns:\n+            `torch.Tensor`: The padded images.\n+        \"\"\"\n+        height, width = images.shape[-2:]\n+        num_channels = images.shape[1]\n+        batch_size = images.shape[0]\n+\n+        if height == width:\n+            return images\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        padded_images = torch.zeros(\n+            (batch_size, num_channels, max_dim, max_dim), dtype=images.dtype, device=images.device\n+        )\n+        for i, color in enumerate(background_color):\n+            padded_images[:, i, :, :] = color\n+        if width > height:\n+            start = (max_dim - height) // 2\n+            padded_images[:, :, start : start + height, :] = images\n+        else:\n+            start = (max_dim - width) // 2\n+            padded_images[:, :, :, start : start + width] = images\n+\n+        return padded_images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        min_size: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        do_pad: bool = True,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, min_size=min_size, interpolation=interpolation\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_pad:\n+                stacked_images = self.pad_to_square(stacked_images, background_color=self.background_color)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"DeepseekVLImageProcessorFast\"]"
        },
        {
            "sha": "b9f3fc37ba7affde541d98fd8b0da02eab8d8315",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -33,6 +33,7 @@\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n from ..idefics.modeling_idefics import IdeficsBaseModelOutputWithPast, IdeficsCausalLMOutputWithPast\n from ..janus.image_processing_janus import JanusImageProcessor\n+from ..janus.image_processing_janus_fast import JanusImageProcessorFast\n from ..janus.modeling_janus import JanusForConditionalGeneration, JanusModel, JanusPreTrainedModel\n \n \n@@ -181,13 +182,24 @@ def generate(self):\n \n \n class DeepseekVLImageProcessor(JanusImageProcessor):\n+    def __init__(self, **super_kwargs):\n+        super().__init__(**super_kwargs)\n+\n     def postprocess(self):\n         raise AttributeError(\"Not needed for DeepseekVL\")\n \n     def unnormalize(self):\n         raise AttributeError(\"Not needed for DeepseekVL\")\n \n \n+class DeepseekVLImageProcessorFast(JanusImageProcessorFast):\n+    def __init__(self, **super_kwargs):\n+        super().__init__(**super_kwargs)\n+\n+    def postprocess(self):\n+        raise AttributeError(\"Not needed for DeepseekVL\")\n+\n+\n class DeepseekVLProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\"padding\": False},\n@@ -322,5 +334,6 @@ def model_input_names(self):\n     \"DeepseekVLModel\",\n     \"DeepseekVLForConditionalGeneration\",\n     \"DeepseekVLImageProcessor\",\n+    \"DeepseekVLImageProcessorFast\",\n     \"DeepseekVLProcessor\",\n ]"
        },
        {
            "sha": "da85178ccc84d9385fd3473a25eb51757dc9d34b",
            "filename": "src/transformers/models/deepseek_vl_hybrid/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2F__init__.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_deepseek_vl_hybrid import *\n     from .image_processing_deepseek_vl_fast_hybrid import *\n     from .image_processing_deepseek_vl_hybrid import *\n+    from .image_processing_deepseek_vl_hybrid_fast import *\n     from .modeling_deepseek_vl_hybrid import *\n     from .processing_deepseek_vl_hybrid import *\n else:"
        },
        {
            "sha": "a2772894b2ae4fe0b93f21c748413fa06ee0b3e0",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -154,14 +154,15 @@ def __init__(\n             self.background_color = tuple([int(x * 255) for x in image_mean])\n \n         if high_res_image_mean is None:\n-            self.background_color = (127, 127, 127)\n+            self.high_res_background_color = (127, 127, 127)\n         else:\n-            self.background_color = tuple([int(x * 255) for x in high_res_image_mean])\n+            self.high_res_background_color = tuple([int(x * 255) for x in high_res_image_mean])\n \n     def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n+        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -173,6 +174,10 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n+            size (`dict[str, int]` or `int`):\n+                The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -191,6 +196,7 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n+        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -222,7 +228,7 @@ def resize(\n         # Expand and pad the images to obtain a square image of dimensions `size x size`\n         image = self.pad_to_square(\n             image=image,\n-            background_color=self.background_color,\n+            background_color=background_color,\n             input_data_format=input_data_format,\n         )\n         return image\n@@ -361,16 +367,20 @@ def preprocess(\n             # high_res_image: resize (high) -> rescale -> normalize (high)\n             # low_res_image:  resize (high) -> rescale -> resize (low) -> normalize (low)\n             high_res_image = image\n-\n             if do_resize:\n                 high_res_image = self.resize(\n                     image=high_res_image,\n                     size=high_res_size_dict,\n+                    background_color=self.high_res_background_color,\n                     resample=high_res_resample,\n                     input_data_format=input_data_format,\n                 )\n                 image = self.resize(\n-                    image=high_res_image, size=size_dict, resample=resample, input_data_format=input_data_format\n+                    image=high_res_image,\n+                    size=size_dict,\n+                    background_color=self.background_color,\n+                    resample=resample,\n+                    input_data_format=input_data_format,\n                 )\n \n             if do_rescale:\n@@ -475,9 +485,5 @@ def pad_to_square(\n \n         return result\n \n-    def postprocess(self):\n-        \"\"\"Applies post-processing to the decoded image tokens by reversing transformations applied during preprocessing.\"\"\"\n-        raise AttributeError(\"Not needed for DeepseekVLHybrid\")\n-\n \n __all__ = [\"DeepseekVLHybridImageProcessor\"]"
        },
        {
            "sha": "2120a65dd429349533158bc48eed51dcdb906a94",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "added",
            "additions": 326,
            "deletions": 0,
            "changes": 326,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -0,0 +1,326 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_deepseek_vl_hybrid.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    get_size_dict,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ChannelDimension, PILImageResampling, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n+    from ...image_utils import pil_torch_interpolation_mapping\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+\n+class DeepseekVLHybridFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+     high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n+        Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n+        method.\n+    high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+        Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+        overridden by the `high_res_resample` parameter in the `preprocess` method.\n+    high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+        Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n+        channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n+    high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n+        Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n+        number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    min_size: int\n+    high_res_size: dict\n+    high_res_resample: \"PILImageResampling\"\n+    high_res_image_mean: list[float]\n+    high_res_image_std: list[float]\n+\n+\n+@auto_docstring\n+class DeepseekVLHybridImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    min_size = 14\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    valid_kwargs = DeepseekVLHybridFastImageProcessorKwargs\n+    high_res_image_mean = OPENAI_CLIP_MEAN\n+    high_res_image_std = OPENAI_CLIP_STD\n+    high_res_size = {\"height\": 1024, \"width\": 1024}\n+    high_res_resample = PILImageResampling.BICUBIC\n+\n+    def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n+        if kwargs.get(\"image_mean\", None) is None:\n+            background_color = (127, 127, 127)\n+        else:\n+            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+        if kwargs.get(\"high_res_image_mean\", None) is None:\n+            high_res_background_color = (127, 127, 127)\n+        else:\n+            high_res_background_color = tuple([int(x * 255) for x in kwargs.get(\"high_res_image_mean\")])\n+        super().__init__(**kwargs)\n+        self.background_color = tuple(background_color)\n+        self.high_res_background_color = tuple(high_res_background_color)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        min_size: int,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        if size.height is None or size.width is None or size.height != size.width:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size.height\n+\n+        height, width = image.shape[-2:]\n+        max_size = max(height, width)\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = SizeDict(\n+            height=max(int(height * delta), min_size),\n+            width=max(int(width * delta), min_size),\n+        )\n+\n+        return super().resize(image, size=output_size_nonpadded, interpolation=interpolation, antialias=antialias)\n+\n+    def pad_to_square(\n+        self,\n+        images: \"torch.Tensor\",\n+        background_color: Union[int, tuple[int, int, int]] = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                The images to pad.\n+            background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+\n+        Returns:\n+            `torch.Tensor`: The padded images.\n+        \"\"\"\n+        height, width = images.shape[-2:]\n+        num_channels = images.shape[1]\n+        batch_size = images.shape[0]\n+\n+        if height == width:\n+            return images\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        padded_images = torch.zeros(\n+            (batch_size, num_channels, max_dim, max_dim), dtype=images.dtype, device=images.device\n+        )\n+        for i, color in enumerate(background_color):\n+            padded_images[:, i, :, :] = color\n+        if width > height:\n+            start = (max_dim - height) // 2\n+            padded_images[:, :, start : start + height, :] = images\n+        else:\n+            start = (max_dim - width) // 2\n+            padded_images[:, :, :, start : start + width] = images\n+\n+        return padded_images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        high_res_size: SizeDict,\n+        min_size: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        high_res_interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        high_res_image_mean: Optional[Union[float, list[float]]],\n+        high_res_image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        do_pad: bool = True,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        high_res_resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_high_res_images = self.resize(\n+                    image=stacked_images, size=high_res_size, min_size=min_size, interpolation=high_res_interpolation\n+                )\n+            high_res_resized_images_grouped[shape] = stacked_high_res_images\n+        high_res_resized_images = reorder_images(high_res_resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_high_res_images, grouped_high_res_images_index = group_images_by_shape(\n+            high_res_resized_images, disable_grouping=disable_grouping\n+        )\n+        high_res_padded_images = {}\n+        high_res_processed_images_grouped = {}\n+        for shape, stacked_high_res_images in grouped_high_res_images.items():\n+            if do_pad:\n+                stacked_high_res_images = self.pad_to_square(\n+                    stacked_high_res_images, background_color=self.high_res_background_color\n+                )\n+                high_res_padded_images[shape] = stacked_high_res_images\n+            # Fused rescale and normalize\n+            stacked_high_res_images = self.rescale_and_normalize(\n+                stacked_high_res_images,\n+                do_rescale,\n+                rescale_factor,\n+                do_normalize,\n+                high_res_image_mean,\n+                high_res_image_std,\n+            )\n+            high_res_processed_images_grouped[shape] = stacked_high_res_images\n+        high_res_processed_images = reorder_images(high_res_processed_images_grouped, grouped_high_res_images_index)\n+        high_res_processed_images = (\n+            torch.stack(high_res_processed_images, dim=0) if return_tensors else high_res_processed_images\n+        )\n+\n+        resized_images_grouped = {}\n+        for shape, stacked_high_res_padded_images in high_res_padded_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_high_res_padded_images, size=size, min_size=min_size, interpolation=interpolation\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_high_res_images_index)\n+\n+        grouped_resized_images, grouped_resized_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_resized_images.items():\n+            if do_pad:\n+                stacked_images = self.pad_to_square(stacked_images, background_color=self.background_color)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_resized_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"high_res_pixel_values\": high_res_processed_images},\n+            tensor_type=return_tensors,\n+        )\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        high_res_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        high_res_image_mean: Optional[Union[float, list[float]]] = None,\n+        high_res_image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if high_res_size is not None:\n+            high_res_size = SizeDict(**get_size_dict(size=high_res_size, default_to_square=default_to_square))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if isinstance(high_res_image_mean, list):\n+            high_res_image_mean = tuple(high_res_image_mean)\n+        if isinstance(high_res_image_std, list):\n+            high_res_image_std = tuple(high_res_image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        high_res_resample = kwargs.pop(\"high_res_resample\")\n+        kwargs[\"high_res_interpolation\"] = (\n+            pil_torch_interpolation_mapping[high_res_resample]\n+            if isinstance(high_res_resample, (int, PILImageResampling))\n+            else high_res_resample\n+        )\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"high_res_size\"] = high_res_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"high_res_image_mean\"] = high_res_image_mean\n+        kwargs[\"high_res_image_std\"] = high_res_image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        return kwargs\n+\n+\n+__all__ = [\"DeepseekVLHybridImageProcessorFast\"]"
        },
        {
            "sha": "e3149b420286ce9572108d8636e616d1e5031055",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 212,
            "deletions": 4,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -20,7 +20,10 @@\n from ...cache_utils import Cache\n from ...image_processing_utils_fast import (\n     BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n     get_size_dict,\n+    group_images_by_shape,\n+    reorder_images,\n )\n from ...image_transforms import convert_to_rgb, to_channel_dimension_format\n from ...image_utils import (\n@@ -29,6 +32,7 @@\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n+    SizeDict,\n     infer_channel_dimension_format,\n     is_scaled_image,\n     make_flat_list_of_images,\n@@ -48,11 +52,14 @@\n     auto_docstring,\n     can_return_tuple,\n     filter_out_non_signature_kwargs,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n     logging,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n from ..deepseek_vl.configuration_deepseek_vl import DeepseekVLConfig\n from ..deepseek_vl.image_processing_deepseek_vl import DeepseekVLImageProcessor\n+from ..deepseek_vl.image_processing_deepseek_vl_fast import DeepseekVLImageProcessorFast\n from ..deepseek_vl.modeling_deepseek_vl import (\n     DeepseekVLForConditionalGeneration,\n     DeepseekVLModel,\n@@ -63,6 +70,16 @@\n from ..sam.modeling_sam import SamLayerNorm, SamVisionNeck\n \n \n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+\n+    from ...image_utils import pil_torch_interpolation_mapping\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -516,9 +533,9 @@ def __init__(\n         )\n \n         if high_res_image_mean is None:\n-            self.background_color = (127, 127, 127)\n+            self.high_res_background_color = (127, 127, 127)\n         else:\n-            self.background_color = tuple([int(x * 255) for x in high_res_image_mean])\n+            self.high_res_background_color = tuple([int(x * 255) for x in high_res_image_mean])\n \n     @filter_out_non_signature_kwargs()\n     def preprocess(\n@@ -654,16 +671,20 @@ def preprocess(\n             # high_res_image: resize (high) -> rescale -> normalize (high)\n             # low_res_image:  resize (high) -> rescale -> resize (low) -> normalize (low)\n             high_res_image = image\n-\n             if do_resize:\n                 high_res_image = self.resize(\n                     image=high_res_image,\n                     size=high_res_size_dict,\n+                    background_color=self.high_res_background_color,\n                     resample=high_res_resample,\n                     input_data_format=input_data_format,\n                 )\n                 image = self.resize(\n-                    image=high_res_image, size=size_dict, resample=resample, input_data_format=input_data_format\n+                    image=high_res_image,\n+                    size=size_dict,\n+                    background_color=self.background_color,\n+                    resample=resample,\n+                    input_data_format=input_data_format,\n                 )\n \n             if do_rescale:\n@@ -695,6 +716,192 @@ def preprocess(\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n \n+class DeepseekVLHybridFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+     high_res_size (`dict`, *optional*, defaults to `{\"height\": 1024, \"width\": 1024}`):\n+        Size of the high resolution output image after resizing. Can be overridden by the `high_res_size` parameter in the `preprocess`\n+        method.\n+    high_res_resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+        Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n+        overridden by the `high_res_resample` parameter in the `preprocess` method.\n+    high_res_image_mean (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_MEAN`):\n+        Mean to use if normalizing the high resolution image. This is a float or list of floats the length of the number of\n+        channels in the image. Can be overridden by the `high_res_image_mean` parameter in the `preprocess` method.\n+    high_res_image_std (`float` or `list[float]`, *optional*, defaults to `OPENAI_CLIP_STD`):\n+        Standard deviation to use if normalizing the high resolution image. This is a float or list of floats the length of the\n+        number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    min_size: int\n+    high_res_size: dict\n+    high_res_resample: \"PILImageResampling\"\n+    high_res_image_mean: list[float]\n+    high_res_image_std: list[float]\n+\n+\n+class DeepseekVLHybridImageProcessorFast(DeepseekVLImageProcessorFast):\n+    high_res_image_mean = OPENAI_CLIP_MEAN\n+    high_res_image_std = OPENAI_CLIP_STD\n+    high_res_size = {\"height\": 1024, \"width\": 1024}\n+    high_res_resample = PILImageResampling.BICUBIC\n+\n+    def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n+        if kwargs.get(\"image_mean\", None) is None:\n+            background_color = (127, 127, 127)\n+        else:\n+            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+        if kwargs.get(\"high_res_image_mean\", None) is None:\n+            high_res_background_color = (127, 127, 127)\n+        else:\n+            high_res_background_color = tuple([int(x * 255) for x in kwargs.get(\"high_res_image_mean\")])\n+        DeepseekVLImageProcessorFast().__init__(**kwargs)\n+        self.background_color = tuple(background_color)\n+        self.high_res_background_color = tuple(high_res_background_color)\n+\n+    def _further_process_kwargs(\n+        self,\n+        size: Optional[SizeDict] = None,\n+        high_res_size: Optional[SizeDict] = None,\n+        default_to_square: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        high_res_image_mean: Optional[Union[float, list[float]]] = None,\n+        high_res_image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[ChannelDimension] = None,\n+        **kwargs,\n+    ) -> dict:\n+        \"\"\"\n+        Update kwargs that need further processing before being validated\n+        Can be overridden by subclasses to customize the processing of kwargs.\n+        \"\"\"\n+        if kwargs is None:\n+            kwargs = {}\n+        if size is not None:\n+            size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n+        if high_res_size is not None:\n+            high_res_size = SizeDict(**get_size_dict(size=high_res_size, default_to_square=default_to_square))\n+        if isinstance(image_mean, list):\n+            image_mean = tuple(image_mean)\n+        if isinstance(image_std, list):\n+            image_std = tuple(image_std)\n+        if isinstance(high_res_image_mean, list):\n+            high_res_image_mean = tuple(high_res_image_mean)\n+        if isinstance(high_res_image_std, list):\n+            high_res_image_std = tuple(high_res_image_std)\n+        if data_format is None:\n+            data_format = ChannelDimension.FIRST\n+\n+        high_res_resample = kwargs.pop(\"high_res_resample\")\n+        kwargs[\"high_res_interpolation\"] = (\n+            pil_torch_interpolation_mapping[high_res_resample]\n+            if isinstance(high_res_resample, (int, PILImageResampling))\n+            else high_res_resample\n+        )\n+\n+        kwargs[\"size\"] = size\n+        kwargs[\"high_res_size\"] = high_res_size\n+        kwargs[\"default_to_square\"] = default_to_square\n+        kwargs[\"image_mean\"] = image_mean\n+        kwargs[\"image_std\"] = image_std\n+        kwargs[\"high_res_image_mean\"] = high_res_image_mean\n+        kwargs[\"high_res_image_std\"] = high_res_image_std\n+        kwargs[\"data_format\"] = data_format\n+\n+        return kwargs\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        high_res_size: SizeDict,\n+        min_size: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        high_res_interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        high_res_image_mean: Optional[Union[float, list[float]]],\n+        high_res_image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        do_pad: bool = True,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        high_res_resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_high_res_images = self.resize(\n+                    image=stacked_images, size=high_res_size, min_size=min_size, interpolation=high_res_interpolation\n+                )\n+            high_res_resized_images_grouped[shape] = stacked_high_res_images\n+        high_res_resized_images = reorder_images(high_res_resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_high_res_images, grouped_high_res_images_index = group_images_by_shape(\n+            high_res_resized_images, disable_grouping=disable_grouping\n+        )\n+        high_res_padded_images = {}\n+        high_res_processed_images_grouped = {}\n+        for shape, stacked_high_res_images in grouped_high_res_images.items():\n+            if do_pad:\n+                stacked_high_res_images = self.pad_to_square(\n+                    stacked_high_res_images, background_color=self.high_res_background_color\n+                )\n+                high_res_padded_images[shape] = stacked_high_res_images\n+            # Fused rescale and normalize\n+            stacked_high_res_images = self.rescale_and_normalize(\n+                stacked_high_res_images,\n+                do_rescale,\n+                rescale_factor,\n+                do_normalize,\n+                high_res_image_mean,\n+                high_res_image_std,\n+            )\n+            high_res_processed_images_grouped[shape] = stacked_high_res_images\n+        high_res_processed_images = reorder_images(high_res_processed_images_grouped, grouped_high_res_images_index)\n+        high_res_processed_images = (\n+            torch.stack(high_res_processed_images, dim=0) if return_tensors else high_res_processed_images\n+        )\n+\n+        resized_images_grouped = {}\n+        for shape, stacked_high_res_padded_images in high_res_padded_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_high_res_padded_images, size=size, min_size=min_size, interpolation=interpolation\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_high_res_images_index)\n+\n+        grouped_resized_images, grouped_resized_images_index = group_images_by_shape(\n+            resized_images, disable_grouping=disable_grouping\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_resized_images.items():\n+            if do_pad:\n+                stacked_images = self.pad_to_square(stacked_images, background_color=self.background_color)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_resized_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(\n+            data={\"pixel_values\": processed_images, \"high_res_pixel_values\": high_res_processed_images},\n+            tensor_type=return_tensors,\n+        )\n+\n+\n class DeepseekVLHybridProcessorKwargs(DeepseekVLProcessorKwargs):\n     pass\n \n@@ -773,5 +980,6 @@ def __call__(\n     \"DeepseekVLHybridModel\",\n     \"DeepseekVLHybridForConditionalGeneration\",\n     \"DeepseekVLHybridImageProcessor\",\n+    \"DeepseekVLHybridImageProcessorFast\",\n     \"DeepseekVLHybridProcessor\",\n ]"
        },
        {
            "sha": "8aacc2ed6fdbc936a0ed5b84c5833335a442793d",
            "filename": "src/transformers/models/janus/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2F__init__.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_janus import *\n     from .image_processing_janus import *\n+    from .image_processing_janus_fast import *\n     from .modeling_janus import *\n     from .processing_janus import *\n else:"
        },
        {
            "sha": "f99041748fcb89fcfaff78947b68270f97a1c254",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -134,6 +134,7 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n+        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -145,6 +146,10 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n+            size (`dict[str, int]` or `int`):\n+                The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -163,6 +168,7 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n+        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -194,7 +200,7 @@ def resize(\n         # Expand and pad the images to obtain a square image of dimensions `size x size`\n         image = self.pad_to_square(\n             image=image,\n-            background_color=self.background_color,\n+            background_color=background_color,\n             input_data_format=input_data_format,\n         )\n         return image"
        },
        {
            "sha": "81f9bafed767cba4690922f3813151665357d257",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "added",
            "additions": 245,
            "deletions": 0,
            "changes": 245,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -0,0 +1,245 @@\n+# coding=utf-8\n+# Copyright 2025 Deepseek AI and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+class JanusFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    r\"\"\"\n+    min_size (`int`, *optional*, defaults to 14):\n+        The minimum allowed size for the resized image. Ensures that neither the height nor width\n+        falls below this value after resizing.\n+    \"\"\"\n+\n+    min_size: int\n+\n+\n+@auto_docstring\n+class JanusImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = OPENAI_CLIP_MEAN\n+    image_std = OPENAI_CLIP_STD\n+    size = {\"height\": 384, \"width\": 384}\n+    min_size = 14\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    valid_kwargs = JanusFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[JanusFastImageProcessorKwargs]):\n+        if kwargs.get(\"image_mean\", None) is None:\n+            background_color = (127, 127, 127)\n+        else:\n+            background_color = tuple([int(x * 255) for x in kwargs.get(\"image_mean\")])\n+        super().__init__(**kwargs)\n+        self.background_color = tuple(background_color)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        min_size: int,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        if size.height is None or size.width is None or size.height != size.width:\n+            raise ValueError(\n+                f\"Output height and width must be the same. Got height={size['height']} and width={size['width']}\"\n+            )\n+        size = size.height\n+\n+        height, width = image.shape[-2:]\n+        max_size = max(height, width)\n+\n+        delta = size / max_size\n+        # Largest side becomes `size` and the other side is scaled according to the aspect ratio.\n+        output_size_nonpadded = SizeDict(\n+            height=max(int(height * delta), min_size),\n+            width=max(int(width * delta), min_size),\n+        )\n+\n+        return super().resize(image, size=output_size_nonpadded, interpolation=interpolation, antialias=antialias)\n+\n+    def pad_to_square(\n+        self,\n+        images: \"torch.Tensor\",\n+        background_color: Union[int, tuple[int, int, int]] = 0,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads an image to a square based on the longest edge.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                The images to pad.\n+            background_color (`int` or `tuple[int, int, int]`, *optional*, defaults to 0):\n+                The color to use for the padding. Can be an integer for single channel or a\n+                tuple of integers representing for multi-channel images. If passed as integer\n+                in mutli-channel mode, it will default to `0` in subsequent channels.\n+\n+        Returns:\n+            `torch.Tensor`: The padded images.\n+        \"\"\"\n+        height, width = images.shape[-2:]\n+        num_channels = images.shape[1]\n+        batch_size = images.shape[0]\n+\n+        if height == width:\n+            return images\n+\n+        max_dim = max(height, width)\n+\n+        # Ensure background_color is the correct shape\n+        if isinstance(background_color, int):\n+            background_color = [background_color]\n+        elif len(background_color) != num_channels:\n+            raise ValueError(\n+                f\"background_color must have no more than {num_channels} elements to match the number of channels\"\n+            )\n+\n+        padded_images = torch.zeros(\n+            (batch_size, num_channels, max_dim, max_dim), dtype=images.dtype, device=images.device\n+        )\n+        for i, color in enumerate(background_color):\n+            padded_images[:, i, :, :] = color\n+        if width > height:\n+            start = (max_dim - height) // 2\n+            padded_images[:, :, start : start + height, :] = images\n+        else:\n+            start = (max_dim - width) // 2\n+            padded_images[:, :, :, start : start + width] = images\n+\n+        return padded_images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        min_size: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        do_pad: bool = True,\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, min_size=min_size, interpolation=interpolation\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_pad:\n+                stacked_images = self.pad_to_square(stacked_images, background_color=self.background_color)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def postprocess(\n+        self,\n+        images: ImageInput,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[list[float]] = None,\n+        image_std: Optional[list[float]] = None,\n+        return_tensors: Optional[str] = None,\n+    ) -> \"torch.Tensor\":\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = 1.0 / self.rescale_factor if rescale_factor is None else rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        image_mean = tuple(-rescale_factor * mean / std for mean, std in zip(image_mean, image_std))\n+        image_std = tuple(1 / std for std in image_std)\n+\n+        images = self.preprocess(\n+            images,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=False,\n+            do_pad=False,\n+            return_tensors=return_tensors,\n+        ).pixel_values\n+        if do_rescale:\n+            images = [image.clip(0, 255).to(torch.uint8) for image in images]\n+\n+        if do_normalize and do_rescale and return_tensors == \"PIL.Image.Image\":\n+            images = [F.to_pil_image(image) for image in images]\n+\n+        data = {\"pixel_values\": images}\n+        return_tensors = return_tensors if return_tensors != \"PIL.Image.Image\" else None\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"JanusImageProcessorFast\"]"
        },
        {
            "sha": "d90f409ee865d45192a1f999a79051ac5a07b7af",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -1437,6 +1437,7 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n+        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -1448,6 +1449,10 @@ def resize(\n         Args:\n             image (`np.ndarray`):\n                 Image to resize.\n+            size (`dict[str, int]` or `int`):\n+                The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -1466,6 +1471,7 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n+        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -1497,7 +1503,7 @@ def resize(\n         # Expand and pad the images to obtain a square image of dimensions `size x size`\n         image = self.pad_to_square(\n             image=image,\n-            background_color=self.background_color,\n+            background_color=background_color,\n             input_data_format=input_data_format,\n         )\n         return image"
        },
        {
            "sha": "3156bd043456b569ab02024f3cae378921f7b8ef",
            "filename": "tests/models/deepseek_vl/test_image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 36,
            "deletions": 3,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/tests%2Fmodels%2Fdeepseek_vl%2Ftest_image_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/tests%2Fmodels%2Fdeepseek_vl%2Ftest_image_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_image_processing_deepseek_vl.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -17,14 +17,21 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n if is_vision_available():\n     from transformers import DeepseekVLImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import DeepseekVLImageProcessorFast\n+\n \n # Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTester with ViT->DeepseekVL\n class DeepseekVLImageProcessingTester:\n@@ -83,10 +90,9 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n \n @require_torch\n @require_vision\n-# Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTest with ViT->DeepseekVL\n class DeepseekVLImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n-    # Ignore copy\n     image_processing_class = DeepseekVLImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DeepseekVLImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -113,6 +119,33 @@ def test_image_processor_from_dict_with_kwargs(self):\n             image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n             self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=None)\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=None)\n+\n+        # Overwrite as the outputs are not always all of the same shape (kept for BC)\n+        for i in range(len(encoding_slow.pixel_values)):\n+            self._assert_slow_fast_tensors_equivalence(\n+                torch.from_numpy(encoding_slow.pixel_values[i]), encoding_fast.pixel_values[i]\n+            )\n+\n     # Ignore copy\n     @unittest.skip(reason=\"Not supported\")\n     def test_call_numpy_4_channels(self):"
        },
        {
            "sha": "554219f1cc4c7c5356f08bc6997c9a3ba418cc7d",
            "filename": "tests/models/deepseek_vl_hybrid/test_image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 59,
            "deletions": 2,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -13,13 +13,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import unittest\n \n import numpy as np\n+import requests\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -32,6 +32,9 @@\n \n     from transformers import DeepseekVLHybridImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import DeepseekVLHybridImageProcessorFast\n+\n \n class DeepseekVLHybridImageProcessingTester:\n     def __init__(\n@@ -104,6 +107,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class DeepseekVLHybridImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = DeepseekVLHybridImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DeepseekVLHybridImageProcessorFast if is_torchvision_available() else None\n \n     # Copied from tests.models.vit.test_image_processing_vit.ViTImageProcessingTester.setUp with ViT->DeepseekVLHybrid\n     def setUp(self):\n@@ -213,6 +217,59 @@ def test_call_pytorch_high_res(self):\n                 (self.image_processor_tester.batch_size, *expected_output_image_shape),\n             )\n \n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.high_res_pixel_values, encoding_fast.high_res_pixel_values\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=None)\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=None)\n+\n+        # Overwrite as the outputs are not always all of the same shape (kept for BC)\n+        for i in range(len(encoding_slow.pixel_values)):\n+            self._assert_slow_fast_tensors_equivalence(\n+                torch.from_numpy(encoding_slow.pixel_values[i]), encoding_fast.pixel_values[i]\n+            )\n+        for i in range(len(encoding_slow.high_res_pixel_values)):\n+            self._assert_slow_fast_tensors_equivalence(\n+                torch.from_numpy(encoding_slow.high_res_pixel_values[i]), encoding_fast.high_res_pixel_values[i]\n+            )\n+\n     @unittest.skip(reason=\"Not supported\")\n     def test_call_numpy_4_channels(self):\n         pass"
        },
        {
            "sha": "843ef834ac9146e51d52e16f527d9ee1f0851bae",
            "filename": "tests/models/janus/test_image_processing_janus.py",
            "status": "modified",
            "additions": 122,
            "deletions": 68,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/tests%2Fmodels%2Fjanus%2Ftest_image_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1/tests%2Fmodels%2Fjanus%2Ftest_image_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_image_processing_janus.py?ref=7b4d9843ba867e33ceb3106dd026a4dbeeed6fb1",
            "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -31,6 +31,9 @@\n \n     from transformers import JanusImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import JanusImageProcessorFast\n+\n \n class JanusImageProcessingTester:\n     def __init__(\n@@ -44,8 +47,8 @@ def __init__(\n         do_resize=True,\n         size=None,\n         do_normalize=True,\n-        image_mean=[1.0, 1.0, 1.0],\n-        image_std=[1.0, 1.0, 1.0],\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n     ):\n         size = size if size is not None else {\"height\": 384, \"width\": 384}\n@@ -89,6 +92,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class JanusImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = JanusImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = JanusImageProcessorFast if is_torchvision_available() else None\n \n     # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.setUp with CLIP->Janus\n     def setUp(self):\n@@ -101,87 +105,137 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 384, \"width\": 384})\n-        self.assertEqual(image_processor.image_mean, [1.0, 1.0, 1.0])\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 384, \"width\": 384})\n+            self.assertEqual(image_processor.image_mean, [0.48145466, 0.4578275, 0.40821073])\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, image_mean=[1.0, 2.0, 1.0]\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n-        self.assertEqual(image_processor.image_mean, [1.0, 2.0, 1.0])\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, image_mean=[1.0, 2.0, 1.0]\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            self.assertEqual(image_processor.image_mean, [1.0, 2.0, 1.0])\n \n     def test_call_pil(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test Non batched input\n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (1, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-\n-        # Test batched\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test Non batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_call_numpy(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n \n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (1, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_call_pytorch(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n \n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n \n-        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (1, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n \n     def test_nested_input(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-\n-        # Test batched as a list of images.\n-        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-\n-        # Test batched as a nested list of images, where each sublist is one batch.\n-        image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n-        encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = (7, 3, 384, 384)\n-        self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n-\n-        # Image processor should return same pixel values, independently of input format.\n-        self.assertTrue((encoded_images_nested == encoded_images).all())\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test batched as a list of images.\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched as a nested list of images, where each sublist is one batch.\n+            image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n+            encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 3, 384, 384)\n+            self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+\n+            # Image processor should return same pixel values, independently of input format.\n+            self.assertTrue((encoded_images_nested == encoded_images).all())\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=None)\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=None)\n+\n+        # Overwrite as the outputs are not always all of the same shape (kept for BC)\n+        for i in range(len(encoding_slow.pixel_values)):\n+            self._assert_slow_fast_tensors_equivalence(\n+                torch.from_numpy(encoding_slow.pixel_values[i]), encoding_fast.pixel_values[i]\n+            )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_postprocess(self):\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        dummy_images = [image / 255.0 for image in dummy_images]\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow.postprocess(dummy_images, return_tensors=None)\n+        encoding_fast = image_processor_fast.postprocess(dummy_images, return_tensors=None)\n+\n+        # Overwrite as the outputs are not always all of the same shape (kept for BC)\n+        for i in range(len(encoding_slow.pixel_values)):\n+            self._assert_slow_fast_tensors_equivalence(\n+                torch.from_numpy(encoding_slow.pixel_values[i]).float(), encoding_fast.pixel_values[i].float()\n+            )\n \n     @unittest.skip(reason=\"Not supported\")\n     def test_call_numpy_4_channels(self):"
        }
    ],
    "stats": {
        "total": 1375,
        "additions": 1273,
        "deletions": 102
    }
}