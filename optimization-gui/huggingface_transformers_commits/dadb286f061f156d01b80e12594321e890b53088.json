{
    "author": "ArthurZucker",
    "message": "protect tensor parallel usage (#34800)\n\nprotect",
    "sha": "dadb286f061f156d01b80e12594321e890b53088",
    "files": [
        {
            "sha": "7672df0b9a0e46505a96bc01a6e2037aa670b7ba",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dadb286f061f156d01b80e12594321e890b53088/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dadb286f061f156d01b80e12594321e890b53088/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=dadb286f061f156d01b80e12594321e890b53088",
            "patch": "@@ -52,6 +52,7 @@\n     find_pruneable_heads_and_indices,\n     id_tensor_storage,\n     is_torch_greater_or_equal_than_1_13,\n+    is_torch_greater_or_equal_than_2_4,\n     prune_conv1d_layer,\n     prune_layer,\n     prune_linear_layer,\n@@ -5005,6 +5006,8 @@ def tensor_parallel(self, device_mesh):\n             device_mesh (`torch.distributed.DeviceMesh`):\n                 The device mesh to use for tensor parallelism.\n         \"\"\"\n+        if not is_torch_greater_or_equal_than_2_4:\n+            raise EnvironmentError(\"tensor parallel is only supported for `torch>=2.5`.\")\n \n         # Tensor parallelize a nn.Module based on the `_tp_plan` attribute of the module.\n         # No op if `_tp_plan` attribute does not exist under the module."
        },
        {
            "sha": "6757f72350ba297fbdb9c7038a002431d7a4a29f",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/dadb286f061f156d01b80e12594321e890b53088/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dadb286f061f156d01b80e12594321e890b53088/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=dadb286f061f156d01b80e12594321e890b53088",
            "patch": "@@ -20,11 +20,6 @@\n from packaging import version\n from safetensors.torch import storage_ptr, storage_size\n from torch import nn\n-from torch.distributed.tensor import Replicate\n-from torch.distributed.tensor.parallel import (\n-    ColwiseParallel,\n-    RowwiseParallel,\n-)\n \n from .utils import is_torch_xla_available, logging\n \n@@ -44,6 +39,14 @@\n is_torch_greater_or_equal_than_1_12 = parsed_torch_version_base >= version.parse(\"1.12\")\n \n \n+if is_torch_greater_or_equal_than_2_4:\n+    from torch.distributed.tensor import Replicate\n+    from torch.distributed.tensor.parallel import (\n+        ColwiseParallel,\n+        RowwiseParallel,\n+    )\n+\n+\n def softmax_backward_data(parent, grad_output, output, dim, self):\n     \"\"\"\n     A function that calls the internal `_softmax_backward_data` PyTorch method and that adjusts the arguments according"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 11,
        "deletions": 5
    }
}