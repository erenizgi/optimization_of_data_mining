{
    "author": "vasqu",
    "message": "[`Mamba2`] Fix caching, slow path, and multi-gpu (#35154)\n\n* fixup mamba2 - caching and several other small fixes\n\n* fixup cached forward\n\n* correct fix this time\n\n* fixup cache - we do not need to extend the attn mask it's handled by generate (gives total ids + mask at each step)\n\n* remove unnecessary (un)squeeze\n\n* fixup cache position\n\n* simplify a few things\n\n* [run-slow] mamba2\n\n* multi gpu attempt two\n\n* [run-slow] mamba2\n\n* [run-slow] mamba2\n\n* [run-slow] mamba2\n\n* [run-slow] mamba2\n\n* add newer slow path fix\n\n* [run-slow] mamba2",
    "sha": "5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95",
    "files": [
        {
            "sha": "550eeb7f9665e4b9e987d82617b9112989303b3b",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 201,
            "deletions": 161,
            "changes": 362,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95",
            "patch": "@@ -44,14 +44,22 @@\n     from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n     from mamba_ssm.ops.triton.ssd_combined import mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined\n else:\n-    selective_state_update = None\n+    mamba_chunk_scan_combined, mamba_split_conv1d_scan_combined, selective_state_update = None, None, None\n \n if is_causal_conv1d_available():\n     from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n else:\n     causal_conv1d_update, causal_conv1d_fn = None, None\n \n-is_fast_path_available = all((selective_state_update, causal_conv1d_fn, causal_conv1d_update))\n+is_fast_path_available = all(\n+    (\n+        selective_state_update,\n+        mamba_chunk_scan_combined,\n+        mamba_split_conv1d_scan_combined,\n+        causal_conv1d_fn,\n+        causal_conv1d_update,\n+    )\n+)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/mamba-codestral-7B-v0.1\"\n _CONFIG_FOR_DOC = \"Mamba2Config\"\n@@ -111,6 +119,17 @@ def segment_sum(input_tensor):\n     return tensor_segsum\n \n \n+def apply_mask_to_padding_states(hidden_states, attention_mask):\n+    \"\"\"\n+    Tunes out the hidden states for padding tokens, see https://github.com/state-spaces/mamba/issues/66\n+    \"\"\"\n+    if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n+        dtype = hidden_states.dtype\n+        hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+    return hidden_states\n+\n+\n class Mamba2Cache:\n     \"\"\"\n     Arguments:\n@@ -120,51 +139,69 @@ class Mamba2Cache:\n         device: torch.device\n \n     Attributes:\n-        seqlen_offset: int\n-        dtype: torch.dtype\n-        conv_states: Dict[int, torch.Tensor] # layer_idx -> [batch_size, intermediate_size, conv_kernel_size]\n-        ssm_states: Dict[int, torch.Tensor] # layer_idx -> [batch_size, intermediate_size, ssm_state_size]\n+        dtype: (`torch.dtype`):\n+            The default `dtype` used to initializing the cache.\n+        conv_kernel_size: (`int`):\n+            Model's convolution kernel size taken from config.\n+        n_groups: (`int`):\n+            Model's number of groups taken from the config - similar to tensor parallel in Transformer.\n+        state_size: (`int`):\n+            Model's SSM state size taken from config.\n+        num_heads: (`int`):\n+            The number of heads used in the linear attention / SSM.\n+        head_dim: (`int`):\n+            The respective dimension of the heads used in the linear attention / SSM.\n+        intermediate_size: (`int`):\n+            Model's intermediate_size based on (expand * hidden_dim) from config.\n+        conv_states: (`torch.Tensor`):\n+            A tensor of shape `[num_layers, batch_size, conv_kernel_size, intermediate_size + 2 * n_groups * state_size]` that holds convolutional states.\n+        ssm_states: (`torch.Tensor`):\n+            A tensor of shape `[num_layers, batch_size, num_heads, head_dim, state_size]` that holds ssm states.\n     \"\"\"\n \n     def __init__(\n         self, config: Mamba2Config, batch_size: int, dtype: torch.dtype = torch.float16, device: Optional[str] = None\n     ):\n-        self.seqlen_offset = 0\n         self.dtype = dtype\n         self.conv_kernel_size = config.conv_kernel\n+        self.n_groups = config.n_groups\n+        self.state_size = config.state_size\n+        self.num_heads = config.num_heads\n+        self.head_dim = config.head_dim\n         self.intermediate_size = int(config.expand * config.hidden_size)\n \n-        self.conv_states = {\n-            i: torch.zeros(\n-                batch_size,\n-                self.intermediate_size + 2 * config.n_groups * config.state_size,\n-                self.conv_kernel_size,\n-                device=device,\n-                dtype=dtype,\n-            )\n-            for i in range(config.num_hidden_layers)\n-        }\n-        self.ssm_states = {\n-            i: torch.zeros(\n-                batch_size, config.num_heads, config.head_dim, config.state_size, device=device, dtype=dtype\n-            )\n-            for i in range(config.num_hidden_layers)\n-        }\n-        self.activation = config.hidden_act\n-        self.act = ACT2FN[config.hidden_act]\n+        self.conv_states = torch.zeros(\n+            config.num_hidden_layers,\n+            batch_size,\n+            self.intermediate_size + 2 * self.n_groups * self.state_size,\n+            self.conv_kernel_size,\n+            device=device,\n+            dtype=dtype,\n+        )\n+        self.ssm_states = torch.zeros(\n+            config.num_hidden_layers,\n+            batch_size,\n+            self.num_heads,\n+            self.head_dim,\n+            self.state_size,\n+            device=device,\n+            dtype=dtype,\n+        )\n \n     def update_conv_state(\n-        self, layer_idx: int, new_conv_state: torch.Tensor, cache_position: torch.LongTensor\n+        self, layer_idx: int, new_conv_state: torch.Tensor, cache_init: bool = False\n     ) -> torch.Tensor:\n-        conv_state = self.conv_states[layer_idx]\n-        cache_position = cache_position.clamp(0, self.conv_kernel_size - 1)\n-\n-        conv_state = conv_state.roll(shifts=-1, dims=-1)\n-        conv_state[:, :, cache_position] = new_conv_state.to(conv_state.device)\n-        self.conv_states[layer_idx].zero_()\n-        self.conv_states[layer_idx] += conv_state\n+        if cache_init:\n+            self.conv_states[layer_idx] = new_conv_state.to(self.conv_states.device)\n+        else:\n+            self.conv_states[layer_idx] = self.conv_states[layer_idx].roll(shifts=-1, dims=-1)\n+            self.conv_states[layer_idx][:, :, -1] = new_conv_state[:, 0, :].to(self.conv_states.device)\n         return self.conv_states[layer_idx]\n \n+    def update_ssm_state(self, layer_idx: int, new_ssm_state: torch.Tensor):\n+        self.ssm_states[layer_idx] = new_ssm_state.to(self.ssm_states.device)\n+        return self.ssm_states[layer_idx]\n+\n     def reset(self):\n         self.conv_states.zero_()\n         self.ssm_states.zero_()\n@@ -269,19 +306,27 @@ def cuda_kernels_forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n     ):\n-        # set up dimensions for reshapes later\n+        # 1. Gated MLP's linear projection\n+        hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n+        projected_states = self.in_proj(hidden_states)\n \n+        # Set up dimensions for reshapes later\n         batch_size, seq_len, _ = hidden_states.shape\n         groups_time_state_size = self.n_groups * self.ssm_state_size\n-        d_to_remove = 2 * self.intermediate_size + 2 * self.n_groups * self.ssm_state_size + self.num_heads\n-\n-        # getting projected states from cache if it exists\n-        if cache_params is not None and cache_params.seqlen_offset > 0:\n-            in_projected_states = self.in_proj(hidden_states.squeeze(1))  # (B 2D)\n-            d_mlp = (in_projected_states.shape[-1] - d_to_remove) // 2\n-            split_projection_dim = [d_mlp, d_mlp, self.intermediate_size, self.conv_dim, self.num_heads]\n-            _, _, gate, hidden_states_B_C, dt = torch.split(in_projected_states, split_projection_dim, dim=-1)\n+        d_mlp = (\n+            projected_states.shape[-1]\n+            - 2 * self.intermediate_size\n+            - 2 * self.n_groups * self.ssm_state_size\n+            - self.num_heads\n+        ) // 2\n+\n+        # Single step calculations via cache\n+        if cache_params is not None and cache_position is not None and cache_position[0] > 0:\n+            _, _, gate, hidden_states_B_C, dt = projected_states.squeeze(1).split(\n+                [d_mlp, d_mlp, self.intermediate_size, self.conv_dim, self.num_heads], dim=-1\n+            )\n \n+            # 2. Convolution sequence transformation\n             hidden_states_B_C = causal_conv1d_update(\n                 hidden_states_B_C,\n                 cache_params.conv_states[self.layer_idx],\n@@ -295,8 +340,9 @@ def cuda_kernels_forward(\n                 [self.intermediate_size, groups_time_state_size, groups_time_state_size],\n                 dim=-1,\n             )\n-            A = -torch.exp(self.A_log.float())  # (nheads,)\n \n+            # 3. SSM transformation\n+            A = -torch.exp(self.A_log.float())  # (nheads,)\n             A = A[:, None, ...][:, :, None].expand(-1, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n             dt = dt[:, :, None].expand(-1, -1, self.head_dim)\n             dt_bias = self.dt_bias[:, None, ...].expand(-1, self.head_dim)\n@@ -318,20 +364,18 @@ def cuda_kernels_forward(\n             )\n             hidden_states = hidden_states.view(batch_size, self.num_heads * self.head_dim)\n             hidden_states = self.norm(hidden_states, gate)\n+\n+            # 4. Final linear projection\n             out = self.out_proj(hidden_states)[:, None, ...]\n-        # if no cache is found, calling the kernel\n+\n+        # Fused calculations or step by step if no initialized cache is found\n         else:\n-            if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-                # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n-                dtype = hidden_states.dtype\n-                hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-            # 1. Gated MLP's linear projection\n-            projected_states = self.in_proj(hidden_states)\n             A = -torch.exp(self.A_log.float())  # (num_heads) or (intermediate_size, state_size)\n             dt_limit_kwargs = {} if self.time_step_limit == (0.0, float(\"inf\")) else {\"dt_limit\": self.time_step_limit}\n \n+            # 2-4. Fused kernel for conv1d, SSM, and the final projection\n             if self.training and cache_params is None:\n-                out, ssm_state = mamba_split_conv1d_scan_combined(\n+                out = mamba_split_conv1d_scan_combined(\n                     projected_states,\n                     self.conv1d.weight.squeeze(1),\n                     self.conv1d.bias,\n@@ -348,41 +392,50 @@ def cuda_kernels_forward(\n                     headdim=self.head_dim,\n                     ngroups=self.n_groups,\n                     norm_before_gate=False,\n-                    return_final_states=True,\n+                    return_final_states=False,\n                     **dt_limit_kwargs,\n                 )\n \n             else:\n-                gate, hidden_states_B_C, time_step = torch.split(\n-                    projected_states,\n-                    [self.intermediate_size, self.conv_dim, self.num_heads],\n-                    dim=-1,\n+                _, _, gate, hidden_states_B_C, dt = projected_states.split(\n+                    [d_mlp, d_mlp, self.intermediate_size, self.conv_dim, self.num_heads], dim=-1\n                 )\n \n-                # 1D Convolution\n-                if causal_conv1d_fn is None or self.activation not in [\"silu\", \"swish\"]:\n+                # 2. Convolution sequence transformation\n+                # Init cache\n+                if cache_params is not None:\n+                    hidden_states_B_C_transposed = hidden_states_B_C.transpose(1, 2)\n+                    conv_states = nn.functional.pad(\n+                        hidden_states_B_C_transposed,\n+                        (cache_params.conv_kernel_size - hidden_states_B_C_transposed.shape[-1], 0),\n+                    )\n+                    cache_params.update_conv_state(\n+                        layer_idx=self.layer_idx, new_conv_state=conv_states, cache_init=True\n+                    )\n+\n+                if self.activation not in [\"silu\", \"swish\"]:\n                     hidden_states_B_C = self.act(\n-                        self.conv1d(hidden_states_B_C.transpose(1, 2)).transpose(1, 2)[:, :seq_len]\n-                    )  # (B, L, self.d_inner + 2 * ngroups * d_state)\n+                        self.conv1d(hidden_states_B_C.transpose(1, 2))[..., :seq_len].transpose(1, 2)\n+                    )\n                 else:\n                     hidden_states_B_C = causal_conv1d_fn(\n                         x=hidden_states_B_C.transpose(1, 2),\n                         weight=self.conv1d.weight.squeeze(1),\n                         bias=self.conv1d.bias,\n                         activation=self.activation,\n-                    ).transpose(1, 2)[:, :seq_len]\n+                    ).transpose(1, 2)\n+\n+                hidden_states_B_C = apply_mask_to_padding_states(hidden_states_B_C, attention_mask)\n                 hidden_states, B, C = torch.split(\n                     hidden_states_B_C,\n                     [self.intermediate_size, groups_time_state_size, groups_time_state_size],\n                     dim=-1,\n                 )\n-                if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-                    # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n-                    dtype = hidden_states.dtype\n-                    hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n+\n+                # 3. SSM transformation\n                 scan_output, ssm_state = mamba_chunk_scan_combined(\n                     hidden_states.view(batch_size, seq_len, -1, self.head_dim),\n-                    time_step,\n+                    dt,\n                     A,\n                     B.view(batch_size, seq_len, self.n_groups, -1),\n                     C.view(batch_size, seq_len, self.n_groups, -1),\n@@ -395,72 +448,81 @@ def cuda_kernels_forward(\n                     dt_softplus=True,\n                     **dt_limit_kwargs,\n                 )\n+\n+                # Init cache\n                 if ssm_state is not None and cache_params is not None:\n-                    cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n+                    cache_params.update_ssm_state(layer_idx=self.layer_idx, new_ssm_state=ssm_state)\n+\n                 scan_output = scan_output.view(batch_size, seq_len, -1)\n                 # Multiply \"gate\" branch and apply extra normalization layer\n                 scan_output = self.norm(scan_output, gate)\n+\n+                # 4. Final linear projection\n                 out = self.out_proj(scan_output)\n         return out\n \n     # fmt: off\n     def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None, cache_position:Optional[torch.LongTensor]=None, attention_mask: Optional[torch.Tensor]=None):\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n-        # Gated MLP's linear projection\n-        projected_states =  self.in_proj(input_states.squeeze(1))\n-        d_mlp = (projected_states.shape[-1] - 2 * self.intermediate_size -  2 * self.n_groups * self.ssm_state_size- self.num_heads) // 2\n-        _, _, gate, hidden_states, dt = projected_states.split(\n+\n+        # 1. Gated MLP's linear projection\n+        input_states = apply_mask_to_padding_states(input_states, attention_mask)\n+        projected_states = self.in_proj(input_states)\n+        d_mlp = (projected_states.shape[-1] - 2 * self.intermediate_size - 2 * self.n_groups * self.ssm_state_size-self.num_heads) // 2\n+        _, _, gate, hidden_states_B_C, dt = projected_states.split(\n                 [d_mlp, d_mlp, self.intermediate_size,  self.conv_dim, self.num_heads], dim=-1\n         )\n \n-        # Convolution sequence transformation\n-        if cache_params is not None:\n-            ssm_state = cache_params.ssm_states[self.layer_idx].clone()\n-            ssm_state = ssm_state.to(hidden_states.device)\n-            if cache_params.seqlen_offset > 0:\n-                conv_state = cache_params.conv_states[self.layer_idx]                   # [batch, intermediate_size, conv_kernel_size]\n-                conv_state = torch.roll(conv_state, shifts=-1, dims=-1)\n-                # handle batched generation - states are copied through\n-                conv_state[:, :, -1] = hidden_states[:, 0, :] if hidden_states.ndim == 3 else hidden_states\n-                cache_params.conv_states[self.layer_idx].copy_(conv_state)\n-                hidden_states = torch.sum(conv_state.to(projected_states.device) * self.conv1d.weight[:, 0, :], dim=-1)\n-                if self.use_conv_bias:\n-                    hidden_states += self.conv1d.bias\n-                hidden_states = self.act(hidden_states).to(dtype)[:, None, ...]         # [batch, 1, intermediate_size] : decoding\n-            else:\n-                hidden_states = hidden_states.transpose(1,2)\n-                conv_state = nn.functional.pad(\n-                    hidden_states,\n-                    (self.conv_kernel_size - hidden_states.shape[-1], 0)\n-                )\n-                cache_params.conv_states[self.layer_idx].copy_(conv_state)\n-                hidden_states = self.act(self.conv1d(hidden_states).transpose(1,2))[:, :seq_len, :]     # [batch, intermediate_size, seq_len]\n-                if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n-                    dtype = hidden_states.dtype\n-                    # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n-                    hidden_states = (hidden_states * attention_mask[:, :, None]).to(dtype)\n-        else:\n-            ssm_state = torch.zeros(\n-                (batch_size, self.num_heads, self.head_dim, self.ssm_state_size),\n-                device=hidden_states.device, dtype=dtype\n+        # 2. Convolution sequence transformation\n+        if cache_params is not None and cache_position is not None and cache_position[0] > 0:\n+            cache_params.update_conv_state(layer_idx=self.layer_idx, new_conv_state=hidden_states_B_C, cache_init=False)\n+\n+            # We need to guarantee that anything regarding the cache is on the same device\n+            conv_states = cache_params.conv_states[self.layer_idx].to(device=self.conv1d.weight.device)\n+\n+            hidden_states_B_C = torch.sum(\n+                conv_states * self.conv1d.weight.squeeze(1), dim=-1\n             )\n-            hidden_states = self.act(self.conv1d(hidden_states.transpose(1, 2))[..., :seq_len].transpose(1, 2))\n-        hidden_states, B, C = torch.split(hidden_states, [self.intermediate_size, self.n_groups * self.ssm_state_size, self.n_groups * self.ssm_state_size], dim=-1)\n+            if self.use_conv_bias:\n+                hidden_states_B_C = hidden_states_B_C + self.conv1d.bias\n+            hidden_states_B_C = self.act(hidden_states_B_C)\n+        else:\n+            # Init cache\n+            if cache_params is not None:\n+                hidden_states_B_C_transposed = hidden_states_B_C.transpose(1, 2)\n+                conv_states = nn.functional.pad(\n+                    hidden_states_B_C_transposed, (cache_params.conv_kernel_size - hidden_states_B_C_transposed.shape[-1], 0)\n+                )\n+                cache_params.update_conv_state(layer_idx=self.layer_idx, new_conv_state=conv_states, cache_init=True)\n+\n+            hidden_states_B_C = self.act(self.conv1d(hidden_states_B_C.transpose(1, 2))[..., :seq_len].transpose(1, 2))\n+\n+        hidden_states_B_C = apply_mask_to_padding_states(hidden_states_B_C, attention_mask)\n+        hidden_states, B, C = torch.split(\n+            hidden_states_B_C,\n+            [self.intermediate_size, self.n_groups * self.ssm_state_size, self.n_groups * self.ssm_state_size],\n+            dim=-1\n+        )\n+\n+        # 3. SSM transformation\n         A = -torch.exp(self.A_log.float())                            # [num_heads]\n-        if cache_params is not None and cache_params.seqlen_offset > 0:\n+        if cache_params is not None and cache_position is not None and cache_position[0] > 0:\n+            # We need to guarantee that anything regarding the cache is on the same device\n+            cache_device = cache_params.ssm_states.device\n+\n             # Note: there is no need to pad parameter matrices here, as there is just one new token\n             # for batched generation\n-            dt = dt[:, None, ...] if dt.ndim == 2 else dt[:, 0, :][:, None, ...]\n+            dt = dt[:, 0, :][:, None, ...]\n             dt = dt.transpose(1, 2).expand(batch_size, dt.shape[-1], self.head_dim)\n             # [num_heads] -> [num_heads, head_dim]\n             dt_bias = self.dt_bias[..., None].expand(self.dt_bias.shape[0], self.head_dim)\n \n             dt = torch.nn.functional.softplus(dt + dt_bias.to(dt.dtype))\n-            dt = torch.clamp(dt, self.time_step_min) #, self.time_step_max)\n+            dt = torch.clamp(dt, self.time_step_limit[0], self.time_step_limit[1])\n             A = A[..., None, None].expand(self.num_heads, self.head_dim, self.ssm_state_size).to(dtype=torch.float32)\n             # [bsz, num_heads, head_dim, state_size]\n-            dA = torch.exp(dt[..., None] * A)\n+            dA = (torch.exp(dt[..., None] * A)).to(device=cache_device)\n \n             # Discretize B\n             # [bsz, n_groups * state_size] -> [bsz, n_groups, 1, state_size] ->\n@@ -474,11 +536,12 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             # Discretize x into dB\n             # [bsz, intermediate_size] -> [bsz, num_heads, head_dim]\n             hidden_states = hidden_states.reshape(batch_size, -1, self.head_dim)\n-            dBx = dB * hidden_states[..., None]\n+            dBx = (dB * hidden_states[..., None]).to(device=cache_device)\n \n             # State calculation\n-            cache_params.ssm_states[self.layer_idx].copy_(\n-                cache_params.ssm_states[self.layer_idx] * dA + dBx\n+            cache_params.update_ssm_state(\n+                layer_idx=self.layer_idx,\n+                new_ssm_state=cache_params.ssm_states[self.layer_idx] * dA + dBx\n             )\n \n             # Subsequent output\n@@ -488,7 +551,7 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             C = C.reshape(batch_size, -1, C.shape[-1])\n             # [bsz, num_heads, head_dim]\n \n-            ssm_states = cache_params.ssm_states[self.layer_idx].to(C.dtype)  # Shape: [b, h, d, n]\n+            ssm_states = cache_params.ssm_states[self.layer_idx].to(device=C.device, dtype=C.dtype)  # Shape: [b, h, d, n]\n             # Reshape ssm_states to merge the first two dimensions\n             ssm_states_reshaped = ssm_states.view(batch_size * self.num_heads, self.head_dim, self.ssm_state_size)  # Shape: [b*h, d, n]\n             C_reshaped = C.view(batch_size * self.num_heads, self.ssm_state_size, 1)  # Shape: [b*h, n, 1]\n@@ -505,9 +568,9 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n         else:\n             # begin ssd naive implementation without einsums\n             dt = nn.functional.softplus(dt + self.dt_bias)\n-            dt = torch.clamp(dt, self.time_step_min)\n+            dt = torch.clamp(dt, self.time_step_limit[0], self.time_step_limit[1])\n             hidden_states = hidden_states.reshape(batch_size, seq_len, -1, self.head_dim).float()\n-            B = B.reshape(batch_size, seq_len,  -1, self.ssm_state_size).float()\n+            B = B.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n             B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n             C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n@@ -522,7 +585,6 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             # Rearrange into blocks/chunks\n             hidden_states, A, B, C = [reshape_into_chunks(t, pad_size, self.chunk_size) for t in (hidden_states, A, B, C)]\n \n-\n             # [bsz, -1, chunk_size, num_heads] -> [bsz, num_heads, -1, chunk_size]\n             A = A.permute(0, 3, 1, 2)\n             A_cumsum = torch.cumsum(A, dim=-1)\n@@ -531,45 +593,43 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             # This is the analog of a causal mask\n             L = torch.exp(segment_sum(A))\n \n-            # First, contraction of C and B to get G (attention-weights like)\n-            G_intermediate = C[:, :, :, None, :, :] * B[:, :, None, :, : ,:]  # shape: (b, c, l, s, h, n)\n+            # Contraction of C and B to get G (attention-weights like)\n+            G_intermediate = C[:, :, :, None, :, :] * B[:, :, None, :, :, :]  # shape: (b, c, l, s, h, n)\n             G = G_intermediate.sum(dim=-1)  # shape: (b, c, l, s, h)\n \n-\n-            # Step 2: Compute M, equivalent to applying attention mask to weights\n+            # Compute M, equivalent to applying attention mask to weights\n             M_intermediate = G[..., None] * L.permute(0, 2, 3, 4, 1)[..., None]\n             M = M_intermediate.sum(dim=-1)\n \n-            # Step 3: Compute Y_diag (apply to values)\n-            Y_diag = (M[..., None] * hidden_states[:, :, None]).sum(3)\n+            # Compute Y_diag (apply to values)\n+            Y_diag = (M[..., None] * hidden_states[:, :, None]).sum(dim=3)\n \n+            # 2. Compute the state for each intra-chunk\n             # (right term of low-rank factorization of off-diagonal blocks; B terms)\n-\n             decay_states = torch.exp((A_cumsum[:, :, :, -1:] - A_cumsum))\n-            B_decay_contraction = B * decay_states.permute(0, 2, 3, 1)[..., None]\n-            # permute back B * decay states\n-            states = (B_decay_contraction.permute(0, 1, 3, 2, 4)[..., None]  * hidden_states.permute(0, 1, 3, 2, 4)[..., None, :]).sum(dim=3).permute(0, 1, 2, 4, 3)\n-            if cache_params is not None and cache_params.seqlen_offset > 0:\n-                previous_states = cache_params.ssm_states[self.layer_idx][:, None, ...]\n+            B_decay = B * decay_states.permute(0, -2, -1, 1)[..., None]\n+            states = (B_decay[..., None, :] * hidden_states[..., None]).sum(dim=2)\n+\n+            # 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries\n+            # (middle term of factorization of off-diag blocks; A terms)\n+            if cache_params is not None and cache_position is not None and cache_position[0] > 0:\n+                previous_states = cache_params.ssm_states[self.layer_idx][:, None, ...].to(device=states.device)\n             else:\n                 previous_states = torch.zeros_like(states[:, :1])\n             states = torch.cat([previous_states, states], dim=1)\n             decay_chunk = torch.exp(segment_sum(nn.functional.pad(A_cumsum[:, :, :, -1], (1, 0))))\n-\n-            states_permuted = states.permute(0, 2, 1, 3, 4)\n-            result = (decay_chunk[..., None, None] * states_permuted[:, :, None, ...]).sum(dim=2)\n-            new_states = result.permute(0, 2, 1, 3, 4)\n+            decay_chunk = decay_chunk.transpose(1, 3)\n+            new_states = (decay_chunk[..., None, None] * states[:, :, None, ...]).sum(dim=1)\n             states, ssm_state = new_states[:, :-1], new_states[:, -1]\n \n-            # Compute state -> output conversion per chunk\n+            # 4. Compute state -> output conversion per chunk\n             # (left term of low-rank factorization of off-diagonal blocks; C terms)\n             state_decay_out = torch.exp(A_cumsum)\n-            # compute Yoff\n             C_times_states = (C[..., None, :] * states[:, :, None, ...])\n             state_decay_out_permuted = state_decay_out.permute(0, 2, 3, 1)\n             Y_off = (C_times_states.sum(-1) * state_decay_out_permuted[..., None])\n-            # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n \n+            # Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)\n             y = Y_diag + Y_off\n             # [bsz, -1, self.chunk_size, num_heads, head_dim] -> [bsz, (padded) seq_len, num_heads, head_dim]\n             y = y.reshape(batch_size, -1, self.num_heads, self.head_dim)\n@@ -579,8 +639,10 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             if pad_size > 0:\n                 y = y[:, :seq_len, :, :]\n             y = y.reshape(batch_size, seq_len, -1)\n+\n+            # Init cache\n             if ssm_state is not None and cache_params is not None:\n-                cache_params.ssm_states[self.layer_idx].copy_(ssm_state)\n+                cache_params.update_ssm_state(layer_idx=self.layer_idx, new_ssm_state=ssm_state)\n \n         scan_output = self.norm(y, gate)\n \n@@ -916,9 +978,6 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if use_cache:\n-            cache_params.seqlen_offset += inputs_embeds.shape[1]\n-\n         hidden_states = self.norm_f(hidden_states)\n \n         if output_hidden_states:\n@@ -975,10 +1034,6 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwitten -- uses `cache_params` as opposed to `past_key_values`\n \n-        if inputs_embeds is not None:\n-            past_len = inputs_embeds.shape[1] + input_ids.shape[1]\n-        else:\n-            past_len = input_ids.shape[1]\n         if use_cache:\n             # `cache_position` should have been initialized in `generate`\n             if cache_position is None:\n@@ -987,33 +1042,18 @@ def prepare_inputs_for_generation(\n                     \"`model.generate`, you are responsible for passing in a valid `cache_position` if \"\n                     \"you are calling `prepare_inputs_for_generation` directly with `use_cache=True`\"\n                 )\n-            # how do we detect that we are in decoding without cache?\n             if cache_position[0] > 0:\n                 input_ids = input_ids[:, -1][..., None]\n-                attention_mask = attention_mask[:, -1][..., None]\n+\n+                if attention_mask is not None:\n+                    attention_mask = None\n             else:\n                 # we initialize the `cache_position` to full size of `conv_states` at prefill stage\n                 # considering padding will be applied when input length is shorter, and truncation\n                 # will be applied when it is longer, so it will be equivalent to always have it match\n                 # the length of `cache_params.conv_states`, which is `config.conv_kernel`\n-                cache_position = torch.arange(0, past_len, device=input_ids.device)\n-                # if the cache is not used, we also do have to extend the attention mask here\n-                # TODO there is likely a cleverer way to do this\n-                extended_mask = torch.ones(\n-                    attention_mask.size(0), past_len - attention_mask.shape[1], device=attention_mask.device\n-                )\n-                attention_mask = torch.cat([attention_mask, extended_mask], dim=1)\n-                cache_params = None\n-\n-        if attention_mask.shape[1] < past_len:\n-            # we have to update manually the attention mask if\n-            # we are in decoding without cache\n-            # and we don't have position_ids here\n-            # TODO but we should be able to use cache_position though at a later time\n-            extended_mask = torch.ones(\n-                attention_mask.size(0), past_len - attention_mask.shape[1], device=attention_mask.device\n-            )\n-            attention_mask = torch.cat([attention_mask, extended_mask], dim=1)\n+                cache_position = torch.arange(0, self.config.conv_kernel, device=input_ids.device)\n+\n         if inputs_embeds is not None and cache_params is None:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:"
        },
        {
            "sha": "c2ef68f2614ea59fd32d21b1d1ff5a26a54e32c2",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 64,
            "deletions": 18,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=5a2aedca1e9b1d7cc7c6ce3e65034c6df7863a95",
            "patch": "@@ -21,6 +21,7 @@\n \n from transformers import AutoTokenizer, Mamba2Config, is_torch_available\n from transformers.testing_utils import require_read_token, require_torch, require_torch_gpu, slow, torch_device\n+from transformers.utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -103,6 +104,10 @@ def prepare_config_and_inputs(\n     ):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n \n+        # Only left padding is valid\n+        attention_mask = torch.ones(size=(self.batch_size, self.seq_length), device=input_ids.device, dtype=torch.long)\n+        attention_mask[0, :1] = 0\n+\n         sequence_labels = None\n         token_labels = None\n         choice_labels = None\n@@ -118,7 +123,7 @@ def prepare_config_and_inputs(\n         return (\n             config,\n             input_ids,\n-            None,\n+            attention_mask,\n             sequence_labels,\n             token_labels,\n             choice_labels,\n@@ -158,6 +163,56 @@ def prepare_config_and_inputs_for_common(self):\n         inputs_dict = {\"input_ids\": input_ids}\n         return config, inputs_dict\n \n+    def create_and_check_mamba2_caching(self, config, input_ids, attention_mask, *args):\n+        model = Mamba2Model(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        output_whole = model(input_ids, attention_mask=attention_mask).last_hidden_state\n+\n+        outputs = model(\n+            input_ids[:, :-1],\n+            attention_mask=attention_mask[:, :-1],\n+            use_cache=True,\n+            cache_position=torch.arange(0, config.conv_kernel, device=input_ids.device),\n+        )\n+        output_one = outputs.last_hidden_state\n+\n+        # Using the state computed on the first inputs, we will get the same output\n+        outputs = model(\n+            input_ids[:, -1:],\n+            attention_mask=attention_mask[:, -1:],\n+            use_cache=True,\n+            cache_params=outputs.cache_params,\n+            cache_position=torch.arange(config.conv_kernel, config.conv_kernel + 1, device=input_ids.device),\n+        )\n+        output_two = outputs.last_hidden_state\n+\n+        self.parent.assertTrue(\n+            torch.allclose(torch.cat([output_one, output_two], dim=1), output_whole, atol=1e-3, rtol=1e-3)\n+        )\n+\n+    def create_and_check_mamba2_slow_vs_fast_forward(self, config, input_ids, *args, gradient_checkpointing=False):\n+        model = Mamba2Model(config)\n+        model.eval()\n+\n+        if not (is_mamba_2_ssm_available() and is_causal_conv1d_available()):\n+            self.parent.skipTest(\n+                \"This test needs the Mamba2 fast path. Skipping as the necessary packages have not been found.\"\n+            )\n+        if torch_device != \"cuda\":\n+            self.parent.skipTest(\"This test needs the Mamba2 fast path. Skipping as we need a cuda capable device.\")\n+\n+        model.to(torch_device)\n+        if gradient_checkpointing:\n+            model.gradient_checkpointing_enable()\n+\n+        token_emb = model.embeddings(input_ids)\n+        outputs_fast = model.layers[0].mixer.cuda_kernels_forward(token_emb)\n+        outputs_slow = model.layers[0].mixer.torch_forward(token_emb)\n+\n+        self.parent.assertTrue(torch.allclose(outputs_fast, outputs_slow, atol=1e-3, rtol=1e-3))\n+\n \n @unittest.skipIf(\n     not is_torch_greater_or_equal_than_2_0, reason=\"See https://github.com/huggingface/transformers/pull/24204\"\n@@ -184,6 +239,14 @@ def setUp(self):\n             self, config_class=Mamba2Config, n_embd=37, common_properties=[\"hidden_size\", \"num_hidden_layers\"]\n         )\n \n+    def test_mamba2_caching(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_mamba2_caching(*config_and_inputs)\n+\n+    def test_mamba2_slow_vs_fast_forward(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_mamba2_slow_vs_fast_forward(*config_and_inputs)\n+\n     def test_initialization(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -199,23 +262,6 @@ def test_initialization(self):\n     def test_tied_weights_keys(self):\n         pass\n \n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    def test_generate_without_input_ids(self):\n-        pass\n-\n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    def test_generate_from_inputs_embeds(self, _, num_beams):\n-        pass\n-\n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    def test_greedy_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing is interacting with beam search\")\n-    def test_beam_search_generate_dict_outputs_use_cache(self):\n-        pass\n-\n     @unittest.skip(reason=\"A large mamba2 would be necessary (and costly) for that\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        }
    ],
    "stats": {
        "total": 444,
        "additions": 265,
        "deletions": 179
    }
}