{
    "author": "sonianuj287",
    "message": "Use canonical get_size_with_aspect_ratio (with max_size) from transformers.image_transforms to fix #37939 (#41284)\n\n* Use canonical get_size_with_aspect_ratio (with max_size) from transformers.image_transforms to fix #37939\n\n* Fix import sorting/style\n\n* Fix import order\n\n* Refactor: use canonical get_size_with_aspect_ratio across image processors (except YOLOS)\n\nThis commit updates image processing utilities in multiple model processors to use the shared\ntransformers.image_transforms.get_size_with_aspect_ratio for consistent resizing logic and\naspect ratio handling.\n\nYOLOS processors are intentionally left unchanged in this commit to preserve their current\nbehavior and avoid breaking model-specific padding/resizing assumptions. YOLOS will be updated\nin a dedicated follow-up PR once compatibility is fully verified.\n\n* ruff fixes\n\n* Fix check_copies.py references for get_size_with_aspect_ratio to use canonical transformers.image_transforms version\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "297a41a6cf3e853291453b17fd6933146bd9a9ca",
    "files": [
        {
            "sha": "6df784585e9b9ea3b2670eeb87a3af3d8beb9441",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 50,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -22,8 +22,10 @@\n \n import numpy as np\n \n+from transformers.image_transforms import get_size_with_aspect_ratio\n+\n from ...feature_extraction_utils import BatchFeature\n-from ...image_processing_utils import BaseImageProcessor, get_size_dict\n+from ...image_processing_utils import BaseImageProcessor, ImagesKwargs, get_size_dict\n from ...image_transforms import (\n     PaddingMode,\n     center_to_corners_format,\n@@ -53,15 +55,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n-from ...processing_utils import ImagesKwargs\n-from ...utils import (\n-    TensorType,\n-    is_scipy_available,\n-    is_torch_available,\n-    is_torch_tensor,\n-    is_vision_available,\n-    logging,\n-)\n+from ...utils import TensorType, is_scipy_available, is_torch_available, is_torch_tensor, is_vision_available, logging\n from ...utils.import_utils import requires\n \n \n@@ -85,46 +79,6 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n # Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,"
        },
        {
            "sha": "eabdb536ff70b2f422ac00110119f272f7738909",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 41,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -28,6 +28,7 @@\n     PaddingMode,\n     center_to_corners_format,\n     corners_to_center_format,\n+    get_size_with_aspect_ratio,\n     id_to_rgb,\n     pad,\n     rescale,\n@@ -107,47 +108,6 @@ class DeformableDetrImageProcessorKwargs(ImagesKwargs):\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n-# Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     size: Union[int, tuple[int, int], list[int]],"
        },
        {
            "sha": "a1264cb188e2e109e9bcc00503ab7a6d797b2ecb",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -26,6 +26,7 @@\n     PaddingMode,\n     center_to_corners_format,\n     corners_to_center_format,\n+    get_size_with_aspect_ratio,\n     pad,\n     rescale,\n     resize,\n@@ -74,45 +75,6 @@\n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n \n \n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     size: Union[int, tuple[int, int], list[int]],"
        },
        {
            "sha": "02261fc2a1297f5d53d42b3c9f13873dafc04eb8",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -27,6 +27,7 @@\n     PaddingMode,\n     center_to_corners_format,\n     corners_to_center_format,\n+    get_size_with_aspect_ratio,\n     id_to_rgb,\n     pad,\n     rescale,\n@@ -107,45 +108,6 @@ class DetrImageProcessorKwargs(ImagesKwargs):\n \n \n # From the original repo: https://github.com/facebookresearch/detr/blob/3af9fa878e73b6894ce3596450a8d9b89d918ca9/datasets/transforms.py#L76\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n def get_image_size_for_max_height_width(\n     input_image: np.ndarray,\n     max_height: int,"
        },
        {
            "sha": "3381e5bcac50ff2501f6a2efa187f528c6bf1f99",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 40,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -22,6 +22,7 @@\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n     PaddingMode,\n+    get_size_with_aspect_ratio,\n     pad,\n     resize,\n )\n@@ -107,46 +108,6 @@ def convert_segmentation_map_to_binary_masks(\n     return binary_masks.astype(np.float32), labels.astype(np.int64)\n \n \n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = round(raw_size * height / width)\n-        else:\n-            oh = round(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = round(raw_size * width / height)\n-        else:\n-            ow = round(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n-# Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects\n def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):\n     \"\"\"\n     Binarize the given masks using `object_mask_threshold`, it returns the associated values of `masks`, `scores` and"
        },
        {
            "sha": "c099d44e3d586dd70db51bc9a958ed8ddba73cc0",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 41,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -28,6 +28,7 @@\n     PaddingMode,\n     center_to_corners_format,\n     corners_to_center_format,\n+    get_size_with_aspect_ratio,\n     id_to_rgb,\n     pad,\n     rescale,\n@@ -115,47 +116,6 @@ class GroundingDinoImageProcessorKwargs(ImagesKwargs):\n     masks_path: Optional[Union[str, pathlib.Path]]\n \n \n-# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n-# Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     size: Union[int, tuple[int, int], list[int]],"
        },
        {
            "sha": "ce79107f05b32d4a2354523f7d0bd4e0317ba7da",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -80,47 +80,6 @@ class Mask2FormerImageProcessorKwargs(ImagesKwargs):\n     num_labels: Optional[int]\n \n \n-# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n-# Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n     Return the maximum value across all indices of an iterable of values."
        },
        {
            "sha": "20a70315e68ca7fc71c81d69711c94584482ad63",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -25,6 +25,8 @@\n from torch import nn\n from torchvision.transforms.v2 import functional as F\n \n+from transformers.image_transforms import get_size_with_aspect_ratio\n+\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -47,7 +49,6 @@\n     Mask2FormerImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_to_rle,\n-    get_size_with_aspect_ratio,\n     remove_low_and_no_objects,\n )\n "
        },
        {
            "sha": "60e703405605da6cb21ffe94ec103a5028a4d72a",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -86,47 +86,6 @@ class MaskFormerImageProcessorKwargs(ImagesKwargs):\n     num_labels: Optional[int]\n \n \n-# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n-# Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> list[Any]:\n     \"\"\"\n     Return the maximum value across all indices of an iterable of values."
        },
        {
            "sha": "23cc282bb27e6e2889ccf6e7844346a4940e0d35",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -22,6 +22,8 @@\n from torch import nn\n from torchvision.transforms.v2 import functional as F\n \n+from transformers.image_transforms import get_size_with_aspect_ratio\n+\n from ...image_processing_utils import BatchFeature, get_size_dict\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -48,7 +50,6 @@\n     MaskFormerImageProcessorKwargs,\n     compute_segments,\n     convert_segmentation_to_rle,\n-    get_size_with_aspect_ratio,\n     remove_low_and_no_objects,\n )\n "
        },
        {
            "sha": "b3c77a8920cd1eb6c6b2b66ce8a621464d1d5df6",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 41,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/297a41a6cf3e853291453b17fd6933146bd9a9ca/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=297a41a6cf3e853291453b17fd6933146bd9a9ca",
            "patch": "@@ -26,6 +26,7 @@\n     PaddingMode,\n     center_to_corners_format,\n     corners_to_center_format,\n+    get_size_with_aspect_ratio,\n     pad,\n     rescale,\n     resize,\n@@ -90,47 +91,6 @@ class RTDetrImageProcessorKwargs(ImagesKwargs):\n     masks_path: Optional[Union[str, pathlib.Path]]\n \n \n-# Copied from transformers.models.detr.image_processing_detr.get_size_with_aspect_ratio\n-def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n-    \"\"\"\n-    Computes the output image size given the input image size and the desired output size.\n-\n-    Args:\n-        image_size (`tuple[int, int]`):\n-            The input image size.\n-        size (`int`):\n-            The desired output size.\n-        max_size (`int`, *optional*):\n-            The maximum allowed output size.\n-    \"\"\"\n-    height, width = image_size\n-    raw_size = None\n-    if max_size is not None:\n-        min_original_size = float(min((height, width)))\n-        max_original_size = float(max((height, width)))\n-        if max_original_size / min_original_size * size > max_size:\n-            raw_size = max_size * min_original_size / max_original_size\n-            size = int(round(raw_size))\n-\n-    if (height <= width and height == size) or (width <= height and width == size):\n-        oh, ow = height, width\n-    elif width < height:\n-        ow = size\n-        if max_size is not None and raw_size is not None:\n-            oh = int(raw_size * height / width)\n-        else:\n-            oh = int(size * height / width)\n-    else:\n-        oh = size\n-        if max_size is not None and raw_size is not None:\n-            ow = int(raw_size * width / height)\n-        else:\n-            ow = int(size * width / height)\n-\n-    return (oh, ow)\n-\n-\n-# Copied from transformers.models.detr.image_processing_detr.get_resize_output_image_size\n def get_resize_output_image_size(\n     input_image: np.ndarray,\n     size: Union[int, tuple[int, int], list[int]],"
        }
    ],
    "stats": {
        "total": 389,
        "additions": 14,
        "deletions": 375
    }
}