{
    "author": "MostHumble",
    "message": "Enhance documentation to explain chat-based few-shot prompting (#37828)\n\n* Enhance documentation to explain chat-based few-shot prompting\n\nUpdates the documentation on few-shot prompting to illustrate how to structure examples using the chat-based format for instruction-tuned models.\n\n* Update docs/source/en/tasks/prompting.md\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Update docs/source/en/tasks/prompting.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/prompting.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/prompting.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/tasks/prompting.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* fix typos\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "c3aeaa80606585dccdc2b75eb8f419de5a5a58ee",
    "files": [
        {
            "sha": "635b1fefa4189d8db19e2c53d1f73f8fdd66853e",
            "filename": "docs/source/en/tasks/prompting.md",
            "status": "modified",
            "additions": 41,
            "deletions": 11,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c3aeaa80606585dccdc2b75eb8f419de5a5a58ee/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c3aeaa80606585dccdc2b75eb8f419de5a5a58ee/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md?ref=c3aeaa80606585dccdc2b75eb8f419de5a5a58ee",
            "patch": "@@ -78,32 +78,62 @@ Crafting a good prompt alone, also known as zero-shot prompting, may not be enou\n \n This section covers a few prompting techniques.\n \n-### Few-shot\n+### Few-shot prompting\n \n-Few-shot prompting improves accuracy and performance by including specific examples of what a model should generate given an input. The explicit examples give the model a better understanding of the task and the output format you're looking for. Try experimenting with different numbers of examples (2, 4, 8, etc.) to see how it affects performance.\n+Few-shot prompting improves accuracy and performance by including specific examples of what a model should generate given an input. The explicit examples give the model a better understanding of the task and the output format youâ€™re looking for. Try experimenting with different numbers of examples (2, 4, 8, etc.) to see how it affects performance. The example below provides the model with 1 example (1-shot) of the output format (a date in MM/DD/YYYY format) it should return.\n \n-The example below provides the model with 1 example (1-shot) of the output format (a date in MM/DD/YYYY format) it should return.\n-\n-```py\n+```python\n from transformers import pipeline\n import torch\n \n pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n prompt = \"\"\"Text: The first human went into space and orbited the Earth on April 12, 1961.\n Date: 04/12/1961\n-Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon. \n+Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\n Date:\"\"\"\n \n outputs = pipeline(prompt, max_new_tokens=12, do_sample=True, top_k=10)\n for output in outputs:\n     print(f\"Result: {output['generated_text']}\")\n-Result: Text: The first human went into space and orbited the Earth on April 12, 1961.\n-Date: 04/12/1961\n-Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon. \n-Date: 09/28/1960\n+# Result: Text: The first human went into space and orbited the Earth on April 12, 1961.\n+# Date: 04/12/1961\n+# Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\n+# Date: 09/28/1960\n+```\n+\n+The downside of few-shot prompting is that you need to create lengthier prompts which increases computation and latency. There is also a limit to prompt lengths. Finally, a model can learn unintended patterns from your examples, and it may not work well on complex reasoning tasks.\n+\n+To improve few-shot prompting for modern instruction-tuned LLMs, use a model's specific [chat template](../conversations). These models are trained on datasets with turn-based conversations between a \"user\" and \"assistant\". Structuring your prompt to align with this can improve performance.\n+\n+Structure your prompt as a turn-based conversation and use the [`apply_chat_template`] method to tokenize and format it.\n+\n+```python\n+from transformers import pipeline\n+import torch\n+\n+pipeline = pipeline(model=\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n+\n+messages = [\n+    {\"role\": \"user\", \"content\": \"Text: The first human went into space and orbited the Earth on April 12, 1961.\"},\n+    {\"role\": \"assistant\", \"content\": \"Date: 04/12/1961\"},\n+    {\"role\": \"user\", \"content\": \"Text: The first-ever televised presidential debate in the United States took place on September 28, 1960, between presidential candidates John F. Kennedy and Richard Nixon.\"}\n+]\n+\n+prompt = pipeline.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+\n+outputs = pipeline(prompt, max_new_tokens=12, do_sample=True, top_k=10)\n+\n+for output in outputs:\n+    print(f\"Result: {output['generated_text']}\")\n ```\n \n-The downside of few-shot prompting is that you need to create lengthier prompts which increases computation and latency. There is also a limit to prompt lengths. Finally, a model can learn unintended patterns from your examples and it doesn't work well on complex reasoning tasks.\n+\n+While the basic few-shot prompting approach embedded examples within a single text string, the chat template format offers the following benefits.\n+\n+- The model may have a potentially improved understanding because it can better recognize the pattern and the expected roles of user input and assistant output.\n+- The model may more consistently output the desired output format because it is structured like its input during training.\n+\n+Always consult a specific instruction-tuned model's documentation to learn more about the format of their chat template so that you can structure your few-shot prompts accordingly.\n \n ### Chain-of-thought\n "
        }
    ],
    "stats": {
        "total": 52,
        "additions": 41,
        "deletions": 11
    }
}