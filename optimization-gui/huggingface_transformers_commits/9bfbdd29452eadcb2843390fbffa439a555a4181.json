{
    "author": "cyyever",
    "message": "Fix default values of getenv (#39867)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "9bfbdd29452eadcb2843390fbffa439a555a4181",
    "files": [
        {
            "sha": "7c6be88099d1a5752523cca6480b2425cb9b7e50",
            "filename": "examples/3D_parallel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/examples%2F3D_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/examples%2F3D_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2F3D_parallel.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -72,9 +72,9 @@\n \n \n def main():\n-    tp_size = int(os.environ.get(\"TP_SIZE\", 1))\n-    dp_size = int(os.environ.get(\"DP_SIZE\", 1))\n-    cp_size = int(os.environ.get(\"CP_SIZE\", 1))  # Add CP size configuration\n+    tp_size = int(os.environ.get(\"TP_SIZE\", \"1\"))\n+    dp_size = int(os.environ.get(\"DP_SIZE\", \"1\"))\n+    cp_size = int(os.environ.get(\"CP_SIZE\", \"1\"))  # Add CP size configuration\n     sdpa_backend = SDPBackend.FLASH_ATTENTION  # For CP\n     # sdpa_backend = SDPBackend.MATH # For CP\n     global_batch_size = 8  # Desired global batch size"
        },
        {
            "sha": "4c14d2644f0a0e7dbc971a7f72edd739c7fb902e",
            "filename": "examples/legacy/seq2seq/old_test_datasets.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/examples%2Flegacy%2Fseq2seq%2Fold_test_datasets.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/examples%2Flegacy%2Fseq2seq%2Fold_test_datasets.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fold_test_datasets.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -184,7 +184,7 @@ def count_pad_tokens(data_loader, k=\"input_ids\"):\n         assert len(sortish_dl) == len(naive_dl)\n \n     def _get_dataset(self, n_obs=1000, max_len=128):\n-        if os.getenv(\"USE_REAL_DATA\", False):\n+        if os.getenv(\"USE_REAL_DATA\", None):\n             data_dir = \"examples/seq2seq/wmt_en_ro\"\n             max_tokens = max_len * 2 * 64\n             if not Path(data_dir).joinpath(\"train.len\").exists():"
        },
        {
            "sha": "f3e3a9a867e670af92a74ba8a7e38f26440f3969",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -56,7 +56,7 @@\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n \n-ignore_sanity_checks = int(os.environ.get(\"IGNORE_SANITY\", 0)) == 1\n+ignore_sanity_checks = int(os.environ.get(\"IGNORE_SANITY\", \"0\")) == 1\n # torch.use_deterministic_algorithms(True)\n torch.backends.cudnn.deterministic = True\n \n@@ -74,9 +74,9 @@\n \n \n def main():\n-    tp_size = int(os.environ.get(\"TP_SIZE\", 1))\n-    dp_size = int(os.environ.get(\"DP_SIZE\", 4))\n-    cp_size = int(os.environ.get(\"CP_SIZE\", 1))  # Add CP size configuration\n+    tp_size = int(os.environ.get(\"TP_SIZE\", \"1\"))\n+    dp_size = int(os.environ.get(\"DP_SIZE\", \"4\"))\n+    cp_size = int(os.environ.get(\"CP_SIZE\", \"1\"))  # Add CP size configuration\n     sdpa_backend = SDPBackend.FLASH_ATTENTION  # For CP\n     # sdpa_backend = SDPBackend.MATH # For CP\n     global_batch_size = 8  # Desired global batch size"
        },
        {
            "sha": "65dbd9cda1f2cfcb1ce8c84fd515120a2beefdf5",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -60,7 +60,7 @@ def initialize_tensor_parallelism(tp_plan, tp_size=None):\n \n             backend_map = {\"cuda\": \"nccl\", \"cpu\": \"gloo\", \"xpu\": \"xccl\", \"hpu\": \"hccl\"}\n             backend = backend_map.get(device_type)\n-            if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", 0)):\n+            if device_type == \"cpu\" and int(os.environ.get(\"CCL_WORKER_COUNT\", \"0\")):\n                 backend = \"ccl\"\n             if device_type == \"xpu\" and not is_torch_greater_or_equal(\"2.8\", accept_dev=True):\n                 backend = \"ccl\""
        },
        {
            "sha": "f0d3df9e22da4fd768e6167a7f4a17ab2eebecb9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -195,7 +195,7 @@ def is_local_dist_rank_0():\n     return (\n         torch.distributed.is_available()\n         and torch.distributed.is_initialized()\n-        and int(os.environ.get(\"LOCAL_RANK\", -1)) == 0\n+        and int(os.environ.get(\"LOCAL_RANK\", \"-1\")) == 0\n     )\n \n \n@@ -4663,7 +4663,7 @@ def from_pretrained(\n                 \"`tp_plan` and `device_map` are mutually exclusive. Choose either one for parallelization.\"\n             )\n \n-        if device_map == \"auto\" and int(os.environ.get(\"WORLD_SIZE\", 0)):\n+        if device_map == \"auto\" and int(os.environ.get(\"WORLD_SIZE\", \"0\")):\n             logger.info(\n                 \"You've set device_map=`auto` while triggering a distributed run with torchrun. This might lead to unexpected behavior. \"\n                 \"If your plan is to load the model on each device, you should set device_map={\""
        },
        {
            "sha": "0580fa7287cd22f8affc8e22dea2d3dfb5c9f383",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -2868,7 +2868,7 @@ def run_test_in_subprocess(test_case, target_func, inputs=None, timeout=None):\n             variable `PYTEST_TIMEOUT` will be checked. If still `None`, its value will be set to `600`.\n     \"\"\"\n     if timeout is None:\n-        timeout = int(os.environ.get(\"PYTEST_TIMEOUT\", 600))\n+        timeout = int(os.environ.get(\"PYTEST_TIMEOUT\", \"600\"))\n \n     start_methohd = \"spawn\"\n     ctx = multiprocessing.get_context(start_methohd)\n@@ -3028,7 +3028,7 @@ class HfDocTestParser(doctest.DocTestParser):\n     # fmt: on\n \n     # !!!!!!!!!!! HF Specific !!!!!!!!!!!\n-    skip_cuda_tests: bool = bool(os.environ.get(\"SKIP_CUDA_DOCTEST\", False))\n+    skip_cuda_tests: bool = bool(os.environ.get(\"SKIP_CUDA_DOCTEST\", \"0\"))\n     # !!!!!!!!!!! HF Specific !!!!!!!!!!!\n \n     def parse(self, string, name=\"<string>\"):"
        },
        {
            "sha": "da0721eee0c92e61fcbce0b21fcb7d0a019aac4b",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -123,7 +123,7 @@ def default_logdir() -> str:\n def get_int_from_env(env_keys, default):\n     \"\"\"Returns the first positive env value found in the `env_keys` list or the default.\"\"\"\n     for e in env_keys:\n-        val = int(os.environ.get(e, -1))\n+        val = int(os.environ.get(e, \"-1\"))\n         if val >= 0:\n             return val\n     return default"
        },
        {
            "sha": "af8aa6b8f6a05f62cdcaa14a54ba1806ec31b631",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -213,8 +213,8 @@ def define_sagemaker_information():\n     sagemaker_object = {\n         \"sm_framework\": os.getenv(\"SM_FRAMEWORK_MODULE\", None),\n         \"sm_region\": os.getenv(\"AWS_REGION\", None),\n-        \"sm_number_gpu\": os.getenv(\"SM_NUM_GPUS\", 0),\n-        \"sm_number_cpu\": os.getenv(\"SM_NUM_CPUS\", 0),\n+        \"sm_number_gpu\": os.getenv(\"SM_NUM_GPUS\", \"0\"),\n+        \"sm_number_cpu\": os.getenv(\"SM_NUM_CPUS\", \"0\"),\n         \"sm_distributed_training\": runs_distributed_training,\n         \"sm_deep_learning_container\": dlc_container_used,\n         \"sm_deep_learning_container_tag\": dlc_tag,"
        },
        {
            "sha": "88a6a9769f65cc8d027059088cb02828e2fa13e8",
            "filename": "src/transformers/utils/logging.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Futils%2Flogging.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/src%2Ftransformers%2Futils%2Flogging.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Flogging.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -307,7 +307,7 @@ def warning_advice(self, *args, **kwargs):\n     This method is identical to `logger.warning()`, but if env var TRANSFORMERS_NO_ADVISORY_WARNINGS=1 is set, this\n     warning will not be printed\n     \"\"\"\n-    no_advisory_warnings = os.getenv(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\", False)\n+    no_advisory_warnings = os.getenv(\"TRANSFORMERS_NO_ADVISORY_WARNINGS\")\n     if no_advisory_warnings:\n         return\n     self.warning(*args, **kwargs)\n@@ -392,7 +392,6 @@ def get_lock(self):\n \n def is_progress_bar_enabled() -> bool:\n     \"\"\"Return a boolean indicating whether tqdm progress bars are enabled.\"\"\"\n-    global _tqdm_active\n     return bool(_tqdm_active)\n \n "
        },
        {
            "sha": "7e60d51d5f91df9cc59b00d7d5a12ed371d98fcd",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -143,7 +143,7 @@ def get_config(self):\n             is_decoder=False,\n             initializer_range=self.initializer_range,\n         )\n-        if test := os.environ.get(\"PYTEST_CURRENT_TEST\", False):\n+        if test := os.environ.get(\"PYTEST_CURRENT_TEST\", None):\n             test_name = test.split(\":\")[-1].split(\" \")[0]\n \n             # If we're testing `test_retain_grad_hidden_states_attentions`, we normally get an error"
        },
        {
            "sha": "a1b010435588137c635bdf70a798cbfc1be82ffc",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -1710,7 +1710,9 @@ def test_large_batched_generation_multilingual(self):\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n         model.to(torch_device)\n \n-        token = os.getenv(\"HF_HUB_READ_TOKEN\", True)\n+        token = os.getenv(\"HF_HUB_READ_TOKEN\", None)\n+        if token is None:\n+            token = True\n         ds = load_dataset(\n             \"hf-internal-testing/fixtures_common_voice\",\n             \"ja\","
        },
        {
            "sha": "2e5bdb80ac81de9d520285925f95106cdf1e9d60",
            "filename": "tests/utils/test_skip_decorators.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bfbdd29452eadcb2843390fbffa439a555a4181/tests%2Futils%2Ftest_skip_decorators.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bfbdd29452eadcb2843390fbffa439a555a4181/tests%2Futils%2Ftest_skip_decorators.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_skip_decorators.py?ref=9bfbdd29452eadcb2843390fbffa439a555a4181",
            "patch": "@@ -43,7 +43,7 @@\n \n # test that we can stack our skip decorators with 3rd party decorators\n def check_slow():\n-    run_slow = bool(os.getenv(\"RUN_SLOW\", 0))\n+    run_slow = bool(os.getenv(\"RUN_SLOW\", \"0\"))\n     if run_slow:\n         assert True\n     else:\n@@ -52,15 +52,15 @@ def check_slow():\n \n # test that we can stack our skip decorators\n def check_slow_torch_cuda():\n-    run_slow = bool(os.getenv(\"RUN_SLOW\", 0))\n+    run_slow = bool(os.getenv(\"RUN_SLOW\", \"0\"))\n     if run_slow and torch_device == \"cuda\":\n         assert True\n     else:\n         assert False, \"should have been skipped\"\n \n \n def check_slow_torch_accelerator():\n-    run_slow = bool(os.getenv(\"RUN_SLOW\", 0))\n+    run_slow = bool(os.getenv(\"RUN_SLOW\", \"0\"))\n     assert run_slow and torch_device in [\"cuda\", \"xpu\"], \"should have been skipped\"\n \n "
        }
    ],
    "stats": {
        "total": 47,
        "additions": 24,
        "deletions": 23
    }
}