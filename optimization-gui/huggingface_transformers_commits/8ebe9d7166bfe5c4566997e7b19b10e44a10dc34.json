{
    "author": "efsotr",
    "message": "Optimize ForCausalLMLoss by removing unnecessary contiguous() call to reduce memory overhead (#35646)\n\nOptimize ForCausalLMLoss by removing unnecessary contiguous() calls to reduce memory overhead",
    "sha": "8ebe9d7166bfe5c4566997e7b19b10e44a10dc34",
    "files": [
        {
            "sha": "0f39fde40a7c49f1ae019d1abe53e3a6740c106b",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ebe9d7166bfe5c4566997e7b19b10e44a10dc34/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ebe9d7166bfe5c4566997e7b19b10e44a10dc34/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=8ebe9d7166bfe5c4566997e7b19b10e44a10dc34",
            "patch": "@@ -36,15 +36,15 @@ def ForCausalLMLoss(\n     logits = logits.float()\n     labels = labels.to(logits.device)\n     # Shift so that tokens < n predict n\n-    shift_logits = logits[..., :-1, :].contiguous()\n+    labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n     shift_labels = labels[..., 1:].contiguous()\n \n     # Flatten the tokens\n-    shift_logits = shift_logits.view(-1, vocab_size)\n+    logits = logits.view(-1, vocab_size)\n     shift_labels = shift_labels.view(-1)\n     # Enable model parallelism\n-    shift_labels = shift_labels.to(shift_logits.device)\n-    loss = fixed_cross_entropy(shift_logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n+    shift_labels = shift_labels.to(logits.device)\n+    loss = fixed_cross_entropy(logits, shift_labels, num_items_in_batch, ignore_index, **kwargs)\n     return loss\n \n "
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}