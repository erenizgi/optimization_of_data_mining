{
    "author": "SunMarc",
    "message": "fix florence kwargs  (#40826)",
    "sha": "ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10",
    "files": [
        {
            "sha": "afa05e8e3c914fbfaa9d9d22e238ed0175a354c8",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10",
            "patch": "@@ -25,7 +25,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -726,7 +725,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, Florence2Seq2SeqModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -777,7 +775,6 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             return_dict=True,\n-            **kwargs,\n         )\n \n         return Florence2Seq2SeqModelOutput(\n@@ -922,7 +919,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **kwargs,\n+            # **kwargs, ## TODO: add back when Bart attention is refactored and takes kwargs\n         )\n \n         hidden_states = outputs[0]"
        },
        {
            "sha": "12bf00ca253dc4365409b7df79ba471e598033fc",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=ada64ce4526b8b6d7d1fd8edacbde3cbb27d8a10",
            "patch": "@@ -24,7 +24,6 @@\n from ...configuration_utils import PretrainedConfig\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import MultiModalData, ProcessorMixin, Unpack\n@@ -1569,7 +1568,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, Florence2Seq2SeqModelOutput]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1620,7 +1618,6 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n             return_dict=True,\n-            **kwargs,\n         )\n \n         return Florence2Seq2SeqModelOutput(\n@@ -1731,7 +1728,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            **kwargs,\n+            # **kwargs, ## TODO: add back when Bart attention is refactored and takes kwargs\n         )\n \n         hidden_states = outputs[0]"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 2,
        "deletions": 8
    }
}