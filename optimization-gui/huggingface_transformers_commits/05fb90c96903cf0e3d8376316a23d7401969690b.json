{
    "author": "cyyever",
    "message": "Fix single quotes in markdown (#41154)\n\nFix typos\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "05fb90c96903cf0e3d8376316a23d7401969690b",
    "files": [
        {
            "sha": "7a2da690945b0b140973274d02189c1ec51e242d",
            "filename": "docs/TRANSLATING.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2FTRANSLATING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2FTRANSLATING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2FTRANSLATING.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -50,7 +50,7 @@ Begin translating the text!\n \n 1. Start with the `_toctree.yml` file that corresponds to your documentation chapter. This file is essential for rendering the table of contents on the website.\n \n-    - If the `_toctree.yml` file doesn’t exist for your language, create one by copying the English version and removing unrelated sections.\n+    - If the `_toctree.yml` file doesn't exist for your language, create one by copying the English version and removing unrelated sections.\n     - Ensure it is placed in the `docs/source/LANG-ID/` directory.\n \n     Here’s an example structure for the `_toctree.yml` file:"
        },
        {
            "sha": "1a3439bc792777c09283b95bba46faf9003a28bb",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -136,7 +136,7 @@ The cache position tracks where to insert new tokens in the attention cache. It\n \n Cache position is used internally for two purposes:\n \n-1. Selecting new tokens to process in the input sequence and ensuring only tokens that haven’t been cached yet are passed to the model's `forward`.\n+1. Selecting new tokens to process in the input sequence and ensuring only tokens that haven't been cached yet are passed to the model's `forward`.\n 2. Storing key/value pairs at the correct positions in the cache. This is especially important for fixed-size caches, that pre-allocates a specific cache length.\n \n The generation loop usually takes care of the cache position, but if you're writing a custom generation method, it is important that cache positions are accurate since they are used to write and read key/value states into fixed slots."
        },
        {
            "sha": "b5f9bafa96ec7ef36301f8d0d7d34529d9b68d28",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -124,7 +124,7 @@ Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopte\n \n > [!WARNING]\n > Some tokenizers add special `<bos>` and `<eos>` tokens. Chat templates should already include all the necessary special tokens, and adding additional special tokens is often incorrect or duplicated, hurting model performance. When you format text with `apply_chat_template(tokenize=False)`, make sure you set `add_special_tokens=False` if you tokenize later to avoid duplicating these tokens.\n-> This isn’t an issue if you use `apply_chat_template(tokenize=True)`, which means it's usually the safer option!\n+> This isn't an issue if you use `apply_chat_template(tokenize=True)`, which means it's usually the safer option!\n \n ### add_generation_prompt\n \n@@ -168,7 +168,7 @@ Can I ask a question?<|im_end|>\n \n When `add_generation_prompt=True`, `<|im_start|>assistant` is added at the end to indicate the start of an `assistant` message. This lets the model know an `assistant` response is next.\n \n-Not all models require generation prompts, and some models, like [Llama](./model_doc/llama), don’t have any special tokens before the `assistant` response. In these cases, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\n+Not all models require generation prompts, and some models, like [Llama](./model_doc/llama), don't have any special tokens before the `assistant` response. In these cases, [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) has no effect.\n \n ### continue_final_message\n \n@@ -187,9 +187,9 @@ model.generate(**formatted_chat)\n ```\n \n > [!WARNING]\n-> You shouldn’t use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n+> You shouldn't use [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) and [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) together. The former adds tokens that start a new message, while the latter removes end of sequence tokens. Using them together returns an error.\n \n-[`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the `assistant` role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don’t support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) argument to the pipeline.\n+[`TextGenerationPipeline`] sets [add_generation_prompt](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.add_generation_prompt) to `True` by default to start a new message. However, if the final message in the chat has the `assistant` role, it assumes the message is a prefill and switches to `continue_final_message=True`. This is because most models don't support multiple consecutive assistant messages. To override this behavior, explicitly pass the [continue_final_message](https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template.continue_final_message) argument to the pipeline.\n \n ## Model training\n "
        },
        {
            "sha": "3b0aec00894b7e752855d962b94c609b5e359576",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -115,7 +115,7 @@ Some vision models also support video inputs. The message format is very similar\n \n - The content `\"type\"` should be `\"video\"` to indicate the content is a video.\n - For videos, it can be a link to the video (`\"url\"`) or it could be a file path (`\"path\"`). Videos loaded from a URL can only be decoded with [PyAV](https://pyav.basswood-io.com/docs/stable/) or [Decord](https://github.com/dmlc/decord).\n-- In addition to loading videos from a URL or file path, you can also pass decoded video data directly. This is useful if you’ve already preprocessed or decoded video frames elsewhere in memory (e.g., using OpenCV, decord, or torchvision). You don't need to save to files or store it in an URL.\n+- In addition to loading videos from a URL or file path, you can also pass decoded video data directly. This is useful if you've already preprocessed or decoded video frames elsewhere in memory (e.g., using OpenCV, decord, or torchvision). You don't need to save to files or store it in an URL.\n \n > [!WARNING]\n > Loading a video from `\"url\"` is only supported by the PyAV or Decord backends."
        },
        {
            "sha": "70e4a33f9ade90e81753641c5c6561328c636894",
            "filename": "docs/source/en/cursor.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fcursor.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fcursor.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcursor.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -23,7 +23,7 @@ where `port` is the port used by `transformers serve` (`8000` by default). On th\n You're now ready to set things up on the app side! In Cursor, while you can't set a new provider, you can change the endpoint for OpenAI requests in the model selection settings. First, navigate to \"Settings\" > \"Cursor Settings\", \"Models\" tab, and expand the \"API Keys\" collapsible. To set your `transformers serve` endpoint, follow this order:\n 1. Unselect ALL models in the list above (e.g. `gpt4`, ...);\n 2. Add and select the model you want to use (e.g. `Qwen/Qwen3-4B`)\n-3. Add some random text to OpenAI API Key. This field won't be used, but it can’t be empty;\n+3. Add some random text to OpenAI API Key. This field won't be used, but it can't be empty;\n 4. Add the https address from `ngrok` to the \"Override OpenAI Base URL\" field, appending `/v1` to the address (i.e. `https://(...).ngrok-free.app/v1`);\n 5. Hit \"Verify\".\n "
        },
        {
            "sha": "61d1ddd59f643523eb6acab9e40bb0f0cd8eb93c",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -486,7 +486,7 @@ To configure Apex-like fp16 mixed precision, set up the config as shown below wi\n > [!TIP]\n > bf16 requires DeepSpeed 0.6.0.\n \n-bf16 has the same dynamic range as fp32, and doesn’t require loss scaling unlike fp16. However, if you use [gradient accumulation](#gradient-accumulation) with bf16, gradients are accumulated in bf16 which may not be desirable because the lower precision can lead to lossy accumulation.\n+bf16 has the same dynamic range as fp32, and doesn't require loss scaling unlike fp16. However, if you use [gradient accumulation](#gradient-accumulation) with bf16, gradients are accumulated in bf16 which may not be desirable because the lower precision can lead to lossy accumulation.\n \n bf16 can be set up in the config file or enabled from the command line when the following arguments are passed: `--bf16` or `--bf16_full_eval`.\n "
        },
        {
            "sha": "425ee109950c090b85617699eec20d5e6b77af3e",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -414,9 +414,9 @@ Recommended practices:\n - Add self-contained examples to enable quick experimentation.\n - Describe soft-requirements such as if the method only works well with a certain family of models.\n \n-### Reusing `generate`’s input preparation\n+### Reusing `generate`'s input preparation\n \n-If you're adding a new decoding loop, you might want to preserve the input preparation present in `generate` (batch expansion, attention masks, logits processors, stopping criteria, etc.). You can also pass a **callable** to `custom_generate` to reuse [`~GenerationMixin.generate`]’s full preparation pipeline while overriding only the decoding loop.\n+If you're adding a new decoding loop, you might want to preserve the input preparation present in `generate` (batch expansion, attention masks, logits processors, stopping criteria, etc.). You can also pass a **callable** to `custom_generate` to reuse [`~GenerationMixin.generate`]'s full preparation pipeline while overriding only the decoding loop.\n \n ```py\n def custom_loop(model, input_ids, attention_mask, logits_processor, stopping_criteria, generation_config, **model_kwargs):\n@@ -437,7 +437,7 @@ output = model.generate(\n ```\n \n > [!TIP]\n-> If you publish a `custom_generate` repository, your `generate` implementation can itself define a callable and pass it to `model.generate()`. This lets you customize the decoding loop while still benefiting from Transformers’ built-in input preparation logic.\n+> If you publish a `custom_generate` repository, your `generate` implementation can itself define a callable and pass it to `model.generate()`. This lets you customize the decoding loop while still benefiting from Transformers' built-in input preparation logic.\n \n ### Finding custom generation methods\n "
        },
        {
            "sha": "f318c73d28a9a951247a2cf4fe84a56b70db4fc6",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -213,7 +213,7 @@ A cache can also work in iterative generation settings where there is back-and-f\n \n For iterative generation with a cache, start by initializing an empty cache class and then you can feed in your new prompts. Keep track of dialogue history with a [chat template](./chat_templating).\n \n-The following example demonstrates [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). If you’re using a different chat-style model, [`~PreTrainedTokenizer.apply_chat_template`] may process messages differently. It might cut out important tokens depending on how the Jinja template is written.\n+The following example demonstrates [Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf). If you're using a different chat-style model, [`~PreTrainedTokenizer.apply_chat_template`] may process messages differently. It might cut out important tokens depending on how the Jinja template is written.\n \n For example, some models use special `<think> ... </think>` tokens during reasoning. These could get lost during re-encoding, causing indexing issues. You might need to manually remove or adjust extra tokens from the completions to keep things stable.\n "
        },
        {
            "sha": "8a9c67846492ad0fec4d4db3ac868bb7b7777c75",
            "filename": "docs/source/en/model_doc/bart.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbart.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n </div>\n \n # BART\n-[BART](https://huggingface.co/papers/1910.13461) is a sequence-to-sequence model that combines the pretraining objectives from BERT and GPT. It’s pretrained by corrupting text in different ways like deleting words, shuffling sentences, or masking tokens and learning how to fix it. The encoder encodes the corrupted document and the corrupted text is fixed by the decoder. As it learns to recover the original text, BART gets really good at both understanding and generating language.\n+[BART](https://huggingface.co/papers/1910.13461) is a sequence-to-sequence model that combines the pretraining objectives from BERT and GPT. It's pretrained by corrupting text in different ways like deleting words, shuffling sentences, or masking tokens and learning how to fix it. The encoder encodes the corrupted document and the corrupted text is fixed by the decoder. As it learns to recover the original text, BART gets really good at both understanding and generating language.\n \n You can find all the original BART checkpoints under the [AI at Meta](https://huggingface.co/facebook?search_models=bart) organization.\n \n@@ -89,7 +89,7 @@ echo -e \"Plants create <mask> through a process known as photosynthesis.\" | tran\n \n - Inputs should be padded on the right because BERT uses absolute position embeddings.\n - The [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn) checkpoint doesn't include `mask_token_id` which means it can't perform mask-filling tasks.\n-- BART doesn’t use `token_type_ids` for sequence classification. Use [`BartTokenizer`] or [`~PreTrainedTokenizerBase.encode`] to get the proper splitting.\n+- BART doesn't use `token_type_ids` for sequence classification. Use [`BartTokenizer`] or [`~PreTrainedTokenizerBase.encode`] to get the proper splitting.\n - The forward pass of [`BartModel`] creates the `decoder_input_ids` if they're not passed. This can be different from other model APIs, but it is a useful feature for mask-filling tasks.\n - Model predictions are intended to be identical to the original implementation when `forced_bos_token_id=0`. This only works if the text passed to `fairseq.encode` begins with a space.\n - [`~GenerationMixin.generate`] should be used for conditional generation tasks like summarization."
        },
        {
            "sha": "917adab47cc5ec36db8bd8cc5edc80140bccc5da",
            "filename": "docs/source/en/model_doc/bertweet.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbertweet.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n ## BERTweet\n \n-[BERTweet](https://huggingface.co/papers/2005.10200) shares the same architecture as [BERT-base](./bert), but it’s pretrained like [RoBERTa](./roberta) on English Tweets. It performs really well on Tweet-related tasks like part-of-speech tagging, named entity recognition, and text classification.\n+[BERTweet](https://huggingface.co/papers/2005.10200) shares the same architecture as [BERT-base](./bert), but it's pretrained like [RoBERTa](./roberta) on English Tweets. It performs really well on Tweet-related tasks like part-of-speech tagging, named entity recognition, and text classification.\n \n You can find all the original BERTweet checkpoints under the [VinAI Research](https://huggingface.co/vinai?search_models=BERTweet) organization.\n \n@@ -88,7 +88,7 @@ echo -e \"Plants create <mask> through a process known as photosynthesis.\" | tran\n </hfoptions>\n \n ## Notes\n-- Use the [`AutoTokenizer`] or [`BertweetTokenizer`] because it’s preloaded with a custom vocabulary adapted to tweet-specific tokens like hashtags (#), mentions (@), emojis, and common abbreviations. Make sure to also install the [emoji](https://pypi.org/project/emoji/) library.\n+- Use the [`AutoTokenizer`] or [`BertweetTokenizer`] because it's preloaded with a custom vocabulary adapted to tweet-specific tokens like hashtags (#), mentions (@), emojis, and common abbreviations. Make sure to also install the [emoji](https://pypi.org/project/emoji/) library.\n - Inputs should be padded on the right (`padding=\"max_length\"`) because BERT uses absolute position embeddings.\n \n ## BertweetTokenizer"
        },
        {
            "sha": "5cf9c81647492fd92d92b88e68c4cafd01ba4294",
            "filename": "docs/source/en/model_doc/canine.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcanine.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -23,7 +23,7 @@ rendered properly in your Markdown viewer.\n \n # CANINE\n \n-[CANINE](https://huggingface.co/papers/2103.06874) is a tokenization-free Transformer. It skips the usual step of splitting text into subwords or wordpieces and processes text character by character. That means it works directly with raw Unicode, making it especially useful for languages with complex or inconsistent tokenization rules and even noisy inputs like typos. Since working with characters means handling longer sequences, CANINE uses a smart trick. The model compresses the input early on (called downsampling) so the transformer doesn’t have to process every character individually. This keeps things fast and efficient.\n+[CANINE](https://huggingface.co/papers/2103.06874) is a tokenization-free Transformer. It skips the usual step of splitting text into subwords or wordpieces and processes text character by character. That means it works directly with raw Unicode, making it especially useful for languages with complex or inconsistent tokenization rules and even noisy inputs like typos. Since working with characters means handling longer sequences, CANINE uses a smart trick. The model compresses the input early on (called downsampling) so the transformer doesn't have to process every character individually. This keeps things fast and efficient.\n \n You can find all the original CANINE checkpoints under the [Google](https://huggingface.co/google?search_models=canine) organization.\n "
        },
        {
            "sha": "1deca4d00eee329d3c4d18c9322453d5dd6bda3b",
            "filename": "docs/source/en/model_doc/cohere.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -122,7 +122,7 @@ visualizer(\"Plants create energy through a process known as\")\n </div>\n \n ## Notes\n-- Don’t use the dtype parameter in [`~AutoModel.from_pretrained`] if you’re using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n+- Don't use the dtype parameter in [`~AutoModel.from_pretrained`] if you're using FlashAttention-2 because it only supports fp16 or bf16. You should use [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html), set fp16 or bf16 to True if using [`Trainer`], or use [torch.autocast](https://pytorch.org/docs/stable/amp.html#torch.autocast).\n \n ## CohereConfig\n "
        },
        {
            "sha": "79fe6bd9f6ad309ba1d5d6fe8e90bca7d37c8110",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -138,7 +138,7 @@ Applying the template to our `messages` list would produce:\n \n This tells the model:  \n 1. The conversation history (user/assistant turns).  \n-2. The model’s turn to generate a response (`<|assistant|>` at the end).  \n+2. The model's turn to generate a response (`<|assistant|>` at the end).  \n \n ---\n "
        },
        {
            "sha": "e779d0ac55f104664c6bfc33a9400199b09b369b",
            "filename": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n \n # DeepseekVLHybrid\n \n-[Deepseek-VL-Hybrid](https://huggingface.co/papers/2403.05525) was introduced by the DeepSeek AI team. It is a vision-language model (VLM) designed to process both text and images for generating contextually relevant responses. The model leverages [LLaMA](./llama) as its text encoder, while [SigLip](./siglip) is used for encoding low-resolution images and [SAM (Segment Anything Model)](./sam) is incorporated to handle high-resolution image encoding, enhancing the model’s ability to process fine-grained visual details. Deepseek-VL-Hybrid is a variant of Deepseek-VL that uses [SAM (Segment Anything Model)](./sam) to handle high-resolution image encoding.\n+[Deepseek-VL-Hybrid](https://huggingface.co/papers/2403.05525) was introduced by the DeepSeek AI team. It is a vision-language model (VLM) designed to process both text and images for generating contextually relevant responses. The model leverages [LLaMA](./llama) as its text encoder, while [SigLip](./siglip) is used for encoding low-resolution images and [SAM (Segment Anything Model)](./sam) is incorporated to handle high-resolution image encoding, enhancing the model's ability to process fine-grained visual details. Deepseek-VL-Hybrid is a variant of Deepseek-VL that uses [SAM (Segment Anything Model)](./sam) to handle high-resolution image encoding.\n \n You can find all the original Deepseek-VL-Hybrid checkpoints under the [DeepSeek-community](https://huggingface.co/deepseek-community) organization.\n "
        },
        {
            "sha": "503291eeb5fb4cd21111bf117b8e1c9a09822c13",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -24,7 +24,7 @@ The [Vision Transformer](vit) (ViT) is a transformer encoder model (BERT-like) o\n \n Next, people figured out ways to make ViT work really well on self-supervised image feature extraction (i.e. learning meaningful features, also called embeddings) on images without requiring any labels. Some example papers here include [DINOv2](dinov2) and [MAE](vit_mae).\n \n-The authors of DINOv2 noticed that ViTs have artifacts in attention maps. It’s due to the model using some image patches as “registers”. The authors propose a fix: just add some new tokens (called \"register\" tokens), which you only use during pre-training (and throw away afterwards). This results in:\n+The authors of DINOv2 noticed that ViTs have artifacts in attention maps. It's due to the model using some image patches as “registers”. The authors propose a fix: just add some new tokens (called \"register\" tokens), which you only use during pre-training (and throw away afterwards). This results in:\n - no artifacts\n - interpretable attention maps\n - and improved performances."
        },
        {
            "sha": "ea86050505994dddc11cfa5a9aab2307d960ed10",
            "filename": "docs/source/en/model_doc/evolla.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fevolla.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -25,7 +25,7 @@ Evolla is an advanced 80-billion-parameter protein-language generative model des\n \n The abstract from the paper is the following:\n \n-*Proteins, nature’s intricate molecular machines, are the products of billions of years of evolution and play fundamental roles in sustaining life. Yet, deciphering their molecular language - that is, understanding how protein sequences and structures encode and determine biological functions - remains a corner-stone challenge in modern biology. Here, we introduce Evolla, an 80 billion frontier protein-language generative model designed to decode the molecular language of proteins. By integrating information from protein sequences, structures, and user queries, Evolla generates precise and contextually nuanced insights into protein function. A key innovation of Evolla lies in its training on an unprecedented AI-generated dataset: 546 million protein question-answer pairs and 150 billion word tokens, designed to reflect the immense complexity and functional diversity of proteins. Post-pretraining, Evolla integrates Direct Preference Optimization (DPO) to refine the model based on preference signals and Retrieval-Augmented Generation (RAG) for external knowledge incorporation, improving response quality and relevance. To evaluate its performance, we propose a novel framework, Instructional Response Space (IRS), demonstrating that Evolla delivers expert-level insights, advancing research in proteomics and functional genomics while shedding light on the molecular logic encoded in proteins. The online demo is available at http://www.chat-protein.com/.*\n+*Proteins, nature's intricate molecular machines, are the products of billions of years of evolution and play fundamental roles in sustaining life. Yet, deciphering their molecular language - that is, understanding how protein sequences and structures encode and determine biological functions - remains a corner-stone challenge in modern biology. Here, we introduce Evolla, an 80 billion frontier protein-language generative model designed to decode the molecular language of proteins. By integrating information from protein sequences, structures, and user queries, Evolla generates precise and contextually nuanced insights into protein function. A key innovation of Evolla lies in its training on an unprecedented AI-generated dataset: 546 million protein question-answer pairs and 150 billion word tokens, designed to reflect the immense complexity and functional diversity of proteins. Post-pretraining, Evolla integrates Direct Preference Optimization (DPO) to refine the model based on preference signals and Retrieval-Augmented Generation (RAG) for external knowledge incorporation, improving response quality and relevance. To evaluate its performance, we propose a novel framework, Instructional Response Space (IRS), demonstrating that Evolla delivers expert-level insights, advancing research in proteomics and functional genomics while shedding light on the molecular logic encoded in proteins. The online demo is available at http://www.chat-protein.com/.*\n \n Examples:\n "
        },
        {
            "sha": "05786d8096febf519ded91c46388e5d7c6bf000e",
            "filename": "docs/source/en/model_doc/glm4.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -21,12 +21,12 @@ rendered properly in your Markdown viewer.\n \n The GLM family welcomes new members [GLM-4-0414](https://huggingface.co/papers/2406.12793) series models.\n \n-The **GLM-4-32B-0414** series models, featuring 32 billion parameters. Its performance is comparable to OpenAI’s GPT\n-series and DeepSeek’s V3/R1 series. It also supports very user-friendly local deployment features. GLM-4-32B-Base-0414\n+The **GLM-4-32B-0414** series models, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT\n+series and DeepSeek's V3/R1 series. It also supports very user-friendly local deployment features. GLM-4-32B-Base-0414\n was pre-trained on 15T of high-quality data, including substantial reasoning-type synthetic data. This lays the\n foundation for subsequent reinforcement learning extensions. In the post-training stage, we employed human preference\n alignment for dialogue scenarios. Additionally, using techniques like rejection sampling and reinforcement learning, we\n-enhanced the model’s performance in instruction following, engineering code, and function calling, thus strengthening\n+enhanced the model's performance in instruction following, engineering code, and function calling, thus strengthening\n the atomic capabilities required for agent tasks. GLM-4-32B-0414 achieves good results in engineering code, Artifact\n generation, function calling, search-based Q&A, and report generation. In particular, on several benchmarks, such as\n code generation or specific Q&A tasks, GLM-4-32B-Base-0414 achieves comparable performance with those larger models like"
        },
        {
            "sha": "cdd4aec7302a9653fcdfd4a10c7557b7de8b8bab",
            "filename": "docs/source/en/model_doc/kyutai_speech_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkyutai_speech_to_text.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n # Kyutai Speech-To-Text\n ## Overview\n \n-[Kyutai STT](https://kyutai.org/next/stt) is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai’s lab has released two model checkpoints:\n+[Kyutai STT](https://kyutai.org/next/stt) is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai's lab has released two model checkpoints:\n - [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\n - [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\n "
        },
        {
            "sha": "ea4020714513f2294732ca4d667dd078897ed9ec",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -54,14 +54,14 @@ The attributes can be obtained from model config, as `model.config.vision_config\n \n ### Formatting Prompts with Chat Templates  \n \n-Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor's `apply_chat_template` method.  \n \n **Important:**  \n - You must construct a conversation history — passing a plain string won't work.  \n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n \n-Here’s an example of how to structure your input.\n+Here's an example of how to structure your input.\n We will use [llava-hf/llava-1.5-7b-hf](https://huggingface.co/llava-hf/llava-1.5-7b-hf) and a conversation history of text and image. Each content field has to be a list of dicts, as follows:\n \n ```python"
        },
        {
            "sha": "cbbe1361d3be73e8f1a6c7fe0ca4eca416eeba2d",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -34,7 +34,7 @@ The introduction from the blog is the following:\n \n On January 30, 2024, we released LLaVA-NeXT, an open-source Large Multimodal Model (LMM) that has been trained exclusively on text-image data. With the proposed AnyRes technique, it boosts capabilities in reasoning, OCR, and world knowledge, demonstrating remarkable performance across a spectrum of image-based multimodal understanding tasks, and even exceeding Gemini-Pro on several image benchmarks, e.g. MMMU and MathVista.\n \n-**In today’s exploration, we delve into the performance of LLaVA-NeXT within the realm of video understanding tasks. We reveal that LLaVA-NeXT surprisingly has strong performance in understanding video content. The current version of LLaVA-NeXT for videos has several improvements:\n+**In today's exploration, we delve into the performance of LLaVA-NeXT within the realm of video understanding tasks. We reveal that LLaVA-NeXT surprisingly has strong performance in understanding video content. The current version of LLaVA-NeXT for videos has several improvements:\n \n - Zero-shot video representation capabilities with AnyRes: The AnyRes technique naturally represents a high-resolution image into multiple images that a pre-trained VIT is able to digest, and forms them into a concatenated sequence. This technique is naturally generalizable to represent videos (consisting of multiple frames), allowing the image-only-trained LLaVA-Next model to perform surprisingly well on video tasks. Notably, this is the first time that LMMs show strong zero-shot modality transfer ability.\n - Inference with length generalization improves on longer videos. The linear scaling technique enables length generalization, allowing LLaVA-NeXT to effectively handle long-video beyond the limitation of the \"max_token_length\" of the LLM.\n@@ -61,14 +61,14 @@ The attributes can be obtained from model config, as `model.config.vision_config\n \n ### Formatting Prompts with Chat Templates  \n \n-Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor’s `apply_chat_template` method.  \n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processor's `apply_chat_template` method.  \n \n **Important:**  \n - You must construct a conversation history — passing a plain string won't work.  \n - Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n - The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n \n-Here’s an example of how to structure your input. We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images.\n+Here's an example of how to structure your input. We will use [LLaVA-NeXT-Video-7B-hf](https://huggingface.co/llava-hf/LLaVA-NeXT-Video-7B-hf) and a conversation history of videos and images.\n \n ```python\n from transformers import LlavaNextVideoProcessor"
        },
        {
            "sha": "b791b4b2afe6456cd6c83b3bc662b281e857574b",
            "filename": "docs/source/en/model_doc/qdqbert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqdqbert.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -115,7 +115,7 @@ tensors. After setting up the tensor quantizers, one can use the following examp\n \n The goal of exporting to ONNX is to deploy inference by [TensorRT](https://developer.nvidia.com/tensorrt). Fake\n quantization will be broken into a pair of QuantizeLinear/DequantizeLinear ONNX ops. After setting static member of\n-TensorQuantizer to use Pytorch’s own fake quantization functions, fake quantized model can be exported to ONNX, follow\n+TensorQuantizer to use Pytorch's own fake quantization functions, fake quantized model can be exported to ONNX, follow\n the instructions in [torch.onnx](https://pytorch.org/docs/stable/onnx.html). Example:\n \n ```python"
        },
        {
            "sha": "583d84d5f11ae95282b7fa982e2b53f14b3d9262",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -29,7 +29,7 @@ The [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unifi\n \n The abstract from the technical report is the following:\n \n-*We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni’s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n+*We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n \n ## Notes\n "
        },
        {
            "sha": "253c883d1df9b85742966de6cf480280766b9300",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -29,7 +29,7 @@ The [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unifi\n \n The abstract from the technical report is the following:\n \n-*We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni’s streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n+*We present Qwen2.5-Omni, an end-to-end multimodal model designed to perceive diverse modalities, including text, images, audio, and video, while simultaneously generating text and natural speech responses in a streaming manner. To enable the streaming of multimodal information inputs, both audio and visual encoders utilize a block-wise processing approach. This strategy effectively decouples the handling of long sequences of multimodal data, assigning the perceptual responsibilities to the multimodal encoder and entrusting the modeling of extended sequences to a large language model. Such a division of labor enhances the fusion of different modalities via the shared attention mechanism. To synchronize the timestamps of video inputs with audio, we organized the audio and video sequentially in an interleaved manner and propose a novel position embedding approach, named TMRoPE (Time-aligned Multimodal RoPE). To concurrently generate text and speech while avoiding interference between the two modalities, we propose Thinker-Talker architecture. In this framework, Thinker functions as a large language model tasked with text generation, while Talker is a dual-track autoregressive model that directly utilizes the hidden representations from the Thinker to produce audio tokens as output. Both the Thinker and Talker models are designed to be trained and inferred in an end-to-end manner. For decoding audio tokens in a streaming manner, we introduce a sliding-window DiT that restricts the receptive field, aiming to reduce the initial package delay. Qwen2.5-Omni outperforms the similarly sized Qwen2-VL and Qwen2-Audio in both image and audio capabilities. Furthermore, Qwen2.5-Omni achieves state-of-the-art performance on multimodal benchmarks like Omni-Bench. Notably, Qwen2.5-Omni is the first open-source model to achieve a level of performance in end-to-end speech instruction following that is comparable to its capabilities with text inputs, as evidenced by benchmarks such as MMLU and GSM8K. As for speech generation, Qwen2.5-Omni's streaming Talker outperform most existing streaming and non-streaming alternatives in robustness and naturalness.*\n \n ## Notes\n "
        },
        {
            "sha": "c556e01ba13cfc6efd4b4e66fc6d1c991e4c526d",
            "filename": "docs/source/en/model_doc/reformer.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -41,8 +41,8 @@ found [here](https://github.com/google/trax/tree/master/trax/models/reformer).\n ## Usage tips\n \n - Reformer does **not** work with *torch.nn.DataParallel* due to a bug in PyTorch, see [issue #36035](https://github.com/pytorch/pytorch/issues/36035).\n-- Use Axial position encoding (see below for more details). It’s a mechanism to avoid having a huge positional encoding matrix (when the sequence length is very big) by factorizing it into smaller matrices.\n-- Replace traditional attention by LSH (local-sensitive hashing) attention (see below for more details). It’s a technique to avoid computing the full product query-key in the attention layers.\n+- Use Axial position encoding (see below for more details). It's a mechanism to avoid having a huge positional encoding matrix (when the sequence length is very big) by factorizing it into smaller matrices.\n+- Replace traditional attention by LSH (local-sensitive hashing) attention (see below for more details). It's a technique to avoid computing the full product query-key in the attention layers.\n - Avoid storing the intermediate results of each layer by using reversible transformer layers to obtain them during the backward pass (subtracting the residuals from the input of the next layer gives them back) or recomputing them for results inside a given layer (less efficient than storing them but saves memory).\n - Compute the feedforward operations by chunks and not on the whole batch.\n "
        },
        {
            "sha": "7b2111e62f2ccc43f3742d98abeb09243b672678",
            "filename": "docs/source/en/model_doc/seamless_m4t_v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fseamless_m4t_v2.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -35,7 +35,7 @@ SeamlessM4T-v2 enables multiple tasks without relying on separate models:\n \n The abstract from the paper is the following:\n \n-*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one’s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a science fiction concept into a real-world technology. Finally, contributions in this work—including models, code, and a watermark detector—are publicly released and accessible at the link below.*\n+*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a science fiction concept into a real-world technology. Finally, contributions in this work—including models, code, and a watermark detector—are publicly released and accessible at the link below.*\n \n ## Usage\n "
        },
        {
            "sha": "61400bac177b5324e0db7292c7bbdf91c4cde627",
            "filename": "docs/source/en/model_doc/smolvlm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -38,7 +38,7 @@ Videos should not be upsampled.\n If `do_resize` is set to `True`, the model resizes images so that the longest edge is 4*512 pixels by default.\n The default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 512}` is the default, but you can change it to a different value if needed.\n \n-Here’s how to control resizing and set a custom size:\n+Here's how to control resizing and set a custom size:\n \n ```python\n image_processor = SmolVLMImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 512}, max_image_size=512)"
        },
        {
            "sha": "23409b0898c371c96dbc7a3ef5a945bb5e13a224",
            "filename": "docs/source/en/model_doc/wav2vec2-bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-bert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-bert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwav2vec2-bert.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -31,7 +31,7 @@ The official results of the model can be found in Section 3.2.1 of the paper.\n \n The abstract from the paper is the following:\n \n-*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one’s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a science fiction concept into a real-world technology. Finally, contributions in this work—including models, code, and a watermark detector—are publicly released and accessible at the link below.*\n+*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a science fiction concept into a real-world technology. Finally, contributions in this work—including models, code, and a watermark detector—are publicly released and accessible at the link below.*\n \n This model was contributed by [ylacombe](https://huggingface.co/ylacombe). The original code can be found [here](https://github.com/facebookresearch/seamless_communication).\n "
        },
        {
            "sha": "9dceefdef8da4da7db323c4202c43e9909f4969b",
            "filename": "docs/source/en/model_memory_anatomy.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -258,7 +258,7 @@ needs to maintain multiple copies of inputs and outputs.\n \n For convolutions and linear layers there are 2x flops in the backward compared to the forward, which generally translates\n into ~2x slower (sometimes more, because sizes in the backward tend to be more awkward). Activations are usually\n-bandwidth-limited, and it’s typical for an activation to have to read more data in the backward than in the forward\n+bandwidth-limited, and it's typical for an activation to have to read more data in the backward than in the forward\n (e.g. activation forward reads once, writes once, activation backward reads twice, gradOutput and output of the forward,\n and writes once, gradInput).\n "
        },
        {
            "sha": "31474e1d3213f68de7398b057e50c4fea5ae05c1",
            "filename": "docs/source/en/quantization/bitnet.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitnet.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -41,7 +41,7 @@ model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\")\n \n ## Kernels\n \n-`@torch.compile` is used to unpack the weights and perform the forward pass. It’s very straightforward to implement and delivers significant speed improvements. Additional optimized kernels will be integrated in future versions.\n+`@torch.compile` is used to unpack the weights and perform the forward pass. It's very straightforward to implement and delivers significant speed improvements. Additional optimized kernels will be integrated in future versions.\n \n ## Resources\n "
        },
        {
            "sha": "4b4b3ba5fa3651745701de858edcc08d65acfd15",
            "filename": "docs/source/en/tasks/image_captioning.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -71,7 +71,7 @@ Many image captioning datasets contain multiple captions per image. In those cas\n \n </Tip>\n \n-Split the dataset’s train split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n+Split the dataset's train split into a train and test set with the [`~datasets.Dataset.train_test_split`] method:\n \n ```python\n ds = ds[\"train\"].train_test_split(test_size=0.1)"
        },
        {
            "sha": "2678792c5f3d42e8b55546632e6a7ff12159982e",
            "filename": "docs/source/en/tasks/prompting.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fprompting.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -80,7 +80,7 @@ This section covers a few prompting techniques.\n \n ### Few-shot prompting\n \n-Few-shot prompting improves accuracy and performance by including specific examples of what a model should generate given an input. The explicit examples give the model a better understanding of the task and the output format you’re looking for. Try experimenting with different numbers of examples (2, 4, 8, etc.) to see how it affects performance. The example below provides the model with 1 example (1-shot) of the output format (a date in MM/DD/YYYY format) it should return.\n+Few-shot prompting improves accuracy and performance by including specific examples of what a model should generate given an input. The explicit examples give the model a better understanding of the task and the output format you're looking for. Try experimenting with different numbers of examples (2, 4, 8, etc.) to see how it affects performance. The example below provides the model with 1 example (1-shot) of the output format (a date in MM/DD/YYYY format) it should return.\n \n ```python\n from transformers import pipeline"
        },
        {
            "sha": "e06283c9ceb12a1ba411ef75ed64e25a1d5a351a",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -241,7 +241,7 @@ As a final step, create a batch of examples using [`DefaultDataCollator`]:\n \n ## Train the model\n \n-You’re ready to start training your model now! Load ViLT with [`ViltForQuestionAnswering`]. Specify the number of labels\n+You're ready to start training your model now! Load ViLT with [`ViltForQuestionAnswering`]. Specify the number of labels\n along with the label mappings:\n \n ```py"
        },
        {
            "sha": "b5e79100317f0a7ed762d297e665a932464c5071",
            "filename": "docs/source/en/testing.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftesting.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -260,8 +260,7 @@ or `pytest.ini`/``tox.ini`` files:\n looponfailroots = transformers tests\n ```\n \n-This would lead to only looking for file changes in the respective directories, specified relatively to the ini-file’s\n-directory.\n+This would lead to only looking for file changes in the respective directories, specified relatively to the ini-file's directory.\n \n [pytest-watch](https://github.com/joeyespo/pytest-watch) is an alternative implementation of this functionality.\n \n@@ -852,7 +851,7 @@ Methods:\n \n -  A **xfail** means that you expect a test to fail for some reason. A common example is a test for a feature not yet\n   implemented, or a bug not yet fixed. When a test passes despite being expected to fail (marked with\n-  pytest.mark.xfail), it’s an xpass and will be reported in the test summary.\n+  pytest.mark.xfail), it's an xpass and will be reported in the test summary.\n \n One of the important differences between the two is that `skip` doesn't run the test, and `xfail` does. So if the\n code that's buggy causes some bad state that will affect other tests, do not use `xfail`.\n@@ -1285,7 +1284,7 @@ You can vote for this feature and see where it is at these CI-specific threads:\n \n ## DeepSpeed integration\n \n-For a PR that involves the DeepSpeed integration, keep in mind our CircleCI PR CI setup doesn't have GPUs. Tests requiring GPUs are run on a different CI nightly. This means if you get a passing CI report in your PR, it doesn’t mean the DeepSpeed tests pass.\n+For a PR that involves the DeepSpeed integration, keep in mind our CircleCI PR CI setup doesn't have GPUs. Tests requiring GPUs are run on a different CI nightly. This means if you get a passing CI report in your PR, it doesn't mean the DeepSpeed tests pass.\n \n To run DeepSpeed tests:\n "
        },
        {
            "sha": "ce5152c2a4a78c7948eef0f6943f93bff35497e0",
            "filename": "docs/source/en/transformers_as_backend.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/05fb90c96903cf0e3d8376316a23d7401969690b/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftransformers_as_backend.md?ref=05fb90c96903cf0e3d8376316a23d7401969690b",
            "patch": "@@ -26,7 +26,7 @@ This guide shows how to use Transformers' models as a backend to some popular in\n \n [vLLM](https://github.com/vllm-project/vllm) is a high-performance inference engine optimized for serving LLMs at scale. It supports many Transformers' models, including all decoder-only LLMs and several vision-language models (VLMs). VLMs currently support image inputs only, with video support planned.\n \n-vLLM automatically selects the best backend, and if a model isn’t natively supported, it falls back to the Transformers model. To explicitly use a Transformers' model, set `model_impl=\"transformers\"`.\n+vLLM automatically selects the best backend, and if a model isn't natively supported, it falls back to the Transformers model. To explicitly use a Transformers' model, set `model_impl=\"transformers\"`.\n \n ```python\n from vllm import LLM\n@@ -47,7 +47,7 @@ Refer to the [vLLM docs](https://docs.vllm.ai/en/latest/models/supported_models.\n \n [SGLang](https://github.com/InternLM/sglang) is a high-performance, OpenAI-compatible server and runtime designed for chat-based LLMs. It offers fast inference, role-based conversation handling, and support for custom pipelines, making it great for building real-world LLM apps.\n \n-SGLang automatically falls back to the Transformers backend if a model isn’t natively supported. To explicitly use a Transformers' model, set `impl=\"transformers\"`.\n+SGLang automatically falls back to the Transformers backend if a model isn't natively supported. To explicitly use a Transformers' model, set `impl=\"transformers\"`.\n \n ```python\n import sglang as sgl"
        }
    ],
    "stats": {
        "total": 101,
        "additions": 50,
        "deletions": 51
    }
}