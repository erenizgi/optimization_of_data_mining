{
    "author": "pstjohn",
    "message": "[ESM] Add flash-attention-2 backend for ESM-2 (#38023)\n\n* Add flash-attention-2 backend for ESM-2\n\nSigned-off-by: Peter St. John <pstjohn@nvidia.com>\n\n* update extended_attention_mask for fa2\n\nSigned-off-by: Peter St. John <pstjohn@nvidia.com>\n\n* add test_flash_attn_2_equivalence test\n\nSigned-off-by: Peter St. John <pstjohn@nvidia.com>\n\n---------\n\nSigned-off-by: Peter St. John <pstjohn@nvidia.com>",
    "sha": "d69945e5fced637e77cf7af5e4955cb897bc298c",
    "files": [
        {
            "sha": "da62ecbea2a450483074c3ee8f7a10ddf37e110d",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 137,
            "deletions": 7,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/d69945e5fced637e77cf7af5e4955cb897bc298c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d69945e5fced637e77cf7af5e4955cb897bc298c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=d69945e5fced637e77cf7af5e4955cb897bc298c",
            "patch": "@@ -1,5 +1,6 @@\n # coding=utf-8\n # Copyright 2022 Meta and The HuggingFace Inc. team. All rights reserved.\n+# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -30,10 +31,14 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n+from ...utils import auto_docstring, is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, logging\n from .configuration_esm import EsmConfig\n \n \n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -111,8 +116,8 @@ def forward(self, q: torch.Tensor, k: torch.Tensor) -> Tuple[torch.Tensor, torch\n         self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)\n \n         return (\n-            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached),\n-            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached),\n+            apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached).to(dtype=q.dtype),\n+            apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached).to(dtype=k.dtype),\n         )\n \n \n@@ -245,6 +250,8 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n class EsmSelfAttention(nn.Module):\n     def __init__(self, config, position_embedding_type=None):\n         super().__init__()\n+        self.config = config\n+\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n@@ -393,10 +400,128 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n+class EsmFlashAttention2(EsmSelfAttention):\n+    \"\"\"\n+    ESM flash attention module. This module inherits from `EsmSelfAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        # Flash attention doesn't support output_attentions or cross attention\n+        if output_attentions or head_mask is not None or encoder_hidden_states is not None:\n+            logger.warning_once(\n+                \"EsmFlashAttention2 does not support output_attentions, head_mask, or cross_attention. \"\n+                \"Falling back to the manual attention implementation. This warning can be removed using \"\n+                'the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+        key_layer = self.transpose_for_scores(self.key(hidden_states))\n+        value_layer = self.transpose_for_scores(self.value(hidden_states))\n+        if past_key_value is not None:\n+            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32.\n+        input_dtype = query_layer.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.query.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_layer = query_layer.to(target_dtype)\n+            key_layer = key_layer.to(target_dtype)\n+            value_layer = value_layer.to(target_dtype)\n+\n+        # Matt: Our BERT model (which this code was derived from) scales attention logits down by sqrt(head_dim).\n+        # ESM scales the query down by the same factor instead. Modulo numerical stability these are equivalent,\n+        # but not when rotary embeddings get involved. Therefore, we scale the query here to match the original\n+        # ESM code and fix rotary embeddings.\n+        query_layer = query_layer * self.attention_head_size**-0.5\n+\n+        if self.position_embedding_type == \"rotary\":\n+            query_layer, key_layer = self.rotary_embeddings(query_layer, key_layer)\n+        elif self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            raise ValueError(f\"ESM flash attention does not support {self.position_embedding_type} embeddings\")\n+\n+        # It would likely be faster to change self.transpose_for_scores to output the correct\n+        # dimensions for flash_attention_2, but that would also mean changing the rotary embedding\n+        # functions. Here we just permute the dimensions to match the expected input.\n+        attn_output = _flash_attention_forward(\n+            query_layer.permute(0, 2, 1, 3),\n+            key_layer.permute(0, 2, 1, 3),\n+            value_layer.permute(0, 2, 1, 3),\n+            attention_mask,\n+            query_length=q_len,\n+            is_causal=self.is_decoder,\n+            softmax_scale=1.0,\n+            dropout=self.dropout_prob if self.training else 0.0,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1)\n+\n+        outputs = (attn_output, None)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+\n+        return outputs\n+\n+\n+ESM_ATTENTION_CLASSES = {\n+    \"eager\": EsmSelfAttention,\n+    \"flash_attention_2\": EsmFlashAttention2,\n+}\n+\n+\n class EsmAttention(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.self = EsmSelfAttention(config)\n+        self.self = ESM_ATTENTION_CLASSES[config._attn_implementation](config)\n         self.output = EsmSelfOutput(config)\n         self.pruned_heads = set()\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -673,6 +798,7 @@ class EsmPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"esm\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"EsmLayer\", \"EsmFoldTriangularSelfAttentionBlock\", \"EsmEmbeddings\"]\n+    _supports_flash_attn_2 = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->EsmLMHead\n     def _init_weights(self, module):\n@@ -806,9 +932,13 @@ def forward(\n         if attention_mask is None:\n             attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            extended_attention_mask = attention_mask\n+\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]"
        },
        {
            "sha": "203aa9a69a390e08082939a071e780df3faabffa",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d69945e5fced637e77cf7af5e4955cb897bc298c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d69945e5fced637e77cf7af5e4955cb897bc298c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=d69945e5fced637e77cf7af5e4955cb897bc298c",
            "patch": "@@ -1980,6 +1980,7 @@ def distogram(coords, min_bin, max_bin, num_bins):\n )\n class EsmForProteinFolding(EsmPreTrainedModel):\n     _no_split_modules = [\"EsmFoldStructureModule\", \"EsmFoldTriangularSelfAttentionBlock\"]\n+    _supports_flash_attn_2 = False\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -2050,6 +2051,7 @@ def forward(\n         position_ids: Optional[torch.Tensor] = None,\n         masking_pattern: Optional[torch.Tensor] = None,\n         num_recycles: Optional[int] = None,\n+        output_hidden_states: Optional[bool] = False,\n     ) -> EsmForProteinFoldingOutput:\n         r\"\"\"\n         masking_pattern (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "18887bb5927cd887b5c73a17175a2317120fb9c9",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 49,
            "deletions": 1,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/d69945e5fced637e77cf7af5e4955cb897bc298c/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d69945e5fced637e77cf7af5e4955cb897bc298c/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=d69945e5fced637e77cf7af5e4955cb897bc298c",
            "patch": "@@ -13,10 +13,22 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch ESM model.\"\"\"\n \n+import tempfile\n import unittest\n \n+import pytest\n+\n from transformers import EsmConfig, is_torch_available\n-from transformers.testing_utils import TestCasePlus, require_bitsandbytes, require_torch, slow, torch_device\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    is_flaky,\n+    require_bitsandbytes,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n@@ -59,6 +71,7 @@ def __init__(\n         num_labels=3,\n         num_choices=4,\n         scope=None,\n+        position_embedding_type=\"rotary\",\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -82,6 +95,7 @@ def __init__(\n         self.num_labels = num_labels\n         self.num_choices = num_choices\n         self.scope = scope\n+        self.position_embedding_type = position_embedding_type\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n@@ -116,6 +130,7 @@ def get_config(self):\n             max_position_embeddings=self.max_position_embeddings,\n             type_vocab_size=self.type_vocab_size,\n             initializer_range=self.initializer_range,\n+            position_embedding_type=self.position_embedding_type,\n         )\n \n     def create_and_check_model(self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels):\n@@ -296,6 +311,39 @@ def test_resize_embeddings_untied(self):\n     def test_resize_tokens_embeddings(self):\n         pass\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @is_flaky()\n+    @slow\n+    def test_flash_attn_2_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(reason=\"Model does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"eager\")\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model_class.main_input_name]\n+                dummy_input = dummy_input.to(torch_device)\n+                outputs = model(dummy_input, output_hidden_states=True)\n+                outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+\n+                logits = outputs.hidden_states[-1]\n+                logits_fa = outputs_fa.hidden_states[-1]\n+\n+                torch.testing.assert_close(logits_fa, logits, atol=1e-2, rtol=1e-3)\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 196,
        "additions": 188,
        "deletions": 8
    }
}