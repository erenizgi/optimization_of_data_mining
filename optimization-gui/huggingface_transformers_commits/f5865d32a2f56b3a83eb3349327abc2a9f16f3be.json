{
    "author": "MekkCyber",
    "message": "Restrict & Explain tp_plan for FBgemm (#37404)\n\n* explain tp_plan\n\n* add llama4 check\n\n* add clarification",
    "sha": "f5865d32a2f56b3a83eb3349327abc2a9f16f3be",
    "files": [
        {
            "sha": "7015d73be1487f4a244050f2d2be0b114ddd2ab8",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 44,
            "deletions": 33,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5865d32a2f56b3a83eb3349327abc2a9f16f3be/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5865d32a2f56b3a83eb3349327abc2a9f16f3be/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=f5865d32a2f56b3a83eb3349327abc2a9f16f3be",
            "patch": "@@ -242,39 +242,50 @@ def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> Li\n         return [k for k in missing_keys if k not in not_missing_keys]\n \n     def update_tp_plan(self, config):\n-        text_plan = {\n-            \"layers.*.self_attn.q_proj.weight\": \"local_colwise\",\n-            \"layers.*.self_attn.q_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.self_attn.k_proj.weight\": \"local_colwise\",\n-            \"layers.*.self_attn.k_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.self_attn.v_proj.weight\": \"local_colwise\",\n-            \"layers.*.self_attn.v_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.self_attn.o_proj.weight\": \"local_rowwise\",\n-            \"layers.*.self_attn\": \"gather\",\n-            \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n-            \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n-            \"norm.weight\": \"sequence_parallel\",\n-            \"layers.*.feed_forward.shared_expert.gate_proj.weight\": \"local_colwise\",\n-            \"layers.*.feed_forward.shared_expert.gate_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.feed_forward.shared_expert.up_proj.weight\": \"local_colwise\",\n-            \"layers.*.feed_forward.shared_expert.up_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.feed_forward.shared_expert.down_proj.weight\": \"local_rowwise\",\n-            \"layers.*.feed_forward.experts\": \"local\",\n-            \"layers.*.feed_forward\": \"gather\",\n-            \"layers.*.feed_forward.experts.*.gate_proj.weight\": \"local_colwise\",\n-            \"layers.*.feed_forward.experts.*.gate_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.feed_forward.experts.*.up_proj.weight\": \"local_colwise\",\n-            \"layers.*.feed_forward.experts.*.up_proj.weight_scale\": \"local_colwise\",\n-            \"layers.*.feed_forward.experts.*.down_proj.weight\": \"local_rowwise\",\n-            # For Fused implementation\n-            \"layers.*.feed_forward.experts.gate_up_proj\": \"local_packed_rowwise\",\n-            \"layers.*.feed_forward.experts.gate_up_proj_scale\": \"local_packed_rowwise\",\n-            \"layers.*.feed_forward.experts.down_proj\": \"local_colwise\",\n-        }\n-        if config.get_text_config() is not None:\n-            config.get_text_config().base_model_tp_plan = text_plan\n-        else:\n-            config.base_model_tp_plan = text_plan\n+        if \"Llama4\" in config.__class__.__name__:\n+            text_plan = {\n+                # We are using a different tp plan with local_colwise and local_rowwise for the attention because fbgemm operations cannot be parallelized\n+                # With local_colwise and local_rowwise, all the operations are done locally, and we add a gather operation to gather the results instead of\n+                # using dtensors\n+                \"layers.*.self_attn.q_proj.weight\": \"local_colwise\",\n+                \"layers.*.self_attn.q_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.self_attn.k_proj.weight\": \"local_colwise\",\n+                \"layers.*.self_attn.k_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.self_attn.v_proj.weight\": \"local_colwise\",\n+                \"layers.*.self_attn.v_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.self_attn.o_proj.weight\": \"local_rowwise\",\n+                \"layers.*.self_attn\": \"gather\",\n+                # We keep the same sequence_parallel plan for layernorms\n+                \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n+                \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n+                \"norm.weight\": \"sequence_parallel\",\n+                # We keep the same local_colwise and local_rowwise plan for the feed forward shared expert\n+                # We also add scales for the shared expert, for local_colwise the scale is also local_colwise\n+                # For local_rowwise the scale is replicated, so we don't need to add it\n+                \"layers.*.feed_forward.shared_expert.gate_proj.weight\": \"local_colwise\",\n+                \"layers.*.feed_forward.shared_expert.gate_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.feed_forward.shared_expert.up_proj.weight\": \"local_colwise\",\n+                \"layers.*.feed_forward.shared_expert.up_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.feed_forward.shared_expert.down_proj.weight\": \"local_rowwise\",\n+                \"layers.*.feed_forward.experts\": \"local\",\n+                \"layers.*.feed_forward\": \"gather\",\n+                \"layers.*.feed_forward.experts.*.gate_proj.weight\": \"local_colwise\",\n+                \"layers.*.feed_forward.experts.*.gate_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.feed_forward.experts.*.up_proj.weight\": \"local_colwise\",\n+                \"layers.*.feed_forward.experts.*.up_proj.weight_scale\": \"local_colwise\",\n+                \"layers.*.feed_forward.experts.*.down_proj.weight\": \"local_rowwise\",\n+                # For Fused implementation we use local_packed_rowwise for the gate_up_proj, and the same for the packed scales\n+                # We use local_colwise for the down_proj, and the scales are replicated so we don't add them\n+                \"layers.*.feed_forward.experts.gate_up_proj\": \"local_packed_rowwise\",\n+                \"layers.*.feed_forward.experts.gate_up_proj_scale\": \"local_packed_rowwise\",\n+                \"layers.*.feed_forward.experts.down_proj\": \"local_colwise\",\n+            }\n+            if config.get_text_config() is not None:\n+                config.get_text_config().base_model_tp_plan = text_plan\n+            else:\n+                config.base_model_tp_plan = text_plan\n+            return config\n+\n         return config\n \n     def is_serializable(self, safe_serialization=None):"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 44,
        "deletions": 33
    }
}