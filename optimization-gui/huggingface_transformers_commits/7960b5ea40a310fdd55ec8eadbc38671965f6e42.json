{
    "author": "patrickvonplaten",
    "message": "[Devstral] Make sure FP8 conversion works correctly (#42715)\n\n* add first generation tutorial\n\n* WIP\n\n* WIP\n\n* WIP\n\n* WIP\n\n* WIP\n\n* uP-\n\n* uP-\n\n* WIP",
    "sha": "7960b5ea40a310fdd55ec8eadbc38671965f6e42",
    "files": [
        {
            "sha": "baf6787204dd3c6f2b4a491c5683adcd3ef849c0",
            "filename": "src/transformers/models/ministral3/convert_ministral3_weights_to_hf.py",
            "status": "modified",
            "additions": 74,
            "deletions": 60,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/7960b5ea40a310fdd55ec8eadbc38671965f6e42/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7960b5ea40a310fdd55ec8eadbc38671965f6e42/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py?ref=7960b5ea40a310fdd55ec8eadbc38671965f6e42",
            "patch": "@@ -35,46 +35,48 @@\n \n \n # fmt: off\n-STATE_DICT_MAPPING = {\n-    # Text model keys\n-    r\"^output.weight\":                            r\"lm_head.weight\",\n-    r\"^norm.weight\":                              r\"model.language_model.norm.weight\",\n-    r\"^tok_embeddings.weight\":                    r\"model.language_model.embed_tokens.weight\",\n-    r\"^layers.(\\d+).attention_norm.weight\":       r\"model.language_model.layers.\\1.input_layernorm.weight\",\n-    r\"^layers.(\\d+).ffn_norm.weight\":             r\"model.language_model.layers.\\1.post_attention_layernorm.weight\",\n-    r\"^layers.(\\d+).attention.w(q|k|v|o).weight\": r\"model.language_model.layers.\\1.self_attn.\\2_proj.weight\",\n-    r\"^layers.(\\d+).feed_forward.w1.weight\":      r\"model.language_model.layers.\\1.mlp.gate_proj.weight\",\n-    r\"^layers.(\\d+).feed_forward.w2.weight\":      r\"model.language_model.layers.\\1.mlp.down_proj.weight\",\n-    r\"^layers.(\\d+).feed_forward.w3.weight\":      r\"model.language_model.layers.\\1.mlp.up_proj.weight\",\n-    r\"^layers.(\\d+).attention.w(q|k|v|o).qscale_act\": r\"model.language_model.layers.\\1.self_attn.\\2_proj.activation_scale\",\n-    r\"^layers.(\\d+).feed_forward.w1.qscale_act\":      r\"model.language_model.layers.\\1.mlp.gate_proj.activation_scale\",\n-    r\"^layers.(\\d+).feed_forward.w2.qscale_act\":      r\"model.language_model.layers.\\1.mlp.down_proj.activation_scale\",\n-    r\"^layers.(\\d+).feed_forward.w3.qscale_act\":      r\"model.language_model.layers.\\1.mlp.up_proj.activation_scale\",\n-    r\"^layers.(\\d+).attention.w(q|k|v|o).qscale_weight\": r\"model.language_model.layers.\\1.self_attn.\\2_proj.weight_scale_inv\",\n-    r\"^layers.(\\d+).feed_forward.w1.qscale_weight\":      r\"model.language_model.layers.\\1.mlp.gate_proj.weight_scale_inv\",\n-    r\"^layers.(\\d+).feed_forward.w2.qscale_weight\":      r\"model.language_model.layers.\\1.mlp.down_proj.weight_scale_inv\",\n-    r\"^layers.(\\d+).feed_forward.w3.qscale_weight\":      r\"model.language_model.layers.\\1.mlp.up_proj.weight_scale_inv\",\n-\n-    # Vision model keys\n-    r\"vision_encoder.transformer.layers.(\\d+).attention_norm.weight\": r\"model.vision_tower.transformer.layers.\\1.attention_norm.weight\",\n-    r\"^vision_encoder.transformer.layers.(\\d+).ffn_norm.weight\": r\"model.vision_tower.transformer.layers.\\1.ffn_norm.weight\",\n-    r\"^vision_encoder.transformer.layers.(\\d+).attention.w(q|k|v|o).weight\": r\"model.vision_tower.transformer.layers.\\1.attention.\\2_proj.weight\",\n-    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w1.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.gate_proj.weight\",\n-    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w2.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.down_proj.weight\",\n-    r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w3.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.up_proj.weight\",\n-    r\"^vision_language_adapter.w_in\": r\"model.multi_modal_projector.linear_1\",\n-    r\"^vision_language_adapter.w_out\": r\"model.multi_modal_projector.linear_2\",\n-    r\"^vision_encoder.ln_pre.weight\": r\"model.vision_tower.ln_pre.weight\",\n-    r\"^vision_encoder.patch_conv.weight\": r\"model.vision_tower.patch_conv.weight\",\n-    r\"^patch_merger.merging_layer.weight\": r\"model.multi_modal_projector.patch_merger.merging_layer.weight\",\n-    r\"^pre_mm_projector_norm.weight\": r\"model.multi_modal_projector.norm.weight\",\n-}\n+def get_sd_mapping(has_vision: bool) -> dict:\n+    model_key = \"model.language_model\" if has_vision else \"model\"\n+    return {\n+        # Text model keys\n+        r\"^output.weight\":                            r\"lm_head.weight\",\n+        r\"^norm.weight\":                              rf\"{model_key}.norm.weight\",\n+        r\"^tok_embeddings.weight\":                    rf\"{model_key}.embed_tokens.weight\",\n+        r\"^layers.(\\d+).attention_norm.weight\":       rf\"{model_key}.layers.\\1.input_layernorm.weight\",\n+        r\"^layers.(\\d+).ffn_norm.weight\":             rf\"{model_key}.layers.\\1.post_attention_layernorm.weight\",\n+        r\"^layers.(\\d+).attention.w(q|k|v|o).weight\": rf\"{model_key}.layers.\\1.self_attn.\\2_proj.weight\",\n+        r\"^layers.(\\d+).feed_forward.w1.weight\":      rf\"{model_key}.layers.\\1.mlp.gate_proj.weight\",\n+        r\"^layers.(\\d+).feed_forward.w2.weight\":      rf\"{model_key}.layers.\\1.mlp.down_proj.weight\",\n+        r\"^layers.(\\d+).feed_forward.w3.weight\":      rf\"{model_key}.layers.\\1.mlp.up_proj.weight\",\n+        r\"^layers.(\\d+).attention.w(q|k|v|o).qscale_act\": rf\"{model_key}.layers.\\1.self_attn.\\2_proj.activation_scale\",\n+        r\"^layers.(\\d+).feed_forward.w1.qscale_act\":      rf\"{model_key}.layers.\\1.mlp.gate_proj.activation_scale\",\n+        r\"^layers.(\\d+).feed_forward.w2.qscale_act\":      rf\"{model_key}.layers.\\1.mlp.down_proj.activation_scale\",\n+        r\"^layers.(\\d+).feed_forward.w3.qscale_act\":      rf\"{model_key}.layers.\\1.mlp.up_proj.activation_scale\",\n+        r\"^layers.(\\d+).attention.w(q|k|v|o).qscale_weight\": rf\"{model_key}.layers.\\1.self_attn.\\2_proj.weight_scale_inv\",\n+        r\"^layers.(\\d+).feed_forward.w1.qscale_weight\":      rf\"{model_key}.layers.\\1.mlp.gate_proj.weight_scale_inv\",\n+        r\"^layers.(\\d+).feed_forward.w2.qscale_weight\":      rf\"{model_key}.layers.\\1.mlp.down_proj.weight_scale_inv\",\n+        r\"^layers.(\\d+).feed_forward.w3.qscale_weight\":      rf\"{model_key}.layers.\\1.mlp.up_proj.weight_scale_inv\",\n+\n+        # Vision model keys\n+        r\"vision_encoder.transformer.layers.(\\d+).attention_norm.weight\": r\"model.vision_tower.transformer.layers.\\1.attention_norm.weight\",\n+        r\"^vision_encoder.transformer.layers.(\\d+).ffn_norm.weight\": r\"model.vision_tower.transformer.layers.\\1.ffn_norm.weight\",\n+        r\"^vision_encoder.transformer.layers.(\\d+).attention.w(q|k|v|o).weight\": r\"model.vision_tower.transformer.layers.\\1.attention.\\2_proj.weight\",\n+        r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w1.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.gate_proj.weight\",\n+        r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w2.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.down_proj.weight\",\n+        r\"^vision_encoder.transformer.layers.(\\d+).feed_forward.w3.weight\": r\"model.vision_tower.transformer.layers.\\1.feed_forward.up_proj.weight\",\n+        r\"^vision_language_adapter.w_in\": r\"model.multi_modal_projector.linear_1\",\n+        r\"^vision_language_adapter.w_out\": r\"model.multi_modal_projector.linear_2\",\n+        r\"^vision_encoder.ln_pre.weight\": r\"model.vision_tower.ln_pre.weight\",\n+        r\"^vision_encoder.patch_conv.weight\": r\"model.vision_tower.patch_conv.weight\",\n+        r\"^patch_merger.merging_layer.weight\": r\"model.multi_modal_projector.patch_merger.merging_layer.weight\",\n+        r\"^pre_mm_projector_norm.weight\": r\"model.multi_modal_projector.norm.weight\",\n+    }\n # fmt: on\n \n \n-def map_old_key_to_new(old_key):\n+def map_old_key_to_new(old_key, mapping):\n     \"\"\"Map of a key of the original state dict to the equivalent key in HF format\"\"\"\n-    for pattern, replacement in STATE_DICT_MAPPING.items():\n+    for pattern, replacement in mapping.items():\n         new_key, n_replace = re.subn(pattern, replacement, old_key)\n         # Early exit of the loop\n         if n_replace > 0:\n@@ -100,11 +102,13 @@ def convert_state_dict(original_state_dict: dict, config: Mistral3Config):\n     \"\"\"Convert a state dict file, when a single `nn.Module` is never sharded in different files (usual case).\"\"\"\n     new_dict = {}\n \n+    is_vision = isinstance(config, Mistral3Config)\n+    mapping = get_sd_mapping(is_vision)\n     for old_key, tensor in original_state_dict.items():\n         if \"fake_quantizer\" in old_key:\n             continue\n \n-        new_key = map_old_key_to_new(old_key)\n+        new_key = map_old_key_to_new(old_key, mapping)\n \n         if \"vision\" in old_key:\n             num_attention_heads = config.vision_config.num_attention_heads\n@@ -114,10 +118,11 @@ def convert_state_dict(original_state_dict: dict, config: Mistral3Config):\n             key_value_dim = head_dim * num_attention_heads\n             query_dim = head_dim * num_attention_heads\n         else:\n-            num_attention_heads = config.text_config.num_attention_heads\n-            hidden_size = config.text_config.hidden_size\n-            head_dim = config.text_config.head_dim\n-            num_key_value_heads = config.text_config.num_key_value_heads\n+            text_config = config.text_config if is_vision else config\n+            num_attention_heads = text_config.num_attention_heads\n+            hidden_size = text_config.hidden_size\n+            head_dim = text_config.head_dim\n+            num_key_value_heads = text_config.num_key_value_heads\n             key_value_dim = head_dim * num_key_value_heads\n             query_dim = head_dim * num_attention_heads\n \n@@ -130,8 +135,11 @@ def convert_state_dict(original_state_dict: dict, config: Mistral3Config):\n     return new_dict\n \n \n-def convert_config(original_config: dict, max_position_embeddings: int = 262144):\n+def convert_config(original_config: dict, max_position_embeddings: int = 262144, is_vision: bool = True):\n     original_vision_config = original_config.pop(\"vision_encoder\", None)\n+    assert is_vision == (original_vision_config is not None), (\n+        f\"is_vision={is_vision} but original_vision_config={original_vision_config}\"\n+    )\n     original_text_config = original_config\n \n     # Text config\n@@ -159,9 +167,9 @@ def convert_config(original_config: dict, max_position_embeddings: int = 262144)\n         \"original_max_position_embeddings\": original_config[\"yarn\"][\"original_max_position_embeddings\"],\n         \"beta_fast\": float(original_config[\"yarn\"][\"beta\"]),\n         \"beta_slow\": float(original_config[\"yarn\"][\"alpha\"]),\n-        \"mscale_all_dim\": 1.0,\n+        \"mscale_all_dim\": 1.0 if is_vision else 0.0,\n         \"mscale\": 1.0,\n-        \"llama_4_scaling_beta\": original_config[\"llama_4_scaling\"][\"beta\"],\n+        \"llama_4_scaling_beta\": original_config.get(\"llama_4_scaling\", {}).get(\"beta\", 0),\n     }\n \n     # These are not always defined depending on `params.json`\n@@ -173,11 +181,25 @@ def convert_config(original_config: dict, max_position_embeddings: int = 262144)\n     if new_text_config_kwargs[\"sliding_window\"] is not None:\n         new_text_config_kwargs[\"sliding_window\"] = int(new_text_config_kwargs[\"sliding_window\"])\n \n-    new_text_config = Ministral3Config(**new_text_config_kwargs)\n+    def get_maybe_quant_config() -> dict:\n+        kwargs = {}\n+        if original_config.get(\"quantization\", {}).get(\"qformat_weight\") == \"fp8_e4m3\":\n+            assert original_config[\"quantization\"][\"qscheme_act\"] == \"TENSOR\"\n+            quantization_config = {\n+                \"activation_scheme\": \"static\",\n+                \"modules_to_not_convert\": [\"model.vision_tower\", \"model.multi_modal_projector\", \"lm_head\"],\n+                \"quant_method\": \"fp8\",\n+                \"weight_block_size\": None,\n+            }\n+            kwargs[\"quantization_config\"] = AutoQuantizationConfig.from_dict(quantization_config)\n+        return kwargs\n \n     # No vision\n     if original_vision_config is None:\n+        new_text_config = Ministral3Config(**new_text_config_kwargs, **get_maybe_quant_config())\n         return new_text_config\n+    else:\n+        new_text_config = Ministral3Config(**new_text_config_kwargs)\n \n     # Vision config\n     new_vision_config = original_vision_config\n@@ -191,25 +213,14 @@ def convert_config(original_config: dict, max_position_embeddings: int = 262144)\n     _ = new_vision_config.pop(\"max_image_size\")\n     new_vision_config = PixtralVisionConfig(hidden_act=\"silu\", **new_vision_config)\n \n-    kwargs = {}\n-    if original_config.get(\"quantization\", {}).get(\"qformat_weight\") == \"fp8_e4m3\":\n-        assert original_config[\"quantization\"][\"qscheme_act\"] == \"TENSOR\"\n-        quantization_config = {\n-            \"activation_scheme\": \"static\",\n-            \"modules_to_not_convert\": [\"model.vision_tower\", \"model.multi_modal_projector\"],\n-            \"quant_method\": \"fp8\",\n-            \"weight_block_size\": None,\n-        }\n-        kwargs[\"quantization_config\"] = AutoQuantizationConfig.from_dict(quantization_config)\n-\n     new_config = Mistral3Config(\n         vision_config=new_vision_config,\n         text_config=new_text_config,\n         multimodal_projector_bias=adapter_bias,\n         image_token_id=image_token_id,\n         spatial_merge_size=spatial_merge_size,\n         vision_feature_layer=-1,\n-        **kwargs,\n+        **get_maybe_quant_config(),\n     )\n     return new_config\n \n@@ -218,7 +229,8 @@ def convert_and_write_model(input_dir: str, output_dir: str, max_position_embedd\n     \"\"\"Convert the model and save it (this implicitly save the config as well).\"\"\"\n     params = read_json(os.path.join(input_dir, \"params.json\"))\n \n-    config = convert_config(params, max_position_embeddings)\n+    is_vision = params.get(\"vision_encoder\") is not None\n+    config = convert_config(params, max_position_embeddings, is_vision)\n \n     full_state_dict = {}\n     # The model may be split between different files, but a single nn.Module is always fully present in a single file\n@@ -228,8 +240,10 @@ def convert_and_write_model(input_dir: str, output_dir: str, max_position_embedd\n         new_dict = convert_state_dict(original_state_dict, config)\n         full_state_dict.update(new_dict)\n \n-    if config.text_config.tie_word_embeddings:\n-        full_state_dict[\"lm_head.weight\"] = full_state_dict[\"model.language_model.embed_tokens.weight\"]\n+    text_config = config.text_config if is_vision else config\n+    if text_config.tie_word_embeddings:\n+        model_key = \"model.language_model\" if is_vision else \"model\"\n+        full_state_dict[\"lm_head.weight\"] = full_state_dict[f\"{model_key}.embed_tokens.weight\"]\n \n     # Load weights into model and resave them\n     with torch.device(\"meta\"):"
        }
    ],
    "stats": {
        "total": 134,
        "additions": 74,
        "deletions": 60
    }
}