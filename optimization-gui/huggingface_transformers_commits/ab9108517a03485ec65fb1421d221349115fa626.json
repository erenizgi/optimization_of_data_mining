{
    "author": "xhaktm00",
    "message": "ğŸŒ [i18n-KO] Translated `pipelines.md` to Korean (#39577)\n\n* docs: ko: pipelines.md\n\n* feat: gpt draft\n\n* Update docs/source/ko/main_classes/pipelines.md\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update docs/source/ko/main_classes/pipelines.md\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update docs/source/ko/main_classes/pipelines.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/ko/main_classes/pipelines.md\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update docs/source/ko/main_classes/pipelines.md\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update _toctree.yml\n\n* Update _toctree.yml\n\në²ˆì—­ ë¬¸ì„œ ìˆ˜ì •\n\n* Update pipelines.md\n\nToC ìˆ˜ì •\n\n* Update pipelines.md\n\n---------\n\nCo-authored-by: xhaktm <tnwjd318@hs.ac.kr>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "ab9108517a03485ec65fb1421d221349115fa626",
    "files": [
        {
            "sha": "6ad09c5ed6beeb36679ad143aa8b57bbfe200868",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab9108517a03485ec65fb1421d221349115fa626/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab9108517a03485ec65fb1421d221349115fa626/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=ab9108517a03485ec65fb1421d221349115fa626",
            "patch": "@@ -418,8 +418,8 @@\n       title: ëª¨ë¸ ì¶œë ¥\n     - local: main_classes/peft\n       title: PEFT\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Pipelines\n+    - local: main_classes/pipelines\n+      title: íŒŒì´í”„ë¼ì¸\n     - local: main_classes/processors\n       title: í”„ë¡œì„¸ì„œ\n     - local: main_classes/quantization"
        },
        {
            "sha": "70248fd51925645d53da5ed3edfb0519989088fd",
            "filename": "docs/source/ko/main_classes/pipelines.md",
            "status": "added",
            "additions": 469,
            "deletions": 0,
            "changes": 469,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab9108517a03485ec65fb1421d221349115fa626/docs%2Fsource%2Fko%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab9108517a03485ec65fb1421d221349115fa626/docs%2Fsource%2Fko%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fpipelines.md?ref=ab9108517a03485ec65fb1421d221349115fa626",
            "patch": "@@ -0,0 +1,469 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# íŒŒì´í”„ë¼ì¸ [[pipelines]]\n+\n+íŒŒì´í”„ë¼ì¸ì€ ëª¨ë¸ì„ ì¶”ë¡ ì— í™œìš©í•  ìˆ˜ ìˆëŠ” í›Œë¥­í•˜ê³  ì‰¬ìš´ ë°©ë²•ì…ë‹ˆë‹¤. ì´ íŒŒì´í”„ë¼ì¸ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ë³µì¡í•œ ì½”ë“œë¥¼ ëŒ€ë¶€ë¶„ ì¶”ìƒí™”í•˜ì—¬, ê°œì²´ëª… ì¸ì‹(Named Entity Recognition), ë§ˆìŠ¤í¬ë“œ ì–¸ì–´ ëª¨ë¸ë§(Masked Language Modeling), ê°ì • ë¶„ì„(Sentiment Analysis), íŠ¹ì„± ì¶”ì¶œ(Feature Extraction), ì§ˆì˜ì‘ë‹µ(Question Answering) ë“±ì˜ ì—¬ëŸ¬ ì‘ì—…ì— íŠ¹í™”ëœ ê°„ë‹¨í•œ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì‚¬ìš© ì˜ˆì‹œëŠ” [ì‘ì—… ìš”ì•½](../task_summary)ì„ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+íŒŒì´í”„ë¼ì¸ ì¶”ìƒí™”ëŠ” ë‹¤ìŒ ë‘ ê°€ì§€ ë²”ì£¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤.\n+\n+- \\[`íŒŒì´í”„ë¼ì¸`]ì€ ë‹¤ë¥¸ ëª¨ë“  íŒŒì´í”„ë¼ì¸ì„ ìº¡ìŠí™”í•˜ëŠ” ê°€ì¥ ê°•ë ¥í•œ ê°ì²´ì…ë‹ˆë‹¤.\n+- ì‘ì—…ë³„ íŒŒì´í”„ë¼ì¸ì€ [ì˜¤ë””ì˜¤](#audio), [ì»´í“¨í„° ë¹„ì „](#computer-vision), [ìì—°ì–´ ì²˜ë¦¬](#natural-language-processing), [ë©€í‹°ëª¨ë‹¬](#multimodal) ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## íŒŒì´í”„ë¼ì¸ ì¶”ìƒí™” [[the-pipeline-abstraction]]\n+\n+*íŒŒì´í”„ë¼ì¸* ì¶”ìƒí™”ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  íŒŒì´í”„ë¼ì¸ì„ ê°ì‹¸ëŠ” ë˜í¼ì…ë‹ˆë‹¤. ë‹¤ë¥¸ íŒŒì´í”„ë¼ì¸ì²˜ëŸ¼ ì¸ìŠ¤í„´ìŠ¤í™”ë˜ë©°, ì¶”ê°€ì ì¸ í¸ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n+\n+ë‹¨ì¼ í•­ëª© í˜¸ì¶œ ì˜ˆì‹œ:\n+\n+```python\n+>>> pipe = pipeline(\"text-classification\")\n+>>> pipe(\"This restaurant is awesome\")\n+[{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n+```\n+\n+[hub](https://huggingface.co)ì—ì„œ íŠ¹ì • ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš°, í•´ë‹¹ ëª¨ë¸ì´ ì´ë¯¸ í—ˆë¸Œì— ì‘ì—…ì„ ì •ì˜í•˜ê³  ìˆë‹¤ë©´ ì‘ì—…ëª…ì„ ìƒëµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```python\n+>>> pipe = pipeline(model=\"FacebookAI/roberta-large-mnli\")\n+>>> pipe(\"This restaurant is awesome\")\n+[{'label': 'NEUTRAL', 'score': 0.7313136458396912}]\n+```\n+\n+ì—¬ëŸ¬ í•­ëª©ì„ ì²˜ë¦¬í•˜ë ¤ë©´ *ë¦¬ìŠ¤íŠ¸*ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.\n+\n+```python\n+>>> pipe = pipeline(\"text-classification\")\n+>>> pipe([\"This restaurant is awesome\", \"This restaurant is awful\"] )\n+[{'label': 'POSITIVE', 'score': 0.9998743534088135},\n+ {'label': 'NEGATIVE', 'score': 0.9996669292449951}]\n+```\n+\n+ì „ì²´ ë°ì´í„°ì…‹ì„ ìˆœíšŒí•˜ë ¤ë©´ `dataset`ì„ ì§ì ‘ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+ì´ë ‡ê²Œ í•˜ë©´ ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ë©”ëª¨ë¦¬ì— ì˜¬ë¦´ í•„ìš”ë„ ì—†ê³ , ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ë”°ë¡œ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.\n+ì´ ë°©ì‹ì€ GPUì—ì„œ ì‚¬ìš©ì ì •ì˜ ë£¨í”„ì™€ ìœ ì‚¬í•œ ì†ë„ë¡œ ì‘ë™í•˜ë©°, ë§Œì•½ ê·¸ë ‡ì§€ ì•Šì„ ê²½ìš° ì´ìŠˆë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”.\n+\n+```python\n+import datasets\n+from transformers import pipeline\n+from transformers.pipelines.pt_utils import KeyDataset\n+from tqdm.auto import tqdm\n+\n+pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\n+dataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n+\n+# KeyDataset (*pt* ì „ìš©)ëŠ” ë°ì´í„°ì…‹ í•­ëª©ì˜ ë”•ì…”ë„ˆë¦¬ì—ì„œ ì§€ì •ëœ í‚¤ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤.\n+# ì´ ì˜ˆì œì—ì„œëŠ” *target* í•­ëª©ì´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ KeyDatasetì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¬¸ì¥ ìŒ ì…ë ¥ì—ëŠ” KeyPairDatasetì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+for out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n+    print(out)\n+    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n+    # {\"text\": ....}\n+    # ....\n+```\n+\n+ë” í¸ë¦¬í•˜ê²Œ ì‚¬ìš©í•˜ë ¤ë©´ ì œë„ˆë ˆì´í„°ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n+\n+```python\n+from transformers import pipeline\n+\n+pipe = pipeline(\"text-classification\")\n+\n+def data():\n+    while True:\n+        # ë°ì´í„°ëŠ” ë°ì´í„°ì…‹, ë°ì´í„°ë² ì´ìŠ¤, í ë˜ëŠ” HTTP ìš”ì²­ì—ì„œ ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+        # ì„œë²„ì—ì„œ\n+        # ì£¼ì˜: ë°˜ë³µì ì´ë¯€ë¡œ `num_workers > 1` ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n+        # ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì—¬ì „íˆ\n+        # ë©”ì¸ ìŠ¤ë ˆë“œê°€ ëŒ€ê·œëª¨ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ë™ì•ˆ í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œê°€ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+        yield \"This is a test\"\n+\n+for out in pipe(data()):\n+    print(out)\n+    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n+    # {\"text\": ....}\n+    # ....\n+```\n+\n+\\[\\[autodoc]] pipeline\n+\n+## íŒŒì´í”„ë¼ì¸ ë°°ì¹˜ ì²˜ë¦¬ [[pipeline-batching]]\n+\n+ëª¨ë“  íŒŒì´í”„ë¼ì¸ì€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸, `Dataset`, `Generator` ì „ë‹¬ ì‹œ ìŠ¤íŠ¸ë¦¬ë° ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ë•Œ ì‘ë™í•©ë‹ˆë‹¤.\n+\n+```python\n+from transformers import pipeline\n+from transformers.pipelines.pt_utils import KeyDataset\n+import datasets\n+\n+dataset = datasets.load_dataset(\"imdb\", name=\"plain_text\", split=\"unsupervised\")\n+pipe = pipeline(\"text-classification\", device=0)\n+for out in pipe(KeyDataset(dataset, \"text\"), batch_size=8, truncation=\"only_first\"):\n+    print(out)\n+    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]\n+    # ì´ì „ê³¼ ë™ì¼í•œ ì¶œë ¥ì´ì§€ë§Œ, ë‚´ìš©ì„ ë°°ì¹˜ë¡œ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n+```\n+\n+<Tip warning={true}>\n+\n+í•˜ì§€ë§Œ ë°°ì¹˜ ì²˜ë¦¬ê°€ í•­ìƒ ì„±ëŠ¥ í–¥ìƒì„ ë³´ì¥í•˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. í•˜ë“œì›¨ì–´, ë°ì´í„°, ëª¨ë¸ì— ë”°ë¼ ì†ë„ê°€ 10ë°°ë¡œ ë¹¨ë¼ì§ˆìˆ˜ë„, 5ë°° ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì£¼ë¡œ ì†ë„ í–¥ìƒì´ ìˆëŠ” ì˜ˆì‹œ:\n+\n+</Tip>\n+\n+```python\n+from transformers import pipeline\n+from torch.utils.data import Dataset\n+from tqdm.auto import tqdm\n+\n+pipe = pipeline(\"text-classification\", device=0)\n+\n+class MyDataset(Dataset):\n+    def __len__(self):\n+        return 5000\n+\n+    def __getitem__(self, i):\n+        return \"This is a test\"\n+\n+dataset = MyDataset()\n+\n+for batch_size in [1, 8, 64, 256]:\n+    print(\"-\" * 30)\n+    print(f\"Streaming batch_size={batch_size}\")\n+    for out in tqdm(pipe(dataset, batch_size=batch_size), total=len(dataset)):\n+        pass\n+```\n+\n+```\n+# On GTX 970\n+------------------------------\n+Streaming no batching\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:26<00:00, 187.52it/s]\n+------------------------------\n+Streaming batch_size=8\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:04<00:00, 1205.95it/s]\n+------------------------------\n+Streaming batch_size=64\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2478.24it/s]\n+------------------------------\n+Streaming batch_size=256\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2554.43it/s]\n+(diminishing returns, saturated the GPU)\n+```\n+\n+ì£¼ë¡œ ì†ë„ ì €í•˜ê°€ ìˆëŠ” ì˜ˆì‹œ:\n+\n+```python\n+class MyDataset(Dataset):\n+    def __len__(self):\n+        return 5000\n+\n+    def __getitem__(self, i):\n+        if i % 64 == 0:\n+            n = 100\n+        else:\n+            n = 1\n+        return \"This is a test\" * n\n+```\n+\n+ì´ëŠ” ë‹¤ë¥¸ ë¬¸ì¥ë“¤ì— ë¹„í•´ ê°„í—ì ìœ¼ë¡œ ë§¤ìš° ê¸´ ë¬¸ì¥ì´ í¬í•¨ëœ ê²½ìš°ì…ë‹ˆë‹¤. ì´ ê²½ìš° **ì „ì²´** ë°°ì¹˜ê°€ 400í† í° ê¸¸ì´ë¡œ  \n+([64, 400]) ë˜ì–´ì•¼ í•˜ë¯€ë¡œ, [64, 4] ëŒ€ì‹  [64, 400]ì´ ë˜ì–´ í¬ê²Œ ì†ë„ê°€ ì €í•˜ë©ë‹ˆë‹¤. ê²Œë‹¤ê°€, ë” í° ë°°ì¹˜ì—ì„œëŠ” í”„ë¡œê·¸ë¨ì´ ì¶©ëŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+\n+```\n+------------------------------\n+Streaming no batching\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:05<00:00, 183.69it/s]\n+------------------------------\n+Streaming batch_size=8\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:03<00:00, 265.74it/s]\n+------------------------------\n+Streaming batch_size=64\n+100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:26<00:00, 37.80it/s]\n+------------------------------\n+Streaming batch_size=256\n+  0%|                                                                                 | 0/1000 [00:00<?, ?it/s]\n+Traceback (most recent call last):\n+  File \"/home/nicolas/src/transformers/test.py\", line 42, in <module>\n+    for out in tqdm(pipe(dataset, batch_size=256), total=len(dataset)):\n+....\n+    q = q / math.sqrt(dim_per_head)  # (bs, n_heads, q_length, dim_per_head)\n+RuntimeError: CUDA out of memory. Tried to allocate 376.00 MiB (GPU 0; 3.95 GiB total capacity; 1.72 GiB already allocated; 354.88 MiB free; 2.46 GiB reserved in total by PyTorch)\n+```\n+\n+ì¼ë°˜ì ì¸ í•´ê²°ì±…ì€ ì—†ìœ¼ë©°, ì‚¬ìš© ì‚¬ë¡€ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì‚¬ìš©ìë¥¼ ìœ„í•œ ê²½í—˜ìƒ ì§€ì¹¨:\n+\n+- **í•˜ë“œì›¨ì–´ì™€ ì‹¤ì œ ì›Œí¬ë¡œë“œë¡œ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì„¸ìš”. ì¸¡ì •ì´ ë‹µì…ë‹ˆë‹¤.**\n+- ì‹¤ì‹œê°„ ì¶”ë¡ (latency)ì´ ì¤‘ìš”í•˜ë‹¤ë©´ ë°°ì¹˜ ì²˜ë¦¬í•˜ì§€ ë§ˆì„¸ìš”.\n+- CPU ì‚¬ìš© ì‹œì—ë„ ë°°ì¹˜ ì²˜ë¦¬í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+- GPUì—ì„œ ì •ì  ë°ì´í„° ì²˜ë¦¬(throughput)ê°€ ëª©ì ì´ë¼ë©´\n+\n+  - ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´(\"ì‹¤ì œ\" ë°ì´í„°)ë¥¼ ì˜ ëª¨ë¥´ëŠ” ê²½ìš°, ê¸°ë³¸ì ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬í•˜ì§€ ë§ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•˜ë©´ì„œ ì„ì‹œë¡œ ë°°ì¹˜ë¥¼ ì ìš©í•´ ë³´ê³ , ì‹¤íŒ¨ ì‹œ ì´ë¥¼ ë³µêµ¬í•  ìˆ˜ ìˆë„ë¡ OOM ê²€ì‚¬ ë¡œì§ì„ ì¶”ê°€í•˜ì„¸ìš”. (ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì œì–´í•˜ì§€ ì•Šìœ¼ë©´ ì–¸ì  ê°€ëŠ” ì‹¤íŒ¨í•˜ê²Œ ë©ë‹ˆë‹¤.)\n+  - ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì¼ì •í•˜ë‹¤ë©´ ë°°ì¹˜ ì²˜ë¦¬ê°€ ìœ ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸¡ì •í•˜ë©° OOMê¹Œì§€ ì‹œë„í•´ ë³´ì„¸ìš”.\n+  - GPU ë©”ëª¨ë¦¬ê°€ í´ìˆ˜ë¡ ë°°ì¹˜ ì²˜ë¦¬ì˜ ì´ì ì´ í½ë‹ˆë‹¤.\n+- ë°°ì¹˜ ì²˜ë¦¬ í™œì„±í™” ì‹œ OOMì„ í•¸ë“¤ë§í•  ìˆ˜ ìˆë„ë¡ ëŒ€ë¹„í•˜ì„¸ìš”.\n+\n+## íŒŒì´í”„ë¼ì¸ ì²­í¬ ë°°ì¹˜ ì²˜ë¦¬ [[pipeline-chunk-batching]]\n+\n+`ì œë¡œìƒ· ë¶„ë¥˜` ë° `ì§ˆì˜ì‘ë‹µ` íŒŒì´í”„ë¼ì¸ì€ ë‹¨ì¼ ì…ë ¥ì´ ì—¬ëŸ¬ í¬ì›Œë“œ íŒ¨ìŠ¤ë¥¼ ìœ ë°œí•  ìˆ˜ ìˆì–´ `ë°°ì¹˜ í¬ê¸°` ì¸ìë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë‘ íŒŒì´í”„ë¼ì¸ì€ `ì²­í¬ íŒŒì´í”„ë¼ì¸` í˜•íƒœë¡œ ë™ì‘í•©ë‹ˆë‹¤. ìš”ì•½í•˜ë©´\n+\n+```python\n+preprocessed = pipe.preprocess(inputs)\n+model_outputs = pipe.forward(preprocessed)\n+outputs = pipe.postprocess(model_outputs)\n+```\n+\n+ì´ì œ ë‚´ë¶€ì ìœ¼ë¡œëŠ”\n+\n+```python\n+all_model_outputs = []\n+for preprocessed in pipe.preprocess(inputs):\n+    model_outputs = pipe.forward(preprocessed)\n+    all_model_outputs.append(model_outputs)\n+outputs = pipe.postprocess(all_model_outputs)\n+```\n+\n+íŒŒì´í”„ë¼ì¸ì˜ ì‚¬ìš© ë°©ì‹ì´ ë™ì¼í•˜ë¯€ë¡œ, ì½”ë“œì—ëŠ” ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤.\n+\n+íŒŒì´í”„ë¼ì¸ì€ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜í–‰í•˜ê¸° ë•Œë¬¸ì— ì…ë ¥ì´ ëª‡ ë²ˆì˜ í¬ì›Œë“œ íŒ¨ìŠ¤ë¥¼ ë°œìƒì‹œí‚¤ëŠ”ì§€ ê³ ë ¤í•  í•„ìš” ì—†ì´, `ë°°ì¹˜ í¬ê¸°`ëŠ” ì…ë ¥ê³¼ ë¬´ê´€í•˜ê²Œ ìµœì í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+ë‹¤ë§Œ ì•ì„œ ì–¸ê¸‰í•œ ì£¼ì˜ì‚¬í•­ì€ ì—¬ì „íˆ ìœ íš¨í•©ë‹ˆë‹¤.\n+\n+## íŒŒì´í”„ë¼ì¸ FP16 ì¶”ë¡  [[pipeline-fp16-inference]]\n+\n+ëª¨ë¸ì€ FP16 ëª¨ë“œë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, GPUì—ì„œ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ë©´ì„œ ì²˜ë¦¬ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ì€ ì„±ëŠ¥ ì €í•˜ ì—†ì´ FP16ì„ ì§€ì›í•˜ë©°, ëª¨ë¸ì´ í´ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±ì€ ë” ë‚®ì•„ì§‘ë‹ˆë‹¤.\n+\n+FP16 ì¶”ë¡ ì„ í™œì„±í™”í•˜ë ¤ë©´ íŒŒì´í”„ë¼ì¸ ìƒì„±ìì— `torch_dtype=torch.float16` ë˜ëŠ” `torch_dtype='float16'`ì„ ì „ë‹¬í•˜ì„¸ìš”. ì´ ê¸°ëŠ¥ì€ íŒŒì´í† ì¹˜ ë°±ì—”ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì—ì„œë§Œ ì‘ë™í•˜ë©°, ì…ë ¥ì€ ë‚´ë¶€ì ìœ¼ë¡œ FP16 í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.\n+\n+## íŒŒì´í”„ë¼ì¸ ì‚¬ìš©ì ì •ì˜ ì½”ë“œ [[pipeline-custom-code]]\n+\n+íŠ¹ì • íŒŒì´í”„ë¼ì¸ì„ ì˜¤ë²„ë¼ì´ë“œí•˜ë ¤ë©´, ë¨¼ì € í•´ë‹¹ ì‘ì—…ì— ëŒ€í•œ ì´ìŠˆë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”. íŒŒì´í”„ë¼ì¸ì˜ ëª©í‘œëŠ” ëŒ€ë¶€ë¶„ì˜ ì‚¬ìš© ì‚¬ë¡€ë¥¼ ì§€ì›í•˜ëŠ” ê²ƒì´ë¯€ë¡œ, `transformers` íŒ€ì´ ì¶”ê°€ ì§€ì›ì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ê°„ë‹¨íˆ ì‹œë„í•˜ë ¤ë©´ íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì„¸ìš”.\n+\n+```python\n+class MyPipeline(TextClassificationPipeline):\n+    def postprocess():\n+        # ì‚¬ìš©ì ì •ì˜ í›„ì²˜ë¦¬ ì½”ë“œ ì‘ì„±\n+        scores = scores * 100\n+        # ì¶”ê°€ ì½”ë“œ ì‘ì„±\n+\n+my_pipeline = MyPipeline(model=model, tokenizer=tokenizer, ...)\n+# ë˜ëŠ” *pipeline* í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²½ìš°:\n+my_pipeline = pipeline(model=\"xxxx\", pipeline_class=MyPipeline)\n+```\n+\n+ì´ë¥¼ í†µí•´ ì›í•˜ëŠ” ëª¨ë“  ì»¤ìŠ¤í…€ ì½”ë“œë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## íŒŒì´í”„ë¼ì¸ êµ¬í˜„í•˜ê¸° [[implementing-a-pipeline]]\n+\n+[ìƒˆ íŒŒì´í”„ë¼ì¸ êµ¬í˜„](../add_new_pipeline)\n+\n+## ì˜¤ë””ì˜¤ [[audio]]\n+\n+ì˜¤ë””ì˜¤ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+### AudioClassificationPipeline [[transformers.AudioClassificationPipeline]]\n+\n+[[autodoc]] AudioClassificationPipeline\n+    - __call__\n+    - all\n+\n+### AutomaticSpeechRecognitionPipeline [[transformers.AutomaticSpeechRecognitionPipeline]]\n+\n+[[autodoc]] AutomaticSpeechRecognitionPipeline\n+    - __call__\n+    - all\n+\n+### TextToAudioPipeline [[transformers.TextToAudioPipeline]]\n+\n+[[autodoc]] TextToAudioPipeline\n+    - __call__\n+    - all\n+\n+\n+### ZeroShotAudioClassificationPipeline [[transformers.ZeroShotAudioClassificationPipeline]]\n+\n+[[autodoc]] ZeroShotAudioClassificationPipeline\n+    - __call__\n+    - all\n+\n+## ì»´í“¨í„° ë¹„ì „ [[computer-vision]]\n+\n+ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+### DepthEstimationPipeline [[transformers.DepthEstimationPipeline]]\n+[[autodoc]] DepthEstimationPipeline\n+    - __call__\n+    - all\n+\n+### ImageClassificationPipeline [[transformers.ImageClassificationPipeline]]\n+\n+[[autodoc]] ImageClassificationPipeline\n+    - __call__\n+    - all\n+\n+### ImageSegmentationPipeline [[transformers.ImageSegmentationPipeline]]\n+\n+[[autodoc]] ImageSegmentationPipeline\n+    - __call__\n+    - all\n+\n+### ImageToImagePipeline [[transformers.ImageToImagePipeline]]\n+\n+[[autodoc]] ImageToImagePipeline\n+    - __call__\n+    - all\n+\n+### ObjectDetectionPipeline [[transformers.ObjectDetectionPipeline]]\n+\n+[[autodoc]] ObjectDetectionPipeline\n+    - __call__\n+    - all\n+\n+### VideoClassificationPipeline [[transformers.VideoClassificationPipeline]]\n+\n+[[autodoc]] VideoClassificationPipeline\n+    - __call__\n+    - all\n+\n+### ZeroShotImageClassificationPipeline [[transformers.ZeroShotImageClassificationPipeline]]\n+\n+[[autodoc]] ZeroShotImageClassificationPipeline\n+    - __call__\n+    - all\n+\n+### ZeroShotObjectDetectionPipeline [[transformers.ZeroShotObjectDetectionPipeline]]\n+\n+[[autodoc]] ZeroShotObjectDetectionPipeline\n+    - __call__\n+    - all\n+\n+## ìì—°ì–´ ì²˜ë¦¬ [[natural-language-processing]]\n+\n+ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+### FillMaskPipeline [[transformers.FillMaskPipeline]]\n+\n+[[autodoc]] FillMaskPipeline\n+    - __call__\n+    - all\n+\n+### QuestionAnsweringPipeline [[transformers.QuestionAnsweringPipeline]]\n+\n+[[autodoc]] QuestionAnsweringPipeline\n+    - __call__\n+    - all\n+\n+### SummarizationPipeline [[transformers.SummarizationPipeline]]\n+\n+[[autodoc]] SummarizationPipeline\n+    - __call__\n+    - all\n+\n+### TableQuestionAnsweringPipeline [[transformers.TableQuestionAnsweringPipeline]]\n+\n+[[autodoc]] TableQuestionAnsweringPipeline\n+    - __call__\n+\n+### TextClassificationPipeline [[transformers.TextClassificationPipeline]]\n+\n+[[autodoc]] TextClassificationPipeline\n+    - __call__\n+    - all\n+\n+### TextGenerationPipeline [[transformers.TextGenerationPipeline]]\n+\n+[[autodoc]] TextGenerationPipeline\n+    - __call__\n+    - all\n+\n+### Text2TextGenerationPipeline [[transformers.Text2TextGenerationPipeline]]\n+\n+[[autodoc]] Text2TextGenerationPipeline\n+    - __call__\n+    - all\n+\n+### TokenClassificationPipeline [[transformers.TokenClassificationPipeline]]\n+\n+[[autodoc]] TokenClassificationPipeline\n+    - __call__\n+    - all\n+\n+### TranslationPipeline [[transformers.TranslationPipeline]]\n+\n+[[autodoc]] TranslationPipeline\n+    - __call__\n+    - all\n+\n+### ZeroShotClassificationPipeline [[transformers.ZeroShotClassificationPipeline]]\n+\n+[[autodoc]] ZeroShotClassificationPipeline\n+    - __call__\n+    - all\n+\n+## ë©€í‹°ëª¨ë‹¬ [[multimodal]]\n+\n+ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” íŒŒì´í”„ë¼ì¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+### DocumentQuestionAnsweringPipeline [[transformers.DocumentQuestionAnsweringPipeline]]\n+\n+[[autodoc]] DocumentQuestionAnsweringPipeline\n+    - __call__\n+    - all\n+\n+### FeatureExtractionPipeline [[transformers.FeatureExtractionPipeline]]\n+\n+[[autodoc]] FeatureExtractionPipeline\n+    - __call__\n+    - all\n+\n+### ImageFeatureExtractionPipeline [[transformers.ImageFeatureExtractionPipeline]]\n+\n+[[autodoc]] ImageFeatureExtractionPipeline\n+    - __call__\n+    - all\n+\n+### ImageToTextPipeline [[transformers.ImageToTextPipeline]]\n+\n+[[autodoc]] ImageToTextPipeline\n+    - __call__\n+    - all\n+\n+### ImageTextToTextPipeline [[transformers.ImageTextToTextPipeline]]\n+\n+[[autodoc]] ImageTextToTextPipeline\n+    - __call__\n+    - all\n+\n+### MaskGenerationPipeline [[transformers.MaskGenerationPipeline]]\n+\n+[[autodoc]] MaskGenerationPipeline\n+    - __call__\n+    - all\n+\n+### VisualQuestionAnsweringPipeline [[transformers.VisualQuestionAnsweringPipeline]]\n+\n+[[autodoc]] VisualQuestionAnsweringPipeline\n+    - __call__\n+    - all\n+\n+## Parent class: `Pipeline` [[transformers.Pipeline]]\n+\n+[[autodoc]] Pipeline"
        }
    ],
    "stats": {
        "total": 473,
        "additions": 471,
        "deletions": 2
    }
}