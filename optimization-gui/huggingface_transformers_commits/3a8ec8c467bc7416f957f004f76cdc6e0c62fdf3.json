{
    "author": "stevhliu",
    "message": "[docs] Attention mask image (#36970)\n\nadd image",
    "sha": "3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3",
    "files": [
        {
            "sha": "72c0c5d76af49b0ea3ce9ff1b6be675522c44a0e",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3",
            "patch": "@@ -164,6 +164,10 @@ visualizer = AttentionMaskVisualizer(\"google/gemma-3-4b-it\")\n visualizer(\"<img>What is shown in this image?\")\n ```\n \n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/gemma-3-attn-mask.png\"/>\n+</div>\n+\n ## Notes\n \n - Use [`Gemma3ForConditionalGeneration`] for image-and-text and image-only inputs."
        },
        {
            "sha": "7dc06608963a3ec3a4aa42bd6ce94b9f1fc164ad",
            "filename": "docs/source/en/model_doc/llama.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama.md?ref=3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3",
            "patch": "@@ -116,6 +116,10 @@ visualizer = AttentionMaskVisualizer(\"huggyllama/llama-7b\")\n visualizer(\"Plants create energy through a process known as\")\n ```\n \n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llama-attn-mask.png\"/>\n+</div>\n+\n ## Notes\n \n - The tokenizer is a byte-pair encoding model based on [SentencePiece](https://github.com/google/sentencepiece). During decoding, if the first token is the start of the word (for example, \"Banana\"), the tokenizer doesn't prepend the prefix space to the string."
        },
        {
            "sha": "ec981890b28414dbf85b9a07aade07b5fa4667ea",
            "filename": "docs/source/en/model_doc/llama2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama2.md?ref=3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3",
            "patch": "@@ -116,6 +116,10 @@ visualizer = AttentionMaskVisualizer(\"meta-llama/Llama-2-7b-hf\")\n visualizer(\"Plants create energy through a process known as\")\n ```\n \n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llama-2-attn-mask.png\"/>\n+</div>\n+\n ## Notes\n \n - Setting `config.pretraining_tp` to a value besides `1` activates a more accurate but slower computation of the linear layers. This matches the original logits better."
        },
        {
            "sha": "fa119a5f8362e6c1cc9d31673d674e7688824b55",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=3a8ec8c467bc7416f957f004f76cdc6e0c62fdf3",
            "patch": "@@ -125,6 +125,10 @@ visualizer = AttentionMaskVisualizer(\"google/paligemma2-3b-mix-224\")\n visualizer(\"<img> What is in this image?\")\n ```\n \n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/paligemma2-attn-mask.png\"/>\n+</div>\n+\n ## Notes\n \n - PaliGemma is not a conversational model and works best when fine-tuned for specific downstream tasks such as image captioning, visual question answering (VQA), object detection, and document understanding."
        }
    ],
    "stats": {
        "total": 16,
        "additions": 16,
        "deletions": 0
    }
}