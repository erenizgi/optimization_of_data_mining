{
    "author": "cyyever",
    "message": " Fix pylint generator warnings (#41258)\n\nFix pylint generator warnings\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
    "files": [
        {
            "sha": "7cb6caccefe11cd07c705905dd5c8a252ae61bf7",
            "filename": "examples/legacy/run_chinese_ref.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/examples%2Flegacy%2Frun_chinese_ref.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/examples%2Flegacy%2Frun_chinese_ref.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Frun_chinese_ref.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -55,7 +55,7 @@ def get_chinese_word(tokens: list[str]):\n def add_sub_symbol(bert_tokens: list[str], chinese_word_set: set()):\n     if not chinese_word_set:\n         return bert_tokens\n-    max_word_len = max([len(w) for w in chinese_word_set])\n+    max_word_len = max(len(w) for w in chinese_word_set)\n \n     bert_word = bert_tokens\n     start, end = 0, len(bert_word)"
        },
        {
            "sha": "a8e3c72de862f1c7d2633c876632c9fafc86b6e8",
            "filename": "examples/pytorch/question-answering/run_qa_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -950,7 +950,7 @@ def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n             all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n             all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n \n-    max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n+    max_len = max(x.shape[1] for x in all_start_logits)  # Get the max_length of the tensor\n \n     # concatenate the numpy array\n     start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n@@ -989,7 +989,7 @@ def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n                 all_start_logits.append(accelerator.gather_for_metrics(start_logits).cpu().numpy())\n                 all_end_logits.append(accelerator.gather_for_metrics(end_logits).cpu().numpy())\n \n-        max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n+        max_len = max(x.shape[1] for x in all_start_logits)  # Get the max_length of the tensor\n         # concatenate the numpy array\n         start_logits_concat = create_and_fill_np_array(all_start_logits, predict_dataset, max_len)\n         end_logits_concat = create_and_fill_np_array(all_end_logits, predict_dataset, max_len)"
        },
        {
            "sha": "5a013a49723d9165248a5fb93009439ad83ce3d0",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -249,7 +249,7 @@ def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_strings: Union[str,\n             token_list, token_indices, tokenizer\n         )\n \n-        self.maximum_token_len = max([len(stop_string) for stop_string in self.stop_strings])\n+        self.maximum_token_len = max(len(stop_string) for stop_string in self.stop_strings)\n         self.num_stop_strings = len(self.stop_strings)\n         self.target_lens = torch.tensor([len(stop_string) for stop_string in stop_strings], dtype=torch.int32)\n "
        },
        {
            "sha": "8a846d47210e066c9f12efe283daec293c7fcac8",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -4103,9 +4103,9 @@ def get_memory_footprint(self, return_buffers=True):\n                 are tensors that do not require gradients and not registered as parameters. E.g. mean and std in batch\n                 norm layers. Please see: https://discuss.pytorch.org/t/what-pytorch-means-by-buffers/120266/2\n         \"\"\"\n-        mem = sum([param.nelement() * param.element_size() for param in self.parameters()])\n+        mem = sum(param.nelement() * param.element_size() for param in self.parameters())\n         if return_buffers:\n-            mem_bufs = sum([buf.nelement() * buf.element_size() for buf in self.buffers()])\n+            mem_bufs = sum(buf.nelement() * buf.element_size() for buf in self.buffers())\n             mem = mem + mem_bufs\n         return mem\n "
        },
        {
            "sha": "71d9081f6c9a28bae97d0b1fb368f2230c6ca1ac",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -96,7 +96,7 @@ def __init__(self, config):\n     def forward(self, input_features):\n         # NOTE: in reference to the NOTE in __init__, right now it just calculates padding as if\n         # there will be just one conv layer.\n-        padding = sum([size // 2 for size in self.kernel_size])  # (7, 7) -> (3, 3)\n+        padding = sum(size // 2 for size in self.kernel_size)  # (7, 7) -> (3, 3)\n \n         input_features = torch.nn.functional.pad(input_features, (0, 0, padding, padding), \"constant\", 0)\n         hidden_states = input_features.transpose(1, 2).contiguous()  # -> Batch x Frame x Time"
        },
        {
            "sha": "ec1cb08abf4be88027b299b325991f955f9e915c",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -425,8 +425,8 @@ def _get_new_num_tokens_layer(self, new_num_tokens, layer):\n \n         new_num_tokens_layer = (\n             new_num_tokens\n-            - sum([emb.weight.shape[0] for emb in embeddings.emb_layers[:layer]])\n-            - sum([emb.weight.shape[0] for emb in embeddings.emb_layers[layer + 1 :]])\n+            - sum(emb.weight.shape[0] for emb in embeddings.emb_layers[:layer])\n+            - sum(emb.weight.shape[0] for emb in embeddings.emb_layers[layer + 1 :])\n         )\n         return new_num_tokens_layer, layer\n "
        },
        {
            "sha": "b9350d31a0197926ba96e04dacfdb7bc71adbfc6",
            "filename": "src/transformers/models/deprecated/tvlt/feature_extraction_tvlt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -202,7 +202,7 @@ def __call__(\n \n         # Create audio attention mask\n         max_patch_len = max(\n-            [ceil(feature.shape[0] / self.patch_size[0]) * self.freq_len for feature in audio_features]\n+            ceil(feature.shape[0] / self.patch_size[0]) * self.freq_len for feature in audio_features\n         )  # The maximum number of audio patches in a batch\n         if return_attention_mask:\n             audio_mask = ["
        },
        {
            "sha": "7a7ae16ebc9f56b68a1db0bbc81f77332145f61d",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -392,7 +392,7 @@ def preprocess(\n                     f\"number of frames must not be greater than the maximum frames of the model {self.num_frames}.\"\n                 )\n \n-        max_num_frames = max([len(video) for video in videos])\n+        max_num_frames = max(len(video) for video in videos)\n         num_patches_per_image = (size[\"shortest_edge\"] // patch_size[0]) ** 2\n         video_masks = np.array(\n             ["
        },
        {
            "sha": "b876a9de96bf7d5aeb0632d99705923edfecc6d7",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -266,8 +266,8 @@ def _pad_for_batching(\n         \"\"\"\n \n         max_shape = (\n-            max([size[0] for size in image_sizes]),\n-            max([size[1] for size in image_sizes]),\n+            max(size[0] for size in image_sizes),\n+            max(size[1] for size in image_sizes),\n         )\n         pixel_values = [\n             pad("
        },
        {
            "sha": "3b0dbe4d422eacaae7fa6afdf8a29249703f6591",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -628,7 +628,7 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n         \"\"\"\n         Computes the average number of target masks across the batch, for normalization purposes.\n         \"\"\"\n-        num_masks = sum([len(classes) for classes in class_labels])\n+        num_masks = sum(len(classes) for classes in class_labels)\n         num_masks = torch.as_tensor(num_masks, dtype=torch.float, device=device)\n         world_size = 1\n         if is_accelerate_available():"
        },
        {
            "sha": "e9701ca07114fb6cea95ad331d062243656bd35f",
            "filename": "src/transformers/models/esm/openfold_utils/protein.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fprotein.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fprotein.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fopenfold_utils%2Fprotein.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -159,7 +159,7 @@ def add_pdb_headers(prot: Protein, pdb_str: str) -> str:\n                 parent_dict.setdefault(str(i), [])\n                 parent_dict[str(i)].append(p)\n \n-            max_idx = max([int(chain_idx) for chain_idx in parent_dict])\n+            max_idx = max(int(chain_idx) for chain_idx in parent_dict)\n             for i in range(max_idx + 1):\n                 chain_parents = parent_dict.get(str(i), [\"N/A\"])\n                 parents_per_chain.append(chain_parents)"
        },
        {
            "sha": "d2d5db61f73930d8b9774a4bf6c6bde31e8add34",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -997,7 +997,7 @@ def forward(\n         elif position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        if sum([x is None for x in [pixel_values, image_encoder_embeddings, perceiver_embeddings]]) != 2:\n+        if sum(x is None for x in [pixel_values, image_encoder_embeddings, perceiver_embeddings]) != 2:\n             raise ValueError(\n                 \"Exactly 1 of pixel_values, image_encoder_embeddings or perceiver_embeddings has to be not-None.\"\n             )"
        },
        {
            "sha": "7877c5b4668ddb03078f0d6805a15dafcc7a8af2",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -522,7 +522,7 @@ def prepare_for_tokenization(self, text, is_split_into_words=False, **kwargs):\n         if (\n             (is_split_into_words or add_prefix_space)\n             and (len(text) > 0 and not text[0].isspace())\n-            and sum([text.startswith(no_split_token) for no_split_token in self.added_tokens_encoder]) == 0\n+            and sum(text.startswith(no_split_token) for no_split_token in self.added_tokens_encoder) == 0\n         ):\n             text = \" \" + text\n         return (text, kwargs)"
        },
        {
            "sha": "553700465f3c9052f733be486a8319df382b534c",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -783,7 +783,7 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n         \"\"\"\n         Computes the average number of target masks across the batch, for normalization purposes.\n         \"\"\"\n-        num_masks = sum([len(classes) for classes in class_labels])\n+        num_masks = sum(len(classes) for classes in class_labels)\n         num_masks = torch.as_tensor(num_masks, dtype=torch.float, device=device)\n         world_size = 1\n         if is_accelerate_available():"
        },
        {
            "sha": "b8ccbbb2fec422d20cc68c9ae7132e24f08a0039",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -1088,7 +1088,7 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n         \"\"\"\n         Computes the average number of target masks across the batch, for normalization purposes.\n         \"\"\"\n-        num_masks = sum([len(classes) for classes in class_labels])\n+        num_masks = sum(len(classes) for classes in class_labels)\n         num_masks = torch.as_tensor(num_masks, dtype=torch.float, device=device)\n         world_size = 1\n         if is_accelerate_available():"
        },
        {
            "sha": "7578ac698db719c0404abf40cfdb6edbfad727e6",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -327,7 +327,7 @@ def build_aspect_ratio_mask(aspect_ratios: list[list[tuple[int, int]]], max_imag\n             The mask contains 1s for valid tiles and 0s for padding.\n     \"\"\"\n     batch_size = len(aspect_ratios)\n-    max_num_images = max([len(row) for row in aspect_ratios])\n+    max_num_images = max(len(row) for row in aspect_ratios)\n \n     aspect_ratio_mask = np.zeros((batch_size, max_num_images, max_image_tiles), dtype=np.int64)\n \n@@ -374,7 +374,7 @@ def pack_images(\n \n     # Determine output shape\n     batch_size = len(batch_images)\n-    max_num_images = max([len(images) for images in batch_images])\n+    max_num_images = max(len(images) for images in batch_images)\n     shapes = [image.shape for images in batch_images for image in images]\n     _, channels, tile_height, tile_width = shapes[0]\n \n@@ -412,7 +412,7 @@ def pack_aspect_ratios(aspect_ratios: list[list[tuple[int, int]]], pad_value: in\n             The aspect ratios stacked into a numpy array with shape (batch_size, max_num_images, 2).\n     \"\"\"\n     batch_size = len(aspect_ratios)\n-    max_num_images = max([len(row) for row in aspect_ratios])\n+    max_num_images = max(len(row) for row in aspect_ratios)\n \n     aspect_ratios_stacked = np.full((batch_size, max_num_images, 2), pad_value, dtype=np.int64)\n     for i, row in enumerate(aspect_ratios):\n@@ -442,7 +442,7 @@ def convert_aspect_ratios_to_ids(aspect_ratios: list[list[tuple[int, int]]], max\n     \"\"\"\n \n     batch_size = len(aspect_ratios)\n-    max_num_images = max([len(row) for row in aspect_ratios])\n+    max_num_images = max(len(row) for row in aspect_ratios)\n     supported_aspect_ratios = get_all_supported_aspect_ratios(max_image_tiles)\n \n     aspect_ratios_ids = np.zeros((batch_size, max_num_images), dtype=np.int64)"
        },
        {
            "sha": "5420a2deeeb1ec0f2e8c23d09a694016fdcfdcc7",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -117,7 +117,7 @@ def convert_sparse_cross_attention_mask_to_dense(\n     \"\"\"\n \n     batch_size = len(cross_attention_token_mask)\n-    max_num_images = max([len(masks) for masks in cross_attention_token_mask])\n+    max_num_images = max(len(masks) for masks in cross_attention_token_mask)\n \n     cross_attention_mask = np.zeros(\n         shape=(batch_size, length, max_num_images, max_num_tiles),"
        },
        {
            "sha": "7f685050930c1eac4055cf2c3e4e7d03063335c4",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -1702,7 +1702,7 @@ def forward(\n \n             if audio_codes is not None:\n                 audio_inputs_embeds = sum(\n-                    [self.embed_tokens[codebook](audio_codes[:, codebook]) for codebook in range(audio_codes.shape[1])]\n+                    self.embed_tokens[codebook](audio_codes[:, codebook]) for codebook in range(audio_codes.shape[1])\n                 )\n                 inputs_embeds = (\n                     audio_inputs_embeds\n@@ -1872,20 +1872,18 @@ def _prepare_inputs_embeds_for_generation(\n             if user_audio_codes is not None and moshi_audio_codes is not None:\n                 audio_codes = torch.cat([moshi_audio_codes, user_audio_codes], dim=1)\n                 audio_inputs_embeds = sum(\n-                    [self.embed_tokens[codebook](audio_codes[:, codebook]) for codebook in range(audio_codes.shape[1])]\n+                    self.embed_tokens[codebook](audio_codes[:, codebook]) for codebook in range(audio_codes.shape[1])\n                 )\n             elif moshi_audio_codes is not None:\n                 audio_codes = moshi_audio_codes\n                 audio_inputs_embeds = sum(\n-                    [self.embed_tokens[codebook](audio_codes[:, codebook]) for codebook in range(audio_codes.shape[1])]\n+                    self.embed_tokens[codebook](audio_codes[:, codebook]) for codebook in range(audio_codes.shape[1])\n                 )\n             elif user_audio_codes is not None:\n                 audio_codes = user_audio_codes\n                 audio_inputs_embeds = sum(\n-                    [\n-                        self.embed_tokens[codebook](audio_codes[:, codebook + self.num_codebooks])\n-                        for codebook in range(audio_codes.shape[1])\n-                    ]\n+                    self.embed_tokens[codebook](audio_codes[:, codebook + self.num_codebooks])\n+                    for codebook in range(audio_codes.shape[1])\n                 )\n \n             if input_ids is not None:"
        },
        {
            "sha": "72466d743fd3c5221af627c45ffc32870d753a2d",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -523,7 +523,7 @@ def forward(\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if inputs_embeds is None:\n-            inputs_embeds = sum([self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks)])\n+            inputs_embeds = sum(self.embed_tokens[codebook](input[:, codebook]) for codebook in range(num_codebooks))\n \n         if encoder_hidden_states is not None:\n             # take care of attention masks"
        },
        {
            "sha": "c6dec96b8473fb7aa4383200eb580ecadf3e5aa2",
            "filename": "src/transformers/models/nllb_moe/convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -85,7 +85,7 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weig\n             )\n             torch.save(expert_state, save_path)\n             sharded_state_dicts.append(expert_state.keys())\n-            total_size += sum([value.numel() for key, value in expert_state.items()]) * (\n+            total_size += sum(value.numel() for key, value in expert_state.items()) * (\n                 expert_state[list(expert_state)[0]].element_size()\n             )\n "
        },
        {
            "sha": "350cf8af1ab7a98f99262af3da4502f81aae8b9c",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -352,7 +352,7 @@ def forward(\n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n         # Ignore copy\n-        total_elements = sum([shape[0] * shape[1] for shape in spatial_shapes_list])\n+        total_elements = sum(shape[0] * shape[1] for shape in spatial_shapes_list)\n         if total_elements != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n@@ -1086,7 +1086,7 @@ def get_cached_task_embeddings(self, tasks_input_ids, tasks_attention_mask):\n                 self.language_cache_prompt.put(not_cached_tasks[idx], (emb, cur_mask))\n \n         # pad before concat if needed\n-        max_len = max([task.shape[0] for task in total_task_features])\n+        max_len = max(task.shape[0] for task in total_task_features)\n         for idx, task in enumerate(total_task_features):\n             if task.shape[0] < max_len:\n                 pad_size = max_len - task.shape[0]"
        },
        {
            "sha": "c22f9b045ae96cb75a961f72ae4e2a0c4d2b8a55",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -139,7 +139,7 @@ def __call__(\n                 encodings = []\n \n                 # Maximum number of queries across batch\n-                max_num_queries = max([len(text_single) for text_single in text])\n+                max_num_queries = max(len(text_single) for text_single in text)\n \n                 # Pad all batch samples to max number of text queries\n                 for text_single in text:"
        },
        {
            "sha": "e4feef67da9dcb184c1f8f92770d1eeed5efb635",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -149,7 +149,7 @@ def __call__(\n                 encodings = []\n \n                 # Maximum number of queries across batch\n-                max_num_queries = max([len(text_single) for text_single in text])\n+                max_num_queries = max(len(text_single) for text_single in text)\n \n                 # Pad all batch samples to max number of text queries\n                 for text_single in text:"
        },
        {
            "sha": "2be667e5dc6e7fa47a4c623f392f71551e6d5cf1",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -302,8 +302,8 @@ def _pad_for_batching(\n         \"\"\"\n \n         max_shape = (\n-            max([size[0] for size in image_sizes]),\n-            max([size[1] for size in image_sizes]),\n+            max(size[0] for size in image_sizes),\n+            max(size[1] for size in image_sizes),\n         )\n         pixel_values = [\n             pad("
        },
        {
            "sha": "b31f910e481713ca916c920b95a3ecabb318de38",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -126,7 +126,7 @@ def _pad_for_batching(\n             list[`torch.Tensor`]: The padded images.\n         \"\"\"\n \n-        max_shape = (max([size[0] for size in image_sizes]), max([size[1] for size in image_sizes]))\n+        max_shape = (max(size[0] for size in image_sizes), max(size[1] for size in image_sizes))\n         pixel_values = [\n             torch.nn.functional.pad(image, pad=(0, max_shape[1] - size[1], 0, max_shape[0] - size[0]))\n             for image, size in zip(pixel_values, image_sizes)"
        },
        {
            "sha": "277fb2c04c6e297990d4124781a2031b3cd4d17f",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -1080,9 +1080,7 @@ def _mask_pads(ll, smooth_obj):\n \n     @staticmethod\n     def _cat_and_pad(tensors, pad_token_id):\n-        output = (\n-            tensors[0].new(sum([t.shape[0] for t in tensors]), max([t.shape[1] for t in tensors])).fill_(pad_token_id)\n-        )\n+        output = tensors[0].new(sum(t.shape[0] for t in tensors), max(t.shape[1] for t in tensors)).fill_(pad_token_id)\n         ind = 0\n         for t in tensors:\n             output[ind : ind + t.shape[0], : t.shape[1]] = t"
        },
        {
            "sha": "c9ccbc93a413595a9f723d9235ca95b304a83fa0",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -190,7 +190,7 @@ def _pad_points_and_labels(self, input_points, input_labels, point_pad_value):\n         r\"\"\"\n         The method pads the 2D points and labels to the maximum number of points in the batch.\n         \"\"\"\n-        expected_nb_points = max([point.shape[0] for point in input_points])\n+        expected_nb_points = max(point.shape[0] for point in input_points)\n         processed_input_points = []\n         for i, point in enumerate(input_points):\n             if point.shape[0] != expected_nb_points:"
        },
        {
            "sha": "42dcecce6a3b023cb8d0e3ba8f5ad8280f5f890d",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -316,7 +316,7 @@ def __call__(\n                 text = [text]\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n-            n_images_in_text = sum([sample.count(self.image_token) for sample in text])\n+            n_images_in_text = sum(sample.count(self.image_token) for sample in text)\n             if n_images_in_text > 0 and (images is None and videos is None):\n                 raise ValueError(f\"We detected {n_images_in_text} tokens in the text but no images/videos were passed\")\n "
        },
        {
            "sha": "517f8fdb1537b7dc8d33ac4bc525b1aa6ae7d8e6",
            "filename": "src/transformers/models/vivit/convert_vivit_flax_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -129,7 +129,7 @@ def transform_state_encoder_block(state_dict, i):\n \n \n def get_n_layers(state_dict):\n-    return sum([1 if \"encoderblock_\" in k else 0 for k in state_dict[\"optimizer\"][\"target\"][\"Transformer\"]])\n+    return sum(1 if \"encoderblock_\" in k else 0 for k in state_dict[\"optimizer\"][\"target\"][\"Transformer\"])\n \n \n def transform_state(state_dict, classification_head=False):"
        },
        {
            "sha": "6eca89c5cb834d5788de835037da6d4dfffc66bf",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -5233,7 +5233,7 @@ def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) ->\n         if count_num_items_in_batch:\n             # For now we don't support object detection\n             try:\n-                num_items_in_batch = sum([(batch[\"labels\"].ne(-100)).sum() for batch in batch_samples])\n+                num_items_in_batch = sum((batch[\"labels\"].ne(-100)).sum() for batch in batch_samples)\n             except (TypeError, AttributeError):\n                 pass\n "
        },
        {
            "sha": "6f3f07851b1924c46a6cfb76f4018072df093eee",
            "filename": "tests/trainer/test_trainer_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5/tests%2Ftrainer%2Ftest_trainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_seq2seq.py?ref=894a2bdd8c1d5447dcdbf6c5ccaa7cdd7213cae5",
            "patch": "@@ -77,7 +77,7 @@ def _compute_metrics(pred):\n             pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n             label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n \n-            accuracy = sum([int(pred_str[i] == label_str[i]) for i in range(len(pred_str))]) / len(pred_str)\n+            accuracy = sum(int(pred_str[i] == label_str[i]) for i in range(len(pred_str))) / len(pred_str)\n \n             return {\"accuracy\": accuracy}\n "
        }
    ],
    "stats": {
        "total": 92,
        "additions": 44,
        "deletions": 48
    }
}