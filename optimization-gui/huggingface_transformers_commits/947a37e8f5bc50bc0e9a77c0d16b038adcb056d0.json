{
    "author": "zucchini-nlp",
    "message": "Update recent processors for vLLM backend (#39583)\n\n* update recent models and make sure it runs withh vLLM\n\n* delete!",
    "sha": "947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
    "files": [
        {
            "sha": "cb0471a9204466de994317ebf6b49dbc3b4d7c89",
            "filename": "src/transformers/models/glm4v/configuration_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconfiguration_glm4v.py?ref=947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
            "patch": "@@ -18,7 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n "
        },
        {
            "sha": "d991a09548f77e31c14b092873d08cf26a4a8bb4",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
            "patch": "@@ -454,11 +454,11 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize(\n-            t=self.temporal_patch_size,\n+            num_frames=self.temporal_patch_size,\n             height=height,\n             width=width,\n             factor=factor,\n-            t_factor=self.temporal_patch_size,\n+            temporal_factor=self.temporal_patch_size,\n         )\n         grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n         return grid_h * grid_w"
        },
        {
            "sha": "9e0e4d4841633b8ed29abc3097ed7e73d2b59195",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
            "patch": "@@ -18,7 +18,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import itertools\n from dataclasses import dataclass\n from typing import Any, Callable, Optional, Union\n@@ -753,6 +752,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = self.post_self_attn_layernorm(hidden_states)"
        },
        {
            "sha": "d839328f5a5bd4493bcf51ccb9f765952c516c73",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
            "patch": "@@ -12,10 +12,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import itertools\n from typing import Callable, Optional, Union\n \n+import numpy as np\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n@@ -822,6 +822,7 @@ def forward(\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = self.post_self_attn_layernorm(hidden_states)\n@@ -1566,6 +1567,7 @@ class Glm4vProcessorKwargs(Qwen2_5_VLProcessorKwargs):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n     }\n \n@@ -1707,9 +1709,15 @@ def __call__(\n \n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n "
        },
        {
            "sha": "2d5b9250ee797b97460e7ef7bfbae49f63aef972",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
            "patch": "@@ -18,9 +18,10 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from typing import Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n@@ -44,6 +45,7 @@ class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n     }\n \n@@ -200,9 +202,15 @@ def __call__(\n \n                 text[i] = text[i].replace(\"<|placeholder|>\", self.image_token)\n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n         text_inputs = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         self._check_special_mm_tokens(text, text_inputs, modalities=[\"image\", \"video\"])\n \n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n         return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n \n     def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwargs):"
        },
        {
            "sha": "5674f15e87ba95e13498fc530228456a8af2286e",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 58,
            "deletions": 3,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/947a37e8f5bc50bc0e9a77c0d16b038adcb056d0/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=947a37e8f5bc50bc0e9a77c0d16b038adcb056d0",
            "patch": "@@ -17,9 +17,11 @@\n \n from typing import Iterable, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -32,6 +34,7 @@ class PerceptionLMProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n+            \"return_mm_token_type_ids\": False,\n         },\n     }\n \n@@ -157,9 +160,17 @@ def __call__(\n             prompt_strings.append(sample)\n \n         return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n-        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        return_mm_token_type_ids = output_kwargs[\"text_kwargs\"].pop(\"return_mm_token_type_ids\", False)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n         self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\", \"video\"])\n-        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n+\n+        if return_mm_token_type_ids:\n+            array_ids = np.array(text_inputs[\"input_ids\"])\n+            mm_token_type_ids = np.zeros_like(text_inputs[\"input_ids\"])\n+            mm_token_type_ids[array_ids == self.image_token_id] = 1\n+            text_inputs[\"mm_token_type_ids\"] = mm_token_type_ids.tolist()\n+\n+        return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n     def _expand_media_tokens(self, sample, media_token: str, media_iter: Iterable):\n         media_count = sample.count(media_token)\n@@ -183,6 +194,50 @@ def _expand_media_tokens(self, sample, media_token: str, media_iter: Iterable):\n             sample += sample_splits[-1]\n         return sample\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = PerceptionLMProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            tile_size = images_kwargs.get(\"tile_size\", None) or self.image_processor.tile_size\n+\n+            num_image_tokens = []\n+            num_image_patches = []\n+            for height, width in image_sizes:\n+                if self.image_processor.vision_input_type == \"thumb+tile\":\n+                    aspect_ratio = self.image_processor._fit_image_to_canvas(\n+                        img_width=width, img_height=height, tile_size=tile_size\n+                    )\n+                    if aspect_ratio is None:\n+                        aspect_ratio = self.image_processor._find_closest_aspect_ratio(\n+                            img_width=width, img_height=height, tile_size=tile_size\n+                        )\n+                    num_tiles = aspect_ratio[0] * aspect_ratio[1] + 1  # base image and tiles\n+                else:\n+                    num_tiles = 1\n+\n+                num_image_tokens.append(\n+                    (tile_size // self.patch_size // self.pooling_ratio)\n+                    * (tile_size // self.patch_size // self.pooling_ratio)\n+                    * num_tiles\n+                )\n+                num_image_patches.append(num_tiles)\n+\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+        return MultiModalData(**vision_data)\n+\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to PerceptionLMTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please"
        }
    ],
    "stats": {
        "total": 88,
        "additions": 79,
        "deletions": 9
    }
}