{
    "author": "echarlaix",
    "message": "[v5] Remove deprecated tranformers.onnx (#41700)\n\n* Remove deprecated tranformers.onnx\n\n* Remove transformers.onnx related doc\n\n* style\n\n* shouldn't have been removed\n\n* fix mismatch between metaclip2 modular en config file\n\n* remove onnx config from not_doctested.txt\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "f39355ec23ce701b642f71c9df183c18989332eb",
    "files": [
        {
            "sha": "ee7aff3dc98699557964d47a9a0534ffeae405ff",
            "filename": "docs/source/ar/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Far%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Far%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fserialization.md?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -32,7 +32,7 @@\n Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNXØŒ Ù‚Ù… Ø£ÙˆÙ„Ø§Ù‹ Ø¨ØªØ«Ø¨ÙŠØª Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¥Ø¶Ø§ÙÙŠ:\n \n ```bash\n-pip install optimum[exporters]\n+pip install optimum-onnx\n ```\n \n Ù„Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ø§Ù…ï»»Øª Ø§Ù„Ù…ØªØ§Ø­Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¬ÙˆØ¹ Ø¥Ù„Ù‰ [ÙˆØ«Ø§Ø¦Ù‚ ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ØŒ Ø£Ùˆ Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙÙŠ Ø³Ø·Ø± Ø§Ù„Ø£ÙˆØ§Ù…Ø±:\n@@ -111,60 +111,3 @@ optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_s\n ### ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù„Ù‡Ù†Ø¯Ø³Ø© ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…Ø©\n \n Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ØºØ¨ ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ø¶Ø§ÙØ© Ø¯Ø¹Ù… Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠÙÙ…ÙƒÙ† ØªØµØ¯ÙŠØ±Ù‡ Ø­Ø§Ù„ÙŠÙ‹Ø§ØŒ ÙÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙƒ Ø£ÙˆÙ„Ø§Ù‹ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§ ÙÙŠ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview)ØŒ ÙˆØ¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…Ø¯Ø¹ÙˆÙ…Ù‹Ø§ØŒ [ÙÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ù…Ø³Ø§Ù‡Ù…Ø© ÙÙŠ ğŸ¤— Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute) Ù…ÙØ¨Ø§Ø´Ø±Ø©Ù‹.\n-\n-### ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `transformers.onnx`\n-\n-<Tip warning={true}>\n-\n-Ù„Ù… ÙŠØ¹Ø¯ ÙŠØªÙ… Ø¯Ø¹Ù… `transformers.onnx`  ÙŠÙØ±Ø¬Ù‰ ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Optimum ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø£Ø¹Ù„Ø§Ù‡. Ø³ÙŠØªÙ… Ø¥Ø²Ø§Ù„Ø© Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©.\n-\n-</Tip>\n-\n-Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ ONNX Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `transformers.onnx`ØŒ Ø«Ø¨Ù‘Øª Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©:\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø­Ø²Ù…Ø© `transformers.onnx` ÙƒÙ†Ù…ÙˆØ°Ø¬ Python Ù„ØªØµØ¯ÙŠØ± Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙƒÙˆÙŠÙ† Ø¬Ø§Ù‡Ø²:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-ÙŠÙØµØ¯Ù‘Ø± Ù‡Ø°Ø§ Ø±Ø³Ù…Ù‹Ø§ Ø¨ÙŠØ§Ù†ÙŠÙ‹Ø§ ONNX Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…ÙØ­Ø¯Ø¯Ø© Ø¨ÙˆØ§Ø³Ø·Ø© ÙˆØ³ÙŠØ·Ø© `--model`. Ù…Ø±Ø± Ø£ÙŠ Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ø¹Ù„Ù‰ ğŸ¤— Hub Ø£Ùˆ Ù†Ù‚Ø·Ø© Ø­ÙØ¸ Ù…ÙØ®Ø²Ù†Ø© Ù…Ø­Ù„ÙŠÙ‹Ø§.\n-ÙŠÙÙ…ÙƒÙ† Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ ØªØ´ØºÙŠÙ„ Ù…Ù„Ù `model.onnx` Ø§Ù„Ù†Ø§ØªØ¬ Ø¹Ù„Ù‰ Ø£Ø­Ø¯ Ø§Ù„Ù…ÙØ³Ø±Ø¹Ø§Øª Ø§Ù„Ø¹Ø¯ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… Ù…Ø¹ÙŠØ§Ø± ONNX. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ONNX Runtime ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ÙŠØªÙˆÙ‚Ø¹ ONNX Runtime Ù…ØµÙÙˆÙØ§Øª NumPy ÙƒÙ…Ø¯Ø®Ù„Ø§Øª\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-ÙŠÙÙ…ÙƒÙ† Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Ù…Ø«Ù„ `[\"last_hidden_state\"]`) Ù…Ù† Ø®Ù„Ø§Ù„ Ø¥Ù„Ù‚Ø§Ø¡ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ ØªÙƒÙˆÙŠÙ† ONNX Ù„ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù€ DistilBERTØŒ Ù„Ø¯ÙŠÙ†Ø§:\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]\n-```\n-\n-Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ù…ÙØªØ·Ø§Ø¨Ù‚Ø© Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø­ÙØ¸ TensorFlow Ø¹Ù„Ù‰ Hub. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ØµØ¯Ù‘Ø± Ù†Ù‚Ø·Ø© Ø­ÙØ¸ TensorFlow Ø®Ø§Ù„ØµØ© ÙƒÙ…Ø§ ÙŠÙ„ÙŠ:\n-\n-```bash\n-python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n-```\n-\n-Ù„ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ®Ø²Ù† Ù…Ø­Ù„ÙŠÙ‹Ø§ØŒ Ø§Ø­ÙØ¸ Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆÙ…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù„ØºÙˆÙ‰ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ø¯Ù„ÙŠÙ„ (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ `local-pt-checkpoint`)ØŒ Ø«Ù… Ù‚Ù… Ø¨ØªØµØ¯ÙŠØ±Ù‡ Ø¥Ù„Ù‰ ONNX Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙˆØ¬ÙŠÙ‡ ÙˆØ³ÙŠØ· `--model` Ù„Ø­Ø²Ù…Ø© `transformers.onnx` Ø¥Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "403f06c8e6cf23f0a1ef8e4dea8d1215dd36a49a",
            "filename": "docs/source/en/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fen%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fen%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserialization.md?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -33,7 +33,7 @@ Export a Transformers model to ONNX with the Optimum CLI or the `optimum.onnxrun\n Run the command below to install Optimum and the [exporters](https://huggingface.co/docs/optimum/exporters/overview) module.\n \n ```bash\n-pip install optimum[exporters]\n+pip install optimum-onnx\n ```\n \n > [!TIP]"
        },
        {
            "sha": "aa5538a6971cbb255e0c3931d6e067f227a84039",
            "filename": "docs/source/ja/main_classes/onnx.md",
            "status": "removed",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995435d96ace8bbf7f95623e5a7487990280fd1/docs%2Fsource%2Fja%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995435d96ace8bbf7f95623e5a7487990280fd1/docs%2Fsource%2Fja%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fonnx.md?ref=5995435d96ace8bbf7f95623e5a7487990280fd1",
            "patch": "@@ -1,50 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Exporting ğŸ¤— Transformers models to ONNX\n-\n-ğŸ¤— Transformers ã¯ `transformers.onnx` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’æä¾›ã—ã¾ã™ã€‚\n-è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ONNXã‚°ãƒ©ãƒ•ã«å¤‰æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-è©³ç´°ã¯[ã‚¬ã‚¤ãƒ‰](../serialization) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n-ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n-\n-## ONNX Configurations\n-\n-ä»¥ä¸‹ã®3ã¤ã®æŠ½è±¡ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n-ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ãŸã„ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ã€ç¶™æ‰¿ã™ã¹ã3ã¤ã®æŠ½è±¡ã‚¯ãƒ©ã‚¹ã‚’æä¾›ã—ã¾ã™ï¼š\n-\n-* ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ [`~onnx.config.OnnxConfig`] ã‚’ç¶™æ‰¿ã—ã¾ã™ã€‚\n-* ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¯ [`~onnx.config.OnnxConfigWithPast`] ã‚’ç¶™æ‰¿ã—ã¾ã™ã€‚\n-* ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ [`~onnx.config.OnnxSeq2SeqConfigWithPast`] ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã™ã€‚\n-\n-\n-### OnnxConfig\n-\n-[[autodoc]] onnx.config.OnnxConfig\n-\n-### OnnxConfigWithPast\n-\n-[[autodoc]] onnx.config.OnnxConfigWithPast\n-\n-### OnnxSeq2SeqConfigWithPast\n-\n-[[autodoc]] onnx.config.OnnxSeq2SeqConfigWithPast\n-\n-## ONNX Features\n-\n-å„ ONNX æ§‹æˆã¯ã€æ¬¡ã®ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹ä¸€é€£ã® _æ©Ÿèƒ½_ ã«é–¢é€£ä»˜ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚\n-ã•ã¾ã–ã¾ãªã‚¿ã‚¤ãƒ—ã®ãƒˆãƒãƒ­ã‚¸ã¾ãŸã¯ã‚¿ã‚¹ã‚¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
        },
        {
            "sha": "e8689e50631653b74c54a1a4635973626b6f5caa",
            "filename": "docs/source/ja/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 62,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fja%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fja%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fserialization.md?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -47,7 +47,7 @@ ONNXå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«ä½¿ç”¨\n ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€ã¾ãšè¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š\n \n ```bash\n-pip install optimum[exporters]\n+pip install optimum-onnx\n ```\n \n ã™ã¹ã¦ã®åˆ©ç”¨å¯èƒ½ãªå¼•æ•°ã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ğŸ¤— Optimumãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã¾ãŸã¯ã€ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤ºã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š\n@@ -128,64 +128,3 @@ CLIã®ä»£ã‚ã‚Šã«ã€ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«ãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«\n ### Exporting a model for an unsupported architecture\n \n ç¾åœ¨ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ããªã„ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«è²¢çŒ®ã—ãŸã„å ´åˆã€ã¾ãš[`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview)ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‹ã©ã†ã‹ã‚’ç¢ºèªã—ã€ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã¯[ğŸ¤— Optimumã«è²¢çŒ®](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã—ã¦ãã ã•ã„ã€‚\n-\n-### Exporting a model with `transformers.onnx`\n-\n-<Tip warning={true}>\n-\n-`transformers.onnx`ã¯ã‚‚ã¯ã‚„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šè¨˜ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ğŸ¤— Optimumã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã¯å°†æ¥ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§å‰Šé™¤ã•ã‚Œã¾ã™ã€‚\n-\n-</Tip>\n-\n-ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ï¼š\n-\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-`transformers.onnx`ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’Pythonãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã€äº‹å‰ã«ç”¨æ„ã•ã‚ŒãŸè¨­å®šã‚’ä½¿ç”¨ã—ã¦ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹æ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-ã“ã®æ–¹æ³•ã¯ã€`--model`å¼•æ•°ã§å®šç¾©ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ONNXã‚°ãƒ©ãƒ•ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚ğŸ¤— Hubã®ã„ãšã‚Œã‹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚ŒãŸ`model.onnx`ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ONNXæ¨™æº–ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹å¤šãã®ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã§å®Ÿè¡Œã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€ONNX Runtimeã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ï¼š\n-\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ONNX Runtime expects NumPy arrays as input\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-å¿…è¦ãªå‡ºåŠ›åï¼ˆä¾‹: `[\"last_hidden_state\"]`ï¼‰ã¯ã€å„ãƒ¢ãƒ‡ãƒ«ã®ONNXæ§‹æˆã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§å–å¾—ã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€DistilBERTã®å ´åˆã€æ¬¡ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ï¼š\n-\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]\n-```\n-\n-ãƒãƒ–ã‹ã‚‰ç´”ç²‹ãªTensorFlowã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ—ãƒ­ã‚°ãƒ©ãƒ çš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã¯ã€ä»¥ä¸‹ã®ã‚ˆã†ã«åŒæ§˜ã§ã™ï¼š\n-\n-```bash\n-python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n-```\n-\n-ãƒ­ãƒ¼ã‚«ãƒ«ã«ä¿å­˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒã˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼š `local-pt-checkpoint`ï¼‰ã€‚ãã®å¾Œã€`transformers.onnx`ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã® `--model`å¼•æ•°ã‚’å¸Œæœ›ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‘ã‘ã¦è¨­å®šã—ã¦ã€ONNXã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¾ã™ï¼š\n-\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```\n-"
        },
        {
            "sha": "ba8faca5f943d3a7520339d4a53a09f9b596b891",
            "filename": "docs/source/ko/main_classes/onnx.md",
            "status": "removed",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995435d96ace8bbf7f95623e5a7487990280fd1/docs%2Fsource%2Fko%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995435d96ace8bbf7f95623e5a7487990280fd1/docs%2Fsource%2Fko%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fonnx.md?ref=5995435d96ace8bbf7f95623e5a7487990280fd1",
            "patch": "@@ -1,45 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# ğŸ¤— Transformers ëª¨ë¸ì„ ONNXë¡œ ë‚´ë³´ë‚´ê¸°[[exporting--transformers-models-to-onnx]]\n-\n-ğŸ¤— íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” `transformers.onnx` íŒ¨í‚¤ì§€ë¥¼ ì œê³µí•˜ë©°, ì´ íŒ¨í‚¤ì§€ëŠ” ì„¤ì • ê°ì²´ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ë¥¼ ONNX ê·¸ë˜í”„ë¡œ ë³€í™˜í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n-\n-ğŸ¤— Transformersì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì´ ê°€ì´ë“œ](../serialization)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n-\n-## ONNX ì„¤ì •[[onnx-configurations]]\n-\n-ë‚´ë³´ë‚´ë ¤ëŠ”(export) ëª¨ë¸ ì•„í‚¤í…ì²˜ì˜ ìœ í˜•ì— ë”°ë¼ ìƒì†ë°›ì•„ì•¼ í•  ì„¸ ê°€ì§€ ì¶”ìƒ í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤:\n-\n-* ì¸ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ [`~onnx.config.OnnxConfig`]ì„ ìƒì†ë°›ìŠµë‹ˆë‹¤.\n-* ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ [`~onnx.config.OnnxConfigWithPast`]ì„ ìƒì†ë°›ìŠµë‹ˆë‹¤.\n-* ì¸ì½”ë”-ë””ì½”ë” ê¸°ë°˜ ëª¨ë¸ì€ [`~onnx.config.OnnxSeq2SeqConfigWithPast`]ì„ ìƒì†ë°›ìŠµë‹ˆë‹¤.\n-\n-### OnnxConfig[[transformers.onnx.OnnxConfig]]\n-\n-[[autodoc]] onnx.config.OnnxConfig\n-\n-### OnnxConfigWithPast[[transformers.onnx.OnnxConfigWithPast]]\n-\n-[[autodoc]] onnx.config.OnnxConfigWithPast\n-\n-### OnnxSeq2SeqConfigWithPast[[OnnxSeq2SeqConfigWithPast]]\n-\n-[[autodoc]] onnx.config.OnnxSeq2SeqConfigWithPast\n-\n-## ONNX íŠ¹ì§•[[onnx-features]]\n-\n-ê° ONNX ì„¤ì •ì€ ë‹¤ì–‘í•œ ìœ í˜•ì˜ í† í´ë¡œì§€ë‚˜ ì‘ì—…ì— ëŒ€í•´ ëª¨ë¸ì„ ë‚´ë³´ë‚¼ ìˆ˜ ìˆê²Œ(exporting) í•´ì£¼ëŠ” _features_ ì„¸íŠ¸ì™€ ì—°ê´€ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        },
        {
            "sha": "312f74f3c914f454ae2b3feb717fd807631b3963",
            "filename": "docs/source/ko/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 57,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fko%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fko%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fserialization.md?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -47,7 +47,7 @@ ONNX í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚¸ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆ\n ğŸ¤— Transformers ëª¨ë¸ì„ ONNXë¡œ ë‚´ë³´ë‚´ë ¤ë©´ ë¨¼ì € ì¶”ê°€ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:\n \n ```bash\n-pip install optimum[exporters]\n+pip install optimum-onnx\n ```\n \n ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ì¸ìˆ˜ë¥¼ í™•ì¸í•˜ë ¤ë©´ [ğŸ¤— Optimum ë¬¸ì„œ](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)ë¥¼ ì°¸ì¡°í•˜ê±°ë‚˜ ëª…ë ¹ì¤„ì—ì„œ ë„ì›€ë§ì„ ë³´ì„¸ìš”.\n@@ -123,59 +123,3 @@ CLI ëŒ€ì‹ ì— `optimum.onnxruntime`ì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ\n ### ì§€ì›ë˜ì§€ ì•ŠëŠ” ì•„í‚¤í…ì²˜ì˜ ëª¨ë¸ ë‚´ë³´ë‚´ê¸° [[exporting-a-model-for-an-unsupported-architecture]]\n \n í˜„ì¬ ë‚´ë³´ë‚¼ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì„ ì§€ì›í•˜ê¸° ìœ„í•´ ê¸°ì—¬í•˜ë ¤ë©´, ë¨¼ì € [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview)ì—ì„œ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•œ í›„ ì§€ì›ë˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” [ğŸ¤— Optimumì— ê¸°ì—¬](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)í•˜ì„¸ìš”.\n-\n-### `transformers.onnx`ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ë‚´ë³´ë‚´ê¸° [[exporting-a-model-with-transformersonnx]]\n-\n-<Tip warning={true}>\n-\n-`tranformers.onnx`ëŠ” ë” ì´ìƒ ìœ ì§€ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìœ„ì—ì„œ ì„¤ëª…í•œ ëŒ€ë¡œ ğŸ¤— Optimumì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‚´ë³´ë‚´ì„¸ìš”. ì´ ì„¹ì…˜ì€ í–¥í›„ ë²„ì „ì—ì„œ ì œê±°ë  ì˜ˆì •ì…ë‹ˆë‹¤.\n-\n-</Tip>\n-\n-ğŸ¤— Transformers ëª¨ë¸ì„ ONNXë¡œ ë‚´ë³´ë‚´ë ¤ë©´ ì¶”ê°€ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-`transformers.onnx` íŒ¨í‚¤ì§€ë¥¼ Python ëª¨ë“ˆë¡œ ì‚¬ìš©í•˜ì—¬ ì¤€ë¹„ëœ êµ¬ì„±ì„ ì‚¬ìš©í•˜ì—¬ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤:\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-ì´ë ‡ê²Œ í•˜ë©´ `--model` ì¸ìˆ˜ì— ì •ì˜ëœ ì²´í¬í¬ì¸íŠ¸ì˜ ONNX ê·¸ë˜í”„ê°€ ë‚´ë³´ë‚´ì§‘ë‹ˆë‹¤. ğŸ¤— Hubì—ì„œ ì œê³µí•˜ëŠ” ì²´í¬í¬ì¸íŠ¸ë‚˜ ë¡œì»¬ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê²°ê³¼ë¡œ ìƒì„±ëœ `model.onnx` íŒŒì¼ì€ ONNX í‘œì¤€ì„ ì§€ì›í•˜ëŠ” ë§ì€ ê°€ì†ê¸° ì¤‘ í•˜ë‚˜ì—ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ ONNX Runtimeì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ONNX Runtime expects NumPy arrays as input\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-í•„ìš”í•œ ì¶œë ¥ ì´ë¦„(ì˜ˆ: `[\"last_hidden_state\"]`)ì€ ê° ëª¨ë¸ì˜ ONNX êµ¬ì„±ì„ í™•ì¸í•˜ì—¬ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, DistilBERTì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]\n-```\n-\n-Hubì˜ TensorFlow ì²´í¬í¬ì¸íŠ¸ì— ëŒ€í•´ì„œë„ ë™ì¼í•œ í”„ë¡œì„¸ìŠ¤ê°€ ì ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ìŒê³¼ ê°™ì´ ìˆœìˆ˜í•œ TensorFlow ì²´í¬í¬ì¸íŠ¸ë¥¼ ë‚´ë³´ëƒ…ë‹ˆë‹¤:\n-\n-```bash\n-python -m transformers.onnx --model=keras-io/transformers-qa onnx/\n-```\n-\n-ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ì„ ë‚´ë³´ë‚´ë ¤ë©´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ íŒŒì¼ê³¼ í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— ì €ì¥í•œ ë‹¤ìŒ, transformers.onnx íŒ¨í‚¤ì§€ì˜ --model ì¸ìˆ˜ë¥¼ ì›í•˜ëŠ” ë””ë ‰í† ë¦¬ë¡œ ì§€ì •í•˜ì—¬ ONNXë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤:\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "ef4cee5f9b489642a7e514a95f0da34ca1f878a4",
            "filename": "docs/source/zh/main_classes/onnx.md",
            "status": "removed",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995435d96ace8bbf7f95623e5a7487990280fd1/docs%2Fsource%2Fzh%2Fmain_classes%2Fonnx.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995435d96ace8bbf7f95623e5a7487990280fd1/docs%2Fsource%2Fzh%2Fmain_classes%2Fonnx.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fonnx.md?ref=5995435d96ace8bbf7f95623e5a7487990280fd1",
            "patch": "@@ -1,45 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# å¯¼å‡º ğŸ¤— Transformers æ¨¡å‹åˆ° ONNX\n-\n-ğŸ¤— Transformersæä¾›äº†ä¸€ä¸ª`transformers.onnx`åŒ…ï¼Œé€šè¿‡åˆ©ç”¨é…ç½®å¯¹è±¡ï¼Œæ‚¨å¯ä»¥å°†æ¨¡å‹checkpointsè½¬æ¢ä¸ºONNXå›¾ã€‚\n-\n-æœ‰å…³æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…å¯¼å‡º ğŸ¤— Transformers æ¨¡å‹çš„[æŒ‡å—](../serialization)ã€‚\n-\n-## ONNX Configurations\n-\n-æˆ‘ä»¬æä¾›äº†ä¸‰ä¸ªæŠ½è±¡ç±»ï¼Œå–å†³äºæ‚¨å¸Œæœ›å¯¼å‡ºçš„æ¨¡å‹æ¶æ„ç±»å‹ï¼š\n-\n-* åŸºäºç¼–ç å™¨çš„æ¨¡å‹ç»§æ‰¿ [`~onnx.config.OnnxConfig`]\n-* åŸºäºè§£ç å™¨çš„æ¨¡å‹ç»§æ‰¿ [`~onnx.config.OnnxConfigWithPast`]\n-* ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ç»§æ‰¿ [`~onnx.config.OnnxSeq2SeqConfigWithPast`]\n-\n-### OnnxConfig\n-\n-[[autodoc]] onnx.config.OnnxConfig\n-\n-### OnnxConfigWithPast\n-\n-[[autodoc]] onnx.config.OnnxConfigWithPast\n-\n-### OnnxSeq2SeqConfigWithPast\n-\n-[[autodoc]] onnx.config.OnnxSeq2SeqConfigWithPast\n-\n-## ONNX Features\n-\n-æ¯ä¸ªONNXé…ç½®ä¸ä¸€ç»„ _ç‰¹æ€§_ ç›¸å…³è”ï¼Œä½¿æ‚¨èƒ½å¤Ÿä¸ºä¸åŒç±»å‹çš„æ‹“æ‰‘ç»“æ„æˆ–ä»»åŠ¡å¯¼å‡ºæ¨¡å‹ã€‚"
        },
        {
            "sha": "fff19e2b8169695eed75f918e3269f403a1d7c35",
            "filename": "docs/source/zh/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 51,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fzh%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/docs%2Fsource%2Fzh%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fserialization.md?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -47,7 +47,7 @@ rendered properly in your Markdown viewer.\n è¦å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œé¦–å…ˆéœ€è¦å®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š\n \n ```bash\n-pip install optimum[exporters]\n+pip install optimum-onnx\n ```\n \n è¯·å‚é˜… [ğŸ¤— Optimum æ–‡æ¡£](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli) ä»¥æŸ¥çœ‹æ‰€æœ‰å¯ç”¨å‚æ•°ï¼Œæˆ–è€…åœ¨å‘½ä»¤è¡Œä¸­æŸ¥çœ‹å¸®åŠ©ï¼š\n@@ -117,53 +117,3 @@ optimum-cli export onnx --model local_path --task question-answering distilbert_\n ### å¯¼å‡ºå°šæœªæ”¯æŒçš„æ¶æ„çš„æ¨¡å‹\n \n å¦‚æœä½ æƒ³è¦ä¸ºå½“å‰æ— æ³•å¯¼å‡ºçš„æ¨¡å‹æ·»åŠ æ”¯æŒï¼Œè¯·å…ˆæ£€æŸ¥ [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) æ˜¯å¦æ”¯æŒè¯¥æ¨¡å‹ï¼Œå¦‚æœä¸æ”¯æŒï¼Œä½ å¯ä»¥ [ç›´æ¥ä¸º ğŸ¤— Optimum è´¡çŒ®ä»£ç ](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)ã€‚\n-\n-### ä½¿ç”¨ `transformers.onnx` å¯¼å‡ºæ¨¡å‹\n-\n-<Tip warning={true}>\n-\n-`transformers.onnx` ä¸å†è¿›è¡Œç»´æŠ¤ï¼Œè¯·å¦‚ä¸Šæ‰€è¿°ï¼Œä½¿ç”¨ ğŸ¤— Optimum å¯¼å‡ºæ¨¡å‹ã€‚è¿™éƒ¨åˆ†å†…å®¹å°†åœ¨æœªæ¥ç‰ˆæœ¬ä¸­åˆ é™¤ã€‚\n-\n-</Tip>\n-\n-è¦ä½¿ç”¨ `transformers.onnx` å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º ONNXï¼Œè¯·å®‰è£…é¢å¤–çš„ä¾èµ–é¡¹ï¼š\n-\n-```bash\n-pip install transformers[onnx]\n-```\n-\n-å°† `transformers.onnx` åŒ…ä½œä¸º Python æ¨¡å—ä½¿ç”¨ï¼Œä»¥ä½¿ç”¨ç°æˆçš„é…ç½®å¯¼å‡ºæ£€æŸ¥ç‚¹ï¼š\n-\n-```bash\n-python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/\n-```\n-\n-ä»¥ä¸Šä»£ç å°†å¯¼å‡ºç”± `--model` å‚æ•°å®šä¹‰çš„æ£€æŸ¥ç‚¹çš„ ONNX å›¾ã€‚ä¼ å…¥ä»»ä½• ğŸ¤— Hub ä¸Šæˆ–è€…å­˜å‚¨ä¸æœ¬åœ°çš„æ£€æŸ¥ç‚¹ã€‚ç”Ÿæˆçš„ `model.onnx` æ–‡ä»¶å¯ä»¥åœ¨æ”¯æŒ ONNX æ ‡å‡†çš„ä¼—å¤šåŠ é€Ÿå¼•æ“ä¸Šè¿è¡Œã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ ONNX Runtime åŠ è½½å¹¶è¿è¡Œæ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š\n-\n-```python\n->>> from transformers import AutoTokenizer\n->>> from onnxruntime import InferenceSession\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> session = InferenceSession(\"onnx/model.onnx\")\n->>> # ONNX Runtime expects NumPy arrays as input\n->>> inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n->>> outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))\n-```\n-\n-å¯ä»¥é€šè¿‡æŸ¥çœ‹æ¯ä¸ªæ¨¡å‹çš„ ONNX é…ç½®æ¥è·å–æ‰€éœ€çš„è¾“å‡ºåï¼ˆä¾‹å¦‚ `[\"last_hidden_state\"]`ï¼‰ã€‚ä¾‹å¦‚ï¼Œå¯¹äº DistilBERTï¼Œå¯ä»¥ç”¨ä»¥ä¸‹ä»£ç è·å–è¾“å‡ºåç§°ï¼š\n-\n-```python\n->>> from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n-\n->>> config = DistilBertConfig()\n->>> onnx_config = DistilBertOnnxConfig(config)\n->>> print(list(onnx_config.outputs.keys()))\n-[\"last_hidden_state\"]\n-```\n-\n-è¦å¯¼å‡ºæœ¬åœ°å­˜å‚¨çš„æ¨¡å‹ï¼Œè¯·å°†æ¨¡å‹çš„æƒé‡å’Œåˆ†è¯å™¨æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ä¸­ï¼ˆä¾‹å¦‚ `local-pt-checkpoint`ï¼‰ï¼Œç„¶åé€šè¿‡å°† `transformers.onnx` åŒ…çš„ `--model` å‚æ•°æŒ‡å‘è¯¥ç›®å½•ï¼Œå°†å…¶å¯¼å‡ºä¸º ONNXï¼š\n-\n-```bash\n-python -m transformers.onnx --model=local-pt-checkpoint onnx/\n-```"
        },
        {
            "sha": "ba7e7dc19fad32f8e95b67c74afe942f2c5eb750",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -129,8 +129,6 @@\n     ],\n     \"loss\": [],\n     \"modelcard\": [\"ModelCard\"],\n-    # Models\n-    \"onnx\": [],\n     \"pipelines\": [\n         \"AudioClassificationPipeline\",\n         \"AutomaticSpeechRecognitionPipeline\","
        },
        {
            "sha": "0bc7bb362bf2ff84941a89aaa79b67268d723c39",
            "filename": "src/transformers/models/albert/configuration_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fconfiguration_albert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"ALBERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n \n \n class AlbertConfig(PreTrainedConfig):\n@@ -142,21 +138,4 @@ def __init__(\n         self.classifier_dropout_prob = classifier_dropout_prob\n \n \n-# Copied from transformers.models.bert.configuration_bert.BertOnnxConfig with Roberta->Albert\n-class AlbertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"AlbertConfig\", \"AlbertOnnxConfig\"]\n+__all__ = [\"AlbertConfig\"]"
        },
        {
            "sha": "ea2e596cb53a705a26e154106fe7a13a470f050c",
            "filename": "src/transformers/models/bart/configuration_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 227,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fconfiguration_bart.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,15 +15,9 @@\n \"\"\"BART model configuration\"\"\"\n \n import warnings\n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n \n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n-from ...utils import is_torch_available, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -180,223 +174,4 @@ def __init__(\n             )\n \n \n-class BartOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-\n-            if self.use_past:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-            else:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-            if self.use_past:\n-                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        elif self.task == \"causal-lm\":\n-            # TODO: figure this case out.\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                ]\n-            )\n-\n-        return common_inputs\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_outputs = super().outputs\n-        else:\n-            common_outputs = super(OnnxConfigWithPast, self).outputs\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        return common_outputs\n-\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length + 3\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_causal_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            # Not using the same length for past_key_values\n-            past_key_values_length = seqlen + 2\n-            num_encoder_layers, _ = self.num_layers\n-            num_encoder_attention_heads, _ = self.num_attention_heads\n-            past_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-\n-            mask_dtype = common_inputs[\"attention_mask\"].dtype\n-            common_inputs[\"attention_mask\"] = torch.cat(\n-                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = [\n-                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_encoder_layers)\n-            ]\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        elif self.task == \"causal-lm\":\n-            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-        else:\n-            common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        return common_inputs\n-\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n-                flattened_output, name, idx, t\n-            )\n-\n-\n-__all__ = [\"BartConfig\", \"BartOnnxConfig\"]\n+__all__ = [\"BartConfig\"]"
        },
        {
            "sha": "46b039e576fcd809897806bbf0e6b384106fea99",
            "filename": "src/transformers/models/beit/configuration_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fconfiguration_beit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,13 +15,8 @@\n \"\"\"BEiT model configuration\"\"\"\n \n import warnings\n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n \n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n@@ -209,21 +204,4 @@ def __init__(\n         self.reshape_hidden_states = reshape_hidden_states\n \n \n-# Copied from transformers.models.vit.configuration_vit.ViTOnnxConfig\n-class BeitOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"BeitConfig\", \"BeitOnnxConfig\"]\n+__all__ = [\"BeitConfig\"]"
        },
        {
            "sha": "a22e0675b46d299f7fcd97e40770e1eef8f51541",
            "filename": "src/transformers/models/bert/configuration_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconfiguration_bert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"BERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -127,20 +123,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class BertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"BertConfig\", \"BertOnnxConfig\"]\n+__all__ = [\"BertConfig\"]"
        },
        {
            "sha": "54d4d18ac2f5b9e399e6c7e9c88900eb571af59a",
            "filename": "src/transformers/models/big_bird/configuration_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fconfiguration_big_bird.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"BigBird model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -158,19 +154,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class BigBirdOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"BigBirdConfig\", \"BigBirdOnnxConfig\"]\n+__all__ = [\"BigBirdConfig\"]"
        },
        {
            "sha": "0da9f97f15bcb6114f65e4aee61e0dfbf15498c9",
            "filename": "src/transformers/models/bigbird_pegasus/configuration_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 229,
            "changes": 231,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fconfiguration_bigbird_pegasus.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,8 @@\n # limitations under the License.\n \"\"\"BigBirdPegasus model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n-from ...utils import is_torch_available, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -186,224 +179,4 @@ def __init__(\n         )\n \n \n-# Copied from transformers.models.bart.configuration_bart.BartOnnxConfig with Bart->BigBirdPegasus\n-class BigBirdPegasusOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-\n-            if self.use_past:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-            else:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-            if self.use_past:\n-                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        elif self.task == \"causal-lm\":\n-            # TODO: figure this case out.\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                ]\n-            )\n-\n-        return common_inputs\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_outputs = super().outputs\n-        else:\n-            common_outputs = super(OnnxConfigWithPast, self).outputs\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        return common_outputs\n-\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length + 3\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_causal_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            # Not using the same length for past_key_values\n-            past_key_values_length = seqlen + 2\n-            num_encoder_layers, _ = self.num_layers\n-            num_encoder_attention_heads, _ = self.num_attention_heads\n-            past_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-\n-            mask_dtype = common_inputs[\"attention_mask\"].dtype\n-            common_inputs[\"attention_mask\"] = torch.cat(\n-                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = [\n-                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_encoder_layers)\n-            ]\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        elif self.task == \"causal-lm\":\n-            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-        else:\n-            common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        return common_inputs\n-\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n-                flattened_output, name, idx, t\n-            )\n-\n-\n-__all__ = [\"BigBirdPegasusConfig\", \"BigBirdPegasusOnnxConfig\"]\n+__all__ = [\"BigBirdPegasusConfig\"]"
        },
        {
            "sha": "e9ea33d3cb39bea9d2eb5e806ca089552e2235d5",
            "filename": "src/transformers/models/blenderbot/configuration_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 232,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fconfiguration_blenderbot.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,7 @@\n # limitations under the License.\n \"\"\"Blenderbot model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...file_utils import is_torch_available\n-from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import logging\n \n \n@@ -166,227 +158,4 @@ def __init__(\n         )\n \n \n-class BlenderbotOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-            else:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-            if self.use_past:\n-                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        elif self.task == \"causal-lm\":\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                _, num_decoder_layers = self.num_layers\n-                for i in range(num_decoder_layers):\n-                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                ]\n-            )\n-\n-        return common_inputs\n-\n-    @property\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig.outputs\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_outputs = super().outputs\n-        else:\n-            common_outputs = super(OnnxConfigWithPast, self).outputs\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        return common_outputs\n-\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = []\n-            _, num_decoder_layers = self.num_layers\n-\n-            for _ in range(num_decoder_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_causal_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            past_key_values_length = seqlen\n-            _, num_decoder_layers = self.num_layers\n-            num_encoder_attention_heads, _ = self.num_attention_heads\n-            past_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            mask_dtype = common_inputs[\"attention_mask\"].dtype\n-            common_inputs[\"attention_mask\"] = torch.cat(\n-                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = [\n-                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_decoder_layers)\n-            ]\n-        return common_inputs\n-\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig._generate_dummy_inputs_for_sequence_classification_and_question_answering\n-    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig.generate_dummy_inputs\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        elif self.task == \"causal-lm\":\n-            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-        else:\n-            common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        return common_inputs\n-\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig._flatten_past_key_values_\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n-                flattened_output, name, idx, t\n-            )\n-\n-    def fill_with_past_key_values_(self, inputs_or_outputs: Mapping[str, Mapping[int, str]], direction: str):\n-        if direction not in [\"inputs\", \"outputs\"]:\n-            raise ValueError(f'direction must either be \"inputs\" or \"outputs\", but {direction} was given')\n-\n-        name = \"past_key_values\" if direction == \"inputs\" else \"present\"\n-        _, num_decoder_layers = self.num_layers\n-\n-        encoder_sequence = \"past_encoder_sequence\"\n-        decoder_sequence = \"past_decoder_sequence\" if direction == \"inputs\" else \"past_decoder_sequence + sequence\"\n-\n-        for i in range(num_decoder_layers):\n-            inputs_or_outputs[f\"{name}.{i}.decoder.key\"] = {0: \"batch\", 2: decoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.decoder.value\"] = {0: \"batch\", 2: decoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.encoder.key\"] = {0: \"batch\", 2: encoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.encoder.value\"] = {0: \"batch\", 2: encoder_sequence}\n-\n-\n-__all__ = [\"BlenderbotConfig\", \"BlenderbotOnnxConfig\"]\n+__all__ = [\"BlenderbotConfig\"]"
        },
        {
            "sha": "33b3e21506a17f7ddbb258095ca68fee698d6ca2",
            "filename": "src/transformers/models/blenderbot_small/configuration_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 229,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fconfiguration_blenderbot_small.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,7 @@\n # limitations under the License.\n \"\"\"BlenderbotSmall model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...file_utils import is_torch_available\n-from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import logging\n \n \n@@ -164,224 +156,4 @@ def __init__(\n         )\n \n \n-# Copied from transformers.models.bart.configuration_bart.BartOnnxConfig with Bart->BlenderbotSmall\n-class BlenderbotSmallOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-\n-            if self.use_past:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-            else:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-            if self.use_past:\n-                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        elif self.task == \"causal-lm\":\n-            # TODO: figure this case out.\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                ]\n-            )\n-\n-        return common_inputs\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_outputs = super().outputs\n-        else:\n-            common_outputs = super(OnnxConfigWithPast, self).outputs\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        return common_outputs\n-\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length + 3\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_causal_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            # Not using the same length for past_key_values\n-            past_key_values_length = seqlen + 2\n-            num_encoder_layers, _ = self.num_layers\n-            num_encoder_attention_heads, _ = self.num_attention_heads\n-            past_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-\n-            mask_dtype = common_inputs[\"attention_mask\"].dtype\n-            common_inputs[\"attention_mask\"] = torch.cat(\n-                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = [\n-                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_encoder_layers)\n-            ]\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        elif self.task == \"causal-lm\":\n-            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-        else:\n-            common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        return common_inputs\n-\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n-                flattened_output, name, idx, t\n-            )\n-\n-\n-__all__ = [\"BlenderbotSmallConfig\", \"BlenderbotSmallOnnxConfig\"]\n+__all__ = [\"BlenderbotSmallConfig\"]"
        },
        {
            "sha": "533c0e4ee98b10b0f46e0d323855ff730998c18a",
            "filename": "src/transformers/models/bloom/configuration_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 108,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconfiguration_bloom.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,19 +14,8 @@\n # limitations under the License.\n \"\"\"Bloom configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, Optional\n-\n-from packaging import version\n-\n-\n-if TYPE_CHECKING:\n-    from ... import PreTrainedTokenizer\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfigWithPast, PatchingSpec\n-from ...utils import is_torch_available, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -142,99 +131,4 @@ def __init__(\n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n \n \n-class BloomOnnxConfig(OnnxConfigWithPast):\n-    torch_onnx_minimum_version = version.parse(\"1.12\")\n-\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        task: str = \"default\",\n-        patching_specs: Optional[list[PatchingSpec]] = None,\n-        use_past: bool = False,\n-    ):\n-        super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)\n-        if not getattr(self._config, \"pad_token_id\", None):\n-            # TODO: how to do that better?\n-            self._config.pad_token_id = 0\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n-        if self.use_past:\n-            # BLOOM stores values on dynamic axis 2. For more details see: https://github.com/huggingface/transformers/pull/18344\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\", inverted_values_shape=True)\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n-\n-        return common_inputs\n-\n-    @property\n-    def num_layers(self) -> int:\n-        return self._config.n_layer\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self._config.n_head\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-3\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: \"PreTrainedTokenizer\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-\n-        # We need to order the input in the way they appears in the forward()\n-        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n-\n-        # Need to add the past_keys\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-\n-                batch, seqlen = common_inputs[\"input_ids\"].shape\n-                # Not using the same length for past_key_values\n-                past_key_values_length = seqlen + 2\n-                head_dim = self._config.hidden_size // self.num_attention_heads\n-                past_key_shape = (\n-                    batch * self.num_attention_heads,\n-                    head_dim,\n-                    past_key_values_length,\n-                )\n-                past_value_shape = (\n-                    batch * self.num_attention_heads,\n-                    past_key_values_length,\n-                    head_dim,\n-                )\n-                ordered_inputs[\"past_key_values\"] = [\n-                    (torch.zeros(past_key_shape), torch.zeros(past_value_shape)) for _ in range(self.num_layers)\n-                ]\n-\n-        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n-        if self.use_past:\n-            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n-            ordered_inputs[\"attention_mask\"] = torch.cat(\n-                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-\n-        return ordered_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"BloomConfig\", \"BloomOnnxConfig\"]\n+__all__ = [\"BloomConfig\"]"
        },
        {
            "sha": "b250c94c9208d9443efe70a4a5c71f449d2358c1",
            "filename": "src/transformers/models/camembert/configuration_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fconfiguration_camembert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"CamemBERT configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -129,19 +125,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class CamembertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"CamembertConfig\", \"CamembertOnnxConfig\"]\n+__all__ = [\"CamembertConfig\"]"
        },
        {
            "sha": "f3e602deb35c61441d60da931a47728877eade5e",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,16 +14,7 @@\n # limitations under the License.\n \"\"\"Chinese-CLIP model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n-\n-if TYPE_CHECKING:\n-    from ...processing_utils import ProcessorMixin\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -368,52 +359,4 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class ChineseCLIPOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"logits_per_image\", {0: \"batch\"}),\n-                (\"logits_per_text\", {0: \"batch\"}),\n-                (\"text_embeds\", {0: \"batch\"}),\n-                (\"image_embeds\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    def generate_dummy_inputs(\n-        self,\n-        processor: \"ProcessorMixin\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-    ) -> Mapping[str, Any]:\n-        text_input_dict = super().generate_dummy_inputs(\n-            processor.tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-        )\n-        image_input_dict = super().generate_dummy_inputs(\n-            processor.image_processor,\n-            batch_size=batch_size,\n-        )\n-        return {**text_input_dict, **image_input_dict}\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 14\n-\n-\n-__all__ = [\"ChineseCLIPConfig\", \"ChineseCLIPOnnxConfig\", \"ChineseCLIPTextConfig\", \"ChineseCLIPVisionConfig\"]\n+__all__ = [\"ChineseCLIPConfig\", \"ChineseCLIPTextConfig\", \"ChineseCLIPVisionConfig\"]"
        },
        {
            "sha": "6990761f8375c2bb4b1e51079212a52a20afd1c7",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,16 +14,7 @@\n # limitations under the License.\n \"\"\"CLIP model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n-\n-if TYPE_CHECKING:\n-    from ...processing_utils import ProcessorMixin\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -364,52 +355,4 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class CLIPOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"logits_per_image\", {0: \"batch\"}),\n-                (\"logits_per_text\", {0: \"batch\"}),\n-                (\"text_embeds\", {0: \"batch\"}),\n-                (\"image_embeds\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    def generate_dummy_inputs(\n-        self,\n-        processor: \"ProcessorMixin\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-    ) -> Mapping[str, Any]:\n-        text_input_dict = super().generate_dummy_inputs(\n-            processor.tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-        )\n-        image_input_dict = super().generate_dummy_inputs(\n-            processor.image_processor,\n-            batch_size=batch_size,\n-        )\n-        return {**text_input_dict, **image_input_dict}\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 14\n-\n-\n-__all__ = [\"CLIPConfig\", \"CLIPOnnxConfig\", \"CLIPTextConfig\", \"CLIPVisionConfig\"]\n+__all__ = [\"CLIPConfig\", \"CLIPTextConfig\", \"CLIPVisionConfig\"]"
        },
        {
            "sha": "32501c00f2cec38509b3c10b17340941b7c02ed0",
            "filename": "src/transformers/models/codegen/configuration_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 88,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fconfiguration_codegen.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"CodeGen model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any, Optional\n-\n-from ... import PreTrainedTokenizer, is_torch_available\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import logging\n \n \n@@ -146,85 +140,4 @@ def __init__(\n         )\n \n \n-# Copied from transformers.models.gpt2.configuration_gpt2.GPT2OnnxConfig with GPT2->CodeGen\n-class CodeGenOnnxConfig(OnnxConfigWithPast):\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        task: str = \"default\",\n-        patching_specs: Optional[list[PatchingSpec]] = None,\n-        use_past: bool = False,\n-    ):\n-        super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)\n-        if not getattr(self._config, \"pad_token_id\", None):\n-            # TODO: how to do that better?\n-            self._config.pad_token_id = 0\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n-\n-        return common_inputs\n-\n-    @property\n-    def num_layers(self) -> int:\n-        return self._config.n_layer\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self._config.n_head\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-        )\n-\n-        # We need to order the input in the way they appears in the forward()\n-        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n-\n-        # Need to add the past_keys\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-\n-                batch, seqlen = common_inputs[\"input_ids\"].shape\n-                # Not using the same length for past_key_values\n-                past_key_values_length = seqlen + 2\n-                past_shape = (\n-                    batch,\n-                    self.num_attention_heads,\n-                    past_key_values_length,\n-                    self._config.hidden_size // self.num_attention_heads,\n-                )\n-                ordered_inputs[\"past_key_values\"] = [\n-                    (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(self.num_layers)\n-                ]\n-\n-        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n-        if self.use_past:\n-            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n-            ordered_inputs[\"attention_mask\"] = torch.cat(\n-                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-\n-        return ordered_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"CodeGenConfig\", \"CodeGenOnnxConfig\"]\n+__all__ = [\"CodeGenConfig\"]"
        },
        {
            "sha": "d9877bbcae25e082bd2a5848bb7e7c2c0ba662d6",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"Conditional DETR model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -247,25 +241,4 @@ def __init__(\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n-class ConditionalDetrOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"pixel_mask\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-5\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-\n-__all__ = [\"ConditionalDetrConfig\", \"ConditionalDetrOnnxConfig\"]\n+__all__ = [\"ConditionalDetrConfig\"]"
        },
        {
            "sha": "9d2ea54179f47eaee6ad5a3f88c4b36be16cdf7d",
            "filename": "src/transformers/models/convbert/configuration_convbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fconfiguration_convbert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"ConvBERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -140,21 +136,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-# Copied from transformers.models.bert.configuration_bert.BertOnnxConfig\n-class ConvBertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"ConvBertConfig\", \"ConvBertOnnxConfig\"]\n+__all__ = [\"ConvBertConfig\"]"
        },
        {
            "sha": "017e881ca69982af86a2be913267ee2991cb071a",
            "filename": "src/transformers/models/convnext/configuration_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconfiguration_convnext.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"ConvNeXT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n@@ -123,20 +117,4 @@ def __init__(\n         )\n \n \n-class ConvNextOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-5\n-\n-\n-__all__ = [\"ConvNextConfig\", \"ConvNextOnnxConfig\"]\n+__all__ = [\"ConvNextConfig\"]"
        },
        {
            "sha": "2c3c1b7f3ed480d84f97c03f61cbd9977248509d",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_text.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"Data2VecText configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -128,19 +124,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class Data2VecTextOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"Data2VecTextConfig\", \"Data2VecTextOnnxConfig\"]\n+__all__ = [\"Data2VecTextConfig\"]"
        },
        {
            "sha": "b75095bf230859a4da66939f1349cf55cd384559",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 24,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_vision.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"Data2VecVision model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -174,21 +168,4 @@ def __init__(\n         self.semantic_loss_ignore_index = semantic_loss_ignore_index\n \n \n-# Copied from transformers.models.vit.configuration_vit.ViTOnnxConfig\n-class Data2VecVisionOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"Data2VecVisionConfig\", \"Data2VecVisionOnnxConfig\"]\n+__all__ = [\"Data2VecVisionConfig\"]"
        },
        {
            "sha": "d37245dc0e4c1758859f582ca6d435b6ae7ef174",
            "filename": "src/transformers/models/deberta/configuration_deberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 47,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fconfiguration_deberta.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,19 +14,10 @@\n # limitations under the License.\n \"\"\"DeBERTa model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, Union\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ... import FeatureExtractionMixin, PreTrainedTokenizerBase\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -159,41 +150,4 @@ def __init__(\n         self.legacy = legacy\n \n \n-# Copied from transformers.models.deberta_v2.configuration_deberta_v2.DebertaV2OnnxConfig\n-class DebertaOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        if self._config.type_vocab_size > 0:\n-            return OrderedDict(\n-                [(\"input_ids\", dynamic_axis), (\"attention_mask\", dynamic_axis), (\"token_type_ids\", dynamic_axis)]\n-            )\n-        else:\n-            return OrderedDict([(\"input_ids\", dynamic_axis), (\"attention_mask\", dynamic_axis)])\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-    def generate_dummy_inputs(\n-        self,\n-        preprocessor: Union[\"PreTrainedTokenizerBase\", \"FeatureExtractionMixin\"],\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        num_choices: int = -1,\n-        is_pair: bool = False,\n-        num_channels: int = 3,\n-        image_width: int = 40,\n-        image_height: int = 40,\n-        tokenizer: \"PreTrainedTokenizerBase\" = None,\n-    ) -> Mapping[str, Any]:\n-        dummy_inputs = super().generate_dummy_inputs(preprocessor=preprocessor)\n-        if self._config.type_vocab_size == 0 and \"token_type_ids\" in dummy_inputs:\n-            del dummy_inputs[\"token_type_ids\"]\n-        return dummy_inputs\n-\n-\n-__all__ = [\"DebertaConfig\", \"DebertaOnnxConfig\"]\n+__all__ = [\"DebertaConfig\"]"
        },
        {
            "sha": "0768a63fd375558e506acbfef425dd8273dcbcea",
            "filename": "src/transformers/models/deberta_v2/configuration_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 46,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fconfiguration_deberta_v2.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,19 +14,10 @@\n # limitations under the License.\n \"\"\"DeBERTa-v2 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, Union\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ... import FeatureExtractionMixin, PreTrainedTokenizerBase\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -159,40 +150,4 @@ def __init__(\n         self.legacy = legacy\n \n \n-class DebertaV2OnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        if self._config.type_vocab_size > 0:\n-            return OrderedDict(\n-                [(\"input_ids\", dynamic_axis), (\"attention_mask\", dynamic_axis), (\"token_type_ids\", dynamic_axis)]\n-            )\n-        else:\n-            return OrderedDict([(\"input_ids\", dynamic_axis), (\"attention_mask\", dynamic_axis)])\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-    def generate_dummy_inputs(\n-        self,\n-        preprocessor: Union[\"PreTrainedTokenizerBase\", \"FeatureExtractionMixin\"],\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        num_choices: int = -1,\n-        is_pair: bool = False,\n-        num_channels: int = 3,\n-        image_width: int = 40,\n-        image_height: int = 40,\n-        tokenizer: \"PreTrainedTokenizerBase\" = None,\n-    ) -> Mapping[str, Any]:\n-        dummy_inputs = super().generate_dummy_inputs(preprocessor=preprocessor)\n-        if self._config.type_vocab_size == 0 and \"token_type_ids\" in dummy_inputs:\n-            del dummy_inputs[\"token_type_ids\"]\n-        return dummy_inputs\n-\n-\n-__all__ = [\"DebertaV2Config\", \"DebertaV2OnnxConfig\"]\n+__all__ = [\"DebertaV2Config\"]"
        },
        {
            "sha": "2f7824ed4b89a51ab446dd3742a5e3fc0da1d5da",
            "filename": "src/transformers/models/deit/configuration_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fconfiguration_deit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"DeiT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -131,20 +125,4 @@ def __init__(\n         self.pooler_act = pooler_act\n \n \n-class DeiTOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"DeiTConfig\", \"DeiTOnnxConfig\"]\n+__all__ = [\"DeiTConfig\"]"
        },
        {
            "sha": "07a54f08f6533ca7c301b3bfcd5be9c67dbdd523",
            "filename": "src/transformers/models/deprecated/mega/configuration_mega.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconfiguration_mega.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"MEGA configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ....configuration_utils import PreTrainedConfig\n-from ....onnx import OnnxConfig\n from ....utils import logging\n \n \n@@ -225,19 +221,4 @@ def __init__(\n         self.num_attention_heads = 1  # not used but required by Hugging Face\n \n \n-class MegaOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"MegaConfig\", \"MegaOnnxConfig\"]\n+__all__ = [\"MegaConfig\"]"
        },
        {
            "sha": "50183409ed512bf9192c5a5394f82eec14e3762b",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"DETR model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -246,25 +240,4 @@ def __init__(\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n-class DetrOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"pixel_mask\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-5\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-\n-__all__ = [\"DetrConfig\", \"DetrOnnxConfig\"]\n+__all__ = [\"DetrConfig\"]"
        },
        {
            "sha": "d115efc5a93ad0137de7d2ec7c07c6e345bdba39",
            "filename": "src/transformers/models/dinov2/configuration_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"DINOv2 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n@@ -160,20 +154,4 @@ def __init__(\n         self.use_mask_token = use_mask_token\n \n \n-class Dinov2OnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"Dinov2Config\", \"Dinov2OnnxConfig\"]\n+__all__ = [\"Dinov2Config\"]"
        },
        {
            "sha": "e65d464f2b43c2c35479374f75aa10f56dc66d19",
            "filename": "src/transformers/models/distilbert/configuration_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fconfiguration_distilbert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"DistilBERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -123,19 +119,4 @@ def __init__(\n         super().__init__(**kwargs, pad_token_id=pad_token_id)\n \n \n-class DistilBertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"DistilBertConfig\", \"DistilBertOnnxConfig\"]\n+__all__ = [\"DistilBertConfig\"]"
        },
        {
            "sha": "ff2dc54b0063367a8b85082e7343439fd1fbd4f6",
            "filename": "src/transformers/models/efficientnet/configuration_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fconfiguration_efficientnet.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"EfficientNet model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -150,20 +144,4 @@ def __init__(\n         self.num_hidden_layers = sum(num_block_repeats) * 4\n \n \n-class EfficientNetOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-5\n-\n-\n-__all__ = [\"EfficientNetConfig\", \"EfficientNetOnnxConfig\"]\n+__all__ = [\"EfficientNetConfig\"]"
        },
        {
            "sha": "e54384c8ab5da2b13f08df7b8fcd4b36504d31b2",
            "filename": "src/transformers/models/electra/configuration_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fconfiguration_electra.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"ELECTRA model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -160,20 +156,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class ElectraOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"ElectraConfig\", \"ElectraOnnxConfig\"]\n+__all__ = [\"ElectraConfig\"]"
        },
        {
            "sha": "aa7a6fba1dde494635cc17dac0b78192a831bd3e",
            "filename": "src/transformers/models/ernie/configuration_ernie.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fconfiguration_ernie.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"ERNIE model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -135,21 +131,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class ErnieOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-                (\"task_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"ErnieConfig\", \"ErnieOnnxConfig\"]\n+__all__ = [\"ErnieConfig\"]"
        },
        {
            "sha": "968a5a2c859f4b3a9001feb09a3f87b975b89817",
            "filename": "src/transformers/models/flaubert/configuration_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fconfiguration_flaubert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"Flaubert configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -217,19 +213,4 @@ def __init__(\n         super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, **kwargs)\n \n \n-class FlaubertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"FlaubertConfig\", \"FlaubertOnnxConfig\"]\n+__all__ = [\"FlaubertConfig\"]"
        },
        {
            "sha": "5afe8dafe87a7084ea7bd661c7928b98dbec6160",
            "filename": "src/transformers/models/gpt2/configuration_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 87,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fconfiguration_gpt2.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,13 +15,7 @@\n # limitations under the License.\n \"\"\"OpenAI GPT-2 configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any, Optional\n-\n-from ... import PreTrainedTokenizer, is_torch_available\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import logging\n \n \n@@ -190,84 +184,4 @@ def __init__(\n         super().__init__(bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n \n \n-class GPT2OnnxConfig(OnnxConfigWithPast):\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        task: str = \"default\",\n-        patching_specs: Optional[list[PatchingSpec]] = None,\n-        use_past: bool = False,\n-    ):\n-        super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)\n-        if not getattr(self._config, \"pad_token_id\", None):\n-            # TODO: how to do that better?\n-            self._config.pad_token_id = 0\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n-\n-        return common_inputs\n-\n-    @property\n-    def num_layers(self) -> int:\n-        return self._config.n_layer\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self._config.n_head\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-        )\n-\n-        # We need to order the input in the way they appears in the forward()\n-        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n-\n-        # Need to add the past_keys\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-\n-                batch, seqlen = common_inputs[\"input_ids\"].shape\n-                # Not using the same length for past_key_values\n-                past_key_values_length = seqlen + 2\n-                past_shape = (\n-                    batch,\n-                    self.num_attention_heads,\n-                    past_key_values_length,\n-                    self._config.hidden_size // self.num_attention_heads,\n-                )\n-                ordered_inputs[\"past_key_values\"] = [\n-                    (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(self.num_layers)\n-                ]\n-\n-        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n-        if self.use_past:\n-            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n-            ordered_inputs[\"attention_mask\"] = torch.cat(\n-                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-\n-        return ordered_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"GPT2Config\", \"GPT2OnnxConfig\"]\n+__all__ = [\"GPT2Config\"]"
        },
        {
            "sha": "de991be3790b51212ef04ab50a6e4f7ce525a146",
            "filename": "src/transformers/models/gpt_neo/configuration_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 74,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fconfiguration_gpt_neo.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"GPT Neo model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer, is_torch_available\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfigWithPast\n from ...utils import logging\n \n \n@@ -205,71 +199,4 @@ def custom_get_block_length_and_num_blocks(seq_length, window_size):\n     return largest_divisor, torch.div(seq_length, largest_divisor, rounding_mode=\"floor\")\n \n \n-class GPTNeoOnnxConfig(OnnxConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n-\n-        return common_inputs\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self._config.num_heads\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-\n-        # We need to order the input in the way they appears in the forward()\n-        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n-\n-        # Need to add the past_keys\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-\n-                batch, seqlen = common_inputs[\"input_ids\"].shape\n-                # Not using the same length for past_key_values\n-                past_key_values_length = seqlen + 2\n-                past_shape = (\n-                    batch,\n-                    self.num_attention_heads,\n-                    past_key_values_length,\n-                    self._config.hidden_size // self.num_attention_heads,\n-                )\n-                ordered_inputs[\"past_key_values\"] = [\n-                    (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(self.num_layers)\n-                ]\n-\n-        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n-        if self.use_past:\n-            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n-            ordered_inputs[\"attention_mask\"] = torch.cat(\n-                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-\n-        return ordered_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"GPTNeoConfig\", \"GPTNeoOnnxConfig\"]\n+__all__ = [\"GPTNeoConfig\"]"
        },
        {
            "sha": "96aea6dad6103ff3dfdc469d5b0ad1b623885601",
            "filename": "src/transformers/models/gptj/configuration_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 88,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fconfiguration_gptj.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"GPT-J model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any, Optional\n-\n-from ... import PreTrainedTokenizer, is_torch_available\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfigWithPast, PatchingSpec\n from ...utils import logging\n \n \n@@ -135,85 +129,4 @@ def __init__(\n         )\n \n \n-# Copied from transformers.models.gpt2.configuration_gpt2.GPT2OnnxConfig\n-class GPTJOnnxConfig(OnnxConfigWithPast):\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        task: str = \"default\",\n-        patching_specs: Optional[list[PatchingSpec]] = None,\n-        use_past: bool = False,\n-    ):\n-        super().__init__(config, task=task, patching_specs=patching_specs, use_past=use_past)\n-        if not getattr(self._config, \"pad_token_id\", None):\n-            # TODO: how to do that better?\n-            self._config.pad_token_id = 0\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict({\"input_ids\": {0: \"batch\", 1: \"sequence\"}})\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"sequence\"}\n-\n-        return common_inputs\n-\n-    @property\n-    def num_layers(self) -> int:\n-        return self._config.n_layer\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self._config.n_head\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-        )\n-\n-        # We need to order the input in the way they appears in the forward()\n-        ordered_inputs = OrderedDict({\"input_ids\": common_inputs[\"input_ids\"]})\n-\n-        # Need to add the past_keys\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-\n-                batch, seqlen = common_inputs[\"input_ids\"].shape\n-                # Not using the same length for past_key_values\n-                past_key_values_length = seqlen + 2\n-                past_shape = (\n-                    batch,\n-                    self.num_attention_heads,\n-                    past_key_values_length,\n-                    self._config.hidden_size // self.num_attention_heads,\n-                )\n-                ordered_inputs[\"past_key_values\"] = [\n-                    (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(self.num_layers)\n-                ]\n-\n-        ordered_inputs[\"attention_mask\"] = common_inputs[\"attention_mask\"]\n-        if self.use_past:\n-            mask_dtype = ordered_inputs[\"attention_mask\"].dtype\n-            ordered_inputs[\"attention_mask\"] = torch.cat(\n-                [ordered_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-\n-        return ordered_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"GPTJConfig\", \"GPTJOnnxConfig\"]\n+__all__ = [\"GPTJConfig\"]"
        },
        {
            "sha": "e33791fdb72d35c50c11c2605b6be91b5dda140b",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,19 +14,10 @@\n # limitations under the License.\n \"\"\"GroupViT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ...processing_utils import ProcessorMixin\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -360,52 +351,4 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class GroupViTOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"logits_per_image\", {0: \"batch\"}),\n-                (\"logits_per_text\", {0: \"batch\"}),\n-                (\"text_embeds\", {0: \"batch\"}),\n-                (\"image_embeds\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    def generate_dummy_inputs(\n-        self,\n-        processor: \"ProcessorMixin\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-    ) -> Mapping[str, Any]:\n-        text_input_dict = super().generate_dummy_inputs(\n-            processor.tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-        )\n-        image_input_dict = super().generate_dummy_inputs(\n-            processor.image_processor,\n-            batch_size=batch_size,\n-        )\n-        return {**text_input_dict, **image_input_dict}\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 14\n-\n-\n-__all__ = [\"GroupViTConfig\", \"GroupViTOnnxConfig\", \"GroupViTTextConfig\", \"GroupViTVisionConfig\"]\n+__all__ = [\"GroupViTConfig\", \"GroupViTTextConfig\", \"GroupViTVisionConfig\"]"
        },
        {
            "sha": "c2c4f40fb933b18c1fffa62f951772bc90894ff5",
            "filename": "src/transformers/models/ibert/configuration_ibert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -16,11 +16,7 @@\n # limitations under the License.\n \"\"\"I-BERT configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -116,19 +112,4 @@ def __init__(\n         self.force_dequant = force_dequant\n \n \n-class IBertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"IBertConfig\", \"IBertOnnxConfig\"]\n+__all__ = [\"IBertConfig\"]"
        },
        {
            "sha": "6ec0d3338ca8e8041d3f1b40e3e725fe3339db52",
            "filename": "src/transformers/models/imagegpt/configuration_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 59,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fconfiguration_imagegpt.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,18 +14,10 @@\n # limitations under the License.\n \"\"\"OpenAI ImageGPT configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ... import FeatureExtractionMixin\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -144,54 +136,4 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-class ImageGPTOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    def generate_dummy_inputs(\n-        self,\n-        preprocessor: \"FeatureExtractionMixin\",\n-        batch_size: int = 1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-        num_channels: int = 3,\n-        image_width: int = 32,\n-        image_height: int = 32,\n-    ) -> Mapping[str, Any]:\n-        \"\"\"\n-        Generate inputs to provide to the ONNX exporter.\n-\n-        Args:\n-            preprocessor ([`PreTrainedTokenizerBase`] or [`FeatureExtractionMixin`]):\n-                The preprocessor associated with this model configuration.\n-            batch_size (`int`, *optional*, defaults to -1):\n-                The batch size to export the model for (-1 means dynamic axis).\n-            num_choices (`int`, *optional*, defaults to -1):\n-                The number of candidate answers provided for multiple choice task (-1 means dynamic axis).\n-            seq_length (`int`, *optional*, defaults to -1):\n-                The sequence length to export the model for (-1 means dynamic axis).\n-            is_pair (`bool`, *optional*, defaults to `False`):\n-                Indicate if the input is a pair (sentence 1, sentence 2)\n-            num_channels (`int`, *optional*, defaults to 3):\n-                The number of channels of the generated images.\n-            image_width (`int`, *optional*, defaults to 40):\n-                The width of the generated images.\n-            image_height (`int`, *optional*, defaults to 40):\n-                The height of the generated images.\n-\n-        Returns:\n-            Mapping[str, Tensor] holding the kwargs to provide to the model's forward function\n-        \"\"\"\n-\n-        input_image = self._generate_dummy_images(batch_size, num_channels, image_height, image_width)\n-        inputs = dict(preprocessor(images=input_image, return_tensors=\"pt\"))\n-\n-        return inputs\n-\n-\n-__all__ = [\"ImageGPTConfig\", \"ImageGPTOnnxConfig\"]\n+__all__ = [\"ImageGPTConfig\"]"
        },
        {
            "sha": "d76f8e2b3a59fc8de8c7aca930fafdf368a36f1f",
            "filename": "src/transformers/models/layoutlm/configuration_layoutlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 68,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fconfiguration_layoutlm.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,8 @@\n # limitations under the License.\n \"\"\"LayoutLM model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any, Optional\n-\n-from ... import PreTrainedConfig, PreTrainedTokenizer\n-from ...onnx import OnnxConfig, PatchingSpec\n-from ...utils import is_torch_available, logging\n+from ... import PreTrainedConfig\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -127,64 +122,4 @@ def __init__(\n         self.max_2d_position_embeddings = max_2d_position_embeddings\n \n \n-class LayoutLMOnnxConfig(OnnxConfig):\n-    def __init__(\n-        self,\n-        config: PreTrainedConfig,\n-        task: str = \"default\",\n-        patching_specs: Optional[list[PatchingSpec]] = None,\n-    ):\n-        super().__init__(config, task=task, patching_specs=patching_specs)\n-        self.max_2d_positions = config.max_2d_position_embeddings - 1\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"bbox\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"token_type_ids\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        \"\"\"\n-        Generate inputs to provide to the ONNX exporter\n-\n-        Args:\n-            tokenizer: The tokenizer associated with this model configuration\n-            batch_size: The batch size (int) to export the model for (-1 means dynamic axis)\n-            seq_length: The sequence length (int) to export the model for (-1 means dynamic axis)\n-            is_pair: Indicate if the input is a pair (sentence 1, sentence 2)\n-\n-        Returns:\n-            Mapping[str, Tensor] holding the kwargs to provide to the model's forward function\n-        \"\"\"\n-\n-        input_dict = super().generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-\n-        # Generate a dummy bbox\n-        box = [48, 84, 73, 128]\n-\n-        if not is_torch_available():\n-            raise ValueError(\"Cannot generate dummy inputs without PyTorch installed.\")\n-        import torch\n-\n-        batch_size, seq_length = input_dict[\"input_ids\"].shape\n-        input_dict[\"bbox\"] = torch.tensor([*[box] * seq_length]).tile(batch_size, 1, 1)\n-        return input_dict\n-\n-\n-__all__ = [\"LayoutLMConfig\", \"LayoutLMOnnxConfig\"]\n+__all__ = [\"LayoutLMConfig\"]"
        },
        {
            "sha": "d67d4a446422524d0db01181644bd2419bd2a130",
            "filename": "src/transformers/models/layoutlmv3/configuration_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 113,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fconfiguration_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fconfiguration_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fconfiguration_layoutlmv3.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,22 +14,10 @@\n # limitations under the License.\n \"\"\"LayoutLMv3 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n-from ...onnx.utils import compute_effective_axis_dimension\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ...processing_utils import ProcessorMixin\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -187,104 +175,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class LayoutLMv3OnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.12\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        # The order of inputs is different for question answering and sequence classification\n-        if self.task in [\"question-answering\", \"sequence-classification\"]:\n-            return OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"bbox\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                ]\n-            )\n-        else:\n-            return OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"bbox\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"pixel_values\", {0: \"batch\", 1: \"num_channels\"}),\n-                ]\n-            )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-5\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-    def generate_dummy_inputs(\n-        self,\n-        processor: \"ProcessorMixin\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-        num_channels: int = 3,\n-        image_width: int = 40,\n-        image_height: int = 40,\n-    ) -> Mapping[str, Any]:\n-        \"\"\"\n-        Generate inputs to provide to the ONNX exporter\n-\n-        Args:\n-            processor ([`ProcessorMixin`]):\n-                The processor associated with this model configuration.\n-            batch_size (`int`, *optional*, defaults to -1):\n-                The batch size to export the model for (-1 means dynamic axis).\n-            seq_length (`int`, *optional*, defaults to -1):\n-                The sequence length to export the model for (-1 means dynamic axis).\n-            is_pair (`bool`, *optional*, defaults to `False`):\n-                Indicate if the input is a pair (sentence 1, sentence 2).\n-            num_channels (`int`, *optional*, defaults to 3):\n-                The number of channels of the generated images.\n-            image_width (`int`, *optional*, defaults to 40):\n-                The width of the generated images.\n-            image_height (`int`, *optional*, defaults to 40):\n-                The height of the generated images.\n-\n-        Returns:\n-            Mapping[str, Any]: holding the kwargs to provide to the model's forward function\n-        \"\"\"\n-\n-        # A dummy image is used so OCR should not be applied\n-        setattr(processor.image_processor, \"apply_ocr\", False)\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = processor.tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_text = [[\" \".join([processor.tokenizer.unk_token]) * seq_length]] * batch_size\n-\n-        # Generate dummy bounding boxes\n-        dummy_bboxes = [[[48, 84, 73, 128]]] * batch_size\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        # batch_size = compute_effective_axis_dimension(batch_size, fixed_dimension=OnnxConfig.default_fixed_batch)\n-        dummy_image = self._generate_dummy_images(batch_size, num_channels, image_height, image_width)\n-\n-        inputs = dict(\n-            processor(\n-                dummy_image,\n-                text=dummy_text,\n-                boxes=dummy_bboxes,\n-                return_tensors=\"pt\",\n-            )\n-        )\n-\n-        return inputs\n-\n-\n-__all__ = [\"LayoutLMv3Config\", \"LayoutLMv3OnnxConfig\"]\n+__all__ = [\"LayoutLMv3Config\"]"
        },
        {
            "sha": "54d72ddd06f632f40e28c90e5c3239f3e7b53b9f",
            "filename": "src/transformers/models/levit/configuration_levit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 24,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flevit%2Fconfiguration_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flevit%2Fconfiguration_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fconfiguration_levit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"LeViT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -124,21 +118,4 @@ def __init__(\n         ]\n \n \n-# Copied from transformers.models.vit.configuration_vit.ViTOnnxConfig\n-class LevitOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"LevitConfig\", \"LevitOnnxConfig\"]\n+__all__ = [\"LevitConfig\"]"
        },
        {
            "sha": "8755e76ba3e524e40f29e1069b0786ea17a26abb",
            "filename": "src/transformers/models/longformer/configuration_longformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 77,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fconfiguration_longformer.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,20 +14,12 @@\n # limitations under the License.\n \"\"\"Longformer configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import Union\n \n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ...onnx.config import PatchingSpec\n-    from ...tokenization_utils_base import PreTrainedTokenizerBase\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -139,71 +131,4 @@ def __init__(\n         self.onnx_export = onnx_export\n \n \n-class LongformerOnnxConfig(OnnxConfig):\n-    def __init__(\n-        self, config: \"PreTrainedConfig\", task: str = \"default\", patching_specs: \"Optional[list[PatchingSpec]]\" = None\n-    ):\n-        super().__init__(config, task, patching_specs)\n-        config.onnx_export = True\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"global_attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        outputs = super().outputs\n-        if self.task == \"default\":\n-            outputs[\"pooler_output\"] = {0: \"batch\"}\n-        return outputs\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        \"\"\"\n-        What absolute tolerance value to use during model conversion validation.\n-\n-        Returns:\n-            Float absolute tolerance value.\n-        \"\"\"\n-        return 1e-4\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        # needs to be >= 14 to support tril operator\n-        return max(super().default_onnx_opset, 14)\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: \"PreTrainedTokenizerBase\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        inputs = super().generate_dummy_inputs(\n-            preprocessor=tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-        import torch\n-\n-        # for some reason, replacing this code by inputs[\"global_attention_mask\"] = torch.randint(2, inputs[\"input_ids\"].shape, dtype=torch.int64)\n-        # makes the export fail randomly\n-        inputs[\"global_attention_mask\"] = torch.zeros_like(inputs[\"input_ids\"])\n-        # make every second token global\n-        inputs[\"global_attention_mask\"][:, ::2] = 1\n-\n-        return inputs\n-\n-\n-__all__ = [\"LongformerConfig\", \"LongformerOnnxConfig\"]\n+__all__ = [\"LongformerConfig\"]"
        },
        {
            "sha": "2f293b04e706f41744f9794c49f2e568cf2685a5",
            "filename": "src/transformers/models/longt5/configuration_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,10 +14,7 @@\n # limitations under the License.\n \"\"\"LongT5 model configuration\"\"\"\n \n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxSeq2SeqConfigWithPast\n from ...utils import logging\n \n \n@@ -152,29 +149,4 @@ def __init__(\n         )\n \n \n-class LongT5OnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = {\n-            \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n-            \"attention_mask\": {0: \"batch\", 1: \"encoder_sequence\"},\n-        }\n-        if self.use_past:\n-            common_inputs[\"attention_mask\"][1] = \"past_encoder_sequence + sequence\"\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        else:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-\n-        return common_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"LongT5Config\", \"LongT5OnnxConfig\"]\n+__all__ = [\"LongT5Config\"]"
        },
        {
            "sha": "43e775873d94c26aceec1f037ad10ca2a5aed019",
            "filename": "src/transformers/models/m2m_100/configuration_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 130,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fconfiguration_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fconfiguration_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fconfiguration_m2m_100.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,8 @@\n # limitations under the License.\n \"\"\"M2M100 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n-from ...utils import is_torch_available, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -158,125 +151,4 @@ def __init__(\n         )\n \n \n-class M2M100OnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-            ]\n-        )\n-\n-        if self.use_past:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        else:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        return common_inputs\n-\n-    # Copied from BartOnnxConfig._generate_dummy_inputs_for_sequence_classification_and_question_answering\n-    # A better name would be _generate_dummy_inputs_for_encoder_and_decoder because sequence classification and question\n-    # answering are not supported for M2M100, but this name is preserved to be able to check that the copy matches what\n-    # was done for BART so that it can be updated if need be.\n-    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig._generate_dummy_inputs_for_default_and_seq2seq_lm\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length + 3\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-        return common_inputs\n-\n-    generate_dummy_inputs = _generate_dummy_inputs_for_default_and_seq2seq_lm\n-\n-\n-__all__ = [\"M2M100Config\", \"M2M100OnnxConfig\"]\n+__all__ = [\"M2M100Config\"]"
        },
        {
            "sha": "523c0da89195aaf2ad1953f0ab3b5150e7a29419",
            "filename": "src/transformers/models/marian/configuration_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 248,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fconfiguration_marian.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,8 @@\n # limitations under the License.\n \"\"\"Marian model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n-from ...utils import is_torch_available, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -164,243 +157,4 @@ def __init__(\n         )\n \n \n-class MarianOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig.inputs\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-\n-            if self.use_past:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-            else:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-            if self.use_past:\n-                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        elif self.task == \"causal-lm\":\n-            # TODO: figure this case out.\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                ]\n-            )\n-\n-        return common_inputs\n-\n-    @property\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig.outputs\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_outputs = super().outputs\n-        else:\n-            common_outputs = super(OnnxConfigWithPast, self).outputs\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        return common_outputs\n-\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_encoder_and_decoder(\n-            tokenizer,\n-            batch_size,\n-            seq_length,\n-            is_pair,\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_encoder_and_decoder(\n-            tokenizer,\n-            batch_size,\n-            decoder_seq_length,\n-            is_pair,\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length + 3\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_causal_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = self._generate_dummy_inputs_for_encoder_and_decoder(\n-            tokenizer,\n-            batch_size,\n-            seq_length,\n-            is_pair,\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            # Not using the same length for past_key_values\n-            past_key_values_length = seqlen + 2\n-            num_encoder_layers, _ = self.num_layers\n-            num_encoder_attention_heads, _ = self.num_attention_heads\n-            past_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-\n-            mask_dtype = common_inputs[\"attention_mask\"].dtype\n-            common_inputs[\"attention_mask\"] = torch.cat(\n-                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = [\n-                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_encoder_layers)\n-            ]\n-        return common_inputs\n-\n-    # Copied from BartOnnxConfig._generate_dummy_inputs_for_sequence_classification_and_question_answering\n-    # We renamed this function because Marian models do not have a sequence classification or question answering head\n-    def _generate_dummy_inputs_for_encoder_and_decoder(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer,\n-                batch_size=batch_size,\n-                seq_length=seq_length,\n-                is_pair=is_pair,\n-            )\n-\n-        else:\n-            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer,\n-                batch_size=batch_size,\n-                seq_length=seq_length,\n-                is_pair=is_pair,\n-            )\n-\n-        return common_inputs\n-\n-    # Copied from transformers.models.bart.configuration_bart.BartOnnxConfig._flatten_past_key_values_\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n-                flattened_output, name, idx, t\n-            )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"MarianConfig\", \"MarianOnnxConfig\"]\n+__all__ = [\"MarianConfig\"]"
        },
        {
            "sha": "dba725a3d0203b58e9e5ff54d8594242d13c3725",
            "filename": "src/transformers/models/mbart/configuration_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 229,
            "changes": 231,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fconfiguration_mbart.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,8 @@\n # limitations under the License.\n \"\"\"MBART model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any\n-\n-from ... import PreTrainedTokenizer\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig, OnnxConfigWithPast, OnnxSeq2SeqConfigWithPast\n-from ...onnx.utils import compute_effective_axis_dimension\n-from ...utils import is_torch_available, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -164,224 +157,4 @@ def __init__(\n         )\n \n \n-# Copied from transformers.models.bart.configuration_bart.BartOnnxConfig with Bart->MBart\n-class MBartOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-\n-            if self.use_past:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-            else:\n-                common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-                common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-            if self.use_past:\n-                self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-        elif self.task == \"causal-lm\":\n-            # TODO: figure this case out.\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                ]\n-            )\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_inputs[f\"past_key_values.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_inputs[f\"past_key_values.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        else:\n-            common_inputs = OrderedDict(\n-                [\n-                    (\"input_ids\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"attention_mask\", {0: \"batch\", 1: \"encoder_sequence\"}),\n-                    (\"decoder_input_ids\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                    (\"decoder_attention_mask\", {0: \"batch\", 1: \"decoder_sequence\"}),\n-                ]\n-            )\n-\n-        return common_inputs\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_outputs = super().outputs\n-        else:\n-            common_outputs = super(OnnxConfigWithPast, self).outputs\n-            if self.use_past:\n-                num_encoder_layers, _ = self.num_layers\n-                for i in range(num_encoder_layers):\n-                    common_outputs[f\"present.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-                    common_outputs[f\"present.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-        return common_outputs\n-\n-    def _generate_dummy_inputs_for_default_and_seq2seq_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, decoder_seq_length, is_pair\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, encoder_seq_length = common_inputs[\"input_ids\"].shape\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_past_length = decoder_seq_length + 3\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                decoder_past_length,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"decoder_attention_mask\"] = torch.cat(\n-                [common_inputs[\"decoder_attention_mask\"], torch.ones(batch, decoder_past_length)], dim=1\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_causal_lm(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-            tokenizer, batch_size, seq_length, is_pair\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            # Not using the same length for past_key_values\n-            past_key_values_length = seqlen + 2\n-            num_encoder_layers, _ = self.num_layers\n-            num_encoder_attention_heads, _ = self.num_attention_heads\n-            past_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-\n-            mask_dtype = common_inputs[\"attention_mask\"].dtype\n-            common_inputs[\"attention_mask\"] = torch.cat(\n-                [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)], dim=1\n-            )\n-            common_inputs[\"past_key_values\"] = [\n-                (torch.zeros(past_shape), torch.zeros(past_shape)) for _ in range(num_encoder_layers)\n-            ]\n-        return common_inputs\n-\n-    def _generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # Copied from OnnxConfig.generate_dummy_inputs\n-        # Did not use super(OnnxConfigWithPast, self).generate_dummy_inputs for code clarity.\n-        # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-        batch_size = compute_effective_axis_dimension(\n-            batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-        )\n-\n-        # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-        token_to_add = tokenizer.num_special_tokens_to_add(is_pair)\n-        seq_length = compute_effective_axis_dimension(\n-            seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-        )\n-\n-        # Generate dummy inputs according to compute batch and sequence\n-        dummy_input = [\" \".join([tokenizer.unk_token]) * seq_length] * batch_size\n-        common_inputs = dict(tokenizer(dummy_input, return_tensors=\"pt\"))\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: PreTrainedTokenizer,\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            common_inputs = self._generate_dummy_inputs_for_default_and_seq2seq_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        elif self.task == \"causal-lm\":\n-            common_inputs = self._generate_dummy_inputs_for_causal_lm(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-        else:\n-            common_inputs = self._generate_dummy_inputs_for_sequence_classification_and_question_answering(\n-                tokenizer, batch_size=batch_size, seq_length=seq_length, is_pair=is_pair\n-            )\n-\n-        return common_inputs\n-\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        if self.task in [\"default\", \"seq2seq-lm\"]:\n-            flattened_output = super()._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super(OnnxSeq2SeqConfigWithPast, self)._flatten_past_key_values_(\n-                flattened_output, name, idx, t\n-            )\n-\n-\n-__all__ = [\"MBartConfig\", \"MBartOnnxConfig\"]\n+__all__ = [\"MBartConfig\"]"
        },
        {
            "sha": "df02924f6b53839bbe0839a3156bcf287a49cf5a",
            "filename": "src/transformers/models/metaclip_2/configuration_metaclip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -4,7 +4,6 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_metaclip_2.py file directly. One of our CI enforces this.\n #                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n "
        },
        {
            "sha": "c7ae058de31301afd071efd8983cb4380ea2f4dd",
            "filename": "src/transformers/models/mobilebert/configuration_mobilebert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fconfiguration_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fconfiguration_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fconfiguration_mobilebert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"MobileBERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -164,21 +160,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-# Copied from transformers.models.bert.configuration_bert.BertOnnxConfig with Bert->MobileBert\n-class MobileBertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"MobileBertConfig\", \"MobileBertOnnxConfig\"]\n+__all__ = [\"MobileBertConfig\"]"
        },
        {
            "sha": "6cfe4b7c07f101763d3e660265eb6afb4c282293",
            "filename": "src/transformers/models/mobilenet_v1/configuration_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fconfiguration_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fconfiguration_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fconfiguration_mobilenet_v1.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"MobileNetV1 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -104,23 +98,4 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n \n \n-class MobileNetV1OnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict([(\"pixel_values\", {0: \"batch\"})])\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"image-classification\":\n-            return OrderedDict([(\"logits\", {0: \"batch\"})])\n-        else:\n-            return OrderedDict([(\"last_hidden_state\", {0: \"batch\"}), (\"pooler_output\", {0: \"batch\"})])\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"MobileNetV1Config\", \"MobileNetV1OnnxConfig\"]\n+__all__ = [\"MobileNetV1Config\"]"
        },
        {
            "sha": "587d70983ed49c37042c8a1034a5719836cdd719",
            "filename": "src/transformers/models/mobilenet_v2/configuration_mobilenet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fconfiguration_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fconfiguration_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fconfiguration_mobilenet_v2.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"MobileNetV2 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -132,23 +126,4 @@ def __init__(\n         self.semantic_loss_ignore_index = semantic_loss_ignore_index\n \n \n-class MobileNetV2OnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict([(\"pixel_values\", {0: \"batch\"})])\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"image-classification\":\n-            return OrderedDict([(\"logits\", {0: \"batch\"})])\n-        else:\n-            return OrderedDict([(\"last_hidden_state\", {0: \"batch\"}), (\"pooler_output\", {0: \"batch\"})])\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"MobileNetV2Config\", \"MobileNetV2OnnxConfig\"]\n+__all__ = [\"MobileNetV2Config\"]"
        },
        {
            "sha": "cd1787c2403c1cfb1aac6cafd77c506e09737e17",
            "filename": "src/transformers/models/mobilevit/configuration_mobilevit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconfiguration_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconfiguration_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconfiguration_mobilevit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"MobileViT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -150,23 +144,4 @@ def __init__(\n         self.semantic_loss_ignore_index = semantic_loss_ignore_index\n \n \n-class MobileViTOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict([(\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"})])\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"image-classification\":\n-            return OrderedDict([(\"logits\", {0: \"batch\"})])\n-        else:\n-            return OrderedDict([(\"last_hidden_state\", {0: \"batch\"}), (\"pooler_output\", {0: \"batch\"})])\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"MobileViTConfig\", \"MobileViTOnnxConfig\"]\n+__all__ = [\"MobileViTConfig\"]"
        },
        {
            "sha": "06bf209cf6d5284c67757a133ae2382a743e6640",
            "filename": "src/transformers/models/mobilevitv2/configuration_mobilevitv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconfiguration_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconfiguration_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconfiguration_mobilevitv2.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"MobileViTV2 model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -146,23 +140,4 @@ def __init__(\n         self.semantic_loss_ignore_index = semantic_loss_ignore_index\n \n \n-class MobileViTV2OnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict([(\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"})])\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"image-classification\":\n-            return OrderedDict([(\"logits\", {0: \"batch\"})])\n-        else:\n-            return OrderedDict([(\"last_hidden_state\", {0: \"batch\"}), (\"pooler_output\", {0: \"batch\"})])\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"MobileViTV2Config\", \"MobileViTV2OnnxConfig\"]\n+__all__ = [\"MobileViTV2Config\"]"
        },
        {
            "sha": "eb2cb7590bab0a075f0512fb6d2d3cd303e56668",
            "filename": "src/transformers/models/mt5/configuration_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 35,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fconfiguration_mt5.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,10 +14,7 @@\n # limitations under the License.\n \"\"\"mT5 model configuration\"\"\"\n \n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxSeq2SeqConfigWithPast\n from ...utils import logging\n \n \n@@ -148,35 +145,4 @@ def __init__(\n         )\n \n \n-class MT5OnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    # Copied from transformers.models.t5.configuration_t5.T5OnnxConfig.inputs\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = {\n-            \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n-            \"attention_mask\": {0: \"batch\", 1: \"encoder_sequence\"},\n-        }\n-        if self.use_past:\n-            common_inputs[\"attention_mask\"][1] = \"past_encoder_sequence + sequence\"\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        else:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-\n-        return common_inputs\n-\n-    @property\n-    # Copied from transformers.models.t5.configuration_t5.T5OnnxConfig.default_onnx_opset\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 5e-4\n-\n-\n-__all__ = [\"MT5Config\", \"MT5OnnxConfig\"]\n+__all__ = [\"MT5Config\"]"
        },
        {
            "sha": "720873e8e47385d864651d25392119bd1d843f97",
            "filename": "src/transformers/models/owlvit/configuration_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconfiguration_owlvit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,16 +14,7 @@\n # limitations under the License.\n \"\"\"OWL-ViT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n-\n-if TYPE_CHECKING:\n-    from ...processing_utils import ProcessorMixin\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -274,52 +265,4 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class OwlViTOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"logits_per_image\", {0: \"batch\"}),\n-                (\"logits_per_text\", {0: \"batch\"}),\n-                (\"text_embeds\", {0: \"batch\"}),\n-                (\"image_embeds\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    def generate_dummy_inputs(\n-        self,\n-        processor: \"ProcessorMixin\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-    ) -> Mapping[str, Any]:\n-        text_input_dict = super().generate_dummy_inputs(\n-            processor.tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-        )\n-        image_input_dict = super().generate_dummy_inputs(\n-            processor.image_processor,\n-            batch_size=batch_size,\n-        )\n-        return {**text_input_dict, **image_input_dict}\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 14\n-\n-\n-__all__ = [\"OwlViTConfig\", \"OwlViTOnnxConfig\", \"OwlViTTextConfig\", \"OwlViTVisionConfig\"]\n+__all__ = [\"OwlViTConfig\", \"OwlViTTextConfig\", \"OwlViTVisionConfig\"]"
        },
        {
            "sha": "d9357ea52eda89ef225a7ff6d3f8f7bad05c7b10",
            "filename": "src/transformers/models/perceiver/configuration_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 68,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fperceiver%2Fconfiguration_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fperceiver%2Fconfiguration_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fconfiguration_perceiver.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,15 +14,7 @@\n # limitations under the License.\n \"\"\"Perceiver model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import Any, Union\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...feature_extraction_utils import FeatureExtractionMixin\n-from ...onnx import OnnxConfig\n-from ...onnx.utils import compute_effective_axis_dimension\n-from ...tokenization_utils_base import PreTrainedTokenizerBase\n from ...utils import logging\n \n \n@@ -182,63 +174,4 @@ def __init__(\n         self._label_trainable_num_channels = _label_trainable_num_channels\n \n \n-class PerceiverOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"inputs\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    def generate_dummy_inputs(\n-        self,\n-        preprocessor: Union[\"PreTrainedTokenizerBase\", \"FeatureExtractionMixin\"],\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        num_choices: int = -1,\n-        is_pair: bool = False,\n-        num_channels: int = 3,\n-        image_width: int = 40,\n-        image_height: int = 40,\n-    ) -> Mapping[str, Any]:\n-        # copied from `transformers.onnx.config.OnnxConfig` and slightly altered/simplified\n-\n-        if isinstance(preprocessor, PreTrainedTokenizerBase):\n-            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-            batch_size = compute_effective_axis_dimension(\n-                batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-            )\n-            # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-            token_to_add = preprocessor.num_special_tokens_to_add(is_pair)\n-            seq_length = compute_effective_axis_dimension(\n-                seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-            )\n-            # Generate dummy inputs according to compute batch and sequence\n-            dummy_input = [\" \".join([\"a\"]) * seq_length] * batch_size\n-            inputs = dict(preprocessor(dummy_input, return_tensors=\"pt\"))\n-            inputs[\"inputs\"] = inputs.pop(\"input_ids\")\n-            return inputs\n-        elif isinstance(preprocessor, FeatureExtractionMixin) and preprocessor.model_input_names[0] == \"pixel_values\":\n-            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-            batch_size = compute_effective_axis_dimension(batch_size, fixed_dimension=OnnxConfig.default_fixed_batch)\n-            dummy_input = self._generate_dummy_images(batch_size, num_channels, image_height, image_width)\n-            inputs = dict(preprocessor(images=dummy_input, return_tensors=\"pt\"))\n-            inputs[\"inputs\"] = inputs.pop(\"pixel_values\")\n-            return inputs\n-        else:\n-            raise ValueError(\n-                \"Unable to generate dummy inputs for the model. Please provide a tokenizer or a preprocessor.\"\n-            )\n-\n-\n-__all__ = [\"PerceiverConfig\", \"PerceiverOnnxConfig\"]\n+__all__ = [\"PerceiverConfig\"]"
        },
        {
            "sha": "374be4a50c222c794dc0140682c32074534ad6f7",
            "filename": "src/transformers/models/plbart/configuration_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fconfiguration_plbart.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"PLBART model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfigWithPast\n from ...utils import logging\n \n \n@@ -165,33 +161,4 @@ def __init__(\n         )\n \n \n-class PLBartOnnxConfig(OnnxConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", {0: \"batch\", 1: \"sequence\"}),\n-                (\"attention_mask\", {0: \"batch\", 1: \"sequence\"}),\n-            ]\n-        )\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.use_past:\n-            return OrderedDict(\n-                [\n-                    (\"last_hidden_state\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"past_keys\", {0: \"batch\", 2: \"sequence\"}),\n-                    (\"encoder_last_hidden_state\", {0: \"batch\", 1: \"sequence\"}),\n-                ]\n-            )\n-        else:\n-            return OrderedDict(\n-                [\n-                    (\"last_hidden_state\", {0: \"batch\", 1: \"sequence\"}),\n-                    (\"encoder_last_hidden_state\", {0: \"batch\", 1: \"sequence\"}),\n-                ]\n-            )\n-\n-\n __all__ = [\"PLBartConfig\"]"
        },
        {
            "sha": "df15851fc69a72e5ddb37033310d820e126e1f2d",
            "filename": "src/transformers/models/poolformer/configuration_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fconfiguration_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fconfiguration_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fconfiguration_poolformer.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"PoolFormer model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -129,20 +123,4 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-class PoolFormerOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 2e-3\n-\n-\n-__all__ = [\"PoolFormerConfig\", \"PoolFormerOnnxConfig\"]\n+__all__ = [\"PoolFormerConfig\"]"
        },
        {
            "sha": "08aed588617cd486a44c72ad43d29d302825bf35",
            "filename": "src/transformers/models/pvt/configuration_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fpvt%2Fconfiguration_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fpvt%2Fconfiguration_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fconfiguration_pvt.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -16,13 +16,9 @@\n # limitations under the License.\n \"\"\"Pvt model configuration\"\"\"\n \n-from collections import OrderedDict\n from collections.abc import Callable, Mapping\n \n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -139,24 +135,4 @@ def __init__(\n         self.qkv_bias = qkv_bias\n \n \n-class PvtOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-\n-__all__ = [\"PvtConfig\", \"PvtOnnxConfig\"]\n+__all__ = [\"PvtConfig\"]"
        },
        {
            "sha": "8a60237d501097db9dbd09e77d1a4b832e2151bf",
            "filename": "src/transformers/models/rembert/configuration_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Frembert%2Fconfiguration_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Frembert%2Fconfiguration_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fconfiguration_rembert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"RemBERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -139,24 +135,4 @@ def __init__(\n         self.tie_word_embeddings = False\n \n \n-class RemBertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"RemBertConfig\", \"RemBertOnnxConfig\"]\n+__all__ = [\"RemBertConfig\"]"
        },
        {
            "sha": "8dd081f371efaf698f23fdfeaa3bcbbb4e6133ed",
            "filename": "src/transformers/models/resnet/configuration_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fresnet%2Fconfiguration_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fresnet%2Fconfiguration_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fconfiguration_resnet.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"ResNet model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n@@ -117,20 +111,4 @@ def __init__(\n         )\n \n \n-class ResNetOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-3\n-\n-\n-__all__ = [\"ResNetConfig\", \"ResNetOnnxConfig\"]\n+__all__ = [\"ResNetConfig\"]"
        },
        {
            "sha": "533f58e2ab001914a93c323931c9d473ac4234ff",
            "filename": "src/transformers/models/roberta/configuration_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fconfiguration_roberta.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"RoBERTa configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -129,19 +125,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-class RobertaOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"RobertaConfig\", \"RobertaOnnxConfig\"]\n+__all__ = [\"RobertaConfig\"]"
        },
        {
            "sha": "8adc3ac2c849e34da537dc2e53f39e7c583438b2",
            "filename": "src/transformers/models/roberta_prelayernorm/configuration_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fconfiguration_roberta_prelayernorm.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"RoBERTa-PreLayerNorm configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -130,20 +126,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-# Copied from transformers.models.roberta.configuration_roberta.RobertaOnnxConfig with Roberta->RobertaPreLayerNorm\n-class RobertaPreLayerNormOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"RobertaPreLayerNormConfig\", \"RobertaPreLayerNormOnnxConfig\"]\n+__all__ = [\"RobertaPreLayerNormConfig\"]"
        },
        {
            "sha": "76f32fa9cb2debc51e59ef00a3145f82ba60c534",
            "filename": "src/transformers/models/roformer/configuration_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Froformer%2Fconfiguration_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Froformer%2Fconfiguration_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fconfiguration_roformer.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"RoFormer model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -130,21 +126,4 @@ def __init__(\n         self.use_cache = use_cache\n \n \n-class RoFormerOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"RoFormerConfig\", \"RoFormerOnnxConfig\"]\n+__all__ = [\"RoFormerConfig\"]"
        },
        {
            "sha": "fb9edd142960358b23d0d70e3115cea5b1d1ffe6",
            "filename": "src/transformers/models/segformer/configuration_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 26,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fsegformer%2Fconfiguration_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fsegformer%2Fconfiguration_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fconfiguration_segformer.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,13 +15,8 @@\n \"\"\"SegFormer model configuration\"\"\"\n \n import warnings\n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n \n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -148,24 +143,4 @@ def __init__(\n         self.semantic_loss_ignore_index = semantic_loss_ignore_index\n \n \n-class SegformerOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-\n-__all__ = [\"SegformerConfig\", \"SegformerOnnxConfig\"]\n+__all__ = [\"SegformerConfig\"]"
        },
        {
            "sha": "64b386531634958c42cd33671e254b96f591ab6c",
            "filename": "src/transformers/models/squeezebert/configuration_squeezebert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fconfiguration_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fconfiguration_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Fconfiguration_squeezebert.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"SqueezeBERT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -147,21 +143,4 @@ def __init__(\n         self.output_groups = output_groups\n \n \n-# # Copied from transformers.models.bert.configuration_bert.BertOnxxConfig with Bert->SqueezeBert\n-class SqueezeBertOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"SqueezeBertConfig\", \"SqueezeBertOnnxConfig\"]\n+__all__ = [\"SqueezeBertConfig\"]"
        },
        {
            "sha": "63c5bb4580d59733253833b29ca6fe33b8e709f4",
            "filename": "src/transformers/models/swiftformer/configuration_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fconfiguration_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fconfiguration_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fconfiguration_swiftformer.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"SwiftFormer model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -129,20 +123,4 @@ def __init__(\n         self.batch_norm_eps = batch_norm_eps\n \n \n-class SwiftFormerOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"SwiftFormerConfig\", \"SwiftFormerOnnxConfig\"]\n+__all__ = [\"SwiftFormerConfig\"]"
        },
        {
            "sha": "6d87c11e96cc3ce58514687d651eb69d0724f48e",
            "filename": "src/transformers/models/swin/configuration_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fswin%2Fconfiguration_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fswin%2Fconfiguration_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fconfiguration_swin.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"Swin Transformer model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n@@ -160,20 +154,4 @@ def __init__(\n         )\n \n \n-class SwinOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"SwinConfig\", \"SwinOnnxConfig\"]\n+__all__ = [\"SwinConfig\"]"
        },
        {
            "sha": "1cf0be33b0f267e745005166b436b99f03921302",
            "filename": "src/transformers/models/t5/configuration_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fconfiguration_t5.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,10 +14,7 @@\n # limitations under the License.\n \"\"\"T5 model configuration\"\"\"\n \n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxSeq2SeqConfigWithPast\n from ...utils import logging\n \n \n@@ -143,29 +140,4 @@ def __init__(\n         )\n \n \n-class T5OnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = {\n-            \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n-            \"attention_mask\": {0: \"batch\", 1: \"encoder_sequence\"},\n-        }\n-        if self.use_past:\n-            common_inputs[\"attention_mask\"][1] = \"past_encoder_sequence + sequence\"\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        else:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-\n-        return common_inputs\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-\n-__all__ = [\"T5Config\", \"T5OnnxConfig\"]\n+__all__ = [\"T5Config\"]"
        },
        {
            "sha": "0151861f3db73c5aa7da814fa22d1ed2138563cd",
            "filename": "src/transformers/models/table_transformer/configuration_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 29,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"Table Transformer model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -247,26 +241,4 @@ def __init__(\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n \n-# Copied from transformers.models.detr.configuration_detr.DetrOnnxConfig\n-class TableTransformerOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-                (\"pixel_mask\", {0: \"batch\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-5\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-\n-__all__ = [\"TableTransformerConfig\", \"TableTransformerOnnxConfig\"]\n+__all__ = [\"TableTransformerConfig\"]"
        },
        {
            "sha": "66c7b09d60ec54c0c4983a4e145275099d8f086e",
            "filename": "src/transformers/models/umt5/configuration_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 35,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fconfiguration_umt5.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,10 +14,7 @@\n # limitations under the License.\n \"\"\"UMT5 model configuration\"\"\"\n \n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxSeq2SeqConfigWithPast\n from ...utils import logging\n \n \n@@ -147,35 +144,4 @@ def __init__(\n         )\n \n \n-class UMT5OnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    # Copied from transformers.models.t5.configuration_t5.T5OnnxConfig.inputs\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = {\n-            \"input_ids\": {0: \"batch\", 1: \"encoder_sequence\"},\n-            \"attention_mask\": {0: \"batch\", 1: \"encoder_sequence\"},\n-        }\n-        if self.use_past:\n-            common_inputs[\"attention_mask\"][1] = \"past_encoder_sequence + sequence\"\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        else:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-            common_inputs[\"decoder_attention_mask\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-\n-        return common_inputs\n-\n-    @property\n-    # Copied from transformers.models.t5.configuration_t5.T5OnnxConfig.default_onnx_opset\n-    def default_onnx_opset(self) -> int:\n-        return 13\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 5e-4\n-\n-\n-__all__ = [\"UMT5Config\", \"UMT5OnnxConfig\"]\n+__all__ = [\"UMT5Config\"]"
        },
        {
            "sha": "2e28f37127328c000d81f5372202f43b7c25bf59",
            "filename": "src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 107,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,21 +14,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n from ..auto.configuration_auto import AutoConfig\n \n \n-if TYPE_CHECKING:\n-    from ... import PreTrainedTokenizerBase\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -118,100 +108,4 @@ def from_encoder_decoder_configs(\n         return cls(encoder=encoder_config.to_dict(), decoder=decoder_config.to_dict(), **kwargs)\n \n \n-class VisionEncoderDecoderEncoderOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict({\"last_hidden_state\": {0: \"batch\", 1: \"encoder_sequence\"}})\n-\n-\n-class VisionEncoderDecoderDecoderOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict()\n-        common_inputs[\"input_ids\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        common_inputs[\"attention_mask\"] = {0: \"batch\", 1: \"past_decoder_sequence + sequence\"}\n-        common_inputs[\"encoder_hidden_states\"] = {0: \"batch\", 1: \"encoder_sequence\"}\n-\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: \"PreTrainedTokenizerBase\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        import torch\n-\n-        common_inputs = OrderedDict()\n-\n-        dummy_input = super().generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-\n-        batch, encoder_sequence = dummy_input[\"input_ids\"].shape\n-        encoder_hidden_states_shape = (batch, encoder_sequence, self._config.encoder_hidden_size)\n-        common_inputs[\"input_ids\"] = dummy_input.pop(\"input_ids\")\n-        common_inputs[\"attention_mask\"] = dummy_input.pop(\"attention_mask\")\n-        common_inputs[\"encoder_hidden_states\"] = torch.zeros(encoder_hidden_states_shape)\n-\n-        return common_inputs\n-\n-\n-class VisionEncoderDecoderOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> None:\n-        pass\n-\n-    def get_encoder_config(self, encoder_config: PreTrainedConfig) -> OnnxConfig:\n-        r\"\"\"\n-        Returns ONNX encoder config for `VisionEncoderDecoder` model.\n-\n-        Args:\n-            encoder_config (`PreTrainedConfig`):\n-                The encoder model's configuration to use when exporting to ONNX.\n-\n-        Returns:\n-            [`VisionEncoderDecoderEncoderOnnxConfig`]: An instance of the ONNX configuration object\n-        \"\"\"\n-        return VisionEncoderDecoderEncoderOnnxConfig(encoder_config)\n-\n-    def get_decoder_config(\n-        self, encoder_config: PreTrainedConfig, decoder_config: PreTrainedConfig, feature: str = \"default\"\n-    ) -> OnnxConfig:\n-        r\"\"\"\n-        Returns ONNX decoder config for `VisionEncoderDecoder` model.\n-\n-        Args:\n-            encoder_config (`PreTrainedConfig`):\n-                The encoder model's configuration to use when exporting to ONNX.\n-            decoder_config (`PreTrainedConfig`):\n-                The decoder model's configuration to use when exporting to ONNX\n-            feature (`str`, *optional*):\n-                The type of feature to export the model with.\n-\n-        Returns:\n-            [`VisionEncoderDecoderDecoderOnnxConfig`]: An instance of the ONNX configuration object.\n-        \"\"\"\n-        decoder_config.encoder_hidden_size = encoder_config.hidden_size\n-        return VisionEncoderDecoderDecoderOnnxConfig(decoder_config, feature)\n-\n-\n-__all__ = [\"VisionEncoderDecoderConfig\", \"VisionEncoderDecoderOnnxConfig\"]\n+__all__ = [\"VisionEncoderDecoderConfig\"]"
        },
        {
            "sha": "cf48c81b913603ace43e2e5c6bed0f1e4d704d8e",
            "filename": "src/transformers/models/vit/configuration_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fvit%2Fconfiguration_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fvit%2Fconfiguration_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fconfiguration_vit.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"ViT model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -130,20 +124,4 @@ def __init__(\n         self.pooler_act = pooler_act\n \n \n-class ViTOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-\n-__all__ = [\"ViTConfig\", \"ViTOnnxConfig\"]\n+__all__ = [\"ViTConfig\"]"
        },
        {
            "sha": "684ba33df8b9880d71cd26aeb6bc3d31b2c8391c",
            "filename": "src/transformers/models/whisper/configuration_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 70,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,19 +14,10 @@\n # limitations under the License.\n \"\"\"Whisper model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-from typing import TYPE_CHECKING, Any, Union\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig, OnnxSeq2SeqConfigWithPast\n from ...utils import logging\n \n \n-if TYPE_CHECKING:\n-    from ...feature_extraction_utils import FeatureExtractionMixin\n-    from ...tokenization_utils_base import PreTrainedTokenizerBase\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -285,64 +276,4 @@ def __init__(\n         )\n \n \n-class WhisperOnnxConfig(OnnxSeq2SeqConfigWithPast):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_inputs = OrderedDict(\n-            [\n-                (\"input_features\", {0: \"batch\", 1: \"feature_size\", 2: \"encoder_sequence\"}),\n-            ]\n-        )\n-        if self.use_past:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\"}\n-        else:\n-            common_inputs[\"decoder_input_ids\"] = {0: \"batch\", 1: \"decoder_sequence\"}\n-\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_inputs, direction=\"inputs\")\n-\n-        return common_inputs\n-\n-    def generate_dummy_inputs(\n-        self,\n-        preprocessor: Union[\"PreTrainedTokenizerBase\", \"FeatureExtractionMixin\"],\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-        sampling_rate: int = 22050,\n-        time_duration: float = 5.0,\n-        frequency: int = 220,\n-    ) -> Mapping[str, Any]:\n-        dummy_inputs = OrderedDict()\n-        encoder_inputs = OnnxConfig.generate_dummy_inputs(\n-            self,\n-            preprocessor=preprocessor.feature_extractor,\n-            batch_size=batch_size,\n-            sampling_rate=sampling_rate,\n-            time_duration=time_duration,\n-            frequency=frequency,\n-        )\n-        encoder_sequence_length = encoder_inputs[\"input_features\"].shape[2]\n-        seq_length = encoder_sequence_length // 2 if self.use_past else seq_length\n-\n-        decoder_inputs = super().generate_dummy_inputs(\n-            preprocessor.tokenizer,\n-            batch_size,\n-            seq_length,\n-            is_pair,\n-        )\n-\n-        dummy_inputs[\"input_features\"] = encoder_inputs.pop(\"input_features\")\n-        dummy_inputs[\"decoder_input_ids\"] = decoder_inputs.pop(\"decoder_input_ids\")\n-\n-        if \"past_key_values\" in decoder_inputs:\n-            dummy_inputs[\"past_key_values\"] = decoder_inputs.pop(\"past_key_values\")\n-\n-        return dummy_inputs\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-3\n-\n-\n-__all__ = [\"WhisperConfig\", \"WhisperOnnxConfig\"]\n+__all__ = [\"WhisperConfig\"]"
        },
        {
            "sha": "cad063e85d52610520fc2ceea3c14fc51be3bbdb",
            "filename": "src/transformers/models/xlm/configuration_xlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 22,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxlm%2Fconfiguration_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxlm%2Fconfiguration_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fconfiguration_xlm.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"XLM configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -221,21 +217,4 @@ def __init__(\n         super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, **kwargs)\n \n \n-# Copied from transformers.models.bert.configuration_bert.BertOnnxConfig\n-class XLMOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-                (\"token_type_ids\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"XLMConfig\", \"XLMOnnxConfig\"]\n+__all__ = [\"XLMConfig\"]"
        },
        {
            "sha": "7b8aae497bb022b3e1fe528a2ab693b8e99ef98b",
            "filename": "src/transformers/models/xlm_roberta/configuration_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fconfiguration_xlm_roberta.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"XLM-RoBERTa configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -130,20 +126,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-# Copied from transformers.models.roberta.configuration_roberta.RobertaOnnxConfig with Roberta->XLMRoberta\n-class XLMRobertaOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"XLMRobertaConfig\", \"XLMRobertaOnnxConfig\"]\n+__all__ = [\"XLMRobertaConfig\"]"
        },
        {
            "sha": "0cb61cfe1e9a14523f91c8adcb962f4c1ccbcae3",
            "filename": "src/transformers/models/xlm_roberta_xl/configuration_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fconfiguration_xlm_roberta_xl.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,11 +14,7 @@\n # limitations under the License.\n \"\"\"XLM_ROBERTa_XL configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -126,20 +122,4 @@ def __init__(\n         self.classifier_dropout = classifier_dropout\n \n \n-# Copied from transformers.models.roberta.configuration_roberta.RobertaOnnxConfig with Roberta->XLMRobertaXL\n-class XLMRobertaXLOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"XLMRobertaXLConfig\", \"XLMRobertaXLOnnxConfig\"]\n+__all__ = [\"XLMRobertaXLConfig\"]"
        },
        {
            "sha": "a20c565352ae20f700b98df72e08cd9223de2df0",
            "filename": "src/transformers/models/xmod/configuration_xmod.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxmod%2Fconfiguration_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fxmod%2Fconfiguration_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fconfiguration_xmod.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -15,11 +15,7 @@\n # limitations under the License.\n \"\"\"X-MOD configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -158,20 +154,4 @@ def __init__(\n         self.default_language = default_language\n \n \n-# Copied from transformers.models.roberta.configuration_roberta.RobertaOnnxConfig with Roberta->Xmod\n-class XmodOnnxConfig(OnnxConfig):\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        if self.task == \"multiple-choice\":\n-            dynamic_axis = {0: \"batch\", 1: \"choice\", 2: \"sequence\"}\n-        else:\n-            dynamic_axis = {0: \"batch\", 1: \"sequence\"}\n-        return OrderedDict(\n-            [\n-                (\"input_ids\", dynamic_axis),\n-                (\"attention_mask\", dynamic_axis),\n-            ]\n-        )\n-\n-\n-__all__ = [\"XmodConfig\", \"XmodOnnxConfig\"]\n+__all__ = [\"XmodConfig\"]"
        },
        {
            "sha": "ad0bb665c49fada408114bb941a899b2c0ef5db3",
            "filename": "src/transformers/models/yolos/configuration_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 27,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fyolos%2Fconfiguration_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Fmodels%2Fyolos%2Fconfiguration_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fconfiguration_yolos.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -14,13 +14,7 @@\n # limitations under the License.\n \"\"\"YOLOS model configuration\"\"\"\n \n-from collections import OrderedDict\n-from collections.abc import Mapping\n-\n-from packaging import version\n-\n from ...configuration_utils import PreTrainedConfig\n-from ...onnx import OnnxConfig\n from ...utils import logging\n \n \n@@ -155,24 +149,4 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n \n \n-class YolosOnnxConfig(OnnxConfig):\n-    torch_onnx_minimum_version = version.parse(\"1.11\")\n-\n-    @property\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        return OrderedDict(\n-            [\n-                (\"pixel_values\", {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}),\n-            ]\n-        )\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        return 1e-4\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        return 12\n-\n-\n-__all__ = [\"YolosConfig\", \"YolosOnnxConfig\"]\n+__all__ = [\"YolosConfig\"]"
        },
        {
            "sha": "8429699f24a0a29b33bf3c4a9717e638952a9f16",
            "filename": "src/transformers/onnx/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995435d96ace8bbf7f95623e5a7487990280fd1/src%2Ftransformers%2Fonnx%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995435d96ace8bbf7f95623e5a7487990280fd1/src%2Ftransformers%2Fonnx%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2F__init__.py?ref=5995435d96ace8bbf7f95623e5a7487990280fd1",
            "patch": "@@ -1,45 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import TYPE_CHECKING\n-\n-from ..utils import _LazyModule\n-\n-\n-_import_structure = {\n-    \"config\": [\n-        \"EXTERNAL_DATA_FORMAT_SIZE_LIMIT\",\n-        \"OnnxConfig\",\n-        \"OnnxConfigWithPast\",\n-        \"OnnxSeq2SeqConfigWithPast\",\n-        \"PatchingSpec\",\n-    ],\n-    \"utils\": [\"ParameterFormat\", \"compute_serialized_parameters_size\"],\n-}\n-\n-\n-if TYPE_CHECKING:\n-    from .config import (\n-        EXTERNAL_DATA_FORMAT_SIZE_LIMIT,\n-        OnnxConfig,\n-        OnnxConfigWithPast,\n-        OnnxSeq2SeqConfigWithPast,\n-        PatchingSpec,\n-    )\n-    from .utils import ParameterFormat, compute_serialized_parameters_size\n-\n-else:\n-    import sys\n-\n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)"
        },
        {
            "sha": "e8f6107d581cbf2e5f3fda6e4b494afead547e4e",
            "filename": "src/transformers/onnx/config.py",
            "status": "removed",
            "additions": 0,
            "deletions": 748,
            "changes": 748,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995435d96ace8bbf7f95623e5a7487990280fd1/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995435d96ace8bbf7f95623e5a7487990280fd1/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconfig.py?ref=5995435d96ace8bbf7f95623e5a7487990280fd1",
            "patch": "@@ -1,748 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import copy\n-import dataclasses\n-import warnings\n-from abc import ABC, abstractmethod\n-from collections import OrderedDict\n-from collections.abc import Callable, Iterable, Mapping\n-from typing import TYPE_CHECKING, Any, Optional, Union\n-\n-import numpy as np\n-from packaging import version\n-\n-from ..utils import is_torch_available, is_vision_available, logging\n-from .utils import ParameterFormat, compute_effective_axis_dimension, compute_serialized_parameters_size\n-\n-\n-if TYPE_CHECKING:\n-    from ..configuration_utils import PreTrainedConfig\n-    from ..feature_extraction_utils import FeatureExtractionMixin\n-    from ..image_processing_utils import ImageProcessingMixin\n-    from ..tokenization_utils_base import PreTrainedTokenizerBase\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-DEFAULT_ONNX_OPSET = 11\n-\n-# 2 Gb\n-EXTERNAL_DATA_FORMAT_SIZE_LIMIT = 2 * 1024 * 1024 * 1024\n-\n-\n-@dataclasses.dataclass\n-class PatchingSpec:\n-    \"\"\"\n-    Data class that holds patching specifications.\n-\n-    Args:\n-        o: Module / object where the op to patch is located\n-        name: Name of the op to monkey patch\n-        custom_op: Custom op that patches the original op\n-        orig_op: Original op that is being patched\n-        op_wrapper: Wrapper (optional) that wraps both the original and custom ops.\n-            It is useful for ops that are class or static methods for instance.\n-    \"\"\"\n-\n-    o: Any\n-    name: str\n-    custom_op: Callable\n-    orig_op: Optional[Callable] = None\n-    op_wrapper: Optional[Callable] = None\n-\n-\n-class OnnxConfig(ABC):\n-    \"\"\"\n-    Base class for ONNX exportable model describing metadata on how to export the model through the ONNX format.\n-    \"\"\"\n-\n-    default_fixed_batch = 2\n-    default_fixed_sequence = 8\n-    default_fixed_num_choices = 4\n-    torch_onnx_minimum_version = version.parse(\"1.8\")\n-    _tasks_to_common_outputs = {\n-        \"causal-lm\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"default\": OrderedDict({\"last_hidden_state\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"image-classification\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"image-segmentation\": OrderedDict(\n-            {\n-                \"logits\": {0: \"batch\", 1: \"sequence\"},\n-                \"pred_boxes\": {0: \"batch\", 1: \"sequence\"},\n-                \"pred_masks\": {0: \"batch\", 1: \"sequence\"},\n-            }\n-        ),\n-        \"masked-im\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"masked-lm\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"multiple-choice\": OrderedDict({\"logits\": {0: \"batch\"}}),\n-        \"object-detection\": OrderedDict(\n-            {\n-                \"logits\": {0: \"batch\", 1: \"sequence\"},\n-                \"pred_boxes\": {0: \"batch\", 1: \"sequence\"},\n-            }\n-        ),\n-        \"question-answering\": OrderedDict(\n-            {\n-                \"start_logits\": {0: \"batch\", 1: \"sequence\"},\n-                \"end_logits\": {0: \"batch\", 1: \"sequence\"},\n-            }\n-        ),\n-        \"semantic-segmentation\": OrderedDict({\"logits\": {0: \"batch\", 1: \"num_labels\", 2: \"height\", 3: \"width\"}}),\n-        \"seq2seq-lm\": OrderedDict({\"logits\": {0: \"batch\", 1: \"decoder_sequence\"}}),\n-        \"sequence-classification\": OrderedDict({\"logits\": {0: \"batch\"}}),\n-        \"token-classification\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"vision2seq-lm\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-        \"speech2seq-lm\": OrderedDict({\"logits\": {0: \"batch\", 1: \"sequence\"}}),\n-    }\n-\n-    def __init__(\n-        self, config: \"PreTrainedConfig\", task: str = \"default\", patching_specs: Optional[list[PatchingSpec]] = None\n-    ):\n-        self._config = config\n-\n-        if task not in self._tasks_to_common_outputs:\n-            raise ValueError(\n-                f\"{task} is not a supported task, supported tasks: {self._tasks_to_common_outputs.keys()}\"\n-            )\n-        self.task = task\n-\n-        self._patching_specs = []\n-        for spec in patching_specs if patching_specs is not None else []:\n-            final_spec = spec\n-            if spec.orig_op is None:\n-                final_spec = dataclasses.replace(spec, orig_op=getattr(spec.o, spec.name))\n-            self._patching_specs.append(final_spec)\n-\n-    @classmethod\n-    def from_model_config(cls, config: \"PreTrainedConfig\", task: str = \"default\") -> \"OnnxConfig\":\n-        \"\"\"\n-        Instantiate a OnnxConfig for a specific model\n-\n-        Args:\n-            config: The model's configuration to use when exporting to ONNX\n-\n-        Returns:\n-            OnnxConfig for this model\n-        \"\"\"\n-        return cls(config, task=task)\n-\n-    @property\n-    @abstractmethod\n-    def inputs(self) -> Mapping[str, Mapping[int, str]]:\n-        \"\"\"\n-        Mapping containing the axis definition of the input tensors to provide to the model\n-\n-        Returns:\n-            For each input: its name associated to the axes symbolic name and the axis position within the tensor\n-        \"\"\"\n-        raise NotImplementedError()\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        \"\"\"\n-        Mapping containing the axis definition of the output tensors to provide to the model\n-\n-        Returns:\n-            For each output: its name associated to the axes symbolic name and the axis position within the tensor\n-        \"\"\"\n-        common_outputs = self._tasks_to_common_outputs[self.task]\n-        return copy.deepcopy(common_outputs)\n-\n-    @property\n-    def values_override(self) -> Optional[Mapping[str, Any]]:\n-        \"\"\"\n-        Dictionary of keys to override in the model's config before exporting\n-\n-        Returns:\n-            Dictionary with the keys (and their corresponding values) to override\n-        \"\"\"\n-        if hasattr(self._config, \"use_cache\"):\n-            return {\"use_cache\": False}\n-\n-        return None\n-\n-    @property\n-    def default_batch_size(self) -> int:\n-        \"\"\"\n-        The default batch size to use if no other indication\n-\n-        Returns:\n-            Integer > 0\n-        \"\"\"\n-        # Using 2 avoid ONNX making assumption about single sample batch\n-        return OnnxConfig.default_fixed_batch\n-\n-    @property\n-    def default_sequence_length(self) -> int:\n-        \"\"\"\n-        The default sequence length to use if no other indication\n-\n-        Returns:\n-            Integer > 0\n-        \"\"\"\n-        return OnnxConfig.default_fixed_sequence\n-\n-    @property\n-    def default_num_choices(self) -> int:\n-        \"\"\"\n-        The default number of choices to use if no other indication\n-\n-        Returns:\n-            Integer > 0\n-        \"\"\"\n-        return OnnxConfig.default_fixed_num_choices\n-\n-    @property\n-    def default_onnx_opset(self) -> int:\n-        \"\"\"\n-        Which onnx opset to use when exporting the model\n-\n-        Returns:\n-            Integer ONNX Opset version\n-        \"\"\"\n-        return DEFAULT_ONNX_OPSET\n-\n-    @property\n-    def atol_for_validation(self) -> float:\n-        \"\"\"\n-        What absolute tolerance value to use during model conversion validation.\n-\n-        Returns:\n-            Float absolute tolerance value.\n-        \"\"\"\n-        return 1e-5\n-\n-    @property\n-    def is_torch_support_available(self) -> bool:\n-        \"\"\"\n-        The minimum PyTorch version required to export the model.\n-\n-        Returns:\n-            `bool`: Whether the installed version of PyTorch is compatible with the model.\n-        \"\"\"\n-        if is_torch_available():\n-            from transformers.utils import get_torch_version\n-\n-            return version.parse(get_torch_version()) >= self.torch_onnx_minimum_version\n-        else:\n-            return False\n-\n-    @staticmethod\n-    def use_external_data_format(num_parameters: int) -> bool:\n-        \"\"\"\n-        Flag indicating if the model requires using external data format\n-\n-        Args:\n-            num_parameters: Number of parameter on the model\n-\n-        Returns:\n-            True if model.num_parameters() * size_of(float32) >= 2Gb False otherwise\n-        \"\"\"\n-\n-        return (\n-            compute_serialized_parameters_size(num_parameters, ParameterFormat.Float)\n-            >= EXTERNAL_DATA_FORMAT_SIZE_LIMIT\n-        )\n-\n-    def _generate_dummy_images(\n-        self, batch_size: int = 2, num_channels: int = 3, image_height: int = 40, image_width: int = 40\n-    ):\n-        images = []\n-        for _ in range(batch_size):\n-            data = np.random.rand(image_height, image_width, num_channels) * 255\n-            images.append(Image.fromarray(data.astype(\"uint8\")).convert(\"RGB\"))\n-        return images\n-\n-    def _generate_dummy_audio(\n-        self, batch_size: int = 2, sampling_rate: int = 22050, time_duration: float = 5.0, frequency: int = 220\n-    ):\n-        audio_data = []\n-        for _ in range(batch_size):\n-            # time variable\n-            t = np.linspace(0, time_duration, int(time_duration * sampling_rate), endpoint=False)\n-\n-            # generate pure sine wave at `frequency` Hz\n-            audio_data.append(0.5 * np.sin(2 * np.pi * frequency * t))\n-\n-        return audio_data\n-\n-    def generate_dummy_inputs(\n-        self,\n-        preprocessor: Union[\"PreTrainedTokenizerBase\", \"FeatureExtractionMixin\", \"ImageProcessingMixin\"],\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        num_choices: int = -1,\n-        is_pair: bool = False,\n-        num_channels: int = 3,\n-        image_width: int = 40,\n-        image_height: int = 40,\n-        sampling_rate: int = 22050,\n-        time_duration: float = 5.0,\n-        frequency: int = 220,\n-        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n-    ) -> Mapping[str, Any]:\n-        \"\"\"\n-        Generate inputs to provide to the ONNX exporter\n-\n-        Args:\n-            preprocessor: ([`PreTrainedTokenizerBase`], [`FeatureExtractionMixin`], or [`ImageProcessingMixin`]):\n-                The preprocessor associated with this model configuration.\n-            batch_size (`int`, *optional*, defaults to -1):\n-                The batch size to export the model for (-1 means dynamic axis).\n-            num_choices (`int`, *optional*, defaults to -1):\n-                The number of candidate answers provided for multiple choice task (-1 means dynamic axis).\n-            seq_length (`int`, *optional*, defaults to -1):\n-                The sequence length to export the model for (-1 means dynamic axis).\n-            is_pair (`bool`, *optional*, defaults to `False`):\n-                Indicate if the input is a pair (sentence 1, sentence 2)\n-            num_channels (`int`, *optional*, defaults to 3):\n-                The number of channels of the generated images.\n-            image_width (`int`, *optional*, defaults to 40):\n-                The width of the generated images.\n-            image_height (`int`, *optional*, defaults to 40):\n-                The height of the generated images.\n-            sampling_rate (`int`, *optional* defaults to 22050)\n-                The sampling rate for audio data generation.\n-            time_duration (`float`, *optional* defaults to 5.0)\n-                Total seconds of sampling for audio data generation.\n-            frequency (`int`, *optional* defaults to 220)\n-                The desired natural frequency of generated audio.\n-\n-        Returns:\n-            Mapping[str, Tensor] holding the kwargs to provide to the model's forward function\n-        \"\"\"\n-        from ..feature_extraction_utils import FeatureExtractionMixin\n-        from ..image_processing_utils import ImageProcessingMixin\n-        from ..tokenization_utils_base import PreTrainedTokenizerBase\n-\n-        if isinstance(preprocessor, PreTrainedTokenizerBase) and tokenizer is not None:\n-            raise ValueError(\"You cannot provide both a tokenizer and a preprocessor to generate dummy inputs.\")\n-        if tokenizer is not None:\n-            warnings.warn(\n-                \"The `tokenizer` argument is deprecated and will be removed in version 5 of Transformers. Use\"\n-                \" `preprocessor` instead.\",\n-                FutureWarning,\n-            )\n-            logger.warning(\"Overwriting the `preprocessor` argument with `tokenizer` to generate dummy inputs.\")\n-            preprocessor = tokenizer\n-        if isinstance(preprocessor, PreTrainedTokenizerBase):\n-            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-            batch_size = compute_effective_axis_dimension(\n-                batch_size, fixed_dimension=OnnxConfig.default_fixed_batch, num_token_to_add=0\n-            )\n-            # If dynamic axis (-1) we forward with a fixed dimension of 8 tokens to avoid optimizations made by ONNX\n-            token_to_add = preprocessor.num_special_tokens_to_add(is_pair)\n-            seq_length = compute_effective_axis_dimension(\n-                seq_length, fixed_dimension=OnnxConfig.default_fixed_sequence, num_token_to_add=token_to_add\n-            )\n-            # Generate dummy inputs according to compute batch and sequence\n-            input_token = (\n-                preprocessor.unk_token\n-                if (preprocessor.unk_token is not None and len(preprocessor.unk_token) > 0)\n-                else \"0\"\n-            )\n-            dummy_input = [\" \".join([input_token]) * seq_length] * batch_size\n-            if self.task == \"multiple-choice\":\n-                # If dynamic axis (-1) we forward with a fixed dimension of 4 candidate answers to avoid optimizations\n-                # made by ONNX\n-                num_choices = compute_effective_axis_dimension(\n-                    num_choices, fixed_dimension=OnnxConfig.default_fixed_num_choices, num_token_to_add=0\n-                )\n-                dummy_input = dummy_input * num_choices\n-                # The shape of the tokenized inputs values is [batch_size * num_choices, seq_length]\n-                tokenized_input = preprocessor(dummy_input, text_pair=dummy_input)\n-                # Unflatten the tokenized inputs values expanding it to the shape [batch_size, num_choices, seq_length]\n-                for k, v in tokenized_input.items():\n-                    tokenized_input[k] = [v[i : i + num_choices] for i in range(0, len(v), num_choices)]\n-                return dict(tokenized_input.convert_to_tensors(tensor_type=\"pt\"))\n-            return dict(preprocessor(dummy_input, return_tensors=\"pt\"))\n-        elif isinstance(preprocessor, ImageProcessingMixin):\n-            if preprocessor.model_input_names[0] != \"pixel_values\":\n-                raise ValueError(\n-                    f\"The `preprocessor` is an image processor ({preprocessor.__class__.__name__}) and expects\"\n-                    f' `model_input_names[0]` to be \"pixel_values\", but got {preprocessor.model_input_names[0]}'\n-                )\n-            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-            batch_size = compute_effective_axis_dimension(batch_size, fixed_dimension=OnnxConfig.default_fixed_batch)\n-            dummy_input = self._generate_dummy_images(batch_size, num_channels, image_height, image_width)\n-            return dict(preprocessor(images=dummy_input, return_tensors=\"pt\"))\n-        elif isinstance(preprocessor, FeatureExtractionMixin) and preprocessor.model_input_names[0] == \"pixel_values\":\n-            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-            batch_size = compute_effective_axis_dimension(batch_size, fixed_dimension=OnnxConfig.default_fixed_batch)\n-            dummy_input = self._generate_dummy_images(batch_size, num_channels, image_height, image_width)\n-            return dict(preprocessor(images=dummy_input, return_tensors=\"pt\"))\n-        elif (\n-            isinstance(preprocessor, FeatureExtractionMixin) and preprocessor.model_input_names[0] == \"input_features\"\n-        ):\n-            # If dynamic axis (-1) we forward with a fixed dimension of 2 samples to avoid optimizations made by ONNX\n-            batch_size = compute_effective_axis_dimension(batch_size, fixed_dimension=OnnxConfig.default_fixed_batch)\n-            dummy_input = self._generate_dummy_audio(batch_size, sampling_rate, time_duration, frequency)\n-            return dict(preprocessor(dummy_input, return_tensors=\"pt\"))\n-        else:\n-            raise ValueError(\n-                \"Unable to generate dummy inputs for the model. Please provide a tokenizer or a preprocessor.\"\n-            )\n-\n-    def generate_dummy_inputs_onnxruntime(self, reference_model_inputs: Mapping[str, Any]) -> Mapping[str, Any]:\n-        \"\"\"\n-        Generate inputs for ONNX Runtime using the reference model inputs. Override this to run inference with seq2seq\n-        models which have the encoder and decoder exported as separate ONNX files.\n-\n-        Args:\n-            reference_model_inputs ([`Mapping[str, Tensor]`):\n-                Reference inputs for the model.\n-\n-        Returns:\n-            `Mapping[str, Tensor]`: The mapping holding the kwargs to provide to the model's forward function\n-        \"\"\"\n-        return reference_model_inputs\n-\n-    def patch_ops(self):\n-        for spec in self._patching_specs:\n-            custom_op = spec.custom_op if spec.op_wrapper is None else spec.op_wrapper(spec.custom_op)\n-            setattr(spec.o, spec.name, custom_op)\n-\n-    def restore_ops(self):\n-        for spec in self._patching_specs:\n-            orig_op = spec.orig_op if spec.op_wrapper is None else spec.op_wrapper(spec.orig_op)\n-            setattr(spec.o, spec.name, orig_op)\n-\n-    @classmethod\n-    def flatten_output_collection_property(cls, name: str, field: Iterable[Any]) -> dict[str, Any]:\n-        \"\"\"\n-        Flatten any potential nested structure expanding the name of the field with the index of the element within the\n-        structure.\n-\n-        Args:\n-            name: The name of the nested structure\n-            field: The structure to, potentially, be flattened\n-\n-        Returns:\n-            (dict[str, Any]): Outputs with flattened structure and key mapping this new structure.\n-\n-        \"\"\"\n-        from itertools import chain\n-\n-        return {f\"{name}.{idx}\": item for idx, item in enumerate(chain.from_iterable(field))}\n-\n-\n-class OnnxConfigWithPast(OnnxConfig, ABC):\n-    def __init__(\n-        self,\n-        config: \"PreTrainedConfig\",\n-        task: str = \"default\",\n-        patching_specs: Optional[list[PatchingSpec]] = None,\n-        use_past: bool = False,\n-    ):\n-        super().__init__(config, task=task, patching_specs=patching_specs)\n-        self.use_past = use_past\n-\n-    @classmethod\n-    def with_past(cls, config: \"PreTrainedConfig\", task: str = \"default\") -> \"OnnxConfigWithPast\":\n-        \"\"\"\n-        Instantiate a OnnxConfig with `use_past` attribute set to True\n-\n-        Args:\n-            config: The underlying model's config to use when exporting to ONNX\n-\n-        Returns:\n-            OnnxConfig with `.use_past = True`\n-        \"\"\"\n-        return cls(config, task=task, use_past=True)\n-\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_outputs = super().outputs\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_outputs, direction=\"outputs\")\n-\n-        return common_outputs\n-\n-    @property\n-    def values_override(self) -> Optional[Mapping[str, Any]]:\n-        if hasattr(self._config, \"use_cache\"):\n-            return {\"use_cache\": self.use_past}\n-\n-        return None\n-\n-    @property\n-    def num_layers(self) -> int:\n-        \"\"\"\n-        The number of layers attribute retrieved from the model config. Override this for model configs where the\n-        number of layers attribute is not called `num_layers`.\n-        \"\"\"\n-        if not hasattr(self._config, \"num_layers\"):\n-            raise AttributeError(\n-                \"could not find the number of layers attribute in the model configuration, override the num_layers\"\n-                \" property of the model OnnxConfig to solve this\"\n-            )\n-        return self._config.num_layers\n-\n-    @property\n-    def num_attention_heads(self) -> int:\n-        \"\"\"\n-        The number of attention heads attribute retrieved from the model config. Override this for model configs where\n-        the number of attention heads attribute is not called `num_attention_heads`.\n-        \"\"\"\n-        if not hasattr(self._config, \"num_attention_heads\"):\n-            raise AttributeError(\n-                \"could not find the number of attention heads attribute in the model configuration, override the\"\n-                \" num_attention_heads property of the model OnnxConfig to solve this\"\n-            )\n-        return self._config.num_attention_heads\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: \"PreTrainedTokenizerBase\",\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        # TODO: should we set seq_length = 1 when self.use_past = True?\n-        common_inputs = super().generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-\n-            batch, seqlen = common_inputs[\"input_ids\"].shape\n-            # Not using the same length for past_key_values\n-            past_key_values_length = seqlen + 2\n-            shape = (\n-                batch,\n-                self.num_attention_heads,\n-                past_key_values_length,\n-                self._config.hidden_size // self.num_attention_heads,\n-            )\n-\n-            if \"attention_mask\" in common_inputs:\n-                mask_dtype = common_inputs[\"attention_mask\"].dtype\n-                common_inputs[\"attention_mask\"] = torch.cat(\n-                    [common_inputs[\"attention_mask\"], torch.ones(batch, past_key_values_length, dtype=mask_dtype)],\n-                    dim=1,\n-                )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            for _ in range(self.num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-\n-        return common_inputs\n-\n-    def fill_with_past_key_values_(\n-        self, inputs_or_outputs: Mapping[str, Mapping[int, str]], direction: str, inverted_values_shape: bool = False\n-    ):\n-        \"\"\"\n-        Fill the input_or_outputs mapping with past_key_values dynamic axes considering.\n-\n-        Args:\n-            inputs_or_outputs: The mapping to fill.\n-            direction: either \"inputs\" or \"outputs\", it specifies whether input_or_outputs is the input mapping or the\n-                output mapping, this is important for axes naming.\n-            inverted_values_shape:\n-                If `True`, store values on dynamic axis 1, else on axis 2.\n-\n-        \"\"\"\n-        if direction not in [\"inputs\", \"outputs\"]:\n-            raise ValueError(f'direction must either be \"inputs\" or \"outputs\", but {direction} was given')\n-\n-        name = \"past_key_values\" if direction == \"inputs\" else \"present\"\n-        for i in range(self.num_layers):\n-            inputs_or_outputs[f\"{name}.{i}.key\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-            if inverted_values_shape:\n-                inputs_or_outputs[f\"{name}.{i}.value\"] = {0: \"batch\", 1: \"past_sequence + sequence\"}\n-            else:\n-                inputs_or_outputs[f\"{name}.{i}.value\"] = {0: \"batch\", 2: \"past_sequence + sequence\"}\n-\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        flattened_output[f\"{name}.{idx}.key\"] = t[0]\n-        flattened_output[f\"{name}.{idx}.value\"] = t[1]\n-\n-    def flatten_output_collection_property(self, name: str, field: Iterable[Any]) -> dict[str, Any]:\n-        flattened_output = {}\n-        if name in [\"present\", \"past_key_values\"]:\n-            for idx, t in enumerate(field):\n-                self._flatten_past_key_values_(flattened_output, name, idx, t)\n-        else:\n-            flattened_output = super().flatten_output_collection_property(name, field)\n-\n-        return flattened_output\n-\n-\n-class OnnxSeq2SeqConfigWithPast(OnnxConfigWithPast):\n-    @property\n-    def outputs(self) -> Mapping[str, Mapping[int, str]]:\n-        common_outputs = super(OnnxConfigWithPast, self).outputs\n-        # Renaming the outputs axes properly.\n-        for name, axes_names in common_outputs.items():\n-            sequence_name = \"encoder_sequence\" if \"encoder\" in name else \"decoder_sequence\"\n-            for axis_idx, name in axes_names.items():\n-                if \"sequence\" in name:\n-                    axes_names[axis_idx] = sequence_name\n-                # We reset the value as the order in common_outputs (OrderedDict) is lost otherwise\n-                else:\n-                    axes_names[axis_idx] = name\n-        if self.use_past:\n-            self.fill_with_past_key_values_(common_outputs, direction=\"outputs\")\n-\n-        return common_outputs\n-\n-    @property\n-    def num_layers(self) -> tuple[int, ...]:\n-        try:\n-            num_layers = super().num_layers\n-            num_layers = (num_layers, num_layers)\n-        except AttributeError:\n-            if hasattr(self._config, \"encoder_layers\") and hasattr(self._config, \"decoder_layers\"):\n-                num_layers = (self._config.encoder_layers, self._config.decoder_layers)\n-            else:\n-                raise AttributeError(\n-                    \"could not find the number of encoder and decoder layers attributes in the model configuration,\"\n-                    \" override the num_layers property of the model OnnxConfig to solve this\"\n-                )\n-\n-        return num_layers\n-\n-    @property\n-    def num_attention_heads(self) -> tuple[int, ...]:\n-        try:\n-            num_attention_heads = super().num_attention_heads\n-            num_attention_heads = (num_attention_heads, num_attention_heads)\n-        except AttributeError:\n-            if hasattr(self._config, \"encoder_attention_heads\") and hasattr(self._config, \"decoder_attention_heads\"):\n-                num_attention_heads = (self._config.encoder_attention_heads, self._config.decoder_attention_heads)\n-            else:\n-                raise AttributeError(\n-                    \"could not find the number of attention heads for the encoder and the decoder attributes in the\"\n-                    \" model configuration, override the num_attention_heads property of the model OnnxConfig to solve\"\n-                    \" this\"\n-                )\n-        return num_attention_heads\n-\n-    def generate_dummy_inputs(\n-        self,\n-        tokenizer: Optional[\"PreTrainedTokenizerBase\"],\n-        batch_size: int = -1,\n-        seq_length: int = -1,\n-        is_pair: bool = False,\n-    ) -> Mapping[str, Any]:\n-        encoder_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=seq_length,\n-            is_pair=is_pair,\n-        )\n-\n-        # Generate decoder inputs\n-        decoder_seq_length = seq_length if not self.use_past else 1\n-        decoder_inputs = super(OnnxConfigWithPast, self).generate_dummy_inputs(\n-            tokenizer,\n-            batch_size=batch_size,\n-            seq_length=decoder_seq_length,\n-            is_pair=is_pair,\n-        )\n-        decoder_inputs = {f\"decoder_{name}\": tensor for name, tensor in decoder_inputs.items()}\n-        common_inputs = dict(**encoder_inputs, **decoder_inputs)\n-\n-        if self.use_past:\n-            if not is_torch_available():\n-                raise ValueError(\"Cannot generate dummy past_keys inputs without PyTorch installed.\")\n-            else:\n-                import torch\n-            batch = common_inputs[\"input_ids\"].shape[0]\n-            encoder_seq_length = common_inputs[\"input_ids\"].shape[1]\n-            decoder_seq_length = common_inputs[\"decoder_input_ids\"].shape[1]\n-            num_encoder_attention_heads, num_decoder_attention_heads = self.num_attention_heads\n-            encoder_shape = (\n-                batch,\n-                num_encoder_attention_heads,\n-                encoder_seq_length,\n-                self._config.hidden_size // num_encoder_attention_heads,\n-            )\n-            decoder_shape = (\n-                batch,\n-                num_decoder_attention_heads,\n-                # Not using the same length for past_key_values\n-                decoder_seq_length + 3,\n-                self._config.hidden_size // num_decoder_attention_heads,\n-            )\n-\n-            common_inputs[\"past_key_values\"] = []\n-            # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-            num_encoder_layers, num_decoder_layers = self.num_layers\n-            min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-            max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-            remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-            for _ in range(min_num_layers):\n-                # For encoder-decoder models, past_key_values contains pre-computed values for both the encoder and the\n-                # decoder layers, hence a tuple of 4 tensors instead of 2\n-                common_inputs[\"past_key_values\"].append(\n-                    (\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(decoder_shape),\n-                        torch.zeros(encoder_shape),\n-                        torch.zeros(encoder_shape),\n-                    )\n-                )\n-\n-            # TODO: test this.\n-            shape = encoder_shape if remaining_side_name == \"encoder\" else decoder_shape\n-            for _ in range(min_num_layers, max_num_layers):\n-                common_inputs[\"past_key_values\"].append((torch.zeros(shape), torch.zeros(shape)))\n-\n-        return common_inputs\n-\n-    def fill_with_past_key_values_(self, inputs_or_outputs: Mapping[str, Mapping[int, str]], direction: str):\n-        if direction not in [\"inputs\", \"outputs\"]:\n-            raise ValueError(f'direction must either be \"inputs\" or \"outputs\", but {direction} was given')\n-\n-        name = \"past_key_values\" if direction == \"inputs\" else \"present\"\n-\n-        # If the number of encoder and decoder layers are present in the model configuration, both are considered\n-        num_encoder_layers, num_decoder_layers = self.num_layers\n-        min_num_layers = min(num_encoder_layers, num_decoder_layers)\n-        max_num_layers = max(num_encoder_layers, num_decoder_layers) - min_num_layers\n-        remaining_side_name = \"encoder\" if num_encoder_layers > num_decoder_layers else \"decoder\"\n-\n-        encoder_sequence = \"past_encoder_sequence\"\n-        decoder_sequence = \"past_decoder_sequence\" if direction == \"inputs\" else \"past_decoder_sequence + sequence\"\n-\n-        for i in range(min_num_layers):\n-            inputs_or_outputs[f\"{name}.{i}.decoder.key\"] = {0: \"batch\", 2: decoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.decoder.value\"] = {0: \"batch\", 2: decoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.encoder.key\"] = {0: \"batch\", 2: encoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.encoder.value\"] = {0: \"batch\", 2: encoder_sequence}\n-\n-        for i in range(min_num_layers, max_num_layers):\n-            if remaining_side_name == \"encoder\":\n-                axes_info = {0: \"batch\", 2: encoder_sequence}\n-            else:\n-                axes_info = {0: \"batch\", 2: decoder_sequence}\n-            inputs_or_outputs[f\"{name}.{i}.{remaining_side_name}.key\"] = axes_info\n-\n-    def _flatten_past_key_values_(self, flattened_output, name, idx, t):\n-        flattened_output[f\"{name}.{idx}.decoder.key\"] = t[0]\n-        flattened_output[f\"{name}.{idx}.decoder.value\"] = t[1]\n-        flattened_output[f\"{name}.{idx}.encoder.key\"] = t[2]\n-        flattened_output[f\"{name}.{idx}.encoder.value\"] = t[3]"
        },
        {
            "sha": "9672b0a96af88ffa2c7e791d1f4d7c818174247f",
            "filename": "src/transformers/onnx/utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 109,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/5995435d96ace8bbf7f95623e5a7487990280fd1/src%2Ftransformers%2Fonnx%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5995435d96ace8bbf7f95623e5a7487990280fd1/src%2Ftransformers%2Fonnx%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Futils.py?ref=5995435d96ace8bbf7f95623e5a7487990280fd1",
            "patch": "@@ -1,109 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from ctypes import c_float, sizeof\n-from enum import Enum\n-from typing import TYPE_CHECKING, Optional, Union\n-\n-\n-if TYPE_CHECKING:\n-    from .. import AutoFeatureExtractor, AutoProcessor, AutoTokenizer  # tests_ignore\n-\n-\n-class ParameterFormat(Enum):\n-    Float = c_float\n-\n-    @property\n-    def size(self) -> int:\n-        \"\"\"\n-        Number of byte required for this data type\n-\n-        Returns:\n-            Integer > 0\n-        \"\"\"\n-        return sizeof(self.value)\n-\n-\n-def compute_effective_axis_dimension(dimension: int, fixed_dimension: int, num_token_to_add: int = 0) -> int:\n-    \"\"\"\n-\n-    Args:\n-        dimension:\n-        fixed_dimension:\n-        num_token_to_add:\n-\n-    Returns:\n-\n-    \"\"\"\n-    # < 0 is possible if using a dynamic axis\n-    if dimension <= 0:\n-        dimension = fixed_dimension\n-\n-    dimension -= num_token_to_add\n-    return dimension\n-\n-\n-def compute_serialized_parameters_size(num_parameters: int, dtype: ParameterFormat) -> int:\n-    \"\"\"\n-    Compute the size taken by all the parameters in the given the storage format when serializing the model\n-\n-    Args:\n-        num_parameters: Number of parameters to be saved\n-        dtype: The data format each parameter will be saved\n-\n-    Returns:\n-        Size (in byte) taken to save all the parameters\n-    \"\"\"\n-    return num_parameters * dtype.size\n-\n-\n-def get_preprocessor(model_name: str) -> Optional[Union[\"AutoTokenizer\", \"AutoFeatureExtractor\", \"AutoProcessor\"]]:\n-    \"\"\"\n-    Gets a preprocessor (tokenizer, feature extractor or processor) that is available for `model_name`.\n-\n-    Args:\n-        model_name (`str`): Name of the model for which a preprocessor are loaded.\n-\n-    Returns:\n-        `Optional[Union[AutoTokenizer, AutoFeatureExtractor, AutoProcessor]]`:\n-            If a processor is found, it is returned. Otherwise, if a tokenizer or a feature extractor exists, it is\n-            returned. If both a tokenizer and a feature extractor exist, an error is raised. The function returns\n-            `None` if no preprocessor is found.\n-    \"\"\"\n-    # Avoid circular imports by only importing this here.\n-    from .. import AutoFeatureExtractor, AutoProcessor, AutoTokenizer  # tests_ignore\n-\n-    try:\n-        return AutoProcessor.from_pretrained(model_name)\n-    except (ValueError, OSError, KeyError):\n-        tokenizer, feature_extractor = None, None\n-        try:\n-            tokenizer = AutoTokenizer.from_pretrained(model_name)\n-        except (OSError, KeyError):\n-            pass\n-        try:\n-            feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n-        except (OSError, KeyError):\n-            pass\n-\n-        if tokenizer is not None and feature_extractor is not None:\n-            raise ValueError(\n-                f\"Couldn't auto-detect preprocessor for {model_name}. Found both a tokenizer and a feature extractor.\"\n-            )\n-        elif tokenizer is None and feature_extractor is None:\n-            return None\n-        elif tokenizer is not None:\n-            return tokenizer\n-        else:\n-            return feature_extractor"
        },
        {
            "sha": "31a6ace2100074dbd9c45b88207370fb716a7b6e",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -2161,7 +2161,7 @@ def create_import_structure_from_path(module_path):\n     {\n         'albert': {\n             frozenset(): {\n-                'configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'}\n+                'configuration_albert': {'AlbertConfig'}\n             },\n             frozenset({'tokenizers'}): {\n                 'tokenization_albert_fast': {'AlbertTokenizerFast'}\n@@ -2337,7 +2337,7 @@ def spread_import_structure(nested_import_structure):\n     {\n         'albert': {\n             frozenset(): {\n-                'configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'}\n+                'configuration_albert': {'AlbertConfig'}\n             },\n             frozenset({'tokenizers'}): {\n                 'tokenization_albert_fast': {'AlbertTokenizerFast'}\n@@ -2364,7 +2364,7 @@ def spread_import_structure(nested_import_structure):\n             'albert.tokenization_albert_fast': {'AlbertTokenizerFast'}\n         },\n         frozenset(): {\n-            'albert.configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'},\n+            'albert.configuration_albert': {'AlbertConfig'},\n             'align.processing_align': {'AlignProcessor'},\n             'align.configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},\n             'altclip.configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},\n@@ -2465,7 +2465,7 @@ def define_import_structure(module_path: str, prefix: str | None = None) -> IMPO\n             'albert.tokenization_albert_fast': {'AlbertTokenizerFast'}\n         },\n         frozenset(): {\n-            'albert.configuration_albert': {'AlbertConfig', 'AlbertOnnxConfig'},\n+            'albert.configuration_albert': {'AlbertConfig'},\n             'align.processing_align': {'AlignProcessor'},\n             'align.configuration_align': {'AlignConfig', 'AlignTextConfig', 'AlignVisionConfig'},\n             'altclip.configuration_altclip': {'AltCLIPConfig', 'AltCLIPTextConfig', 'AltCLIPVisionConfig'},"
        },
        {
            "sha": "0c3d8fff917bb7d30ebdb126adba9594fcc577eb",
            "filename": "tests/utils/test_import_structure.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/tests%2Futils%2Ftest_import_structure.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/tests%2Futils%2Ftest_import_structure.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_import_structure.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -132,7 +132,7 @@ def test_import_spread(self):\n             \"models\": {\n                 frozenset(): {\"dummy_config\": {\"DummyConfig\"}},\n                 \"albert\": {\n-                    frozenset(): {\"configuration_albert\": {\"AlbertConfig\", \"AlbertOnnxConfig\"}},\n+                    frozenset(): {\"configuration_albert\": {\"AlbertConfig\"}},\n                     frozenset({\"torch\"}): {\n                         \"modeling_albert\": {\n                             \"AlbertForMaskedLM\",\n@@ -174,7 +174,7 @@ def test_import_spread(self):\n             frozenset(): {\n                 \"dummy_non_model\": {\"DummyObject\"},\n                 \"models.dummy_config\": {\"DummyConfig\"},\n-                \"models.albert.configuration_albert\": {\"AlbertConfig\", \"AlbertOnnxConfig\"},\n+                \"models.albert.configuration_albert\": {\"AlbertConfig\"},\n                 \"models.llama.configuration_llama\": {\"LlamaConfig\"},\n                 \"models.deprecated.transfo_xl.configuration_transfo_xl\": {\"TransfoXLConfig\"},\n                 \"models.deprecated.transfo_xl.tokenization_transfo_xl\": {\"TransfoXLCorpus\", \"TransfoXLTokenizer\"},"
        },
        {
            "sha": "4dbfb8e0bd8a989c87173a20d0826bf892d64bb9",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -1045,7 +1045,6 @@ def ignore_undocumented(name: str) -> bool:\n         or name.endswith(\"Layer\")\n         or name.endswith(\"Embeddings\")\n         or name.endswith(\"Attention\")\n-        or name.endswith(\"OnnxConfig\")\n     ):\n         return True\n     # Submodules are not documented."
        },
        {
            "sha": "a67667942d3ef73b4b996aacd6e3b74052042246",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f39355ec23ce701b642f71c9df183c18989332eb/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/f39355ec23ce701b642f71c9df183c18989332eb/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=f39355ec23ce701b642f71c9df183c18989332eb",
            "patch": "@@ -738,7 +738,6 @@ src/transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py\n src/transformers/models/yoso/modeling_yoso.py\n src/transformers/models/zamba/configuration_zamba.py\n src/transformers/models/zamba/modeling_zamba.py\n-src/transformers/onnx/config.py\n src/transformers/optimization.py\n src/transformers/pipelines/audio_classification.py\n src/transformers/pipelines/audio_utils.py"
        }
    ],
    "stats": {
        "total": 5402,
        "additions": 93,
        "deletions": 5309
    }
}