{
    "author": "zucchini-nlp",
    "message": "[`compile`] re-enable for Qwen-VL models (#38127)\n\n* compile qwen models\n\n* delete TODO comment\n\n* fix embeds test\n\n* fix assisted decoding\n\n* add comments",
    "sha": "a21f11fca229e0a4b9c3cbcf444703ca004ad39a",
    "files": [
        {
            "sha": "b416948a4abe9ebca052a0be9a92cf8e7b8e2fe1",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 81,
            "deletions": 69,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=a21f11fca229e0a4b9c3cbcf444703ca004ad39a",
            "patch": "@@ -40,7 +40,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n \n@@ -358,7 +358,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n@@ -1659,9 +1659,9 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_embeds.shape[0]\n-                if n_image_tokens != n_image_features:\n+                if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n                     raise ValueError(\n                         f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                     )\n@@ -1676,9 +1676,9 @@ def forward(\n \n             if pixel_values_videos is not None:\n                 video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_embeds.shape[0]\n-                if n_video_tokens != n_video_features:\n+                if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n                     raise ValueError(\n                         f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                     )\n@@ -1694,20 +1694,32 @@ def forward(\n             if attention_mask is not None:\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n-        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n-        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n-            # calculate RoPE index once per generation in the pre-fill stage only\n-            if (\n+        if position_ids is None:\n+            attention_mask_2d = attention_mask\n+            if attention_mask is not None and attention_mask.ndim == 4:\n+                attention_mask_2d = torch.diagonal(attention_mask_2d[:, 0], dim1=1, dim2=2)\n+                attention_mask_2d = attention_mask_2d / torch.finfo(attention_mask_2d.dtype).min\n+                attention_mask_2d = (1.0 - attention_mask_2d).int()\n+\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n                 (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n                 or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            ):\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n                     video_grid_thw,\n-                    second_per_grid_ts,\n-                    attention_mask,\n+                    second_per_grid_ts=second_per_grid_ts,\n+                    attention_mask=attention_mask_2d,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n@@ -1747,6 +1759,61 @@ def forward(\n         )\n         return output if return_dict else output.to_tuple()\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n @dataclass\n class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n@@ -2108,60 +2175,5 @@ def _expand_dict_for_generation(dict_to_expand):\n \n         return input_ids, model_kwargs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"Qwen2_5_VLForConditionalGeneration\", \"Qwen2_5_VLModel\", \"Qwen2_5_VLPreTrainedModel\", \"Qwen2_5_VLTextModel\"]"
        },
        {
            "sha": "b4307161bd78f7104f6906a1242534f7524306d4",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 25,
            "deletions": 13,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=a21f11fca229e0a4b9c3cbcf444703ca004ad39a",
            "patch": "@@ -50,7 +50,7 @@\n from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import is_torchdynamo_compiling, logging\n from ...video_utils import VideoInput\n \n \n@@ -647,9 +647,9 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_embeds.shape[0]\n-                if n_image_tokens != n_image_features:\n+                if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n                     raise ValueError(\n                         f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                     )\n@@ -664,9 +664,9 @@ def forward(\n \n             if pixel_values_videos is not None:\n                 video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_embeds.shape[0]\n-                if n_video_tokens != n_video_features:\n+                if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n                     raise ValueError(\n                         f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                     )\n@@ -682,20 +682,32 @@ def forward(\n             if attention_mask is not None:\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n-        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n-        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n-            # calculate RoPE index once per generation in the pre-fill stage only\n-            if (\n+        if position_ids is None:\n+            attention_mask_2d = attention_mask\n+            if attention_mask is not None and attention_mask.ndim == 4:\n+                attention_mask_2d = torch.diagonal(attention_mask_2d[:, 0], dim1=1, dim2=2)\n+                attention_mask_2d = attention_mask_2d / torch.finfo(attention_mask_2d.dtype).min\n+                attention_mask_2d = (1.0 - attention_mask_2d).int()\n+\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n                 (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n                 or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            ):\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index(\n                     input_ids,\n                     image_grid_thw,\n                     video_grid_thw,\n-                    second_per_grid_ts,\n-                    attention_mask,\n+                    second_per_grid_ts=second_per_grid_ts,\n+                    attention_mask=attention_mask_2d,\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids"
        },
        {
            "sha": "f5e5a08cdd4c7ba353140e6efc3dee1b2a7fdb34",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 76,
            "deletions": 64,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=a21f11fca229e0a4b9c3cbcf444703ca004ad39a",
            "patch": "@@ -924,7 +924,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = False  # TODO (joao): fix. torch.compile failing probably due to `cache_positions`\n+    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.get_text_config().initializer_range\n@@ -1616,16 +1616,28 @@ def forward(\n             if attention_mask is not None:\n                 attention_mask = attention_mask.to(inputs_embeds.device)\n \n-        # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n-        if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n-            # calculate RoPE index once per generation in the pre-fill stage only\n-            if (\n+        if position_ids is None:\n+            attention_mask_2d = attention_mask\n+            if attention_mask is not None and attention_mask.ndim == 4:\n+                attention_mask_2d = torch.diagonal(attention_mask_2d[:, 0], dim1=1, dim2=2)\n+                attention_mask_2d = attention_mask_2d / torch.finfo(attention_mask_2d.dtype).min\n+                attention_mask_2d = (1.0 - attention_mask_2d).int()\n+\n+            # Calculate RoPE index once per generation in the pre-fill stage only.\n+            # When compiling, we can't check tensor values thus we check only input length\n+            # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n+            # models currently cannot do asssisted decoding\n+            prefill_compiled_stage = is_torchdynamo_compiling() and (\n+                (input_ids is not None and input_ids.shape[1] != 1)\n+                or (inputs_embeds is not None and inputs_embeds.shape[1] != 1)\n+            )\n+            prefill_noncompiled_stage = not is_torchdynamo_compiling() and (\n                 (cache_position is not None and cache_position[0] == 0)\n-                or self.rope_deltas is None\n                 or (past_key_values is None or past_key_values.get_seq_length() == 0)\n-            ):\n+            )\n+            if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:\n                 position_ids, rope_deltas = self.get_rope_index(\n-                    input_ids, image_grid_thw, video_grid_thw, attention_mask\n+                    input_ids, image_grid_thw, video_grid_thw, attention_mask_2d\n                 )\n                 self.rope_deltas = rope_deltas\n             # then use the prev pre-calculated rope-deltas to get the correct position ids\n@@ -1662,6 +1674,62 @@ def forward(\n         )\n         return output if return_dict else output.to_tuple()\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n     _checkpoint_conversion_mapping = {\n@@ -1974,61 +2042,5 @@ def _expand_dict_for_generation(dict_to_expand):\n \n         return input_ids, model_kwargs\n \n-    @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"Qwen2VLForConditionalGeneration\", \"Qwen2VLModel\", \"Qwen2VLPreTrainedModel\", \"Qwen2VLTextModel\"]"
        },
        {
            "sha": "232dd7f644bad4ff5e178331803524b997b5f57f",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=a21f11fca229e0a4b9c3cbcf444703ca004ad39a",
            "patch": "@@ -346,10 +346,6 @@ def test_disk_offload_safetensors(self):\n     def test_model_parallelism(self):\n         pass\n \n-    @unittest.skip(reason=\"Compile not yet supported because in Qwen2_5_VL models\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n     @unittest.skip(reason=\"Compile not yet supported because in Qwen2_5_VL models\")\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n@@ -368,10 +364,6 @@ def test_model_is_small(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"Can't compile fullgraph due to dynamic control flow in `prepare_inputs_for_generate`\")\n-    def test_generate_compile_fullgraph(self):\n-        pass\n-\n     @is_flaky()  # TODO (joao/raushan): Investigate why this test is flaky on this model\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         super().test_prompt_lookup_decoding_matches_greedy_search()"
        },
        {
            "sha": "ab2799f7ab7d12c57dff39255076796083722ffe",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a21f11fca229e0a4b9c3cbcf444703ca004ad39a/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=a21f11fca229e0a4b9c3cbcf444703ca004ad39a",
            "patch": "@@ -300,10 +300,6 @@ def test_disk_offload_safetensors(self):\n     def test_model_parallelism(self):\n         pass\n \n-    @unittest.skip(reason=\"Compile not yet supported because in Qwen2VL models\")\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n     @unittest.skip(reason=\"Compile not yet supported because in Qwen2VL models\")\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass"
        }
    ],
    "stats": {
        "total": 340,
        "additions": 182,
        "deletions": 158
    }
}