{
    "author": "remi-or",
    "message": "Enable fa in amd docker (#41069)\n\n* Add FA to docker\n\n* Use caching mechanism for qwen2_5\n\n* Fix a typo in important models list\n\n* Partial fixes for gemma3\n\n* Added a commit ID for FA repo\n\n* Detailled  the expectation storage format\n\n* Rebase fix\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "50d2448a1a7b75354c3d0ca879afd124abd244ac",
    "files": [
        {
            "sha": "245a6736864a0cae1e706a19e1169677bd0a59dc",
            "filename": "docker/transformers-pytorch-amd-gpu/Dockerfile",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/50d2448a1a7b75354c3d0ca879afd124abd244ac/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/50d2448a1a7b75354c3d0ca879afd124abd244ac/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-amd-gpu%2FDockerfile?ref=50d2448a1a7b75354c3d0ca879afd124abd244ac",
            "patch": "@@ -35,3 +35,10 @@ RUN python3 -m pip uninstall -y kernels\n \n # On ROCm, torchcodec is required to decode audio files and 0.4 or 0.6 fails\n RUN python3 -m pip install --no-cache-dir \"torchcodec==0.5\"\n+\n+# Install flash attention from source. Tested with commit 6387433156558135a998d5568a9d74c1778666d8\n+RUN git clone https://github.com/ROCm/flash-attention/ -b tridao && \\\n+    cd flash-attention && \\\n+    GPU_ARCHS=\"gfx942\" python setup.py install\n+\n+RUN python3 -m pip install --no-cache-dir einops"
        },
        {
            "sha": "88ac6479777d8277e3fc0fe48e03e628cae9d86a",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/50d2448a1a7b75354c3d0ca879afd124abd244ac/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50d2448a1a7b75354c3d0ca879afd124abd244ac/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=50d2448a1a7b75354c3d0ca879afd124abd244ac",
            "patch": "@@ -3244,7 +3244,9 @@ def unpack_device_properties(\n class Expectations(UserDict[PackedDeviceProperties, Any]):\n     def get_expectation(self) -> Any:\n         \"\"\"\n-        Find best matching expectation based on environment device properties.\n+        Find best matching expectation based on environment device properties. We look at device_type, major and minor\n+        versions of the drivers. Expectations are stored as a dictionary with keys of the form\n+        (device_type, (major, minor)). If the major and minor versions are not provided, we use None.\n         \"\"\"\n         return self.find_expectation(get_device_properties())\n "
        },
        {
            "sha": "70fd2d376a782e1f3e02ee8facc37ad934fc8ea8",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 25,
            "deletions": 47,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/50d2448a1a7b75354c3d0ca879afd124abd244ac/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50d2448a1a7b75354c3d0ca879afd124abd244ac/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=50d2448a1a7b75354c3d0ca879afd124abd244ac",
            "patch": "@@ -785,7 +785,10 @@ def test_model_4b_bf16(self):\n \n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n-        EXPECTED_TEXTS = ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly']  # fmt: skip\n+        EXPECTED_TEXTS = Expectations({\n+            (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n+            (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. The sky is blue with a few white clouds. The'],\n+        }).get_expectation()  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_with_audio(self):\n@@ -866,18 +869,11 @@ def test_model_4b_batch(self):\n \n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n-\n-        # fmt: off\n-        EXPECTATIONS = Expectations(\n-            {\n-                (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"],\n-                (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. The cow is facing the viewer with its head slightly turned', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"],\n-            }\n-        )\n-\n-        EXPECTED_TEXTS = EXPECTATIONS.get_expectation()\n-        # fmt: on\n-\n+        EXPECTED_TEXTS = Expectations({\n+            (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"],\n+            (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject Matter:** The first image shows a\"],\n+            (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. The cow is facing the viewer with its head slightly turned', \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n* **Subject:** The first image features a cow\"],\n+        }).get_expectation()  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     def test_model_4b_image(self):\n@@ -899,18 +895,11 @@ def test_model_4b_image(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 1  # Gemma3n does not support crops\n-\n-        # fmt: off\n-        EXPECTATIONS = Expectations(\n-            {\n-                (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n-                (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n-            }\n-        )\n-\n-        EXPECTED_TEXTS = EXPECTATIONS.get_expectation()\n-        # fmt: on\n-\n+        EXPECTED_TEXTS = Expectations({\n+            (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n+            (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a clear blue ocean. The cow is facing the viewer with its head slightly'],\n+            (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. The sky is blue with a few white clouds. The'],\n+        }).get_expectation()  # fmt: skip\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n@@ -948,17 +937,11 @@ def test_model_4b_multiimage(self):\n         output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n-        # fmt: off\n-        EXPECTATIONS = Expectations(\n-            {\n-                (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are some key elements:\\n\\n* **A prominent red'],\n-                (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are the key elements:\\n\\n* **A prominent red'],\n-            }\n-        )\n-\n-        EXPECTED_TEXTS = EXPECTATIONS.get_expectation()\n-        # fmt: on\n-\n+        EXPECTED_TEXTS = Expectations({\n+            (\"cuda\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are some key elements:\\n\\n* **A prominent red'],\n+            (\"xpu\", None): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. Here are the key elements:\\n\\n* **A prominent red'],\n+            (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nIn the image, I see a street scene in what appears to be a Chinatown district. \\n\\nHere are some key elements:\\n\\n* **A'],\n+        }).get_expectation()  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n     @unittest.skip(\"For now, using a gemma model with the 3n class is not supported\")\n@@ -1034,15 +1017,10 @@ def test_generation_beyond_sliding_window_with_generation_config(self):\n         ]\n         output_text = tokenizer.batch_decode(out)\n \n-        # fmt: off\n-        EXPECTATIONS = Expectations(\n-            {\n-                (\"cuda\", None): [\" and I am glad to be here. This is a nice place. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"],\n-                (\"xpu\", None): [\" and I think it is very nice. I think it is nice. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"],\n-            }\n-        )\n-\n-        EXPECTED_COMPLETIONS = EXPECTATIONS.get_expectation()\n-        # fmt: on\n-\n+        EXPECTED_COMPLETIONS = Expectations({\n+            # FIXME: This test is VERY flaky on ROCm\n+            (\"cuda\", None): [\" and I am glad to be here. This is a nice place. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"],\n+            (\"rocm\", (9, 4)): [' and I think it makes this place special. This is a nice place. This is a nice place', ', green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are'],\n+            (\"xpu\", None): [\" and I think it is very nice. I think it is nice. This is a nice place.\", \", green, yellow, purple, orange, pink, brown, black, white.\\n\\nHere are\"],\n+        }).get_expectation()  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)"
        },
        {
            "sha": "e2f0e8581837f4ae53ca1accd5bd9112bda613e3",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/50d2448a1a7b75354c3d0ca879afd124abd244ac/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50d2448a1a7b75354c3d0ca879afd124abd244ac/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=50d2448a1a7b75354c3d0ca879afd124abd244ac",
            "patch": "@@ -27,6 +27,7 @@\n     is_torch_available,\n     is_vision_available,\n )\n+from transformers.image_utils import load_image\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n@@ -47,6 +48,7 @@\n     floats_tensor,\n     ids_tensor,\n )\n+from ...test_processing_common import url_to_local_path\n \n \n if is_cv2_available():\n@@ -454,8 +456,8 @@ def setUp(self):\n                 ],\n             }\n         ]\n-        url = \"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/demo_small.jpg\"\n-        self.image = Image.open(requests.get(url, stream=True).raw)\n+        img_url = url_to_local_path(\"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/demo_small.jpg\")\n+        self.image = load_image(img_url).convert(\"RGB\")\n \n         cleanup(torch_device, gc_collect=True)\n "
        },
        {
            "sha": "e5e3a84be9564ecd70979148ea838edaade62033",
            "filename": "utils/important_files.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/50d2448a1a7b75354c3d0ca879afd124abd244ac/utils%2Fimportant_files.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50d2448a1a7b75354c3d0ca879afd124abd244ac/utils%2Fimportant_files.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fimportant_files.py?ref=50d2448a1a7b75354c3d0ca879afd124abd244ac",
            "patch": "@@ -5,7 +5,8 @@\n     \"gpt2\",\n     \"t5\",\n     \"modernbert\",\n-    \"vit,clip\",\n+    \"vit\",\n+    \"clip\",\n     \"detr\",\n     \"table_transformer\",\n     \"got_ocr2\","
        }
    ],
    "stats": {
        "total": 92,
        "additions": 41,
        "deletions": 51
    }
}