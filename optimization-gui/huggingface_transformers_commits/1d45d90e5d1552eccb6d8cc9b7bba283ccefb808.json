{
    "author": "gante",
    "message": "[tests] remove TF tests (uses of `require_tf`) (#38944)\n\n* remove uses of require_tf\n\n* remove redundant import guards\n\n* this class has no tests\n\n* nits\n\n* del tf rng comment",
    "sha": "1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
    "files": [
        {
            "sha": "07be15f31ece9c085a8ec9eedb7fbf06bb708592",
            "filename": "docs/source/de/testing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fde%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fde%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Ftesting.md?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -473,13 +473,6 @@ Hier ist zum Beispiel ein Test, der nur ausgeführt werden muss, wenn 2 oder meh\n def test_example_with_multi_gpu():\n ```\n \n-Wenn ein Test `tensorflow` benötigt, verwenden Sie den Dekorator `require_tf`. Zum Beispiel:\n-\n-```python no-style\n-@require_tf\n-def test_tf_thing_with_tensorflow():\n-```\n-\n Diese Dekors können gestapelt werden. Wenn zum Beispiel ein Test langsam ist und mindestens eine GPU unter pytorch benötigt, können Sie\n wie Sie ihn einrichten können:\n \n@@ -1204,9 +1197,6 @@ if torch.cuda.is_available():\n import numpy as np\n \n np.random.seed(seed)\n-\n-# tf RNG\n-tf.random.set_seed(seed)\n ```\n \n ### Tests debuggen"
        },
        {
            "sha": "ddcb363f8cb82c789b45fb1e2c7a0decba8fc428",
            "filename": "docs/source/en/testing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fen%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fen%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftesting.md?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -474,13 +474,6 @@ For example, here is a test that must be run only when there are 2 or more GPUs\n def test_example_with_multi_gpu():\n ```\n \n-If a test requires `tensorflow` use the `require_tf` decorator. For example:\n-\n-```python no-style\n-@require_tf\n-def test_tf_thing_with_tensorflow():\n-```\n-\n These decorators can be stacked. For example, if a test is slow and requires at least one GPU under pytorch, here is\n how to set it up:\n \n@@ -1226,11 +1219,6 @@ if torch.cuda.is_available():\n import numpy as np\n \n np.random.seed(seed)\n-\n-# tf RNG\n-import tensorflow as tf \n-\n-tf.random.set_seed(seed)\n ```\n \n ### Debugging tests"
        },
        {
            "sha": "5425861a1d19a660797688004da094332cc9dceb",
            "filename": "docs/source/ja/testing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fja%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fja%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftesting.md?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -445,13 +445,6 @@ CUDA_VISIBLE_DEVICES=\"1\" pytest tests/utils/test_logging.py\n def test_example_with_multi_gpu():\n ```\n \n-テストに `tensorflow` が必要な場合は、`require_tf` デコレータを使用します。例えば：\n-\n-```python no-style\n-@require_tf\n-def test_tf_thing_with_tensorflow():\n-```\n-\n これらのデコレータは積み重ねることができます。たとえば、テストが遅く、pytorch で少なくとも 1 つの GPU が必要な場合は、次のようになります。\n 設定方法:\n \n@@ -1135,9 +1128,6 @@ if torch.cuda.is_available():\n import numpy as np\n \n np.random.seed(seed)\n-\n-# tf RNG\n-tf.random.set_seed(seed)\n ```\n \n "
        },
        {
            "sha": "0a9e8ee47acad0a1860e15ead6cb9c8acc4a80a9",
            "filename": "docs/source/ko/testing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fko%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/docs%2Fsource%2Fko%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftesting.md?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -473,13 +473,6 @@ GPU 요구 사항을 표로 정리하면 아래와 같습니디ㅏ:\n def test_example_with_multi_gpu():\n ```\n \n-`tensorflow`가 필요한 경우 `require_tf` 데코레이터를 사용합니다. 예를 들어 다음과 같습니다:\n-\n-```python no-style\n-@require_tf\n-def test_tf_thing_with_tensorflow():\n-```\n-\n 이러한 데코레이터는 중첩될 수 있습니다.\n 예를 들어, 느린 테스트로 진행되고 pytorch에서 적어도 하나의 GPU가 필요한 경우 다음과 같이 설정할 수 있습니다:\n "
        },
        {
            "sha": "10f31b81c8f5bea43a4acf1377cdc83d13d355e5",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -705,6 +705,9 @@ def require_tf(test_case):\n     \"\"\"\n     Decorator marking a test that requires TensorFlow. These tests are skipped when TensorFlow isn't installed.\n     \"\"\"\n+    logger.warning_once(\n+        \"TensorFlow test-related code, including `require_tf`, is deprecated and will be removed in Transformers v4.55\"\n+    )\n     return unittest.skipUnless(is_tf_available(), \"test requires TensorFlow\")(test_case)\n \n "
        },
        {
            "sha": "0539613a10fe3410d089516773ad26716bc28167",
            "filename": "tests/models/bert/test_tokenization_bert_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 106,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_tokenization_bert_tf.py?ref=d37f7517972f67e3f2194c000ed0f87f064e5099",
            "patch": "@@ -1,106 +0,0 @@\n-import unittest\n-from pathlib import Path\n-from tempfile import TemporaryDirectory\n-\n-from transformers import AutoConfig, TFAutoModel, is_tensorflow_text_available, is_tf_available\n-from transformers.models.bert.tokenization_bert import BertTokenizer\n-from transformers.testing_utils import require_tensorflow_text, require_tf, slow\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.modeling_tf_utils import keras\n-\n-if is_tensorflow_text_available():\n-    from transformers.models.bert import TFBertTokenizer\n-\n-\n-TOKENIZER_CHECKPOINTS = [\"google-bert/bert-base-uncased\", \"google-bert/bert-base-cased\"]\n-TINY_MODEL_CHECKPOINT = \"hf-internal-testing/tiny-bert-tf-only\"\n-\n-if is_tf_available():\n-    from transformers.modeling_tf_utils import keras\n-\n-    class ModelToSave(keras.Model):\n-        def __init__(self, tokenizer):\n-            super().__init__()\n-            self.tokenizer = tokenizer\n-            config = AutoConfig.from_pretrained(TINY_MODEL_CHECKPOINT)\n-            self.bert = TFAutoModel.from_config(config)\n-\n-        def call(self, inputs):\n-            tokenized = self.tokenizer(inputs)\n-            out = self.bert(tokenized)\n-            return out[\"pooler_output\"]\n-\n-\n-@require_tf\n-@require_tensorflow_text\n-class BertTokenizationTest(unittest.TestCase):\n-    # The TF tokenizers are usually going to be used as pretrained tokenizers from existing model checkpoints,\n-    # so that's what we focus on here.\n-\n-    def setUp(self):\n-        super().setUp()\n-\n-        self.tokenizers = [BertTokenizer.from_pretrained(checkpoint) for checkpoint in TOKENIZER_CHECKPOINTS]\n-        self.tf_tokenizers = [TFBertTokenizer.from_pretrained(checkpoint) for checkpoint in TOKENIZER_CHECKPOINTS]\n-        assert len(self.tokenizers) == len(self.tf_tokenizers)\n-\n-        self.test_sentences = [\n-            \"This is a straightforward English test sentence.\",\n-            \"This one has some weird characters\\rto\\nsee\\r\\nif  those\\u00e9break things.\",\n-            \"Now we're going to add some Chinese: 一 二 三 一二三\",\n-            \"And some much more rare Chinese: 齉 堃 齉堃\",\n-            \"Je vais aussi écrire en français pour tester les accents\",\n-            \"Classical Irish also has some unusual characters, so in they go: Gaelaċ, ꝼ\",\n-        ]\n-        self.paired_sentences = list(zip(self.test_sentences, self.test_sentences[::-1]))\n-\n-    def test_output_equivalence(self):\n-        for tokenizer, tf_tokenizer in zip(self.tokenizers, self.tf_tokenizers):\n-            for test_inputs in (self.test_sentences, self.paired_sentences):\n-                python_outputs = tokenizer(test_inputs, return_tensors=\"tf\", padding=\"longest\")\n-                tf_outputs = tf_tokenizer(test_inputs)\n-\n-                for key in python_outputs.keys():\n-                    self.assertTrue(tf.reduce_all(python_outputs[key].shape == tf_outputs[key].shape))\n-                    self.assertTrue(tf.reduce_all(tf.cast(python_outputs[key], tf.int64) == tf_outputs[key]))\n-\n-    @slow\n-    def test_different_pairing_styles(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            merged_outputs = tf_tokenizer(self.paired_sentences)\n-            separated_outputs = tf_tokenizer(\n-                text=[sentence[0] for sentence in self.paired_sentences],\n-                text_pair=[sentence[1] for sentence in self.paired_sentences],\n-            )\n-            for key in merged_outputs.keys():\n-                self.assertTrue(tf.reduce_all(tf.cast(merged_outputs[key], tf.int64) == separated_outputs[key]))\n-\n-    @slow\n-    def test_graph_mode(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            compiled_tokenizer = tf.function(tf_tokenizer)\n-            for test_inputs in (self.test_sentences, self.paired_sentences):\n-                test_inputs = tf.constant(test_inputs)\n-                compiled_outputs = compiled_tokenizer(test_inputs)\n-                eager_outputs = tf_tokenizer(test_inputs)\n-\n-                for key in eager_outputs.keys():\n-                    self.assertTrue(tf.reduce_all(eager_outputs[key] == compiled_outputs[key]))\n-\n-    @slow\n-    def test_export_for_inference(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            model = ModelToSave(tokenizer=tf_tokenizer)\n-            test_inputs = tf.convert_to_tensor(self.test_sentences)\n-            out = model(test_inputs)  # Build model with some sample inputs\n-            with TemporaryDirectory() as tempdir:\n-                save_path = Path(tempdir) / \"saved.model\"\n-                model.export(save_path)\n-                loaded_model = tf.saved_model.load(save_path)\n-            loaded_output = loaded_model.serve(test_inputs)\n-            # We may see small differences because the loaded model is compiled, so we need an epsilon for the test\n-            self.assertLessEqual(tf.reduce_max(tf.abs(out - loaded_output)), 1e-5)"
        },
        {
            "sha": "06f16c36e31711552490602618d0234fbc24354e",
            "filename": "tests/models/gpt2/test_tokenization_gpt2_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 131,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2_tf.py?ref=d37f7517972f67e3f2194c000ed0f87f064e5099",
            "patch": "@@ -1,131 +0,0 @@\n-import unittest\n-from pathlib import Path\n-from tempfile import TemporaryDirectory\n-\n-from transformers import AutoConfig, TFGPT2LMHeadModel, is_keras_nlp_available, is_tf_available\n-from transformers.models.gpt2.tokenization_gpt2 import GPT2Tokenizer\n-from transformers.testing_utils import require_keras_nlp, require_tf, slow\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-\n-if is_keras_nlp_available():\n-    from transformers.models.gpt2 import TFGPT2Tokenizer\n-\n-\n-TOKENIZER_CHECKPOINTS = [\"openai-community/gpt2\"]\n-TINY_MODEL_CHECKPOINT = \"openai-community/gpt2\"\n-\n-if is_tf_available():\n-\n-    class ModelToSave(tf.Module):\n-        def __init__(self, tokenizer):\n-            super().__init__()\n-            self.tokenizer = tokenizer\n-            config = AutoConfig.from_pretrained(TINY_MODEL_CHECKPOINT)\n-            self.model = TFGPT2LMHeadModel.from_config(config)\n-\n-        @tf.function(input_signature=(tf.TensorSpec((None,), tf.string, name=\"text\"),))\n-        def serving(self, text):\n-            tokenized = self.tokenizer(text)\n-            input_ids_dense = tokenized[\"input_ids\"].to_tensor()\n-\n-            input_mask = tf.cast(input_ids_dense > 0, tf.int32)\n-            # input_mask = tf.reshape(input_mask, [-1, MAX_SEQ_LEN])\n-\n-            outputs = self.model(input_ids=input_ids_dense, attention_mask=input_mask)[\"logits\"]\n-\n-            return outputs\n-\n-\n-@require_tf\n-@require_keras_nlp\n-class GPTTokenizationTest(unittest.TestCase):\n-    # The TF tokenizers are usually going to be used as pretrained tokenizers from existing model checkpoints,\n-    # so that's what we focus on here.\n-\n-    def setUp(self):\n-        super().setUp()\n-\n-        self.tokenizers = [GPT2Tokenizer.from_pretrained(checkpoint) for checkpoint in (TOKENIZER_CHECKPOINTS)]\n-        self.tf_tokenizers = [TFGPT2Tokenizer.from_pretrained(checkpoint) for checkpoint in TOKENIZER_CHECKPOINTS]\n-        assert len(self.tokenizers) == len(self.tf_tokenizers)\n-\n-        self.test_sentences = [\n-            \"This is a straightforward English test sentence.\",\n-            \"This one has some weird characters\\rto\\nsee\\r\\nif  those\\u00e9break things.\",\n-            \"Now we're going to add some Chinese: 一 二 三 一二三\",\n-            \"And some much more rare Chinese: 齉 堃 齉堃\",\n-            \"Je vais aussi écrire en français pour tester les accents\",\n-            \"Classical Irish also has some unusual characters, so in they go: Gaelaċ, ꝼ\",\n-        ]\n-        self.paired_sentences = list(zip(self.test_sentences, self.test_sentences[::-1]))\n-\n-    def test_output_equivalence(self):\n-        for tokenizer, tf_tokenizer in zip(self.tokenizers, self.tf_tokenizers):\n-            for test_inputs in self.test_sentences:\n-                python_outputs = tokenizer([test_inputs], return_tensors=\"tf\")\n-                tf_outputs = tf_tokenizer([test_inputs])\n-\n-                for key in python_outputs.keys():\n-                    # convert them to numpy to avoid messing with ragged tensors\n-                    python_outputs_values = python_outputs[key].numpy()\n-                    tf_outputs_values = tf_outputs[key].numpy()\n-\n-                    self.assertTrue(tf.reduce_all(python_outputs_values.shape == tf_outputs_values.shape))\n-                    self.assertTrue(tf.reduce_all(tf.cast(python_outputs_values, tf.int64) == tf_outputs_values))\n-\n-    @slow\n-    def test_graph_mode(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            compiled_tokenizer = tf.function(tf_tokenizer)\n-            for test_inputs in self.test_sentences:\n-                test_inputs = tf.constant(test_inputs)\n-                compiled_outputs = compiled_tokenizer(test_inputs)\n-                eager_outputs = tf_tokenizer(test_inputs)\n-\n-                for key in eager_outputs.keys():\n-                    self.assertTrue(tf.reduce_all(eager_outputs[key] == compiled_outputs[key]))\n-\n-    @slow\n-    def test_saved_model(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            model = ModelToSave(tokenizer=tf_tokenizer)\n-            test_inputs = tf.convert_to_tensor([self.test_sentences[0]])\n-            out = model.serving(test_inputs)  # Build model with some sample inputs\n-            with TemporaryDirectory() as tempdir:\n-                save_path = Path(tempdir) / \"saved.model\"\n-                tf.saved_model.save(model, save_path, signatures={\"serving_default\": model.serving})\n-                loaded_model = tf.saved_model.load(save_path)\n-            loaded_output = loaded_model.signatures[\"serving_default\"](test_inputs)[\"output_0\"]\n-            # We may see small differences because the loaded model is compiled, so we need an epsilon for the test\n-            self.assertTrue(tf.reduce_all(out == loaded_output))\n-\n-    @slow\n-    def test_from_config(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            test_inputs = tf.convert_to_tensor([self.test_sentences[0]])\n-            out = tf_tokenizer(test_inputs)  # Build model with some sample inputs\n-\n-            config = tf_tokenizer.get_config()\n-            model_from_config = TFGPT2Tokenizer.from_config(config)\n-            from_config_output = model_from_config(test_inputs)\n-\n-            for key in from_config_output.keys():\n-                self.assertTrue(tf.reduce_all(from_config_output[key] == out[key]))\n-\n-    @slow\n-    def test_padding(self):\n-        for tf_tokenizer in self.tf_tokenizers:\n-            # for the test to run\n-            tf_tokenizer.pad_token_id = 123123\n-\n-            for max_length in [3, 5, 1024]:\n-                test_inputs = tf.convert_to_tensor([self.test_sentences[0]])\n-                out = tf_tokenizer(test_inputs, max_length=max_length)\n-\n-                out_length = out[\"input_ids\"].numpy().shape[1]\n-\n-                assert out_length == max_length"
        },
        {
            "sha": "deee8d31d24c4b3213ad3470edfd42e33ebd5852",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -34,7 +34,6 @@\n from transformers.models.layoutlmv3.tokenization_layoutlmv3 import VOCAB_FILES_NAMES, LayoutLMv3Tokenizer\n from transformers.testing_utils import (\n     require_pandas,\n-    require_tf,\n     require_tokenizers,\n     require_torch,\n     slow,\n@@ -2306,42 +2305,6 @@ def test_layoutlmv3_integration_test(self):\n     def test_np_encode_plus_sent_to_model(self):\n         pass\n \n-    @require_tf\n-    @slow\n-    def test_tf_encode_plus_sent_to_model(self):\n-        from transformers import TF_MODEL_MAPPING, TOKENIZER_MAPPING\n-\n-        MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(TF_MODEL_MAPPING, TOKENIZER_MAPPING)\n-\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n-                    self.skipTest(f\"{tokenizer.__class__} is not in the MODEL_TOKENIZER_MAPPING\")\n-\n-                config_class, model_class = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n-                config = config_class()\n-\n-                if config.is_encoder_decoder or config.pad_token_id is None:\n-                    self.skipTest(reason=\"Model is an encoder-decoder or has no pad token id set.\")\n-\n-                model = model_class(config)\n-\n-                # Make sure the model contains at least the full vocabulary size in its embedding matrix\n-                self.assertGreaterEqual(model.config.vocab_size, len(tokenizer))\n-\n-                # Build sequence\n-                first_ten_tokens = list(tokenizer.get_vocab().keys())[:10]\n-                boxes = [[1000, 1000, 1000, 1000] for _ in range(len(first_ten_tokens))]\n-                encoded_sequence = tokenizer.encode_plus(first_ten_tokens, boxes=boxes, return_tensors=\"tf\")\n-                batch_encoded_sequence = tokenizer.batch_encode_plus(\n-                    [first_ten_tokens, first_ten_tokens], boxes=[boxes, boxes], return_tensors=\"tf\"\n-                )\n-\n-                # This should not fail\n-                model(encoded_sequence)\n-                model(batch_encoded_sequence)\n-\n     @unittest.skip(reason=\"Chat is not supported\")\n     def test_chat_template(self):\n         pass"
        },
        {
            "sha": "7a744a68e3b751a24873c342b90e00a3d578bc28",
            "filename": "tests/models/pop2piano/test_feature_extraction_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -24,7 +24,6 @@\n     require_essentia,\n     require_librosa,\n     require_scipy,\n-    require_tf,\n     require_torch,\n )\n from transformers.utils.import_utils import (\n@@ -231,28 +230,6 @@ def test_batch_feature_pt(self):\n         # check shape\n         self.assertEqual(len(input_features[\"input_features\"].shape), 3)\n \n-    @require_tf\n-    def test_batch_feature_tf(self):\n-        import tensorflow as tf\n-\n-        feature_extractor = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())\n-        speech_input1 = np.zeros([1_000_000], dtype=np.float32)\n-        speech_input2 = np.ones([2_000_000], dtype=np.float32)\n-        speech_input3 = np.random.randint(low=0, high=10, size=500_000).astype(np.float32)\n-\n-        input_features = feature_extractor(\n-            [speech_input1, speech_input2, speech_input3],\n-            sampling_rate=[44_100, 16_000, 48_000],\n-            return_tensors=\"tf\",\n-            return_attention_mask=True,\n-        )\n-\n-        # check tf tensor or not\n-        self.assertTrue(tf.is_tensor(input_features[\"input_features\"]))\n-\n-        # check shape\n-        self.assertEqual(len(input_features[\"input_features\"].shape), 3)\n-\n     @unittest.skip(\n         \"Pop2PianoFeatureExtractor does not supports padding externally (while processing audios in batches padding is automatically applied to max_length)\"\n     )"
        },
        {
            "sha": "15cd2b0297c6281181b8cf4105bfd5d4cbe97f4b",
            "filename": "tests/models/sam/test_processor_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 153,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_processor_sam.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -17,15 +17,10 @@\n \n import numpy as np\n \n-from transformers.testing_utils import (\n-    require_tf,\n-    require_torch,\n-    require_torchvision,\n-    require_vision,\n-)\n-from transformers.utils import is_tf_available, is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n \n-from ...test_processing_common import ProcessorTesterMixin, prepare_image_inputs\n+from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n@@ -38,11 +33,6 @@\n \n     from transformers.models.sam.image_processing_sam import _mask_to_rle_pytorch\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.models.sam.image_processing_sam import _mask_to_rle_tf\n-\n \n @require_vision\n @require_torchvision\n@@ -202,143 +192,3 @@ def test_rle_encoding(self):\n         self.assertEqual(len(rle), 1)\n         self.assertEqual(rle[0][\"size\"], [2, 2])\n         self.assertEqual(rle[0][\"counts\"], [1, 3])  # 1 zero, followed by 3 ones\n-\n-\n-@require_vision\n-@require_tf\n-class TFSamProcessorTest(unittest.TestCase):\n-    def setUp(self):\n-        self.tmpdirname = tempfile.mkdtemp()\n-        image_processor = SamImageProcessor()\n-        processor = SamProcessor(image_processor)\n-        processor.save_pretrained(self.tmpdirname)\n-\n-    def get_image_processor(self, **kwargs):\n-        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n-\n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n-\n-    # This is to avoid repeating the skipping of the common tests\n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images.\"\"\"\n-        return prepare_image_inputs()\n-\n-    def test_save_load_pretrained_additional_features(self):\n-        processor = SamProcessor(image_processor=self.get_image_processor())\n-        processor.save_pretrained(self.tmpdirname)\n-\n-        image_processor_add_kwargs = self.get_image_processor(do_normalize=False, padding_value=1.0)\n-\n-        processor = SamProcessor.from_pretrained(self.tmpdirname, do_normalize=False, padding_value=1.0)\n-\n-        self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n-        self.assertIsInstance(processor.image_processor, SamImageProcessor)\n-\n-    def test_image_processor(self):\n-        image_processor = self.get_image_processor()\n-\n-        processor = SamProcessor(image_processor=image_processor)\n-\n-        image_input = self.prepare_image_inputs()\n-\n-        input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n-        input_processor = processor(images=image_input, return_tensors=\"np\")\n-\n-        input_feat_extract.pop(\"original_sizes\")  # pop original_sizes as it is popped in the processor\n-        input_feat_extract.pop(\"reshaped_input_sizes\")  # pop reshaped_input_sizes as it is popped in the processor\n-\n-        for key in input_feat_extract.keys():\n-            self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n-\n-    @require_tf\n-    def test_post_process_masks(self):\n-        image_processor = self.get_image_processor()\n-\n-        processor = SamProcessor(image_processor=image_processor)\n-        dummy_masks = [tf.ones((1, 3, 5, 5))]\n-\n-        original_sizes = [[1764, 2646]]\n-\n-        reshaped_input_size = [[683, 1024]]\n-        masks = processor.post_process_masks(dummy_masks, original_sizes, reshaped_input_size, return_tensors=\"tf\")\n-        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n-\n-        masks = processor.post_process_masks(\n-            dummy_masks,\n-            tf.convert_to_tensor(original_sizes),\n-            tf.convert_to_tensor(reshaped_input_size),\n-            return_tensors=\"tf\",\n-        )\n-        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n-\n-        # should also work with np\n-        dummy_masks = [np.ones((1, 3, 5, 5))]\n-        masks = processor.post_process_masks(\n-            dummy_masks, np.array(original_sizes), np.array(reshaped_input_size), return_tensors=\"tf\"\n-        )\n-\n-        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n-\n-        dummy_masks = [[1, 0], [0, 1]]\n-        with self.assertRaises(tf.errors.InvalidArgumentError):\n-            masks = processor.post_process_masks(\n-                dummy_masks, np.array(original_sizes), np.array(reshaped_input_size), return_tensors=\"tf\"\n-            )\n-\n-    def test_rle_encoding(self):\n-        \"\"\"\n-        Test the run-length encoding function.\n-        \"\"\"\n-        # Test that a mask of all zeros returns a single run [height * width].\n-        input_mask = tf.zeros((1, 2, 2), dtype=tf.int64)  # shape: 1 x 2 x 2\n-        rle = _mask_to_rle_tf(input_mask)\n-\n-        self.assertEqual(len(rle), 1)\n-        self.assertEqual(rle[0][\"size\"], [2, 2])\n-        # For a 2x2 all-zero mask, we expect a single run of length 4:\n-        self.assertEqual(rle[0][\"counts\"], [4])\n-\n-        # Test that a mask of all ones returns [0, height * width].\n-        input_mask = tf.ones((1, 2, 2), dtype=tf.int64)  # shape: 1 x 2 x 2\n-        rle = _mask_to_rle_tf(input_mask)\n-\n-        self.assertEqual(len(rle), 1)\n-        self.assertEqual(rle[0][\"size\"], [2, 2])\n-        # For a 2x2 all-one mask, we expect two runs: [0, 4].\n-        self.assertEqual(rle[0][\"counts\"], [0, 4])\n-\n-        # Test a mask with mixed 0s and 1s to ensure the run-length encoding is correct.\n-        # Example mask:\n-        # Row 0: [0, 1]\n-        # Row 1: [1, 1]\n-        # This is shape (1, 2, 2).\n-        # Flattened in Fortran order -> [0, 1, 1, 1].\n-        # The RLE for [0,1,1,1] is [1, 3].\n-        input_mask = tf.constant([[[0, 1], [1, 1]]], dtype=tf.int64)\n-        rle = _mask_to_rle_tf(input_mask)\n-\n-        self.assertEqual(len(rle), 1)\n-        self.assertEqual(rle[0][\"size\"], [2, 2])\n-        self.assertEqual(rle[0][\"counts\"], [1, 3])  # 1 zero, followed by 3 ones\n-\n-\n-@require_vision\n-@require_torchvision\n-class SamProcessorEquivalenceTest(unittest.TestCase):\n-    def setUp(self):\n-        self.tmpdirname = tempfile.mkdtemp()\n-        image_processor = SamImageProcessor()\n-        processor = SamProcessor(image_processor)\n-        processor.save_pretrained(self.tmpdirname)\n-\n-    def get_image_processor(self, **kwargs):\n-        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n-\n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n-\n-    # This is to avoid repeating the skipping of the common tests\n-    def prepare_image_inputs(self):\n-        \"\"\"This function prepares a list of PIL images.\"\"\"\n-        return prepare_image_inputs()"
        },
        {
            "sha": "45ba9c401b8a3102c94812a82e40256593196716",
            "filename": "tests/models/whisper/test_tokenization_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_tokenization_whisper.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -18,7 +18,7 @@\n \n from transformers.models.whisper import WhisperTokenizer, WhisperTokenizerFast\n from transformers.models.whisper.tokenization_whisper import _combine_tokens_into_words, _find_longest_common_sequence\n-from transformers.testing_utils import require_flax, require_tf, require_torch, slow\n+from transformers.testing_utils import require_flax, require_torch, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -588,15 +588,6 @@ def test_convert_to_list_np(self):\n         self.assertListEqual(WhisperTokenizer._convert_to_list(np_array), test_list)\n         self.assertListEqual(WhisperTokenizerFast._convert_to_list(np_array), test_list)\n \n-    @require_tf\n-    def test_convert_to_list_tf(self):\n-        import tensorflow as tf\n-\n-        test_list = [[1, 2, 3], [4, 5, 6]]\n-        tf_tensor = tf.constant(test_list)\n-        self.assertListEqual(WhisperTokenizer._convert_to_list(tf_tensor), test_list)\n-        self.assertListEqual(WhisperTokenizerFast._convert_to_list(tf_tensor), test_list)\n-\n     @require_flax\n     def test_convert_to_list_jax(self):\n         import jax.numpy as jnp"
        },
        {
            "sha": "d3a948c938dfdc440cae1ad16c0e5c25056b1aa3",
            "filename": "tests/optimization/test_optimization_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 100,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Foptimization%2Ftest_optimization_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Foptimization%2Ftest_optimization_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Foptimization%2Ftest_optimization_tf.py?ref=d37f7517972f67e3f2194c000ed0f87f064e5099",
            "patch": "@@ -1,100 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_tf\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-    from tensorflow.python.eager import context\n-    from tensorflow.python.framework import ops\n-\n-    from transformers import GradientAccumulator, create_optimizer\n-\n-\n-@require_tf\n-class OptimizationFTest(unittest.TestCase):\n-    def assertListAlmostEqual(self, list1, list2, tol):\n-        self.assertEqual(len(list1), len(list2))\n-        for a, b in zip(list1, list2):\n-            self.assertAlmostEqual(a, b, delta=tol)\n-\n-    def testGradientAccumulator(self):\n-        accumulator = GradientAccumulator()\n-        accumulator([tf.constant([1.0, 2.0])])\n-        accumulator([tf.constant([-2.0, 1.0])])\n-        accumulator([tf.constant([-1.0, 2.0])])\n-        with self.assertRaises(ValueError):\n-            accumulator([tf.constant([1.0, 1.0]), tf.constant([2.0, 2.0])])\n-        self.assertEqual(accumulator.step, 3)\n-        self.assertEqual(len(accumulator.gradients), 1)\n-        self.assertListAlmostEqual(accumulator.gradients[0].numpy().tolist(), [-2.0, 5.0], tol=1e-2)\n-        accumulator.reset()\n-        self.assertEqual(accumulator.step, 0)\n-        self.assertListAlmostEqual(accumulator.gradients[0].numpy().tolist(), [0.0, 0.0], tol=1e-2)\n-\n-    def testGradientAccumulatorDistributionStrategy(self):\n-        context._context = None\n-        ops.enable_eager_execution_internal()\n-        physical_devices = tf.config.list_physical_devices(\"CPU\")\n-        if len(physical_devices) == 1:\n-            tf.config.set_logical_device_configuration(\n-                physical_devices[0], [tf.config.LogicalDeviceConfiguration(), tf.config.LogicalDeviceConfiguration()]\n-            )\n-        devices = tf.config.list_logical_devices(device_type=\"CPU\")\n-        strategy = tf.distribute.MirroredStrategy(devices=devices[:2])\n-\n-        with strategy.scope():\n-            accumulator = GradientAccumulator()\n-            variable = tf.Variable([4.0, 3.0])\n-            optimizer, _ = create_optimizer(5e-5, 10, 5)\n-            gradient_placeholder = tf.Variable([0.0, 0.0], trainable=False)\n-\n-        def accumulate_on_replica(gradient):\n-            accumulator([gradient])\n-\n-        def apply_on_replica():\n-            optimizer.apply_gradients(list(zip(accumulator.gradients, [variable])))\n-\n-        @tf.function\n-        def accumulate(grad1, grad2):\n-            with strategy.scope():\n-                local_variables = strategy.experimental_local_results(gradient_placeholder)\n-                local_variables[0].assign(grad1)\n-                local_variables[1].assign(grad2)\n-                strategy.run(accumulate_on_replica, args=(gradient_placeholder,))\n-\n-        @tf.function\n-        def apply_grad():\n-            with strategy.scope():\n-                strategy.run(apply_on_replica)\n-\n-        def _check_local_values(grad1, grad2):\n-            values = strategy.experimental_local_results(accumulator._gradients[0])\n-            self.assertListAlmostEqual(values[0].value(), grad1, tol=1e-2)\n-            self.assertListAlmostEqual(values[1].value(), grad2, tol=1e-2)\n-\n-        accumulate([1.0, 2.0], [-1.0, 1.0])\n-        accumulate([3.0, -1.0], [-1.0, -1.0])\n-        accumulate([-2.0, 2.0], [3.0, -2.0])\n-        self.assertEqual(accumulator.step, 3)\n-        _check_local_values([2.0, 3.0], [1.0, -2.0])\n-        apply_grad()\n-        self.assertListAlmostEqual(variable.value(), [4.0, 3.0], tol=1e-2)\n-        accumulator.reset()\n-        self.assertEqual(accumulator.step, 0)\n-        _check_local_values([0.0, 0.0], [0.0, 0.0])"
        },
        {
            "sha": "2871467ac907add357750fb25224e6b1ca78e2c8",
            "filename": "tests/pipelines/test_pipelines_audio_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -28,7 +28,6 @@\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torchaudio,\n     slow,\n@@ -193,11 +192,6 @@ def test_large_model_pt(self):\n             ],\n         )\n \n-    @require_tf\n-    @unittest.skip(reason=\"Audio classification is not implemented for TF\")\n-    def test_small_model_tf(self):\n-        pass\n-\n     @require_torch\n     @slow\n     def test_top_k_none_returns_all_labels(self):"
        },
        {
            "sha": "a9977d912c577283a06d561ca755824ad06055c1",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -40,7 +40,6 @@\n     is_torch_available,\n     nested_simplify,\n     require_pyctcdecode,\n-    require_tf,\n     require_torch,\n     require_torch_accelerator,\n     require_torchaudio,\n@@ -326,10 +325,6 @@ def test_large_model_pt_with_lm(self):\n         ):\n             _ = speech_recognizer(filename, return_timestamps=\"char\")\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        self.skipTest(reason=\"Tensorflow not supported yet.\")\n-\n     @require_torch\n     @unittest.skip(\"TODO (joao, eustache): this test is failing, find the breaking PR and fix the cause or the test\")\n     def test_torch_small_no_tokenizer_files(self):"
        },
        {
            "sha": "bc85f0749b1f32c917e42f954341583c152ce648",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 73,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -48,8 +48,6 @@\n     is_pipeline_test,\n     is_staging_test,\n     nested_simplify,\n-    require_tensorflow_probability,\n-    require_tf,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_multi_accelerator,\n@@ -177,20 +175,6 @@ def data(n: int):\n             results.append(out)\n         self.assertEqual(len(results), 10)\n \n-    @require_tf\n-    def test_iterator_data_tf(self):\n-        def data(n: int):\n-            for _ in range(n):\n-                yield \"This is a test\"\n-\n-        pipe = pipeline(model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\")\n-        out = pipe(\"This is a test\")\n-        results = []\n-        for out in pipe(data(10)):\n-            self.assertEqual(nested_simplify(out), {\"label\": \"LABEL_0\", \"score\": 0.504})\n-            results.append(out)\n-        self.assertEqual(len(results), 10)\n-\n     @require_torch\n     def test_unbatch_attentions_hidden_states(self):\n         model = DistilBertForSequenceClassification.from_pretrained(\n@@ -262,9 +246,9 @@ def test_pipeline_with_task_parameters_no_side_effects(self):\n \n \n @is_pipeline_test\n+@require_torch\n class PipelineScikitCompatTest(unittest.TestCase):\n-    @require_torch\n-    def test_pipeline_predict_pt(self):\n+    def test_pipeline_predict(self):\n         data = [\"This is a test\"]\n \n         text_classifier = pipeline(\n@@ -275,20 +259,7 @@ def test_pipeline_predict_pt(self):\n         actual_output = text_classifier.predict(data)\n         self.assertEqual(expected_output, actual_output)\n \n-    @require_tf\n-    def test_pipeline_predict_tf(self):\n-        data = [\"This is a test\"]\n-\n-        text_classifier = pipeline(\n-            task=\"text-classification\", model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\"\n-        )\n-\n-        expected_output = [{\"label\": ANY(str), \"score\": ANY(float)}]\n-        actual_output = text_classifier.predict(data)\n-        self.assertEqual(expected_output, actual_output)\n-\n-    @require_torch\n-    def test_pipeline_transform_pt(self):\n+    def test_pipeline_transform(self):\n         data = [\"This is a test\"]\n \n         text_classifier = pipeline(\n@@ -299,18 +270,6 @@ def test_pipeline_transform_pt(self):\n         actual_output = text_classifier.transform(data)\n         self.assertEqual(expected_output, actual_output)\n \n-    @require_tf\n-    def test_pipeline_transform_tf(self):\n-        data = [\"This is a test\"]\n-\n-        text_classifier = pipeline(\n-            task=\"text-classification\", model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\"\n-        )\n-\n-        expected_output = [{\"label\": ANY(str), \"score\": ANY(float)}]\n-        actual_output = text_classifier.transform(data)\n-        self.assertEqual(expected_output, actual_output)\n-\n \n @is_pipeline_test\n class PipelinePadTest(unittest.TestCase):\n@@ -620,23 +579,6 @@ def test_load_default_pipelines_pt(self):\n             gc.collect()\n             backend_empty_cache(torch_device)\n \n-    @slow\n-    @require_tf\n-    def test_load_default_pipelines_tf(self):\n-        from transformers.modeling_tf_utils import keras\n-        from transformers.pipelines import SUPPORTED_TASKS\n-\n-        set_seed_fn = lambda: keras.utils.set_random_seed(0)  # noqa: E731\n-        for task in SUPPORTED_TASKS.keys():\n-            if task == \"table-question-answering\":\n-                # test table in separate test due to more dependencies\n-                continue\n-\n-            self.check_default_pipeline(task, \"tf\", set_seed_fn, self.check_models_equal_tf)\n-\n-            # clean-up as much as possible GPU memory occupied by TF\n-            gc.collect()\n-\n     @slow\n     @require_torch\n     def test_load_default_pipelines_pt_table_qa(self):\n@@ -663,18 +605,6 @@ def test_pipeline_accelerator_indexed(self):\n         pipe = pipeline(\"text-generation\", device=torch_device)\n         _ = pipe(\"Hello\")\n \n-    @slow\n-    @require_tf\n-    @require_tensorflow_probability\n-    def test_load_default_pipelines_tf_table_qa(self):\n-        import tensorflow as tf\n-\n-        set_seed_fn = lambda: tf.random.set_seed(0)  # noqa: E731\n-        self.check_default_pipeline(\"table-question-answering\", \"tf\", set_seed_fn, self.check_models_equal_tf)\n-\n-        # clean-up as much as possible GPU memory occupied by PyTorch\n-        gc.collect()\n-\n     def check_default_pipeline(self, task, framework, set_seed_fn, check_models_equal_fn):\n         from transformers.pipelines import SUPPORTED_TASKS, pipeline\n "
        },
        {
            "sha": "130d386abe2943068477d27f82cdc26ec62dbf01",
            "filename": "tests/pipelines/test_pipelines_depth_estimation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_depth_estimation.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -24,7 +24,6 @@\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_timm,\n     require_torch,\n     require_vision,\n@@ -123,11 +122,6 @@ def run_pipeline_test(self, depth_estimator, examples):\n         for single_output in outputs:\n             compare_pipeline_output_to_hub_spec(single_output, DepthEstimationOutput)\n \n-    @require_tf\n-    @unittest.skip(reason=\"Depth estimation is not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass\n-\n     @slow\n     @require_torch\n     def test_large_model_pt(self):"
        },
        {
            "sha": "0900b1e10307f89a18bd1c68dbd7b4ccffb64a59",
            "filename": "tests/pipelines/test_pipelines_document_question_answering.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_document_question_answering.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -27,7 +27,6 @@\n     nested_simplify,\n     require_detectron2,\n     require_pytesseract,\n-    require_tf,\n     require_torch,\n     require_torch_bf16,\n     require_vision,\n@@ -423,8 +422,3 @@ def test_large_model_pt_donut(self):\n         question = \"What is the invoice number?\"\n         outputs = dqa_pipeline(image=image, question=question, top_k=2)\n         self.assertEqual(nested_simplify(outputs, decimals=4), [{\"answer\": \"us-001\"}])\n-\n-    @require_tf\n-    @unittest.skip(reason=\"Document question answering not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass"
        },
        {
            "sha": "ff6669e19b3107665945a2e3471f682011b0837c",
            "filename": "tests/pipelines/test_pipelines_feature_extraction.py",
            "status": "modified",
            "additions": 1,
            "deletions": 63,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_feature_extraction.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -23,19 +23,15 @@\n     TF_MODEL_MAPPING,\n     FeatureExtractionPipeline,\n     LxmertConfig,\n-    is_tf_available,\n     is_torch_available,\n     pipeline,\n )\n-from transformers.testing_utils import is_pipeline_test, nested_simplify, require_tf, require_torch\n+from transformers.testing_utils import is_pipeline_test, nested_simplify, require_torch\n \n \n if is_torch_available():\n     import torch\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n \n @is_pipeline_test\n class FeatureExtractionPipelineTests(unittest.TestCase):\n@@ -52,16 +48,6 @@ def test_small_model_pt(self):\n             nested_simplify(outputs),\n             [[[2.287, 1.234, 0.042, 1.53, 1.306, 0.879, -0.526, -1.71, -1.276, 0.756, -0.775, -1.048, -0.25, -0.595, -0.137, -0.598, 2.022, -0.812, 0.284, -0.488, -0.391, -0.403, -0.525, -0.061, -0.228, 1.086, 0.378, -0.14, 0.599, -0.087, -2.259, -0.098], [1.676, 0.232, -1.508, -0.145, 1.798, -1.388, 1.331, -0.37, -0.939, 0.043, 0.06, -0.414, -1.408, 0.24, 0.622, -0.55, -0.569, 1.873, -0.706, 1.924, -0.254, 1.927, -0.423, 0.152, -0.952, 0.509, -0.496, -0.968, 0.093, -1.049, -0.65, 0.312], [0.207, -0.775, -1.822, 0.321, -0.71, -0.201, 0.3, 1.146, -0.233, -0.753, -0.305, 1.309, -1.47, -0.21, 1.802, -1.555, -1.175, 1.323, -0.303, 0.722, -0.076, 0.103, -1.406, 1.931, 0.091, 0.237, 1.172, 1.607, 0.253, -0.9, -1.068, 0.438], [0.615, 1.077, 0.171, -0.175, 1.3, 0.901, -0.653, -0.138, 0.341, -0.654, -0.184, -0.441, -0.424, 0.356, -0.075, 0.26, -1.023, 0.814, 0.524, -0.904, -0.204, -0.623, 1.234, -1.03, 2.594, 0.56, 1.831, -0.199, -1.508, -0.492, -1.687, -2.165], [0.129, 0.008, -1.279, -0.412, -0.004, 1.663, 0.196, 0.104, 0.123, 0.119, 0.635, 1.757, 2.334, -0.799, -1.626, -1.26, 0.595, -0.316, -1.399, 0.232, 0.264, 1.386, -1.171, -0.256, -0.256, -1.944, 1.168, -0.368, -0.714, -0.51, 0.454, 1.148], [-0.32, 0.29, -1.309, -0.177, 0.453, 0.636, -0.024, 0.509, 0.931, -1.754, -1.575, 0.786, 0.046, -1.165, -1.416, 1.373, 1.293, -0.285, -1.541, -1.186, -0.106, -0.994, 2.001, 0.972, -0.02, 1.654, -0.236, 0.643, 1.02, 0.572, -0.914, -0.154], [0.7, -0.937, 0.441, 0.25, 0.78, -0.022, 0.282, -0.095, 1.558, -0.336, 1.706, 0.884, 1.28, 0.198, -0.796, 1.218, -1.769, 1.197, -0.342, -0.177, -0.645, 1.364, 0.008, -0.597, -0.484, -2.772, -0.696, -0.632, -0.34, -1.527, -0.562, 0.862], [2.504, 0.831, -1.271, -0.033, 0.298, -0.735, 1.339, 1.74, 0.233, -1.424, -0.819, -0.761, 0.291, 0.853, -0.092, -0.885, 0.164, 1.025, 0.907, 0.749, -1.515, -0.545, -1.365, 0.271, 0.034, -2.005, 0.031, 0.244, 0.621, 0.176, 0.336, -1.196], [-0.711, 0.591, -1.001, -0.946, 0.784, -1.66, 1.545, 0.799, -0.857, 1.148, 0.213, -0.285, 0.464, -0.139, 0.79, -1.663, -1.121, 0.575, -0.178, -0.508, 1.565, -0.242, -0.346, 1.024, -1.135, -0.158, -2.101, 0.275, 2.009, -0.425, 0.716, 0.981], [0.912, -1.186, -0.846, -0.421, -1.315, -0.827, 0.309, 0.533, 1.029, -2.343, 1.513, -1.238, 1.487, -0.849, 0.896, -0.927, -0.459, 0.159, 0.177, 0.873, 0.935, 1.433, -0.485, 0.737, 1.327, -0.338, 1.608, -0.47, -0.445, -1.118, -0.213, -0.446], [-0.434, -1.362, -1.098, -1.068, 1.507, 0.003, 0.413, -0.395, 0.897, -0.237, 1.405, -0.344, 1.693, 0.677, 0.097, -0.257, -0.602, 1.026, -1.229, 0.855, -0.713, 1.014, 0.443, 0.238, 0.425, -2.184, 1.933, -1.157, -1.132, -0.597, -0.785, 0.967], [0.58, -0.971, 0.789, -0.468, -0.576, 1.779, 1.747, 1.715, -1.939, 0.125, 0.656, -0.042, -1.024, -1.767, 0.107, -0.408, -0.866, -1.774, 1.248, 0.939, -0.033, 1.523, 1.168, -0.744, 0.209, -0.168, -0.316, 0.207, -0.432, 0.047, -0.646, -0.664], [-0.185, -0.613, -1.695, 1.602, -0.32, -0.277, 0.967, 0.728, -0.965, -0.234, 1.069, -0.63, -1.631, 0.711, 0.426, 1.298, -0.191, -0.467, -0.771, 0.971, -0.118, -1.577, -2.064, -0.055, -0.59, 0.642, -0.997, 1.251, 0.538, 1.367, 0.106, 1.704]]])  # fmt: skip\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"feature-extraction\", model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\"\n-        )\n-        outputs = feature_extractor(\"This is a test\")\n-        self.assertEqual(\n-            nested_simplify(outputs),\n-            [[[2.287, 1.234, 0.042, 1.53, 1.306, 0.879, -0.526, -1.71, -1.276, 0.756, -0.775, -1.048, -0.25, -0.595, -0.137, -0.598, 2.022, -0.812, 0.284, -0.488, -0.391, -0.403, -0.525, -0.061, -0.228, 1.086, 0.378, -0.14, 0.599, -0.087, -2.259, -0.098], [1.676, 0.232, -1.508, -0.145, 1.798, -1.388, 1.331, -0.37, -0.939, 0.043, 0.06, -0.414, -1.408, 0.24, 0.622, -0.55, -0.569, 1.873, -0.706, 1.924, -0.254, 1.927, -0.423, 0.152, -0.952, 0.509, -0.496, -0.968, 0.093, -1.049, -0.65, 0.312], [0.207, -0.775, -1.822, 0.321, -0.71, -0.201, 0.3, 1.146, -0.233, -0.753, -0.305, 1.309, -1.47, -0.21, 1.802, -1.555, -1.175, 1.323, -0.303, 0.722, -0.076, 0.103, -1.406, 1.931, 0.091, 0.237, 1.172, 1.607, 0.253, -0.9, -1.068, 0.438], [0.615, 1.077, 0.171, -0.175, 1.3, 0.901, -0.653, -0.138, 0.341, -0.654, -0.184, -0.441, -0.424, 0.356, -0.075, 0.26, -1.023, 0.814, 0.524, -0.904, -0.204, -0.623, 1.234, -1.03, 2.594, 0.56, 1.831, -0.199, -1.508, -0.492, -1.687, -2.165], [0.129, 0.008, -1.279, -0.412, -0.004, 1.663, 0.196, 0.104, 0.123, 0.119, 0.635, 1.757, 2.334, -0.799, -1.626, -1.26, 0.595, -0.316, -1.399, 0.232, 0.264, 1.386, -1.171, -0.256, -0.256, -1.944, 1.168, -0.368, -0.714, -0.51, 0.454, 1.148], [-0.32, 0.29, -1.309, -0.177, 0.453, 0.636, -0.024, 0.509, 0.931, -1.754, -1.575, 0.786, 0.046, -1.165, -1.416, 1.373, 1.293, -0.285, -1.541, -1.186, -0.106, -0.994, 2.001, 0.972, -0.02, 1.654, -0.236, 0.643, 1.02, 0.572, -0.914, -0.154], [0.7, -0.937, 0.441, 0.25, 0.78, -0.022, 0.282, -0.095, 1.558, -0.336, 1.706, 0.884, 1.28, 0.198, -0.796, 1.218, -1.769, 1.197, -0.342, -0.177, -0.645, 1.364, 0.008, -0.597, -0.484, -2.772, -0.696, -0.632, -0.34, -1.527, -0.562, 0.862], [2.504, 0.831, -1.271, -0.033, 0.298, -0.735, 1.339, 1.74, 0.233, -1.424, -0.819, -0.761, 0.291, 0.853, -0.092, -0.885, 0.164, 1.025, 0.907, 0.749, -1.515, -0.545, -1.365, 0.271, 0.034, -2.005, 0.031, 0.244, 0.621, 0.176, 0.336, -1.196], [-0.711, 0.591, -1.001, -0.946, 0.784, -1.66, 1.545, 0.799, -0.857, 1.148, 0.213, -0.285, 0.464, -0.139, 0.79, -1.663, -1.121, 0.575, -0.178, -0.508, 1.565, -0.242, -0.346, 1.024, -1.135, -0.158, -2.101, 0.275, 2.009, -0.425, 0.716, 0.981], [0.912, -1.186, -0.846, -0.421, -1.315, -0.827, 0.309, 0.533, 1.029, -2.343, 1.513, -1.238, 1.487, -0.849, 0.896, -0.927, -0.459, 0.159, 0.177, 0.873, 0.935, 1.433, -0.485, 0.737, 1.327, -0.338, 1.608, -0.47, -0.445, -1.118, -0.213, -0.446], [-0.434, -1.362, -1.098, -1.068, 1.507, 0.003, 0.413, -0.395, 0.897, -0.237, 1.405, -0.344, 1.693, 0.677, 0.097, -0.257, -0.602, 1.026, -1.229, 0.855, -0.713, 1.014, 0.443, 0.238, 0.425, -2.184, 1.933, -1.157, -1.132, -0.597, -0.785, 0.967], [0.58, -0.971, 0.789, -0.468, -0.576, 1.779, 1.747, 1.715, -1.939, 0.125, 0.656, -0.042, -1.024, -1.767, 0.107, -0.408, -0.866, -1.774, 1.248, 0.939, -0.033, 1.523, 1.168, -0.744, 0.209, -0.168, -0.316, 0.207, -0.432, 0.047, -0.646, -0.664], [-0.185, -0.613, -1.695, 1.602, -0.32, -0.277, 0.967, 0.728, -0.965, -0.234, 1.069, -0.63, -1.631, 0.711, 0.426, 1.298, -0.191, -0.467, -0.771, 0.971, -0.118, -1.577, -2.064, -0.055, -0.59, 0.642, -0.997, 1.251, 0.538, 1.367, 0.106, 1.704]]])  # fmt: skip\n-\n     @require_torch\n     def test_tokenization_small_model_pt(self):\n         feature_extractor = pipeline(\n@@ -102,46 +88,6 @@ def test_tokenization_small_model_pt(self):\n                 tokenize_kwargs=tokenize_kwargs,\n             )\n \n-    @require_tf\n-    def test_tokenization_small_model_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"feature-extraction\", model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\"\n-        )\n-        # test with empty parameters\n-        outputs = feature_extractor(\"This is a test\")\n-        self.assertEqual(\n-            nested_simplify(outputs),\n-            [[[2.287, 1.234, 0.042, 1.53, 1.306, 0.879, -0.526, -1.71, -1.276, 0.756, -0.775, -1.048, -0.25, -0.595, -0.137, -0.598, 2.022, -0.812, 0.284, -0.488, -0.391, -0.403, -0.525, -0.061, -0.228, 1.086, 0.378, -0.14, 0.599, -0.087, -2.259, -0.098], [1.676, 0.232, -1.508, -0.145, 1.798, -1.388, 1.331, -0.37, -0.939, 0.043, 0.06, -0.414, -1.408, 0.24, 0.622, -0.55, -0.569, 1.873, -0.706, 1.924, -0.254, 1.927, -0.423, 0.152, -0.952, 0.509, -0.496, -0.968, 0.093, -1.049, -0.65, 0.312], [0.207, -0.775, -1.822, 0.321, -0.71, -0.201, 0.3, 1.146, -0.233, -0.753, -0.305, 1.309, -1.47, -0.21, 1.802, -1.555, -1.175, 1.323, -0.303, 0.722, -0.076, 0.103, -1.406, 1.931, 0.091, 0.237, 1.172, 1.607, 0.253, -0.9, -1.068, 0.438], [0.615, 1.077, 0.171, -0.175, 1.3, 0.901, -0.653, -0.138, 0.341, -0.654, -0.184, -0.441, -0.424, 0.356, -0.075, 0.26, -1.023, 0.814, 0.524, -0.904, -0.204, -0.623, 1.234, -1.03, 2.594, 0.56, 1.831, -0.199, -1.508, -0.492, -1.687, -2.165], [0.129, 0.008, -1.279, -0.412, -0.004, 1.663, 0.196, 0.104, 0.123, 0.119, 0.635, 1.757, 2.334, -0.799, -1.626, -1.26, 0.595, -0.316, -1.399, 0.232, 0.264, 1.386, -1.171, -0.256, -0.256, -1.944, 1.168, -0.368, -0.714, -0.51, 0.454, 1.148], [-0.32, 0.29, -1.309, -0.177, 0.453, 0.636, -0.024, 0.509, 0.931, -1.754, -1.575, 0.786, 0.046, -1.165, -1.416, 1.373, 1.293, -0.285, -1.541, -1.186, -0.106, -0.994, 2.001, 0.972, -0.02, 1.654, -0.236, 0.643, 1.02, 0.572, -0.914, -0.154], [0.7, -0.937, 0.441, 0.25, 0.78, -0.022, 0.282, -0.095, 1.558, -0.336, 1.706, 0.884, 1.28, 0.198, -0.796, 1.218, -1.769, 1.197, -0.342, -0.177, -0.645, 1.364, 0.008, -0.597, -0.484, -2.772, -0.696, -0.632, -0.34, -1.527, -0.562, 0.862], [2.504, 0.831, -1.271, -0.033, 0.298, -0.735, 1.339, 1.74, 0.233, -1.424, -0.819, -0.761, 0.291, 0.853, -0.092, -0.885, 0.164, 1.025, 0.907, 0.749, -1.515, -0.545, -1.365, 0.271, 0.034, -2.005, 0.031, 0.244, 0.621, 0.176, 0.336, -1.196], [-0.711, 0.591, -1.001, -0.946, 0.784, -1.66, 1.545, 0.799, -0.857, 1.148, 0.213, -0.285, 0.464, -0.139, 0.79, -1.663, -1.121, 0.575, -0.178, -0.508, 1.565, -0.242, -0.346, 1.024, -1.135, -0.158, -2.101, 0.275, 2.009, -0.425, 0.716, 0.981], [0.912, -1.186, -0.846, -0.421, -1.315, -0.827, 0.309, 0.533, 1.029, -2.343, 1.513, -1.238, 1.487, -0.849, 0.896, -0.927, -0.459, 0.159, 0.177, 0.873, 0.935, 1.433, -0.485, 0.737, 1.327, -0.338, 1.608, -0.47, -0.445, -1.118, -0.213, -0.446], [-0.434, -1.362, -1.098, -1.068, 1.507, 0.003, 0.413, -0.395, 0.897, -0.237, 1.405, -0.344, 1.693, 0.677, 0.097, -0.257, -0.602, 1.026, -1.229, 0.855, -0.713, 1.014, 0.443, 0.238, 0.425, -2.184, 1.933, -1.157, -1.132, -0.597, -0.785, 0.967], [0.58, -0.971, 0.789, -0.468, -0.576, 1.779, 1.747, 1.715, -1.939, 0.125, 0.656, -0.042, -1.024, -1.767, 0.107, -0.408, -0.866, -1.774, 1.248, 0.939, -0.033, 1.523, 1.168, -0.744, 0.209, -0.168, -0.316, 0.207, -0.432, 0.047, -0.646, -0.664], [-0.185, -0.613, -1.695, 1.602, -0.32, -0.277, 0.967, 0.728, -0.965, -0.234, 1.069, -0.63, -1.631, 0.711, 0.426, 1.298, -0.191, -0.467, -0.771, 0.971, -0.118, -1.577, -2.064, -0.055, -0.59, 0.642, -0.997, 1.251, 0.538, 1.367, 0.106, 1.704]]])  # fmt: skip\n-\n-        # test with various tokenizer parameters\n-        tokenize_kwargs = {\"max_length\": 3}\n-        outputs = feature_extractor(\"This is a test\", tokenize_kwargs=tokenize_kwargs)\n-        self.assertEqual(np.squeeze(outputs).shape, (3, 32))\n-\n-        tokenize_kwargs = {\"truncation\": True, \"padding\": True, \"max_length\": 4}\n-        outputs = feature_extractor(\n-            [\"This is a test\", \"This\", \"This is\", \"This is a\", \"This is a test test test test\"],\n-            tokenize_kwargs=tokenize_kwargs,\n-        )\n-        self.assertEqual(np.squeeze(outputs).shape, (5, 4, 32))\n-\n-        tokenize_kwargs = {\"padding\": True, \"max_length\": 4}\n-        outputs = feature_extractor(\n-            [\"This is a test\", \"This\", \"This is\", \"This is a\", \"This is a test test test test\"],\n-            truncation=True,\n-            tokenize_kwargs=tokenize_kwargs,\n-        )\n-        self.assertEqual(np.squeeze(outputs).shape, (5, 4, 32))\n-\n-        # raise value error if truncation parameter given for two places\n-        tokenize_kwargs = {\"truncation\": True}\n-        with self.assertRaises(ValueError):\n-            _ = feature_extractor(\n-                [\"This is a test\", \"This\", \"This is\", \"This is a\", \"This is a test test test test\"],\n-                truncation=True,\n-                tokenize_kwargs=tokenize_kwargs,\n-            )\n-\n     @require_torch\n     def test_return_tensors_pt(self):\n         feature_extractor = pipeline(\n@@ -150,14 +96,6 @@ def test_return_tensors_pt(self):\n         outputs = feature_extractor(\"This is a test\", return_tensors=True)\n         self.assertTrue(torch.is_tensor(outputs))\n \n-    @require_tf\n-    def test_return_tensors_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"feature-extraction\", model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\"\n-        )\n-        outputs = feature_extractor(\"This is a test\", return_tensors=True)\n-        self.assertTrue(tf.is_tensor(outputs))\n-\n     def get_shape(self, input_, shape=None):\n         if shape is None:\n             shape = []"
        },
        {
            "sha": "fc563f9edd404310636fa27f4ebc889ba4dd2e0d",
            "filename": "tests/pipelines/test_pipelines_fill_mask.py",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_fill_mask.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_fill_mask.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_fill_mask.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -22,7 +22,6 @@\n     is_pipeline_test,\n     is_torch_available,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torch_accelerator,\n     slow,\n@@ -44,47 +43,6 @@ def tearDown(self):\n         if is_torch_available():\n             backend_empty_cache(torch_device)\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        unmasker = pipeline(task=\"fill-mask\", model=\"sshleifer/tiny-distilroberta-base\", top_k=2, framework=\"tf\")\n-        outputs = unmasker(\"My name is <mask>\")\n-        self.assertEqual(\n-            nested_simplify(outputs, decimals=6),\n-            [\n-                {\"sequence\": \"My name is grouped\", \"score\": 2.1e-05, \"token\": 38015, \"token_str\": \" grouped\"},\n-                {\"sequence\": \"My name is accuser\", \"score\": 2.1e-05, \"token\": 25506, \"token_str\": \" accuser\"},\n-            ],\n-        )\n-\n-        outputs = unmasker(\"The largest city in France is <mask>\")\n-        self.assertEqual(\n-            nested_simplify(outputs, decimals=6),\n-            [\n-                {\n-                    \"sequence\": \"The largest city in France is grouped\",\n-                    \"score\": 2.1e-05,\n-                    \"token\": 38015,\n-                    \"token_str\": \" grouped\",\n-                },\n-                {\n-                    \"sequence\": \"The largest city in France is accuser\",\n-                    \"score\": 2.1e-05,\n-                    \"token\": 25506,\n-                    \"token_str\": \" accuser\",\n-                },\n-            ],\n-        )\n-\n-        outputs = unmasker(\"My name is <mask>\", targets=[\" Patrick\", \" Clara\", \" Teven\"], top_k=3)\n-        self.assertEqual(\n-            nested_simplify(outputs, decimals=6),\n-            [\n-                {\"sequence\": \"My name is Clara\", \"score\": 2e-05, \"token\": 13606, \"token_str\": \" Clara\"},\n-                {\"sequence\": \"My name is Patrick\", \"score\": 2e-05, \"token\": 3499, \"token_str\": \" Patrick\"},\n-                {\"sequence\": \"My name is Te\", \"score\": 1.9e-05, \"token\": 2941, \"token_str\": \" Te\"},\n-            ],\n-        )\n-\n     @require_torch\n     def test_small_model_pt(self):\n         unmasker = pipeline(task=\"fill-mask\", model=\"sshleifer/tiny-distilroberta-base\", top_k=2, framework=\"pt\")\n@@ -172,12 +130,6 @@ def test_large_model_pt(self):\n         unmasker = pipeline(task=\"fill-mask\", model=\"distilbert/distilroberta-base\", top_k=2, framework=\"pt\")\n         self.run_large_test(unmasker)\n \n-    @slow\n-    @require_tf\n-    def test_large_model_tf(self):\n-        unmasker = pipeline(task=\"fill-mask\", model=\"distilbert/distilroberta-base\", top_k=2, framework=\"tf\")\n-        self.run_large_test(unmasker)\n-\n     def run_large_test(self, unmasker):\n         outputs = unmasker(\"My name is <mask>\")\n         self.assertEqual(\n@@ -244,13 +196,6 @@ def test_model_no_pad_pt(self):\n         unmasker.tokenizer.pad_token = None\n         self.run_pipeline_test(unmasker, [])\n \n-    @require_tf\n-    def test_model_no_pad_tf(self):\n-        unmasker = pipeline(task=\"fill-mask\", model=\"sshleifer/tiny-distilroberta-base\", framework=\"tf\")\n-        unmasker.tokenizer.pad_token_id = None\n-        unmasker.tokenizer.pad_token = None\n-        self.run_pipeline_test(unmasker, [])\n-\n     def get_test_pipeline(\n         self,\n         model,"
        },
        {
            "sha": "a57774211ec27d5f8c3641a378d59030497b6966",
            "filename": "tests/pipelines/test_pipelines_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_classification.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -29,7 +29,6 @@\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torch_or_tf,\n     require_vision,\n@@ -175,32 +174,6 @@ def test_small_model_pt(self):\n             ],\n         )\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        small_model = \"hf-internal-testing/tiny-random-vit\"\n-        image_classifier = pipeline(\"image-classification\", model=small_model, framework=\"tf\")\n-\n-        outputs = image_classifier(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n-        self.assertEqual(\n-            nested_simplify(outputs, decimals=4),\n-            [{\"label\": \"LABEL_1\", \"score\": 0.574}, {\"label\": \"LABEL_0\", \"score\": 0.426}],\n-        )\n-\n-        outputs = image_classifier(\n-            [\n-                \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n-                \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n-            ],\n-            top_k=2,\n-        )\n-        self.assertEqual(\n-            nested_simplify(outputs, decimals=4),\n-            [\n-                [{\"label\": \"LABEL_1\", \"score\": 0.574}, {\"label\": \"LABEL_0\", \"score\": 0.426}],\n-                [{\"label\": \"LABEL_1\", \"score\": 0.574}, {\"label\": \"LABEL_0\", \"score\": 0.426}],\n-            ],\n-        )\n-\n     def test_custom_tokenizer(self):\n         tokenizer = PreTrainedTokenizerBase()\n "
        },
        {
            "sha": "e17d34714c362dca89c2ebc1f95746b74a898cf4",
            "filename": "tests/pipelines/test_pipelines_image_feature_extraction.py",
            "status": "modified",
            "additions": 1,
            "deletions": 58,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_feature_extraction.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -22,20 +22,16 @@\n     TF_MODEL_MAPPING,\n     TOKENIZER_MAPPING,\n     ImageFeatureExtractionPipeline,\n-    is_tf_available,\n     is_torch_available,\n     is_vision_available,\n     pipeline,\n )\n-from transformers.testing_utils import is_pipeline_test, nested_simplify, require_tf, require_torch\n+from transformers.testing_utils import is_pipeline_test, nested_simplify, require_torch\n \n \n if is_torch_available():\n     import torch\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n if is_vision_available():\n     from PIL import Image\n \n@@ -73,28 +69,6 @@ def test_small_model_w_pooler_pt(self):\n             nested_simplify(outputs[0]),\n             [-0.056,  0.083,  0.021,  0.038,  0.242, -0.279, -0.033, -0.003, 0.200, -0.192,  0.045, -0.095, -0.077,  0.017, -0.058, -0.063, -0.029, -0.204,  0.014,  0.042,  0.305, -0.205, -0.099,  0.146, -0.287,  0.020,  0.168, -0.052,  0.046,  0.048, -0.156,  0.093])  # fmt: skip\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"image-feature-extraction\", model=\"hf-internal-testing/tiny-random-vit-w-pooler\", framework=\"tf\"\n-        )\n-        img = prepare_img()\n-        outputs = feature_extractor(img)\n-        self.assertEqual(\n-            nested_simplify(outputs[0][0]),\n-            [-1.417, -0.392, -1.264, -1.196, 1.648, 0.885, 0.56, -0.606, -1.175, 0.823, 1.912, 0.081, -0.053, 1.119, -0.062, -1.757, -0.571, 0.075, 0.959, 0.118, 1.201, -0.672, -0.498, 0.364, 0.937, -1.623, 0.228, 0.19, 1.697, -1.115, 0.583, -0.981])  # fmt: skip\n-\n-    @require_tf\n-    def test_small_model_w_pooler_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"image-feature-extraction\", model=\"hf-internal-testing/tiny-random-vit-w-pooler\", framework=\"tf\"\n-        )\n-        img = prepare_img()\n-        outputs = feature_extractor(img, pool=True)\n-        self.assertEqual(\n-            nested_simplify(outputs[0]),\n-            [-0.056,  0.083,  0.021,  0.038,  0.242, -0.279, -0.033, -0.003, 0.200, -0.192,  0.045, -0.095, -0.077,  0.017, -0.058, -0.063, -0.029, -0.204,  0.014,  0.042,  0.305, -0.205, -0.099,  0.146, -0.287,  0.020,  0.168, -0.052,  0.046,  0.048, -0.156,  0.093])  # fmt: skip\n-\n     @require_torch\n     def test_image_processing_small_model_pt(self):\n         feature_extractor = pipeline(\n@@ -117,28 +91,6 @@ def test_image_processing_small_model_pt(self):\n         outputs = feature_extractor(img, pool=True)\n         self.assertEqual(np.squeeze(outputs).shape, (32,))\n \n-    @require_tf\n-    def test_image_processing_small_model_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"image-feature-extraction\", model=\"hf-internal-testing/tiny-random-vit\", framework=\"tf\"\n-        )\n-\n-        # test with image processor parameters\n-        image_processor_kwargs = {\"size\": {\"height\": 300, \"width\": 300}}\n-        img = prepare_img()\n-        with pytest.raises(ValueError):\n-            # Image doesn't match model input size\n-            feature_extractor(img, image_processor_kwargs=image_processor_kwargs)\n-\n-        image_processor_kwargs = {\"image_mean\": [0, 0, 0], \"image_std\": [1, 1, 1]}\n-        img = prepare_img()\n-        outputs = feature_extractor(img, image_processor_kwargs=image_processor_kwargs)\n-        self.assertEqual(np.squeeze(outputs).shape, (226, 32))\n-\n-        # Test pooling option\n-        outputs = feature_extractor(img, pool=True)\n-        self.assertEqual(np.squeeze(outputs).shape, (32,))\n-\n     @require_torch\n     def test_return_tensors_pt(self):\n         feature_extractor = pipeline(\n@@ -148,15 +100,6 @@ def test_return_tensors_pt(self):\n         outputs = feature_extractor(img, return_tensors=True)\n         self.assertTrue(torch.is_tensor(outputs))\n \n-    @require_tf\n-    def test_return_tensors_tf(self):\n-        feature_extractor = pipeline(\n-            task=\"image-feature-extraction\", model=\"hf-internal-testing/tiny-random-vit\", framework=\"tf\"\n-        )\n-        img = prepare_img()\n-        outputs = feature_extractor(img, return_tensors=True)\n-        self.assertTrue(tf.is_tensor(outputs))\n-\n     def get_test_pipeline(\n         self,\n         model,"
        },
        {
            "sha": "3860a39d6e83a0ede8ecc24091d0dd8fce051007",
            "filename": "tests/pipelines/test_pipelines_image_segmentation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_segmentation.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -39,7 +39,6 @@\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_timm,\n     require_torch,\n     require_vision,\n@@ -202,11 +201,6 @@ def run_pipeline_test(self, image_segmenter, examples):\n             for output_element in single_output:\n                 compare_pipeline_output_to_hub_spec(output_element, ImageSegmentationOutputElement)\n \n-    @require_tf\n-    @unittest.skip(reason=\"Image segmentation not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass\n-\n     @require_torch\n     def test_small_model_pt_no_panoptic(self):\n         model_id = \"hf-internal-testing/tiny-random-mobilevit\""
        },
        {
            "sha": "d7ce7091583c6e1fd3c6f32c43774b92930733a0",
            "filename": "tests/pipelines/test_pipelines_mask_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_mask_generation.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -29,7 +29,6 @@\n     Expectations,\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_vision,\n     slow,\n@@ -103,11 +102,6 @@ def get_test_pipeline(\n     def run_pipeline_test(self, mask_generator, examples):\n         pass\n \n-    @require_tf\n-    @unittest.skip(reason=\"Image segmentation not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass\n-\n     @slow\n     @require_torch\n     def test_small_model_pt(self):"
        },
        {
            "sha": "6e2e3ee77c3b3b4f688c05a97eee1ba068db624b",
            "filename": "tests/pipelines/test_pipelines_object_detection.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_object_detection.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -30,7 +30,6 @@\n     is_pipeline_test,\n     nested_simplify,\n     require_pytesseract,\n-    require_tf,\n     require_timm,\n     require_torch,\n     require_vision,\n@@ -128,11 +127,6 @@ def run_pipeline_test(self, object_detector, examples):\n                 )\n                 compare_pipeline_output_to_hub_spec(detected_object, ObjectDetectionOutputElement)\n \n-    @require_tf\n-    @unittest.skip(reason=\"Object detection not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass\n-\n     @require_torch\n     def test_small_model_pt(self):\n         model_id = \"hf-internal-testing/tiny-detr-mobilenetsv3\""
        },
        {
            "sha": "2de1de20d2ed6409239066a1eba18140c07efe77",
            "filename": "tests/pipelines/test_pipelines_question_answering.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_question_answering.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -29,7 +29,6 @@\n     is_pipeline_test,\n     is_torch_available,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torch_or_tf,\n     slow,\n@@ -296,17 +295,6 @@ def test_duplicate_handling(self):\n         answers = [output[\"answer\"] for output in outputs]\n         self.assertEqual(len(answers), len(set(answers)), \"There are duplicate answers in the outputs.\")\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        question_answerer = pipeline(\n-            \"question-answering\", model=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\", framework=\"tf\"\n-        )\n-        outputs = question_answerer(\n-            question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\"\n-        )\n-\n-        self.assertEqual(nested_simplify(outputs), {\"score\": 0.011, \"start\": 0, \"end\": 11, \"answer\": \"HuggingFace\"})\n-\n     @slow\n     @require_torch\n     def test_large_model_pt(self):\n@@ -421,16 +409,6 @@ def test_large_model_course(self):\n             {\"answer\": \"Jax, PyTorch and TensorFlow\", \"end\": 1919, \"score\": 0.971, \"start\": 1892},\n         )\n \n-    @slow\n-    @require_tf\n-    def test_large_model_tf(self):\n-        question_answerer = pipeline(\"question-answering\", framework=\"tf\")\n-        outputs = question_answerer(\n-            question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\"\n-        )\n-\n-        self.assertEqual(nested_simplify(outputs), {\"score\": 0.979, \"start\": 27, \"end\": 32, \"answer\": \"Paris\"})\n-\n \n @require_torch_or_tf\n class QuestionAnsweringArgumentHandlerTests(unittest.TestCase):"
        },
        {
            "sha": "1a5f2839e59421dd3a4f890dff3598d651e7ab58",
            "filename": "tests/pipelines/test_pipelines_table_question_answering.py",
            "status": "modified",
            "additions": 0,
            "deletions": 228,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_table_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_table_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_table_question_answering.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -26,7 +26,6 @@\n     is_pipeline_test,\n     require_pandas,\n     require_tensorflow_probability,\n-    require_tf,\n     require_torch,\n     slow,\n )\n@@ -38,111 +37,6 @@ class TQAPipelineTests(unittest.TestCase):\n     # which are needed to generate automatic tests\n     model_mapping = MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\n \n-    @require_tensorflow_probability\n-    @require_pandas\n-    @require_tf\n-    @require_torch\n-    def test_small_model_tf(self):\n-        model_id = \"lysandre/tiny-tapas-random-wtq\"\n-        model = TFAutoModelForTableQuestionAnswering.from_pretrained(model_id, from_pt=True)\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        self.assertIsInstance(model.config.aggregation_labels, dict)\n-        self.assertIsInstance(model.config.no_aggregation_label_index, int)\n-\n-        table_querier = TableQuestionAnsweringPipeline(model=model, tokenizer=tokenizer, max_new_tokens=20)\n-        outputs = table_querier(\n-            table={\n-                \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n-                \"age\": [\"56\", \"45\", \"59\"],\n-                \"number of movies\": [\"87\", \"53\", \"69\"],\n-                \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"],\n-            },\n-            query=\"how many movies has george clooney played in?\",\n-        )\n-        self.assertEqual(\n-            outputs,\n-            {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-        )\n-        outputs = table_querier(\n-            table={\n-                \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n-                \"age\": [\"56\", \"45\", \"59\"],\n-                \"number of movies\": [\"87\", \"53\", \"69\"],\n-                \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"],\n-            },\n-            query=[\"how many movies has george clooney played in?\", \"how old is he?\", \"what's his date of birth?\"],\n-        )\n-        self.assertEqual(\n-            outputs,\n-            [\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-            ],\n-        )\n-        outputs = table_querier(\n-            table={\n-                \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                \"Contributors\": [\"651\", \"77\", \"34\"],\n-                \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-            },\n-            query=[\n-                \"What repository has the largest number of stars?\",\n-                \"Given that the numbers of stars defines if a repository is active, what repository is the most\"\n-                \" active?\",\n-                \"What is the number of repositories?\",\n-                \"What is the average number of stars?\",\n-                \"What is the total amount of stars?\",\n-            ],\n-        )\n-        self.assertEqual(\n-            outputs,\n-            [\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-                {\"answer\": \"AVERAGE > \", \"coordinates\": [], \"cells\": [], \"aggregator\": \"AVERAGE\"},\n-            ],\n-        )\n-\n-        with self.assertRaises(ValueError):\n-            table_querier(query=\"What does it do with empty context ?\", table=None)\n-        with self.assertRaises(ValueError):\n-            table_querier(query=\"What does it do with empty context ?\", table=\"\")\n-        with self.assertRaises(ValueError):\n-            table_querier(query=\"What does it do with empty context ?\", table={})\n-        with self.assertRaises(ValueError):\n-            table_querier(\n-                table={\n-                    \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                    \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                    \"Contributors\": [\"651\", \"77\", \"34\"],\n-                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-                }\n-            )\n-        with self.assertRaises(ValueError):\n-            table_querier(\n-                query=\"\",\n-                table={\n-                    \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                    \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                    \"Contributors\": [\"651\", \"77\", \"34\"],\n-                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-                },\n-            )\n-        with self.assertRaises(ValueError):\n-            table_querier(\n-                query=None,\n-                table={\n-                    \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                    \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                    \"Contributors\": [\"651\", \"77\", \"34\"],\n-                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-                },\n-            )\n-\n     @require_torch\n     def test_small_model_pt(self, torch_dtype=\"float32\"):\n         model_id = \"lysandre/tiny-tapas-random-wtq\"\n@@ -372,128 +266,6 @@ def test_slow_tokenizer_sqa_pt(self, torch_dtype=\"float32\"):\n     def test_slow_tokenizer_sqa_pt_fp16(self):\n         self.test_slow_tokenizer_sqa_pt(torch_dtype=\"float16\")\n \n-    @require_tf\n-    @require_tensorflow_probability\n-    @require_pandas\n-    @require_torch\n-    def test_slow_tokenizer_sqa_tf(self):\n-        model_id = \"lysandre/tiny-tapas-random-sqa\"\n-        model = TFAutoModelForTableQuestionAnswering.from_pretrained(model_id, from_pt=True)\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        table_querier = TableQuestionAnsweringPipeline(model=model, tokenizer=tokenizer, max_new_tokens=20)\n-\n-        inputs = {\n-            \"table\": {\n-                \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n-                \"age\": [\"56\", \"45\", \"59\"],\n-                \"number of movies\": [\"87\", \"53\", \"69\"],\n-                \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"],\n-            },\n-            \"query\": [\"how many movies has george clooney played in?\", \"how old is he?\", \"what's his date of birth?\"],\n-        }\n-        sequential_outputs = table_querier(**inputs, sequential=True)\n-        batch_outputs = table_querier(**inputs, sequential=False)\n-\n-        self.assertEqual(len(sequential_outputs), 3)\n-        self.assertEqual(len(batch_outputs), 3)\n-        self.assertEqual(sequential_outputs[0], batch_outputs[0])\n-        self.assertNotEqual(sequential_outputs[1], batch_outputs[1])\n-        # self.assertNotEqual(sequential_outputs[2], batch_outputs[2])\n-\n-        table_querier = TableQuestionAnsweringPipeline(model=model, tokenizer=tokenizer, max_new_tokens=20)\n-        outputs = table_querier(\n-            table={\n-                \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n-                \"age\": [\"56\", \"45\", \"59\"],\n-                \"number of movies\": [\"87\", \"53\", \"69\"],\n-                \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"],\n-            },\n-            query=\"how many movies has george clooney played in?\",\n-        )\n-        self.assertEqual(\n-            outputs,\n-            {\"answer\": \"7 february 1967\", \"coordinates\": [(0, 3)], \"cells\": [\"7 february 1967\"]},\n-        )\n-        outputs = table_querier(\n-            table={\n-                \"actors\": [\"brad pitt\", \"leonardo di caprio\", \"george clooney\"],\n-                \"age\": [\"56\", \"45\", \"59\"],\n-                \"number of movies\": [\"87\", \"53\", \"69\"],\n-                \"date of birth\": [\"7 february 1967\", \"10 june 1996\", \"28 november 1967\"],\n-            },\n-            query=[\"how many movies has george clooney played in?\", \"how old is he?\", \"what's his date of birth?\"],\n-        )\n-        self.assertEqual(\n-            outputs,\n-            [\n-                {\"answer\": \"7 february 1967\", \"coordinates\": [(0, 3)], \"cells\": [\"7 february 1967\"]},\n-                {\"answer\": \"7 february 1967\", \"coordinates\": [(0, 3)], \"cells\": [\"7 february 1967\"]},\n-                {\"answer\": \"7 february 1967\", \"coordinates\": [(0, 3)], \"cells\": [\"7 february 1967\"]},\n-            ],\n-        )\n-        outputs = table_querier(\n-            table={\n-                \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                \"Contributors\": [\"651\", \"77\", \"34\"],\n-                \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-            },\n-            query=[\n-                \"What repository has the largest number of stars?\",\n-                \"Given that the numbers of stars defines if a repository is active, what repository is the most\"\n-                \" active?\",\n-                \"What is the number of repositories?\",\n-                \"What is the average number of stars?\",\n-                \"What is the total amount of stars?\",\n-            ],\n-        )\n-        self.assertEqual(\n-            outputs,\n-            [\n-                {\"answer\": \"Python, Python\", \"coordinates\": [(0, 3), (1, 3)], \"cells\": [\"Python\", \"Python\"]},\n-                {\"answer\": \"Python, Python\", \"coordinates\": [(0, 3), (1, 3)], \"cells\": [\"Python\", \"Python\"]},\n-                {\"answer\": \"Python, Python\", \"coordinates\": [(0, 3), (1, 3)], \"cells\": [\"Python\", \"Python\"]},\n-                {\"answer\": \"Python, Python\", \"coordinates\": [(0, 3), (1, 3)], \"cells\": [\"Python\", \"Python\"]},\n-                {\"answer\": \"Python, Python\", \"coordinates\": [(0, 3), (1, 3)], \"cells\": [\"Python\", \"Python\"]},\n-            ],\n-        )\n-\n-        with self.assertRaises(ValueError):\n-            table_querier(query=\"What does it do with empty context ?\", table=None)\n-        with self.assertRaises(ValueError):\n-            table_querier(query=\"What does it do with empty context ?\", table=\"\")\n-        with self.assertRaises(ValueError):\n-            table_querier(query=\"What does it do with empty context ?\", table={})\n-        with self.assertRaises(ValueError):\n-            table_querier(\n-                table={\n-                    \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                    \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                    \"Contributors\": [\"651\", \"77\", \"34\"],\n-                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-                }\n-            )\n-        with self.assertRaises(ValueError):\n-            table_querier(\n-                query=\"\",\n-                table={\n-                    \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                    \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                    \"Contributors\": [\"651\", \"77\", \"34\"],\n-                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-                },\n-            )\n-        with self.assertRaises(ValueError):\n-            table_querier(\n-                query=None,\n-                table={\n-                    \"Repository\": [\"Transformers\", \"Datasets\", \"Tokenizers\"],\n-                    \"Stars\": [\"36542\", \"4512\", \"3934\"],\n-                    \"Contributors\": [\"651\", \"77\", \"34\"],\n-                    \"Programming language\": [\"Python\", \"Python\", \"Rust, Python and NodeJS\"],\n-                },\n-            )\n-\n     @slow\n     @require_torch\n     def test_integration_wtq_pt(self, torch_dtype=\"float32\"):"
        },
        {
            "sha": "8f29bde9f8a9997dddbfd6288d3cada4dc2cfc1c",
            "filename": "tests/pipelines/test_pipelines_text_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_text_classification.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -24,7 +24,6 @@\n     is_pipeline_test,\n     is_torch_available,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torch_bf16,\n     require_torch_fp16,\n@@ -152,15 +151,6 @@ def test_accepts_torch_bf16(self):\n         outputs = text_classifier(\"This is great !\")\n         self.assertEqual(nested_simplify(outputs), [{\"label\": \"LABEL_0\", \"score\": 0.504}])\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        text_classifier = pipeline(\n-            task=\"text-classification\", model=\"hf-internal-testing/tiny-random-distilbert\", framework=\"tf\"\n-        )\n-\n-        outputs = text_classifier(\"This is great !\")\n-        self.assertEqual(nested_simplify(outputs), [{\"label\": \"LABEL_0\", \"score\": 0.504}])\n-\n     @slow\n     @require_torch\n     def test_pt_bert(self):\n@@ -173,18 +163,6 @@ def test_pt_bert(self):\n         outputs = text_classifier(\"Birds are a type of animal\")\n         self.assertEqual(nested_simplify(outputs), [{\"label\": \"POSITIVE\", \"score\": 0.988}])\n \n-    @slow\n-    @require_tf\n-    def test_tf_bert(self):\n-        text_classifier = pipeline(\"text-classification\", framework=\"tf\")\n-\n-        outputs = text_classifier(\"This is great !\")\n-        self.assertEqual(nested_simplify(outputs), [{\"label\": \"POSITIVE\", \"score\": 1.0}])\n-        outputs = text_classifier(\"This is bad !\")\n-        self.assertEqual(nested_simplify(outputs), [{\"label\": \"NEGATIVE\", \"score\": 1.0}])\n-        outputs = text_classifier(\"Birds are a type of animal\")\n-        self.assertEqual(nested_simplify(outputs), [{\"label\": \"POSITIVE\", \"score\": 0.988}])\n-\n     def get_test_pipeline(\n         self,\n         model,"
        },
        {
            "sha": "16767b342c898ecf845f3b82a724cde80ad6d65d",
            "filename": "tests/pipelines/test_pipelines_token_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_token_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_token_classification.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -29,7 +29,6 @@\n     is_pipeline_test,\n     is_torch_available,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torch_accelerator,\n     slow,\n@@ -823,26 +822,6 @@ def test_word_heuristic_leading_space(self):\n             [(\"▁I\", False), (\"▁play\", False), (\"▁the\", False), (\"▁there\", False), (\"min\", True)],\n         )\n \n-    @require_tf\n-    def test_tf_only(self):\n-        model_name = \"hf-internal-testing/tiny-random-bert-tf-only\"  # This model only has a TensorFlow version\n-        # We test that if we don't specify framework='tf', it gets detected automatically\n-        token_classifier = pipeline(task=\"ner\", model=model_name)\n-        self.assertEqual(token_classifier.framework, \"tf\")\n-\n-    @require_tf\n-    def test_small_model_tf(self):\n-        model_name = \"hf-internal-testing/tiny-bert-for-token-classification\"\n-        token_classifier = pipeline(task=\"token-classification\", model=model_name, framework=\"tf\")\n-        outputs = token_classifier(\"This is a test !\")\n-        self.assertEqual(\n-            nested_simplify(outputs),\n-            [\n-                {\"entity\": \"I-MISC\", \"score\": 0.115, \"index\": 1, \"word\": \"this\", \"start\": 0, \"end\": 4},\n-                {\"entity\": \"I-MISC\", \"score\": 0.115, \"index\": 2, \"word\": \"is\", \"start\": 5, \"end\": 7},\n-            ],\n-        )\n-\n     @require_torch\n     def test_no_offset_tokenizer(self):\n         model_name = \"hf-internal-testing/tiny-bert-for-token-classification\""
        },
        {
            "sha": "5043c1f6b327a63807326485395fc253b78afff9",
            "filename": "tests/pipelines/test_pipelines_video_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_video_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_video_classification.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -23,7 +23,6 @@\n     is_pipeline_test,\n     nested_simplify,\n     require_av,\n-    require_tf,\n     require_torch,\n     require_torch_or_tf,\n     require_vision,\n@@ -124,8 +123,3 @@ def test_small_model_pt(self):\n         for output in outputs:\n             for element in output:\n                 compare_pipeline_output_to_hub_spec(element, VideoClassificationOutputElement)\n-\n-    @require_tf\n-    @unittest.skip\n-    def test_small_model_tf(self):\n-        pass"
        },
        {
            "sha": "8066c885bfd4ce938dc99613f62d2e75f583023f",
            "filename": "tests/pipelines/test_pipelines_visual_question_answering.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_visual_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_visual_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_visual_question_answering.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -22,7 +22,6 @@\n     is_pipeline_test,\n     is_torch_available,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_torch_accelerator,\n     require_vision,\n@@ -246,8 +245,3 @@ def test_small_model_pt_dataset(self):\n                 [{\"score\": ANY(float), \"answer\": ANY(str)}],\n             ],\n         )\n-\n-    @require_tf\n-    @unittest.skip(reason=\"Visual question answering not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass"
        },
        {
            "sha": "17553915f434bd376c87741a72f1aba8cf4e6b6a",
            "filename": "tests/pipelines/test_pipelines_zero_shot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 78,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -25,7 +25,6 @@\n     is_pipeline_test,\n     is_torch_available,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     slow,\n )\n@@ -243,26 +242,6 @@ def test_small_model_pt_bf16(self):\n             },\n         )\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        zero_shot_classifier = pipeline(\n-            \"zero-shot-classification\",\n-            model=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n-            framework=\"tf\",\n-        )\n-        outputs = zero_shot_classifier(\n-            \"Who are you voting for in 2020?\", candidate_labels=[\"politics\", \"public health\", \"science\"]\n-        )\n-\n-        self.assertEqual(\n-            nested_simplify(outputs),\n-            {\n-                \"sequence\": \"Who are you voting for in 2020?\",\n-                \"labels\": [\"science\", \"public health\", \"politics\"],\n-                \"scores\": [0.333, 0.333, 0.333],\n-            },\n-        )\n-\n     @slow\n     @require_torch\n     def test_large_model_pt(self):\n@@ -319,60 +298,3 @@ def test_large_model_pt(self):\n                 \"scores\": [0.817, 0.713, 0.018, 0.018],\n             },\n         )\n-\n-    @slow\n-    @require_tf\n-    def test_large_model_tf(self):\n-        zero_shot_classifier = pipeline(\n-            \"zero-shot-classification\", model=\"FacebookAI/roberta-large-mnli\", framework=\"tf\"\n-        )\n-        outputs = zero_shot_classifier(\n-            \"Who are you voting for in 2020?\", candidate_labels=[\"politics\", \"public health\", \"science\"]\n-        )\n-\n-        self.assertEqual(\n-            nested_simplify(outputs),\n-            {\n-                \"sequence\": \"Who are you voting for in 2020?\",\n-                \"labels\": [\"politics\", \"public health\", \"science\"],\n-                \"scores\": [0.976, 0.015, 0.009],\n-            },\n-        )\n-        outputs = zero_shot_classifier(\n-            \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks\"\n-            \" in an encoder-decoder configuration. The best performing models also connect the encoder and decoder\"\n-            \" through an attention mechanism. We propose a new simple network architecture, the Transformer, based\"\n-            \" solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two\"\n-            \" machine translation tasks show these models to be superior in quality while being more parallelizable\"\n-            \" and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014\"\n-            \" English-to-German translation task, improving over the existing best results, including ensembles by\"\n-            \" over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new\"\n-            \" single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small\"\n-            \" fraction of the training costs of the best models from the literature. We show that the Transformer\"\n-            \" generalizes well to other tasks by applying it successfully to English constituency parsing both with\"\n-            \" large and limited training data.\",\n-            candidate_labels=[\"machine learning\", \"statistics\", \"translation\", \"vision\"],\n-            multi_label=True,\n-        )\n-        self.assertEqual(\n-            nested_simplify(outputs),\n-            {\n-                \"sequence\": (\n-                    \"The dominant sequence transduction models are based on complex recurrent or convolutional neural\"\n-                    \" networks in an encoder-decoder configuration. The best performing models also connect the\"\n-                    \" encoder and decoder through an attention mechanism. We propose a new simple network\"\n-                    \" architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence\"\n-                    \" and convolutions entirely. Experiments on two machine translation tasks show these models to be\"\n-                    \" superior in quality while being more parallelizable and requiring significantly less time to\"\n-                    \" train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task,\"\n-                    \" improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014\"\n-                    \" English-to-French translation task, our model establishes a new single-model state-of-the-art\"\n-                    \" BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training\"\n-                    \" costs of the best models from the literature. We show that the Transformer generalizes well to\"\n-                    \" other tasks by applying it successfully to English constituency parsing both with large and\"\n-                    \" limited training data.\"\n-                ),\n-                \"labels\": [\"translation\", \"machine learning\", \"vision\", \"statistics\"],\n-                \"scores\": [0.817, 0.713, 0.018, 0.018],\n-            },\n-        )"
        },
        {
            "sha": "39cc712ab7268dc613a3326fd11e97a0fcef291e",
            "filename": "tests/pipelines/test_pipelines_zero_shot_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 83,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot_image_classification.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -22,7 +22,6 @@\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_vision,\n     slow,\n@@ -137,57 +136,6 @@ def test_small_model_pt(self, torch_dtype=\"float32\"):\n     def test_small_model_pt_fp16(self):\n         self.test_small_model_pt(torch_dtype=\"float16\")\n \n-    @require_tf\n-    def test_small_model_tf(self):\n-        image_classifier = pipeline(\n-            model=\"hf-internal-testing/tiny-random-clip-zero-shot-image-classification\", framework=\"tf\"\n-        )\n-        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-        output = image_classifier(image, candidate_labels=[\"a\", \"b\", \"c\"])\n-\n-        self.assertEqual(\n-            nested_simplify(output),\n-            [{\"score\": 0.333, \"label\": \"a\"}, {\"score\": 0.333, \"label\": \"b\"}, {\"score\": 0.333, \"label\": \"c\"}],\n-        )\n-\n-        output = image_classifier([image] * 5, candidate_labels=[\"A\", \"B\", \"C\"], batch_size=2)\n-        self.assertEqual(\n-            nested_simplify(output),\n-            # Pipeline outputs are supposed to be deterministic and\n-            # So we could in theory have real values \"A\", \"B\", \"C\" instead\n-            # of ANY(str).\n-            # However it seems that in this particular case, the floating\n-            # scores are so close, we enter floating error approximation\n-            # and the order is not guaranteed anymore with batching.\n-            [\n-                [\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                ],\n-                [\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                ],\n-                [\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                ],\n-                [\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                ],\n-                [\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                    {\"score\": 0.333, \"label\": ANY(str)},\n-                ],\n-            ],\n-        )\n-\n     @slow\n     @require_torch\n     def test_large_model_pt(self):\n@@ -221,37 +169,6 @@ def test_large_model_pt(self):\n             * 5,\n         )\n \n-    @slow\n-    @require_tf\n-    def test_large_model_tf(self):\n-        image_classifier = pipeline(\n-            task=\"zero-shot-image-classification\", model=\"openai/clip-vit-base-patch32\", framework=\"tf\"\n-        )\n-        # This is an image of 2 cats with remotes and no planes\n-        image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-        output = image_classifier(image, candidate_labels=[\"cat\", \"plane\", \"remote\"])\n-        self.assertEqual(\n-            nested_simplify(output),\n-            [\n-                {\"score\": 0.511, \"label\": \"remote\"},\n-                {\"score\": 0.485, \"label\": \"cat\"},\n-                {\"score\": 0.004, \"label\": \"plane\"},\n-            ],\n-        )\n-\n-        output = image_classifier([image] * 5, candidate_labels=[\"cat\", \"plane\", \"remote\"], batch_size=2)\n-        self.assertEqual(\n-            nested_simplify(output),\n-            [\n-                [\n-                    {\"score\": 0.511, \"label\": \"remote\"},\n-                    {\"score\": 0.485, \"label\": \"cat\"},\n-                    {\"score\": 0.004, \"label\": \"plane\"},\n-                ],\n-            ]\n-            * 5,\n-        )\n-\n     @slow\n     @require_torch\n     def test_siglip_model_pt(self):"
        },
        {
            "sha": "8d5afbe3dedfcccc487c486bb831007469d823e8",
            "filename": "tests/pipelines/test_pipelines_zero_shot_object_detection.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_zero_shot_object_detection.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -23,7 +23,6 @@\n from transformers.testing_utils import (\n     is_pipeline_test,\n     nested_simplify,\n-    require_tf,\n     require_torch,\n     require_vision,\n     slow,\n@@ -90,11 +89,6 @@ def run_pipeline_test(self, object_detector, examples):\n             ],\n         )\n \n-    @require_tf\n-    @unittest.skip(reason=\"Zero Shot Object Detection not implemented in TF\")\n-    def test_small_model_tf(self):\n-        pass\n-\n     @require_torch\n     def test_small_model_pt(self):\n         object_detector = pipeline(\n@@ -201,11 +195,6 @@ def test_large_model_pt(self):\n             ],\n         )\n \n-    @require_tf\n-    @unittest.skip(reason=\"Zero Shot Object Detection not implemented in TF\")\n-    def test_large_model_tf(self):\n-        pass\n-\n     @require_torch\n     @slow\n     def test_threshold(self):"
        },
        {
            "sha": "b18d79ec98a8fcfcd8fa33a059141e649dedcefa",
            "filename": "tests/test_image_transforms.py",
            "status": "modified",
            "additions": 2,
            "deletions": 19,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftest_image_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftest_image_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_transforms.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -17,16 +17,13 @@\n import numpy as np\n from parameterized import parameterized\n \n-from transformers.testing_utils import require_flax, require_tf, require_torch, require_vision\n-from transformers.utils.import_utils import is_flax_available, is_tf_available, is_torch_available, is_vision_available\n+from transformers.testing_utils import require_flax, require_torch, require_vision\n+from transformers.utils.import_utils import is_flax_available, is_torch_available, is_vision_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n if is_flax_available():\n     import jax\n \n@@ -122,20 +119,6 @@ def test_to_pil_image_from_mask(self):\n         self.assertTrue(np_img.min() == 0)\n         self.assertTrue(np_img.max() == 1)\n \n-    @require_tf\n-    def test_to_pil_image_from_tensorflow(self):\n-        # channels_first\n-        image = tf.random.uniform((3, 4, 5))\n-        pil_image = to_pil_image(image)\n-        self.assertIsInstance(pil_image, PIL.Image.Image)\n-        self.assertEqual(pil_image.size, (5, 4))\n-\n-        # channels_last\n-        image = tf.random.uniform((4, 5, 3))\n-        pil_image = to_pil_image(image)\n-        self.assertIsInstance(pil_image, PIL.Image.Image)\n-        self.assertEqual(pil_image.size, (5, 4))\n-\n     @require_torch\n     def test_to_pil_image_from_torch(self):\n         # channels first"
        },
        {
            "sha": "6fd55978e4cef7f436138aa5b56ccf89219cae1f",
            "filename": "tests/test_sequence_feature_extraction_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 32,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftest_sequence_feature_extraction_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftest_sequence_feature_extraction_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_sequence_feature_extraction_common.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -16,7 +16,7 @@\n import numpy as np\n \n from transformers import BatchFeature\n-from transformers.testing_utils import require_tf, require_torch\n+from transformers.testing_utils import require_torch\n \n from .test_feature_extraction_common import FeatureExtractionSavingTestMixin\n \n@@ -76,24 +76,6 @@ def test_batch_feature_pt(self):\n             == (self.feat_extract_tester.batch_size, len(speech_inputs[0]), self.feat_extract_tester.feature_size)\n         )\n \n-    @require_tf\n-    def test_batch_feature_tf(self):\n-        speech_inputs = self.feat_extract_tester.prepare_inputs_for_common(equal_length=True)\n-        feat_extract = self.feature_extraction_class(**self.feat_extract_dict)\n-        input_name = feat_extract.model_input_names[0]\n-\n-        processed_features = BatchFeature({input_name: speech_inputs}, tensor_type=\"tf\")\n-\n-        batch_features_input = processed_features[input_name]\n-\n-        if len(batch_features_input.shape) < 3:\n-            batch_features_input = batch_features_input[:, :, None]\n-\n-        self.assertTrue(\n-            batch_features_input.shape\n-            == (self.feat_extract_tester.batch_size, len(speech_inputs[0]), self.feat_extract_tester.feature_size)\n-        )\n-\n     def _check_padding(self, numpify=False):\n         def _inputs_have_equal_length(input):\n             length = len(input[0])\n@@ -372,19 +354,6 @@ def test_padding_accepts_tensors_pt(self):\n \n         self.assertTrue(abs(input_np.astype(np.float32).sum() - input_pt.numpy().astype(np.float32).sum()) < 1e-2)\n \n-    @require_tf\n-    def test_padding_accepts_tensors_tf(self):\n-        feat_extract = self.feature_extraction_class(**self.feat_extract_dict)\n-        speech_inputs = self.feat_extract_tester.prepare_inputs_for_common()\n-        input_name = feat_extract.model_input_names[0]\n-\n-        processed_features = BatchFeature({input_name: speech_inputs})\n-\n-        input_np = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"np\")[input_name]\n-        input_tf = feat_extract.pad(processed_features, padding=\"longest\", return_tensors=\"tf\")[input_name]\n-\n-        self.assertTrue(abs(input_np.astype(np.float32).sum() - input_tf.numpy().astype(np.float32).sum()) < 1e-2)\n-\n     def test_attention_mask(self):\n         feat_dict = self.feat_extract_dict\n         feat_dict[\"return_attention_mask\"] = True"
        },
        {
            "sha": "b18fa36f095fea45dad07c1c15917beddc31728c",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -53,7 +53,6 @@\n     get_tests_dir,\n     require_jinja,\n     require_read_token,\n-    require_tf,\n     require_tokenizers,\n     require_torch,\n     run_test_in_subprocess,\n@@ -3106,40 +3105,6 @@ def test_torch_encode_plus_sent_to_model(self):\n         #     model(**encoded_sequence_fast)\n         #     model(**batch_encoded_sequence_fast)\n \n-    @require_tf\n-    @slow\n-    def test_tf_encode_plus_sent_to_model(self):\n-        from transformers import TF_MODEL_MAPPING, TOKENIZER_MAPPING\n-\n-        MODEL_TOKENIZER_MAPPING = merge_model_tokenizer_mappings(TF_MODEL_MAPPING, TOKENIZER_MAPPING)\n-\n-        tokenizers = self.get_tokenizers(do_lower_case=False)\n-        for tokenizer in tokenizers:\n-            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                if tokenizer.__class__ not in MODEL_TOKENIZER_MAPPING:\n-                    self.skipTest(f\"{tokenizer.__class__.__name__} is not in the MODEL_TOKENIZER_MAPPING\")\n-\n-                config_class, model_class = MODEL_TOKENIZER_MAPPING[tokenizer.__class__]\n-                config = config_class()\n-\n-                if config.is_encoder_decoder or config.pad_token_id is None:\n-                    self.skipTest(reason=\"Model is not an encoder-decoder model or has no set pad token id\")\n-\n-                model = model_class(config)\n-\n-                # Make sure the model contains at least the full vocabulary size in its embedding matrix\n-                self.assertGreaterEqual(model.config.vocab_size, len(tokenizer))\n-\n-                # Build sequence\n-                first_ten_tokens = list(tokenizer.get_vocab().keys())[:10]\n-                sequence = \" \".join(first_ten_tokens)\n-                encoded_sequence = tokenizer.encode_plus(sequence, return_tensors=\"tf\")\n-                batch_encoded_sequence = tokenizer.batch_encode_plus([sequence, sequence], return_tensors=\"tf\")\n-\n-                # This should not fail\n-                model(encoded_sequence)\n-                model(batch_encoded_sequence)\n-\n     # TODO: Check if require_torch is the best to test for numpy here ... Maybe move to require_flax when available\n     @require_torch\n     @slow"
        },
        {
            "sha": "0a2960672c38fa2734963deab2de37d7f351543f",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -39,7 +39,6 @@\n     CaptureStderr,\n     require_flax,\n     require_sentencepiece,\n-    require_tf,\n     require_tokenizers,\n     require_torch,\n     slow,\n@@ -121,27 +120,6 @@ def test_batch_encoding_pickle(self):\n                 tokenizer_r(\"Small example to encode\", return_tensors=TensorType.NUMPY), np.array_equal\n             )\n \n-    @require_tf\n-    @require_tokenizers\n-    def test_batch_encoding_pickle_tf(self):\n-        import tensorflow as tf\n-\n-        def tf_array_equals(t1, t2):\n-            return tf.reduce_all(tf.equal(t1, t2))\n-\n-        tokenizer_p = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_r = BertTokenizerFast.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        with self.subTest(\"BatchEncoding (Python, return_tensors=TENSORFLOW)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_p(\"Small example to encode\", return_tensors=TensorType.TENSORFLOW), tf_array_equals\n-            )\n-\n-        with self.subTest(\"BatchEncoding (Rust, return_tensors=TENSORFLOW)\"):\n-            self.assert_dump_and_restore(\n-                tokenizer_r(\"Small example to encode\", return_tensors=TensorType.TENSORFLOW), tf_array_equals\n-            )\n-\n     @require_torch\n     @require_tokenizers\n     def test_batch_encoding_pickle_pt(self):\n@@ -211,22 +189,6 @@ def test_batch_encoding_with_labels_pt(self):\n         self.assertEqual(tensor_batch[\"inputs\"].shape, (1, 3))\n         self.assertEqual(tensor_batch[\"labels\"].shape, (1,))\n \n-    @require_tf\n-    def test_batch_encoding_with_labels_tf(self):\n-        batch = BatchEncoding({\"inputs\": [[1, 2, 3], [4, 5, 6]], \"labels\": [0, 1]})\n-        tensor_batch = batch.convert_to_tensors(tensor_type=\"tf\")\n-        self.assertEqual(tensor_batch[\"inputs\"].shape, (2, 3))\n-        self.assertEqual(tensor_batch[\"labels\"].shape, (2,))\n-        # test converting the converted\n-        with CaptureStderr() as cs:\n-            tensor_batch = batch.convert_to_tensors(tensor_type=\"tf\")\n-        self.assertFalse(len(cs.err), msg=f\"should have no warning, but got {cs.err}\")\n-\n-        batch = BatchEncoding({\"inputs\": [1, 2, 3], \"labels\": 0})\n-        tensor_batch = batch.convert_to_tensors(tensor_type=\"tf\", prepend_batch_axis=True)\n-        self.assertEqual(tensor_batch[\"inputs\"].shape, (1, 3))\n-        self.assertEqual(tensor_batch[\"labels\"].shape, (1,))\n-\n     @require_flax\n     def test_batch_encoding_with_labels_jax(self):\n         batch = BatchEncoding({\"inputs\": [[1, 2, 3], [4, 5, 6]], \"labels\": [0, 1]})\n@@ -381,20 +343,6 @@ def test_padding_accepts_tensors_pt(self):\n         self.assertTrue(isinstance(batch[\"input_ids\"], torch.Tensor))\n         self.assertEqual(batch[\"input_ids\"].tolist(), [[0, 1, 2, tokenizer.pad_token_id], [0, 1, 2, 3]])\n \n-    @require_tf\n-    def test_padding_accepts_tensors_tf(self):\n-        import tensorflow as tf\n-\n-        features = [{\"input_ids\": tf.constant([0, 1, 2])}, {\"input_ids\": tf.constant([0, 1, 2, 3])}]\n-        tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-        batch = tokenizer.pad(features, padding=True)\n-        self.assertTrue(isinstance(batch[\"input_ids\"], tf.Tensor))\n-        self.assertEqual(batch[\"input_ids\"].numpy().tolist(), [[0, 1, 2, tokenizer.pad_token_id], [0, 1, 2, 3]])\n-        batch = tokenizer.pad(features, padding=True, return_tensors=\"tf\")\n-        self.assertTrue(isinstance(batch[\"input_ids\"], tf.Tensor))\n-        self.assertEqual(batch[\"input_ids\"].numpy().tolist(), [[0, 1, 2, tokenizer.pad_token_id], [0, 1, 2, 3]])\n-\n     @require_tokenizers\n     def test_instantiation_from_tokenizers(self):\n         bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
        },
        {
            "sha": "d25aa7ceba9aa12feabac21aa4d74f77b3de1969",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 794,
            "changes": 795,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -29,20 +29,16 @@\n     DataCollatorWithFlattening,\n     DataCollatorWithPadding,\n     default_data_collator,\n-    is_tf_available,\n     is_torch_available,\n     set_seed,\n )\n-from transformers.testing_utils import require_tf, require_torch\n+from transformers.testing_utils import require_torch\n from transformers.utils import PaddingStrategy\n \n \n if is_torch_available():\n     import torch\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n \n @require_torch\n class DataCollatorIntegrationTest(unittest.TestCase):\n@@ -1022,795 +1018,6 @@ def test_sentence_order_prediction_collator_immutability(self):\n         )\n \n \n-@require_tf\n-class TFDataCollatorIntegrationTest(unittest.TestCase):\n-    def setUp(self):\n-        super().setUp()\n-        self.tmpdirname = tempfile.mkdtemp()\n-\n-        vocab_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n-        self.vocab_file = os.path.join(self.tmpdirname, \"vocab.txt\")\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n-            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n-\n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n-\n-    def test_default_with_dict(self):\n-        features = [{\"label\": i, \"inputs\": [0, 1, 2, 3, 4, 5]} for i in range(8)]\n-        batch = default_data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].numpy().tolist(), list(range(8)))\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-        self.assertEqual(batch[\"inputs\"].shape.as_list(), [8, 6])\n-\n-        # With label_ids\n-        features = [{\"label_ids\": [0, 1, 2], \"inputs\": [0, 1, 2, 3, 4, 5]} for i in range(8)]\n-        batch = default_data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].numpy().tolist(), ([[0, 1, 2]] * 8))\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-        self.assertEqual(batch[\"inputs\"].shape.as_list(), [8, 6])\n-\n-        # Features can already be tensors\n-        features = [{\"label\": i, \"inputs\": np.random.randint(0, 10, [10])} for i in range(8)]\n-        batch = default_data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].numpy().tolist(), (list(range(8))))\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-        self.assertEqual(batch[\"inputs\"].shape.as_list(), [8, 10])\n-\n-        # Labels can already be tensors\n-        features = [{\"label\": np.array(i), \"inputs\": np.random.randint(0, 10, [10])} for i in range(8)]\n-        batch = default_data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-        self.assertEqual(batch[\"labels\"].numpy().tolist(), list(range(8)))\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-        self.assertEqual(batch[\"inputs\"].shape.as_list(), [8, 10])\n-\n-    def test_numpy_dtype_preservation(self):\n-        data_collator = default_data_collator\n-\n-        # Confirms that numpy inputs are handled correctly even when scalars\n-        features = [{\"input_ids\": np.array([0, 1, 2, 3, 4]), \"label\": np.int64(i)} for i in range(4)]\n-        batch = data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-\n-    def test_default_classification_and_regression(self):\n-        data_collator = default_data_collator\n-\n-        features = [{\"input_ids\": [0, 1, 2, 3, 4], \"label\": i} for i in range(4)]\n-        batch = data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].dtype, tf.int64)\n-\n-        features = [{\"input_ids\": [0, 1, 2, 3, 4], \"label\": float(i)} for i in range(4)]\n-        batch = data_collator(features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"labels\"].dtype, tf.float32)\n-\n-    def test_default_with_no_labels(self):\n-        features = [{\"label\": None, \"inputs\": [0, 1, 2, 3, 4, 5]} for i in range(8)]\n-        batch = default_data_collator(features, return_tensors=\"tf\")\n-        self.assertTrue(\"labels\" not in batch)\n-        self.assertEqual(batch[\"inputs\"].shape.as_list(), [8, 6])\n-\n-        # With label_ids\n-        features = [{\"label_ids\": None, \"inputs\": [0, 1, 2, 3, 4, 5]} for i in range(8)]\n-        batch = default_data_collator(features, return_tensors=\"tf\")\n-        self.assertTrue(\"labels\" not in batch)\n-        self.assertEqual(batch[\"inputs\"].shape.as_list(), [8, 6])\n-\n-    def test_data_collator_with_padding(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [{\"input_ids\": [0, 1, 2]}, {\"input_ids\": [0, 1, 2, 3, 4, 5]}]\n-\n-        data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), [0, 1, 2] + [tokenizer.pad_token_id] * 3)\n-\n-        data_collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=10, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape, [2, 8])\n-\n-    def test_data_collator_for_token_classification(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [\n-            {\"input_ids\": [0, 1, 2], \"labels\": [0, 1, 2]},\n-            {\"input_ids\": [0, 1, 2, 3, 4, 5], \"labels\": [0, 1, 2, 3, 4, 5]},\n-        ]\n-\n-        data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), [0, 1, 2] + [tokenizer.pad_token_id] * 3)\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"labels\"][0].numpy().tolist(), [0, 1, 2] + [-100] * 3)\n-\n-        data_collator = DataCollatorForTokenClassification(\n-            tokenizer, padding=\"max_length\", max_length=10, return_tensors=\"tf\"\n-        )\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 8])\n-\n-        data_collator = DataCollatorForTokenClassification(tokenizer, label_pad_token_id=-1, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), [0, 1, 2] + [tokenizer.pad_token_id] * 3)\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"labels\"][0].numpy().tolist(), [0, 1, 2] + [-1] * 3)\n-\n-    def test_data_collator_for_seq2seq(self):\n-        def create_features():\n-            return [\n-                {\"input_ids\": list(range(3)), \"labels\": list(range(3))},\n-                {\"input_ids\": list(range(6)), \"labels\": list(range(6))},\n-            ]\n-\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = create_features()\n-\n-        data_collator = DataCollatorForSeq2Seq(tokenizer, padding=PaddingStrategy.LONGEST, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), list(range(3)) + [tokenizer.pad_token_id] * 3)\n-        self.assertEqual(batch[\"input_ids\"][1].numpy().tolist(), list(range(6)))\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"labels\"][0].numpy().tolist(), list(range(3)) + [-100] * 3)\n-        self.assertEqual(batch[\"labels\"][1].numpy().tolist(), list(range(6)))\n-\n-        data_collator = DataCollatorForSeq2Seq(\n-            tokenizer, padding=PaddingStrategy.MAX_LENGTH, max_length=7, return_tensors=\"tf\"\n-        )\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 7])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), list(range(3)) + [tokenizer.pad_token_id] * 4)\n-        self.assertEqual(batch[\"input_ids\"][1].numpy().tolist(), list(range(6)) + [tokenizer.pad_token_id] * 1)\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 7])\n-        self.assertEqual(batch[\"labels\"][0].numpy().tolist(), list(range(3)) + [-100] * 4)\n-        self.assertEqual(batch[\"labels\"][1].numpy().tolist(), list(range(6)) + [-100] * 1)\n-\n-        data_collator = DataCollatorForSeq2Seq(tokenizer, padding=PaddingStrategy.DO_NOT_PAD, return_tensors=\"tf\")\n-        with self.assertRaises(ValueError):\n-            # expects an error due to unequal shapes to create tensor\n-            data_collator(features)\n-        batch = data_collator([features[0], features[0]])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), features[0][\"input_ids\"])\n-        self.assertEqual(batch[\"input_ids\"][1].numpy().tolist(), features[0][\"input_ids\"])\n-        self.assertEqual(batch[\"labels\"][0].numpy().tolist(), features[0][\"labels\"])\n-        self.assertEqual(batch[\"labels\"][1].numpy().tolist(), features[0][\"labels\"])\n-\n-        data_collator = DataCollatorForSeq2Seq(\n-            tokenizer, padding=PaddingStrategy.LONGEST, pad_to_multiple_of=8, return_tensors=\"tf\"\n-        )\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 8])\n-\n-        # side effects on labels cause mismatch on longest strategy\n-        features = create_features()\n-\n-        data_collator = DataCollatorForSeq2Seq(\n-            tokenizer, padding=PaddingStrategy.LONGEST, label_pad_token_id=-1, return_tensors=\"tf\"\n-        )\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), list(range(3)) + [tokenizer.pad_token_id] * 3)\n-        self.assertEqual(batch[\"input_ids\"][1].numpy().tolist(), list(range(6)))\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"labels\"][0].numpy().tolist(), list(range(3)) + [-1] * 3)\n-        self.assertEqual(batch[\"labels\"][1].numpy().tolist(), list(range(6)))\n-\n-        for feature in features:\n-            feature.pop(\"labels\")\n-\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 6])\n-        self.assertEqual(batch[\"input_ids\"][0].numpy().tolist(), list(range(3)) + [tokenizer.pad_token_id] * 3)\n-\n-    def _test_no_pad_and_pad(self, no_pad_features, pad_features):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")\n-        batch = data_collator(no_pad_features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        batch = data_collator(pad_features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        data_collator = DataCollatorForLanguageModeling(\n-            tokenizer, mlm=False, pad_to_multiple_of=8, return_tensors=\"tf\"\n-        )\n-        batch = data_collator(no_pad_features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 16])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 16])\n-\n-        batch = data_collator(pad_features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 16])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 16])\n-\n-        tokenizer.pad_token = None\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")\n-        with self.assertRaises(ValueError):\n-            # Expect error due to padding token missing\n-            data_collator(pad_features)\n-\n-        set_seed(42)  # For reproducibility\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, return_tensors=\"tf\")\n-        batch = data_collator(no_pad_features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        masked_tokens = batch[\"input_ids\"] == tokenizer.mask_token_id\n-        self.assertTrue(tf.reduce_any(masked_tokens))\n-        # self.assertTrue(all(x == -100 for x in batch[\"labels\"].numpy()[~masked_tokens.numpy()].tolist()))\n-\n-        batch = data_collator(pad_features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        masked_tokens = batch[\"input_ids\"] == tokenizer.mask_token_id\n-        self.assertTrue(tf.reduce_any(masked_tokens))\n-        # self.assertTrue(all(x == -100 for x in batch[\"labels\"].numpy()[~masked_tokens.numpy()].tolist()))\n-\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        batch = data_collator(no_pad_features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 16])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 16])\n-\n-        masked_tokens = batch[\"input_ids\"] == tokenizer.mask_token_id\n-        self.assertTrue(tf.reduce_any(masked_tokens))\n-        # self.assertTrue(all(x == -100 for x in batch[\"labels\"].numpy()[~masked_tokens.numpy()].tolist()))\n-\n-        batch = data_collator(pad_features, return_tensors=\"tf\")\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 16])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 16])\n-\n-        masked_tokens = batch[\"input_ids\"] == tokenizer.mask_token_id\n-        self.assertTrue(tf.reduce_any(masked_tokens))\n-        # self.assertTrue(all(x == -100 for x in batch[\"labels\"].numpy()[~masked_tokens.numpy()].tolist()))\n-\n-    def test_probability_sum_error(self):\n-        \"\"\"Test that the sum of mask_replace_prob and random_replace_prob exceeding 1 raises an error.\"\"\"\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        with self.assertRaises(ValueError):\n-            DataCollatorForLanguageModeling(tokenizer=tokenizer, mask_replace_prob=0.9, random_replace_prob=0.2)\n-\n-    def test_all_mask_replacement(self):\n-        \"\"\"Test behavior when mask_replace_prob=1.\"\"\"\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        # pytorch call\n-        collator = DataCollatorForLanguageModeling(\n-            tokenizer=tokenizer, mask_replace_prob=1, random_replace_prob=0, return_tensors=\"pt\"\n-        )\n-\n-        inputs = torch.tensor([0, 1, 2, 3, 4, 5])\n-        features = [{\"input_ids\": inputs} for _ in range(8)]\n-        batch = collator(features)\n-\n-        # confirm that every token is either the original token or [MASK]\n-        self.assertTrue(torch.all((batch[\"input_ids\"] == inputs) | (batch[\"input_ids\"] == tokenizer.mask_token_id)))\n-\n-        # tf call\n-        collator = DataCollatorForLanguageModeling(\n-            tokenizer=tokenizer, mask_replace_prob=1, random_replace_prob=0, return_tensors=\"tf\"\n-        )\n-        inputs = tf.constant([0, 1, 2, 3, 4, 5])\n-        features = [{\"input_ids\": inputs} for _ in range(8)]\n-        batch = collator(features)\n-\n-        # confirm that every token is either the original token or [MASK]\n-        self.assertTrue(\n-            tf.reduce_all(\n-                (batch[\"input_ids\"] == tf.cast(inputs, tf.int64)) | (batch[\"input_ids\"] == tokenizer.mask_token_id)\n-            )\n-        )\n-\n-        # numpy call\n-        collator = DataCollatorForLanguageModeling(\n-            tokenizer=tokenizer, mask_replace_prob=1, random_replace_prob=0, return_tensors=\"np\"\n-        )\n-        inputs = np.array([0, 1, 2, 3, 4, 5])\n-        features = [{\"input_ids\": inputs} for _ in range(8)]\n-        batch = collator(features)\n-\n-        # confirm that every token is either the original token or [MASK]\n-        self.assertTrue(np.all((batch[\"input_ids\"] == inputs) | (batch[\"input_ids\"] == tokenizer.mask_token_id)))\n-\n-    def test_data_collator_for_language_modeling(self):\n-        no_pad_features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n-        pad_features = [{\"input_ids\": list(range(5))}, {\"input_ids\": list(range(10))}]\n-        self._test_no_pad_and_pad(no_pad_features, pad_features)\n-\n-        no_pad_features = [list(range(10)), list(range(10))]\n-        pad_features = [list(range(5)), list(range(10))]\n-        self._test_no_pad_and_pad(no_pad_features, pad_features)\n-\n-    def test_data_collator_for_language_modeling_with_seed(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n-\n-        # check if seed is respected between two different DataCollatorForLanguageModeling instances\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42, return_tensors=\"tf\")\n-        batch_1 = data_collator(features)\n-        self.assertEqual(batch_1[\"input_ids\"].shape.as_list(), [2, 1000])\n-        self.assertEqual(batch_1[\"labels\"].shape.as_list(), [2, 1000])\n-\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=42, return_tensors=\"tf\")\n-        batch_2 = data_collator(features)\n-        self.assertEqual(batch_2[\"input_ids\"].shape.as_list(), [2, 1000])\n-        self.assertEqual(batch_2[\"labels\"].shape.as_list(), [2, 1000])\n-\n-        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n-        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n-\n-        # try with different seed\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, seed=43, return_tensors=\"tf\")\n-        batch_3 = data_collator(features)\n-        self.assertEqual(batch_3[\"input_ids\"].shape.as_list(), [2, 1000])\n-        self.assertEqual(batch_3[\"labels\"].shape.as_list(), [2, 1000])\n-\n-        self.assertFalse(np.all(batch_1[\"input_ids\"] == batch_3[\"input_ids\"]))\n-        self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n-\n-    def test_data_collator_for_whole_word_mask(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        data_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"tf\")\n-\n-        features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        # Features can already be tensors\n-        features = [{\"input_ids\": np.arange(10)}, {\"input_ids\": np.arange(10)}]\n-        batch = data_collator(features)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-    def test_data_collator_for_whole_word_mask_with_seed(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [{\"input_ids\": list(range(1000))}, {\"input_ids\": list(range(1000))}]\n-\n-        # check if seed is respected between two different DataCollatorForWholeWordMask instances\n-        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"tf\")\n-        batch_1 = data_collator(features)\n-        self.assertEqual(batch_1[\"input_ids\"].shape.as_list(), [2, 1000])\n-        self.assertEqual(batch_1[\"labels\"].shape.as_list(), [2, 1000])\n-\n-        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=42, return_tensors=\"tf\")\n-        batch_2 = data_collator(features)\n-        self.assertEqual(batch_2[\"input_ids\"].shape.as_list(), [2, 1000])\n-        self.assertEqual(batch_2[\"labels\"].shape.as_list(), [2, 1000])\n-\n-        self.assertTrue(np.all(batch_1[\"input_ids\"] == batch_2[\"input_ids\"]))\n-        self.assertTrue(np.all(batch_1[\"labels\"] == batch_2[\"labels\"]))\n-\n-        # try with different seed\n-        data_collator = DataCollatorForWholeWordMask(tokenizer, seed=43, return_tensors=\"tf\")\n-        batch_3 = data_collator(features)\n-        self.assertEqual(batch_3[\"input_ids\"].shape.as_list(), [2, 1000])\n-        self.assertEqual(batch_3[\"labels\"].shape.as_list(), [2, 1000])\n-\n-        self.assertFalse(np.all(batch_1[\"input_ids\"] == batch_3[\"input_ids\"]))\n-        self.assertFalse(np.all(batch_1[\"labels\"] == batch_3[\"labels\"]))\n-\n-    def test_plm(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        no_pad_features = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n-        pad_features = [{\"input_ids\": list(range(5))}, {\"input_ids\": list(range(10))}]\n-\n-        data_collator = DataCollatorForPermutationLanguageModeling(tokenizer, return_tensors=\"tf\")\n-\n-        batch = data_collator(pad_features)\n-        self.assertIsInstance(batch, dict)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"perm_mask\"].shape.as_list(), [2, 10, 10])\n-        self.assertEqual(batch[\"target_mapping\"].shape.as_list(), [2, 10, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        batch = data_collator(no_pad_features)\n-        self.assertIsInstance(batch, dict)\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 10])\n-        self.assertEqual(batch[\"perm_mask\"].shape.as_list(), [2, 10, 10])\n-        self.assertEqual(batch[\"target_mapping\"].shape.as_list(), [2, 10, 10])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 10])\n-\n-        example = [np.random.randint(0, 5, [5])]\n-        with self.assertRaises(ValueError):\n-            # Expect error due to odd sequence length\n-            data_collator(example)\n-\n-    def test_nsp(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [\n-            {\"input_ids\": [0, 1, 2, 3, 4], \"token_type_ids\": [0, 1, 2, 3, 4], \"next_sentence_label\": i}\n-            for i in range(2)\n-        ]\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 5])\n-        self.assertEqual(batch[\"token_type_ids\"].shape.as_list(), [2, 5])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 5])\n-        self.assertEqual(batch[\"next_sentence_label\"].shape.as_list(), [2])\n-\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"token_type_ids\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"next_sentence_label\"].shape.as_list(), [2])\n-\n-    def test_sop(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-        features = [\n-            {\n-                \"input_ids\": tf.convert_to_tensor([0, 1, 2, 3, 4]),\n-                \"token_type_ids\": tf.convert_to_tensor([0, 1, 2, 3, 4]),\n-                \"sentence_order_label\": i,\n-            }\n-            for i in range(2)\n-        ]\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 5])\n-        self.assertEqual(batch[\"token_type_ids\"].shape.as_list(), [2, 5])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 5])\n-        self.assertEqual(batch[\"sentence_order_label\"].shape.as_list(), [2])\n-\n-        data_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        batch = data_collator(features)\n-\n-        self.assertEqual(batch[\"input_ids\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"token_type_ids\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"labels\"].shape.as_list(), [2, 8])\n-        self.assertEqual(batch[\"sentence_order_label\"].shape.as_list(), [2])\n-\n-\n-@require_tf\n-class TFDataCollatorImmutabilityTest(unittest.TestCase):\n-    def setUp(self):\n-        self.tmpdirname = tempfile.mkdtemp()\n-\n-        vocab_tokens = [\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n-        self.vocab_file = os.path.join(self.tmpdirname, \"vocab.txt\")\n-        with open(self.vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n-            vocab_writer.write(\"\".join([x + \"\\n\" for x in vocab_tokens]))\n-\n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdirname)\n-\n-    def _turn_to_none(self, item):\n-        \"\"\"used to convert `item` to `None` type\"\"\"\n-        return None\n-\n-    def _validate_original_data_against_collated_data(self, collator, original_data, batch_data):\n-        # we only care about side effects, the results are tested elsewhere\n-        collator(batch_data)\n-\n-        # we go through every item and convert to `primitive` datatypes if necessary\n-        # then compares for equivalence for the original data and the data that has been passed through the collator\n-        for original, batch in zip(original_data, batch_data):\n-            for original_val, batch_val in zip(original.values(), batch.values()):\n-                if isinstance(original_val, np.ndarray):\n-                    self.assertEqual(original_val.tolist(), batch_val.tolist())\n-                elif isinstance(original_val, tf.Tensor):\n-                    self.assertEqual(original_val.numpy().tolist(), batch_val.numpy().tolist())\n-                else:\n-                    self.assertEqual(original_val, batch_val)\n-\n-    def _validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-        self, collator, base_data, input_key, input_datatype, label_key, label_datatype, ignore_label=False\n-    ):\n-        # using the arguments to recreate the features with their respective (potentially new) datatypes\n-        features_original = [\n-            {label_key: label_datatype(sample[label_key]), input_key: input_datatype(sample[input_key])}\n-            for sample in base_data\n-        ]\n-        features_batch = [\n-            {label_key: label_datatype(sample[label_key]), input_key: input_datatype(sample[input_key])}\n-            for sample in base_data\n-        ]\n-\n-        # some collators do not use labels, or sometimes we want to check if the collator with labels can handle such cases\n-        if ignore_label:\n-            for original, batch in zip(features_original, features_batch):\n-                original.pop(label_key)\n-                batch.pop(label_key)\n-\n-        self._validate_original_data_against_collated_data(\n-            collator=collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-    def test_default_collator_immutability(self):\n-        features_base_single_label = [{\"label\": i, \"inputs\": (0, 1, 2, 3, 4, 5)} for i in range(4)]\n-        features_base_multiple_labels = [{\"label\": (0, 1, 2), \"inputs\": (0, 1, 2, 3, 4, 5)} for i in range(4)]\n-\n-        for datatype_input, datatype_label in [\n-            (list, int),\n-            (list, float),\n-            (np.array, int),\n-            (np.array, tf.constant),\n-            (list, self._turn_to_none),\n-        ]:\n-            self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                collator=lambda x: default_data_collator(x, return_tensors=\"tf\"),\n-                base_data=features_base_single_label,\n-                input_key=\"inputs\",\n-                input_datatype=datatype_input,\n-                label_key=\"label\",\n-                label_datatype=datatype_label,\n-            )\n-\n-        for datatype_input, datatype_label in [(list, list), (list, self._turn_to_none)]:\n-            self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                collator=lambda x: default_data_collator(x, return_tensors=\"tf\"),\n-                base_data=features_base_multiple_labels,\n-                input_key=\"inputs\",\n-                input_datatype=datatype_input,\n-                label_key=\"label\",\n-                label_datatype=datatype_label,\n-            )\n-\n-        features_base_single_label_alt = [{\"input_ids\": (0, 1, 2, 3, 4), \"label\": float(i)} for i in range(4)]\n-        self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-            collator=lambda x: default_data_collator(x, return_tensors=\"tf\"),\n-            base_data=features_base_single_label_alt,\n-            input_key=\"input_ids\",\n-            input_datatype=list,\n-            label_key=\"label\",\n-            label_datatype=float,\n-        )\n-\n-    def test_with_padding_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_original = [{\"input_ids\": [0, 1, 2]}, {\"input_ids\": [0, 1, 2, 3, 4, 5]}]\n-        features_batch = [{\"input_ids\": [0, 1, 2]}, {\"input_ids\": [0, 1, 2, 3, 4, 5]}]\n-\n-        data_collator = DataCollatorWithPadding(tokenizer, padding=\"max_length\", max_length=10, return_tensors=\"tf\")\n-        self._validate_original_data_against_collated_data(\n-            collator=data_collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        self._validate_original_data_against_collated_data(\n-            collator=data_collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-    def test_for_token_classification_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_base = [\n-            {\"input_ids\": (0, 1, 2), \"labels\": (0, 1, 2)},\n-            {\"input_ids\": (0, 1, 2, 3, 4, 5), \"labels\": (0, 1, 2, 3, 4, 5)},\n-        ]\n-        token_classification_collators = [\n-            DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\"),\n-            DataCollatorForTokenClassification(tokenizer, padding=\"max_length\", max_length=10, return_tensors=\"tf\"),\n-            DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\"),\n-            DataCollatorForTokenClassification(tokenizer, label_pad_token_id=-1, return_tensors=\"tf\"),\n-        ]\n-\n-        for datatype_input, datatype_label in [(list, list)]:\n-            for collator in token_classification_collators:\n-                self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                    collator=collator,\n-                    base_data=features_base,\n-                    input_key=\"input_ids\",\n-                    input_datatype=datatype_input,\n-                    label_key=\"labels\",\n-                    label_datatype=datatype_label,\n-                )\n-\n-        self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-            collator=token_classification_collators[-1],\n-            base_data=features_base,\n-            input_key=\"input_ids\",\n-            input_datatype=datatype_input,\n-            label_key=\"labels\",\n-            label_datatype=datatype_label,\n-            ignore_label=True,\n-        )\n-\n-    def test_seq2seq_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_base = [\n-            {\"input_ids\": list(range(3)), \"labels\": list(range(3))},\n-            {\"input_ids\": list(range(6)), \"labels\": list(range(6))},\n-        ]\n-        seq2seq_collators = [\n-            DataCollatorForSeq2Seq(tokenizer, padding=PaddingStrategy.LONGEST, return_tensors=\"tf\"),\n-            DataCollatorForSeq2Seq(tokenizer, padding=PaddingStrategy.MAX_LENGTH, max_length=7, return_tensors=\"tf\"),\n-            DataCollatorForSeq2Seq(\n-                tokenizer, padding=PaddingStrategy.LONGEST, pad_to_multiple_of=8, return_tensors=\"tf\"\n-            ),\n-            DataCollatorForSeq2Seq(\n-                tokenizer, padding=PaddingStrategy.LONGEST, label_pad_token_id=-1, return_tensors=\"tf\"\n-            ),\n-        ]\n-\n-        for datatype_input, datatype_label in [(list, list)]:\n-            for collator in seq2seq_collators:\n-                self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                    collator=collator,\n-                    base_data=features_base,\n-                    input_key=\"input_ids\",\n-                    input_datatype=datatype_input,\n-                    label_key=\"labels\",\n-                    label_datatype=datatype_label,\n-                )\n-\n-        self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-            collator=seq2seq_collators[-1],\n-            base_data=features_base,\n-            input_key=\"input_ids\",\n-            input_datatype=datatype_input,\n-            label_key=\"labels\",\n-            label_datatype=datatype_label,\n-            ignore_label=True,\n-        )\n-\n-        features_base_no_pad = [\n-            {\"input_ids\": list(range(3)), \"labels\": list(range(3))},\n-            {\"input_ids\": list(range(3)), \"labels\": list(range(3))},\n-        ]\n-        seq2seq_no_padding_collator = DataCollatorForSeq2Seq(\n-            tokenizer, padding=PaddingStrategy.DO_NOT_PAD, return_tensors=\"tf\"\n-        )\n-        for datatype_input, datatype_label in [(list, list)]:\n-            self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                collator=seq2seq_no_padding_collator,\n-                base_data=features_base_no_pad,\n-                input_key=\"input_ids\",\n-                input_datatype=datatype_input,\n-                label_key=\"labels\",\n-                label_datatype=datatype_label,\n-            )\n-\n-    def test_language_modelling_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_base_no_pad = [\n-            {\"input_ids\": tuple(range(10)), \"labels\": (1,)},\n-            {\"input_ids\": tuple(range(10)), \"labels\": (1,)},\n-        ]\n-        features_base_pad = [\n-            {\"input_ids\": tuple(range(5)), \"labels\": (1,)},\n-            {\"input_ids\": tuple(range(5)), \"labels\": (1,)},\n-        ]\n-        lm_collators = [\n-            DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\"),\n-            DataCollatorForLanguageModeling(tokenizer, mlm=False, pad_to_multiple_of=8, return_tensors=\"tf\"),\n-            DataCollatorForLanguageModeling(tokenizer, return_tensors=\"tf\"),\n-            DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\"),\n-        ]\n-\n-        for datatype_input, datatype_label in [(list, list)]:\n-            for collator in lm_collators:\n-                self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                    collator=collator,\n-                    base_data=features_base_no_pad,\n-                    input_key=\"input_ids\",\n-                    input_datatype=datatype_input,\n-                    label_key=\"labels\",\n-                    label_datatype=datatype_label,\n-                    ignore_label=True,\n-                )\n-\n-                self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                    collator=collator,\n-                    base_data=features_base_pad,\n-                    input_key=\"input_ids\",\n-                    input_datatype=datatype_input,\n-                    label_key=\"labels\",\n-                    label_datatype=datatype_label,\n-                    ignore_label=True,\n-                )\n-\n-    def test_whole_world_masking_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_base = [\n-            {\"input_ids\": list(range(10)), \"labels\": (1,)},\n-            {\"input_ids\": list(range(10)), \"labels\": (1,)},\n-        ]\n-        whole_word_masking_collator = DataCollatorForWholeWordMask(tokenizer, return_tensors=\"tf\")\n-\n-        for datatype_input, datatype_label in [(list, list), (np.array, np.array)]:\n-            self._validate_original_data_against_collated_data_on_specified_keys_and_datatypes(\n-                collator=whole_word_masking_collator,\n-                base_data=features_base,\n-                input_key=\"input_ids\",\n-                input_datatype=datatype_input,\n-                label_key=\"labels\",\n-                label_datatype=datatype_label,\n-                ignore_label=True,\n-            )\n-\n-    def test_permutation_language_modelling_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        plm_collator = DataCollatorForPermutationLanguageModeling(tokenizer, return_tensors=\"tf\")\n-\n-        no_pad_features_original = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n-        no_pad_features_batch = [{\"input_ids\": list(range(10))}, {\"input_ids\": list(range(10))}]\n-        self._validate_original_data_against_collated_data(\n-            collator=plm_collator, original_data=no_pad_features_original, batch_data=no_pad_features_batch\n-        )\n-\n-        pad_features_original = [{\"input_ids\": list(range(5))}, {\"input_ids\": list(range(10))}]\n-        pad_features_batch = [{\"input_ids\": list(range(5))}, {\"input_ids\": list(range(10))}]\n-        self._validate_original_data_against_collated_data(\n-            collator=plm_collator, original_data=pad_features_original, batch_data=pad_features_batch\n-        )\n-\n-    def test_next_sentence_prediction_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_original = [\n-            {\"input_ids\": [0, 1, 2, 3, 4], \"token_type_ids\": [0, 1, 2, 3, 4], \"next_sentence_label\": i}\n-            for i in range(2)\n-        ]\n-        features_batch = [\n-            {\"input_ids\": [0, 1, 2, 3, 4], \"token_type_ids\": [0, 1, 2, 3, 4], \"next_sentence_label\": i}\n-            for i in range(2)\n-        ]\n-\n-        nsp_collator = DataCollatorForLanguageModeling(tokenizer, return_tensors=\"tf\")\n-        self._validate_original_data_against_collated_data(\n-            collator=nsp_collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-        nsp_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        self._validate_original_data_against_collated_data(\n-            collator=nsp_collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-    def test_sentence_order_prediction_collator_immutability(self):\n-        tokenizer = BertTokenizer(self.vocab_file)\n-\n-        features_original = [\n-            {\n-                \"input_ids\": tf.convert_to_tensor([0, 1, 2, 3, 4]),\n-                \"token_type_ids\": tf.convert_to_tensor([0, 1, 2, 3, 4]),\n-                \"sentence_order_label\": i,\n-            }\n-            for i in range(2)\n-        ]\n-        features_batch = [\n-            {\n-                \"input_ids\": tf.convert_to_tensor([0, 1, 2, 3, 4]),\n-                \"token_type_ids\": tf.convert_to_tensor([0, 1, 2, 3, 4]),\n-                \"sentence_order_label\": i,\n-            }\n-            for i in range(2)\n-        ]\n-\n-        sop_collator = DataCollatorForLanguageModeling(tokenizer, return_tensors=\"tf\")\n-        self._validate_original_data_against_collated_data(\n-            collator=sop_collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-        sop_collator = DataCollatorForLanguageModeling(tokenizer, pad_to_multiple_of=8, return_tensors=\"tf\")\n-        self._validate_original_data_against_collated_data(\n-            collator=sop_collator, original_data=features_original, batch_data=features_batch\n-        )\n-\n-\n class NumpyDataCollatorIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.tmpdirname = tempfile.mkdtemp()"
        },
        {
            "sha": "8d418d7fe3fc78f8af266b0a27c8489771fea4e2",
            "filename": "tests/utils/test_activations_tf.py",
            "status": "removed",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Futils%2Ftest_activations_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d37f7517972f67e3f2194c000ed0f87f064e5099/tests%2Futils%2Ftest_activations_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_activations_tf.py?ref=d37f7517972f67e3f2194c000ed0f87f064e5099",
            "patch": "@@ -1,60 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_tf\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.activations_tf import get_tf_activation\n-\n-\n-@require_tf\n-class TestTFActivations(unittest.TestCase):\n-    def test_gelu_10(self):\n-        x = tf.constant([-100, -1.0, -0.1, 0, 0.1, 1.0, 100.0])\n-        gelu = get_tf_activation(\"gelu\")\n-        gelu10 = get_tf_activation(\"gelu_10\")\n-\n-        y_gelu = gelu(x)\n-        y_gelu_10 = gelu10(x)\n-\n-        clipped_mask = tf.where(y_gelu_10 < 10.0, 1.0, 0.0)\n-\n-        self.assertEqual(tf.math.reduce_max(y_gelu_10).numpy().item(), 10.0)\n-        self.assertTrue(np.allclose(y_gelu * clipped_mask, y_gelu_10 * clipped_mask))\n-\n-    def test_get_activation(self):\n-        get_tf_activation(\"gelu\")\n-        get_tf_activation(\"gelu_10\")\n-        get_tf_activation(\"gelu_fast\")\n-        get_tf_activation(\"gelu_new\")\n-        get_tf_activation(\"glu\")\n-        get_tf_activation(\"mish\")\n-        get_tf_activation(\"quick_gelu\")\n-        get_tf_activation(\"relu\")\n-        get_tf_activation(\"sigmoid\")\n-        get_tf_activation(\"silu\")\n-        get_tf_activation(\"swish\")\n-        get_tf_activation(\"tanh\")\n-        with self.assertRaises(KeyError):\n-            get_tf_activation(\"bogus\")\n-        with self.assertRaises(KeyError):\n-            get_tf_activation(None)"
        },
        {
            "sha": "725474291ca97b9b57dc22a10abd5d7b77e4f020",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -36,7 +36,7 @@\n     retrieve_model_classes,\n     simplify_replacements,\n )\n-from transformers.testing_utils import require_flax, require_tf, require_torch\n+from transformers.testing_utils import require_flax, require_torch\n \n \n BERT_MODEL_FILES = {\n@@ -84,7 +84,6 @@\n \n \n @require_torch\n-@require_tf\n @require_flax\n class TestAddNewModelLike(unittest.TestCase):\n     def init_file(self, file_name, content):"
        },
        {
            "sha": "7a5150232c1d4851ab62f013b472af107372fbb5",
            "filename": "tests/utils/test_doc_samples.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_doc_samples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_doc_samples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_doc_samples.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -19,15 +19,14 @@\n from typing import Union\n \n import transformers\n-from transformers.testing_utils import require_tf, require_torch, slow\n+from transformers.testing_utils import require_torch, slow\n \n \n logger = logging.getLogger()\n \n \n @unittest.skip(reason=\"Temporarily disable the doc tests.\")\n @require_torch\n-@require_tf\n @slow\n class TestCodeExamples(unittest.TestCase):\n     def analyze_directory("
        },
        {
            "sha": "162b327197b6ce30443fec6a602d1225fd0320f7",
            "filename": "tests/utils/test_file_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_file_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_file_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_file_utils.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -21,16 +21,13 @@\n \n # Try to import everything from transformers to ensure every object can be loaded.\n from transformers import *  # noqa F406\n-from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, require_flax, require_tf, require_torch\n-from transformers.utils import ContextManagers, find_labels, is_flax_available, is_tf_available, is_torch_available\n+from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, require_flax, require_torch\n+from transformers.utils import ContextManagers, find_labels, is_flax_available, is_torch_available\n \n \n if is_torch_available():\n     from transformers import BertForPreTraining, BertForQuestionAnswering, BertForSequenceClassification\n \n-if is_tf_available():\n-    from transformers import TFBertForPreTraining, TFBertForQuestionAnswering, TFBertForSequenceClassification\n-\n if is_flax_available():\n     from transformers import FlaxBertForPreTraining, FlaxBertForQuestionAnswering, FlaxBertForSequenceClassification\n \n@@ -107,18 +104,6 @@ class DummyModel(BertForSequenceClassification):\n \n         self.assertEqual(find_labels(DummyModel), [\"labels\"])\n \n-    @require_tf\n-    def test_find_labels_tf(self):\n-        self.assertEqual(find_labels(TFBertForSequenceClassification), [\"labels\"])\n-        self.assertEqual(find_labels(TFBertForPreTraining), [\"labels\", \"next_sentence_label\"])\n-        self.assertEqual(find_labels(TFBertForQuestionAnswering), [\"start_positions\", \"end_positions\"])\n-\n-        # find_labels works regardless of the class name (it detects the framework through inheritance)\n-        class DummyModel(TFBertForSequenceClassification):\n-            pass\n-\n-        self.assertEqual(find_labels(DummyModel), [\"labels\"])\n-\n     @require_flax\n     def test_find_labels_flax(self):\n         # Flax models don't have labels"
        },
        {
            "sha": "a230da5dc33072c4cbf1c1fa8264a3b63b0553bf",
            "filename": "tests/utils/test_generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 72,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_generic.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_generic.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_generic.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -19,14 +19,13 @@\n \n from transformers.configuration_utils import PretrainedConfig\n from transformers.modeling_outputs import BaseModelOutput\n-from transformers.testing_utils import require_flax, require_tf, require_torch\n+from transformers.testing_utils import require_flax, require_torch\n from transformers.utils import (\n     can_return_tuple,\n     expand_dims,\n     filter_out_non_signature_kwargs,\n     flatten_dict,\n     is_flax_available,\n-    is_tf_available,\n     is_torch_available,\n     reshape,\n     squeeze,\n@@ -38,9 +37,6 @@\n if is_flax_available():\n     import jax.numpy as jnp\n \n-if is_tf_available():\n-    import tensorflow as tf\n-\n if is_torch_available():\n     import torch\n \n@@ -88,16 +84,6 @@ def test_transpose_torch(self):\n         t = torch.tensor(x)\n         self.assertTrue(np.allclose(transpose(x, axes=(1, 2, 0)), transpose(t, axes=(1, 2, 0)).numpy()))\n \n-    @require_tf\n-    def test_transpose_tf(self):\n-        x = np.random.randn(3, 4)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(transpose(x), transpose(t).numpy()))\n-\n-        x = np.random.randn(3, 4, 5)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(transpose(x, axes=(1, 2, 0)), transpose(t, axes=(1, 2, 0)).numpy()))\n-\n     @require_flax\n     def test_transpose_flax(self):\n         x = np.random.randn(3, 4)\n@@ -125,16 +111,6 @@ def test_reshape_torch(self):\n         t = torch.tensor(x)\n         self.assertTrue(np.allclose(reshape(x, (12, 5)), reshape(t, (12, 5)).numpy()))\n \n-    @require_tf\n-    def test_reshape_tf(self):\n-        x = np.random.randn(3, 4)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(reshape(x, (4, 3)), reshape(t, (4, 3)).numpy()))\n-\n-        x = np.random.randn(3, 4, 5)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(reshape(x, (12, 5)), reshape(t, (12, 5)).numpy()))\n-\n     @require_flax\n     def test_reshape_flax(self):\n         x = np.random.randn(3, 4)\n@@ -162,16 +138,6 @@ def test_squeeze_torch(self):\n         t = torch.tensor(x)\n         self.assertTrue(np.allclose(squeeze(x, axis=2), squeeze(t, axis=2).numpy()))\n \n-    @require_tf\n-    def test_squeeze_tf(self):\n-        x = np.random.randn(1, 3, 4)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(squeeze(x), squeeze(t).numpy()))\n-\n-        x = np.random.randn(1, 4, 1, 5)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(squeeze(x, axis=2), squeeze(t, axis=2).numpy()))\n-\n     @require_flax\n     def test_squeeze_flax(self):\n         x = np.random.randn(1, 3, 4)\n@@ -192,12 +158,6 @@ def test_expand_dims_torch(self):\n         t = torch.tensor(x)\n         self.assertTrue(np.allclose(expand_dims(x, axis=1), expand_dims(t, axis=1).numpy()))\n \n-    @require_tf\n-    def test_expand_dims_tf(self):\n-        x = np.random.randn(3, 4)\n-        t = tf.constant(x)\n-        self.assertTrue(np.allclose(expand_dims(x, axis=1), expand_dims(t, axis=1).numpy()))\n-\n     @require_flax\n     def test_expand_dims_flax(self):\n         x = np.random.randn(3, 4)\n@@ -232,18 +192,6 @@ def test_to_py_obj_torch(self):\n \n         self.assertTrue(to_py_obj([t1, t2]) == [x1, x2])\n \n-    @require_tf\n-    def test_to_py_obj_tf(self):\n-        x1 = [[1, 2, 3], [4, 5, 6]]\n-        t1 = tf.constant(x1)\n-        self.assertTrue(to_py_obj(t1) == x1)\n-\n-        x2 = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n-        t2 = tf.constant(x2)\n-        self.assertTrue(to_py_obj(t2) == x2)\n-\n-        self.assertTrue(to_py_obj([t1, t2]) == [x1, x2])\n-\n     @require_flax\n     def test_to_py_obj_flax(self):\n         x1 = [[1, 2, 3], [4, 5, 6]]\n@@ -256,25 +204,6 @@ def test_to_py_obj_flax(self):\n \n         self.assertTrue(to_py_obj([t1, t2]) == [x1, x2])\n \n-    @require_torch\n-    @require_tf\n-    @require_flax\n-    def test_to_py_obj_mixed(self):\n-        x1 = [[1], [2]]\n-        t1 = np.array(x1)\n-\n-        x2 = [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n-        t2 = torch.tensor(x2)\n-\n-        x3 = [1, 2, 3]\n-        t3 = tf.constant(x3)\n-\n-        x4 = [[[1.0, 2.0]]]\n-        t4 = jnp.array(x4)\n-\n-        mixed = [(t1, t2), (t3, t4)]\n-        self.assertTrue(to_py_obj(mixed) == [[x1, x2], [x3, x4]])\n-\n \n class ValidationDecoratorTester(unittest.TestCase):\n     def test_cases_no_warning(self):"
        },
        {
            "sha": "e13fee27283d17a5a5ce3d348270955dbfe42e7e",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d45d90e5d1552eccb6d8cc9b7bba283ccefb808/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=1d45d90e5d1552eccb6d8cc9b7bba283ccefb808",
            "patch": "@@ -61,7 +61,6 @@\n     require_non_hpu,\n     require_read_token,\n     require_safetensors,\n-    require_tf,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_multi_accelerator,\n@@ -79,7 +78,6 @@\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n     is_flax_available,\n-    is_tf_available,\n     is_torch_npu_available,\n     is_torch_sdpa_available,\n )\n@@ -322,9 +320,6 @@ def forward(self):\n if is_flax_available():\n     from transformers import FlaxBertModel\n \n-if is_tf_available():\n-    from transformers import TFBertModel\n-\n \n TINY_T5 = \"patrickvonplaten/t5-tiny-random\"\n TINY_BERT_FOR_TOKEN_CLASSIFICATION = \"hf-internal-testing/tiny-bert-for-token-classification\"\n@@ -1535,27 +1530,6 @@ def test_safetensors_torch_from_flax(self):\n         for p1, p2 in zip(hub_model.parameters(), new_model.parameters()):\n             self.assertTrue(torch.equal(p1, p2))\n \n-    @require_tf\n-    @require_safetensors\n-    def test_safetensors_torch_from_tf(self):\n-        hub_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n-        model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-tf-only\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n-            new_model = BertModel.from_pretrained(tmp_dir)\n-\n-        for p1, p2 in zip(hub_model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.equal(p1, p2))\n-\n-    @require_tf\n-    def test_torch_from_tf(self):\n-        model = TFBertModel.from_pretrained(\"hf-internal-testing/tiny-bert-tf-only\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir)\n-            _ = BertModel.from_pretrained(tmp_dir, from_tf=True)\n-\n     @require_safetensors\n     def test_safetensors_torch_from_torch_sharded(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")"
        }
    ],
    "stats": {
        "total": 2525,
        "additions": 21,
        "deletions": 2504
    }
}