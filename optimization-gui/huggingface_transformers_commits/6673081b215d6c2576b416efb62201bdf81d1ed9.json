{
    "author": "yao-matrix",
    "message": "enable 6 granite cases on xpu (#37569)\n\n* enable 6 granite cases on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* make them all pass on A100\n\nSigned-off-by: N <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nSigned-off-by: N <matrix.yao@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "6673081b215d6c2576b416efb62201bdf81d1ed9",
    "files": [
        {
            "sha": "be1b5841ff8a7a857b2d373f11a0b8f990e9d9d6",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 28,
            "deletions": 8,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -19,9 +19,10 @@\n \n from transformers import GraniteConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -302,7 +303,7 @@ def test_model_rope_scaling(self):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class GraniteIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -328,15 +329,27 @@ def test_model_3b_logits_bf16(self):\n         # Expected mean on dim = -1\n \n         # fmt: off\n-        EXPECTED_MEAN = torch.tensor([[-1.9798, -3.1626, -2.8062, -2.3777, -2.7091, -2.2338, -2.5924, -2.3974]])\n+        EXPECTED_MEANS = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[-3.1406, -2.5469, -2.6250, -2.1250, -2.6250, -2.6562, -2.6875, -2.9688]]),\n+                    (\"cuda\", 7): torch.tensor([[-1.9798, -3.1626, -2.8062, -2.3777, -2.7091, -2.2338, -2.5924, -2.3974]]),\n+                    (\"cuda\", 8): torch.tensor([[-3.1406, -2.5469, -2.6250, -2.1250, -2.6250, -2.6562, -2.6875, -2.9688]]),\n+                }\n+            )\n+        EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n \n-        torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1), rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1).float(), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n-        EXPECTED_SLICE = torch.tensor([[4.8750, -2.1875, -2.1875, -2.1875, -2.1875, -2.8438, -2.1875, -2.1875,\n-        -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875]])\n+        EXPECTED_SLICES = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[2.2031, -5.0625, -5.0625, -5.0625, -5.0625, -0.9180, -5.0625, -5.0625, -5.0625, -5.0625, -5.5312, -2.1719, -1.7891, -0.4922, -2.5469]]),\n+                    (\"cuda\", 7): torch.tensor([[4.8750, -2.1875, -2.1875, -2.1875, -2.1875, -2.8438, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875, -2.1875]]),\n+                    (\"cuda\", 8): torch.tensor([[2.0938, -5.0312, -5.0312, -5.0312, -5.0312, -1.0469, -5.0312, -5.0312, -5.0312, -5.0312, -5.5625, -2.1875, -1.7891, -0.5820, -2.6250]]),\n+                }\n+            )\n+        EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n         # fmt: on\n-\n         self.assertTrue(\n             torch.allclose(\n                 EXPECTED_SLICE.to(torch_device),\n@@ -358,6 +371,13 @@ def test_model_3b_logits(self):\n \n         # fmt: off\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-2.0984, -3.1294, -2.8153, -2.3568, -2.7337, -2.2624, -2.6016, -2.4022]])\n+        EXPECTED_MEANS = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[-3.2693, -2.5957, -2.6234, -2.1675, -2.6386, -2.6850, -2.7039, -2.9656]]),\n+                    (\"cuda\", 7): torch.tensor([[-2.0984, -3.1294, -2.8153, -2.3568, -2.7337, -2.2624, -2.6016, -2.4022]]),\n+                    (\"cuda\", 8): torch.tensor([[-3.2934, -2.6019, -2.6258, -2.1691, -2.6394, -2.6876, -2.7032, -2.9688]]),\n+                }\n+            )\n+        EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n \n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)"
        },
        {
            "sha": "e451ff30c84eb695911624e86596e7be887d76a9",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 38,
            "deletions": 8,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -19,9 +19,10 @@\n \n from transformers import AutoTokenizer, GraniteMoeConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -301,7 +302,7 @@ def test_model_rope_scaling(self):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class GraniteMoeIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -325,13 +326,26 @@ def test_model_3b_logits(self):\n \n         # fmt: off\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]])\n+        EXPECTED_MEANS = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+                    (\"cuda\", 7): torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]]),\n+                    (\"cuda\", 8): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+                }\n+            )\n+        EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n \n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n-        EXPECTED_SLICE = torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892,\n-        -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]])\n+        EXPECTED_SLICES = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n+                    (\"cuda\", 7): torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892, -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]]),\n+                    (\"cuda\", 8): torch.tensor([[2.5479, -9.2124, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2162, -9.2122, -6.3101, -3.6224, -3.6377, -5.2542, -5.2524]]),\n+                }\n+            )\n+        EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n         # fmt: on\n \n         self.assertTrue(\n@@ -346,10 +360,26 @@ def test_model_3b_logits(self):\n     @slow\n     def test_model_3b_generation(self):\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n-        EXPECTED_TEXT_COMPLETION = (\n-            \"Simply put, the theory of relativity states that \\n$$\\n\\\\frac{d^2x^\\\\mu}{d\\\\tau^2} = \"\n-            \"\\\\frac{1}{c^2}\\\\frac{d^2x^\\\\mu}{dt^2}\\n$$\\nwhere $x^\\\\mu$ is a four-vector, $\\\\tau$ is the proper time\"\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"xpu\", 3): (\n+                    \"Simply put, the theory of relativity states that 1) the speed of light is constant, and 2) the speed of light is the same for all observers.\\n\\n\"\n+                    \"The first part is easy to understand. The second part is a little more difficult.\\n\\n\"\n+                    \"The second part of the theory of relativity is a little more difficult to understand.\\n\"\n+                ),\n+                (\"cuda\", 7): (\n+                    \"Simply put, the theory of relativity states that \\n$$\\n\\\\frac{d^2x^\\\\mu}{d\\\\tau^2} = \"\n+                    \"\\\\frac{1}{c^2}\\\\frac{d^2x^\\\\mu}{dt^2}\\n$$\\nwhere $x^\\\\mu$ is a four-vector, $\\\\tau$ is the proper time\"\n+                ),\n+                (\"cuda\", 8): (\n+                    \"Simply put, the theory of relativity states that 1) the speed of light is constant, and 2) the speed of light is the same for all observers.\\n\\n\"\n+                    \"The first part is easy to understand. The second part is a little more difficult.\\n\\n\"\n+                    \"The second part of the theory of relativity is a little more difficult to understand.\\n\"\n+                ),\n+            }\n         )\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n+\n         prompt = \"Simply put, the theory of relativity states that \"\n         tokenizer = AutoTokenizer.from_pretrained(\"ibm/PowerMoE-3b\")\n         model = GraniteMoeForCausalLM.from_pretrained(\"ibm/PowerMoE-3b\", device_map=\"auto\")"
        },
        {
            "sha": "5de3552c20f2bdc46d0c394b8aa3b3338c7badf9",
            "filename": "tests/models/granitemoeshared/test_modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 38,
            "deletions": 8,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/6673081b215d6c2576b416efb62201bdf81d1ed9/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6673081b215d6c2576b416efb62201bdf81d1ed9/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoeshared%2Ftest_modeling_granitemoeshared.py?ref=6673081b215d6c2576b416efb62201bdf81d1ed9",
            "patch": "@@ -19,9 +19,10 @@\n \n from transformers import AutoTokenizer, GraniteMoeSharedConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -304,7 +305,7 @@ def test_model_rope_scaling(self):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class GraniteMoeSharedIntegrationTest(unittest.TestCase):\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n     # Depending on the hardware we get different logits / generations\n@@ -328,13 +329,26 @@ def test_model_3b_logits(self):\n \n         # fmt: off\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]])\n+        EXPECTED_MEANS = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+                    (\"cuda\", 7): torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]]),\n+                    (\"cuda\", 8): torch.tensor([[-4.4005, -3.6689, -3.6187, -2.8308, -3.9871, -3.1001, -2.8738, -2.8063]]),\n+                }\n+            )\n \n+        EXPECTED_MEAN = EXPECTED_MEANS.get_expectation()\n         torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n-        EXPECTED_SLICE = torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892,\n-        -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]])\n+        EXPECTED_SLICES = Expectations(\n+                {\n+                    (\"xpu\", 3): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n+                    (\"cuda\", 7): torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892, -2.2895, -2.2891, -2.2887, -2.2882, -2.2889, -2.2898, -2.2892]]),\n+                    (\"cuda\", 8): torch.tensor([[2.5479, -9.2123, -9.2121, -9.2175, -9.2122, -1.5024, -9.2121, -9.2122, -9.2161, -9.2122, -6.3100, -3.6223, -3.6377, -5.2542, -5.2523]]),\n+                }\n+            )\n+        EXPECTED_SLICE = EXPECTED_SLICES.get_expectation()\n         # fmt: on\n \n         self.assertTrue(\n@@ -349,10 +363,26 @@ def test_model_3b_logits(self):\n     @slow\n     def test_model_3b_generation(self):\n         # ground truth text generated with dola_layers=\"low\", repetition_penalty=1.2\n-        EXPECTED_TEXT_COMPLETION = (\n-            \"Simply put, the theory of relativity states that \\n$$\\n\\\\frac{d^2x^\\\\mu}{d\\\\tau^2} = \"\n-            \"\\\\frac{1}{c^2}\\\\frac{d^2x^\\\\mu}{dt^2}\\n$$\\nwhere $x^\\\\mu$ is a four-vector, $\\\\tau$ is the proper time\"\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"xpu\", 3): (\n+                    \"Simply put, the theory of relativity states that 1) the speed of light is constant, and 2) the speed of light is the same for all observers.\\n\\n\"\n+                    \"The first part is easy to understand. The second part is a little more difficult.\\n\\n\"\n+                    \"The second part of the theory of relativity is a little more difficult to understand.\\n\"\n+                ),\n+                (\"cuda\", 7): (\n+                    \"Simply put, the theory of relativity states that \\n$$\\n\\\\frac{d^2x^\\\\mu}{d\\\\tau^2} = \"\n+                    \"\\\\frac{1}{c^2}\\\\frac{d^2x^\\\\mu}{dt^2}\\n$$\\nwhere $x^\\\\mu$ is a four-vector, $\\\\tau$ is the proper time\"\n+                ),\n+                (\"cuda\", 8): (\n+                    \"Simply put, the theory of relativity states that 1) the speed of light is constant, and 2) the speed of light is the same for all observers.\\n\\n\"\n+                    \"The first part is easy to understand. The second part is a little more difficult.\\n\\n\"\n+                    \"The second part of the theory of relativity is a little more difficult to understand.\\n\"\n+                ),\n+            }\n         )\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n+\n         prompt = \"Simply put, the theory of relativity states that \"\n         tokenizer = AutoTokenizer.from_pretrained(\"ibm/PowerMoE-3b\")\n         model = GraniteMoeSharedForCausalLM.from_pretrained(\"ibm/PowerMoE-3b\", device_map=\"auto\")"
        }
    ],
    "stats": {
        "total": 128,
        "additions": 104,
        "deletions": 24
    }
}