{
    "author": "gante",
    "message": "[agents] remove agents ğŸ§¹  (#37368)",
    "sha": "aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
    "files": [
        {
            "sha": "71a03b0be04d21e1a6b07352d70c25e903a69102",
            "filename": "SECURITY.md",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/SECURITY.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/SECURITY.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/SECURITY.md?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -27,13 +27,6 @@ These models require the `trust_remote_code=True` parameter to be set when using\n the content of the modeling files when using this argument. We recommend setting a revision in order to ensure you\n protect yourself from updates on the repository.\n \n-#### Tools\n-\n-Through the `Agent` framework, remote tools can be downloaded to be used by the Agent. You're to specify these tools\n-yourself, but please keep in mind that their code will be run on your machine if the Agent chooses to run them.\n-\n-Please inspect the code of the tools before passing them to the Agent to protect your runtime and local setup.\n-\n ## Reporting a Vulnerability\n \n Feel free to submit vulnerability reports to [security@huggingface.co](mailto:security@huggingface.co), where someone from the HF security team will review and recommend next steps. If reporting a vulnerability specific to open source, please note [Huntr](https://huntr.com) is a vulnerability disclosure program for open source software."
        },
        {
            "sha": "6b3100c84e77ce3bb508a0ccd70d6d117e003480",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -66,7 +66,6 @@\n     \"ModelTester::test_pipeline_\",\n     \"/repo_utils/\",\n     \"/utils/\",\n-    \"/agents/\",\n }\n \n # allow having multiple repository checkouts and not needing to remember to rerun\n@@ -83,7 +82,6 @@ def pytest_configure(config):\n     config.addinivalue_line(\"markers\", \"is_pipeline_test: mark test to run only when pipelines are tested\")\n     config.addinivalue_line(\"markers\", \"is_staging_test: mark test to run only in the staging environment\")\n     config.addinivalue_line(\"markers\", \"accelerate_tests: mark test that require accelerate\")\n-    config.addinivalue_line(\"markers\", \"agent_tests: mark the agent tests that are run on their specific schedule\")\n     config.addinivalue_line(\"markers\", \"not_device_test: mark the tests always running on cpu\")\n \n "
        },
        {
            "sha": "a754abc76c9517ce2baf1bc92f0f38171130b4b7",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -23,8 +23,6 @@\n     title: ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø®ØµØµØ© ÙˆØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— PEFT\n   - local: model_sharing\n     title: Ù…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ\n-  - local: agents\n-    title: Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡\n   - local: llm_tutorial\n     title: Ø§Ù„ØªÙˆÙ„ÙŠØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLMs\n   - local: conversations\n@@ -252,8 +250,6 @@\n   title: Ø£Ø·Ø± Ù…ÙØ§Ù‡ÙŠÙ…ÙŠØ©\n # - sections:\n #   - sections:\n-#     - local: main_classes/agent\n-#       title: Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª\n #     - local: model_doc/auto\n #       title: ÙØ¦Ø§Øª ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§\n #     - local: main_classes/backbones"
        },
        {
            "sha": "c7efd8f02f48c07471ea699b4fe57f685e3c8dfa",
            "filename": "docs/source/ar/agents.md",
            "status": "removed",
            "additions": 0,
            "deletions": 539,
            "changes": 539,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Far%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Far%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fagents.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,539 +0,0 @@\n-# Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª\n-\n-[[open-in-colab]]\n-\n-### Ù…Ø§ Ù‡Ùˆ Ø§Ù„ÙˆÙƒÙŠÙ„ØŸ\n-\n-ÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ø¸Ù… Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (LLMs) Ø§Ù„ØªÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ø¹Ù„Ù‰ Ø£Ø¯Ø§Ø¡ [Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©](./tasks/language_modeling.) Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ù‡Ø§Ù…ØŒ ÙˆÙ„ÙƒÙ†Ù‡Ø§ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªÙˆØ§Ø¬Ù‡ ØµØ¹ÙˆØ¨Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ ÙˆØ§Ù„Ø­Ø³Ø§Ø¨ ÙˆØ§Ù„Ø¨Ø­Ø«. ÙˆØ¹Ù†Ø¯Ù…Ø§ ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ ÙÙŠ Ù…Ø¬Ø§Ù„Ø§Øª Ù„Ø§ ØªØ¤Ø¯ÙŠ ÙÙŠÙ‡Ø§ Ø£Ø¯Ø§Ø¡Ù‹ Ø¬ÙŠØ¯Ù‹Ø§ØŒ ÙØ¥Ù†Ù‡Ø§ ØºØ§Ù„Ø¨Ù‹Ø§ Ù…Ø§ ØªÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„ØªÙŠ Ù†ØªÙˆÙ‚Ø¹Ù‡Ø§ Ù…Ù†Ù‡Ø§.\n-\n-ÙŠØªÙ…Ø«Ù„ Ø£Ø­Ø¯ Ø§Ù„Ù†Ù‡Ø¬ Ù„Ù„ØªØºÙ„Ø¨ Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ù‚ØµÙˆØ± ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ \"ÙˆÙƒÙŠÙ„\".\n-\n-Ø§Ù„ÙˆÙƒÙŠÙ„ Ù‡Ùˆ Ù†Ø¸Ø§Ù… ÙŠØ³ØªØ®Ø¯Ù… LLM ÙƒÙ…Ø­Ø±Ùƒ Ù„Ù‡ØŒ ÙˆÙ„Ø¯ÙŠÙ‡ Ø­Ù‚ Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ ÙˆØ¸Ø§Ø¦Ù ØªØ³Ù…Ù‰ \"Ø£Ø¯ÙˆØ§Øª\".\n-\n-Ù‡Ø°Ù‡ \"Ø§Ù„Ø£Ø¯ÙˆØ§Øª\" Ù‡ÙŠ ÙˆØ¸Ø§Ø¦Ù Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù‡Ù…Ø©ØŒ ÙˆØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆØµØ§Ù Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù„ÙˆÙƒÙŠÙ„ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.\n-\n-ÙŠÙ…ÙƒÙ† Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ÙˆÙƒÙŠÙ„ Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ù…Ø§ ÙŠÙ„ÙŠ:\n-- ÙˆØ¶Ø¹ Ø³Ù„Ø³Ù„Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª/Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØªØ´ØºÙŠÙ„Ù‡Ø§ Ø¬Ù…ÙŠØ¹Ù‹Ø§ ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª Ù…Ø«Ù„ [`CodeAgent`] Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„\n-- Ø§Ù„ØªØ®Ø·ÙŠØ· Ù„Ù„Ø§Ø¬Ø±Ø§Ø¡Ø§Øª/Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØªÙ†ÙÙŠØ°Ù‡Ø§ ÙˆØ§Ø­Ø¯Ø© ØªÙ„Ùˆ Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆØ§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ Ø§Ù†ØªÙ‡Ø§Ø¡ ÙƒÙ„ Ø¥Ø¬Ø±Ø§Ø¡ Ù‚Ø¨Ù„ Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„ØªØ§Ù„ÙŠ Ù…Ø«Ù„ [`ReactJsonAgent`] Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„\n-\n-### Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡\n-\n-#### Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ (Code agent)\n-\n-ÙŠØªÙ…ØªØ¹ Ù‡Ø°Ø§ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙŠØªØ¨Ø¹ Ø®Ø·ÙˆØ§Øª Ù…Ø­Ø¯Ø¯Ø©: Ø£ÙˆÙ„Ù‹Ø§ØŒ ÙŠØ®Ø·Ø· Ù„Ø³Ù„Ø³Ù„Ø© Ù…Ù† Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ±ÙŠØ¯ ØªÙ†ÙÙŠØ°Ù‡Ø§ØŒ Ø«Ù… Ø´ÙØ±Ø© Python Ù„ØªÙ†ÙÙŠØ° Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª. ÙˆÙ‡Ùˆ ÙŠØªØ¹Ø§Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø£ØµÙ„ÙŠ Ù…Ø¹ Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ù„Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ØŒ ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠ ÙÙ‡Ùˆ Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡ Ù„Ù„Ù…Ù‡Ø§Ù… Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„ÙˆØ³Ø§Ø¦Ø·.\n-\n-#### ÙˆÙƒÙ„Ø§Ø¡ Ø§Ù„ØªÙØ§Ø¹Ù„\n-\n-Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø°ÙŠ ÙŠØªÙ… Ø§Ù„Ù„Ø¬ÙˆØ¡ Ø¥Ù„ÙŠÙ‡ Ù„Ø­Ù„ Ù…Ù‡Ø§Ù… Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø­ÙŠØ« ÙŠØ¬Ø¹Ù„ Ø¥Ø·Ø§Ø± ReAct ([Yao et al.ØŒ 2022](https://huggingface.co/papers/2210.03629)) Ù…Ù† Ø§Ù„ÙƒÙØ§Ø¡Ø© Ø­Ù‚Ù‹Ø§ Ø§Ù„ØªÙÙƒÙŠØ± Ø¹Ù„Ù‰ Ø£Ø³Ø§Ø³ Ù…Ù„Ø§Ø­Ø¸Ø§ØªÙ‡ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©.\n-\n-Ù†Ù‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° Ø¥ØµØ¯Ø§Ø±ÙŠÙ† Ù…Ù† ReactJsonAgent: \n-- [`ReactJsonAgent`] ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙƒÙ€ JSON ÙÙŠ Ø¥Ø®Ø±Ø§Ø¬Ù‡Ø§.\n-- [`ReactCodeAgent`] Ù‡Ùˆ Ù†ÙˆØ¹ Ø¬Ø¯ÙŠØ¯ Ù…Ù† ReactJsonAgent ÙŠÙ‚ÙˆÙ… Ø¨ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø£Ø¯ÙˆØ§ØªÙ‡ ÙƒÙ…Ù‚Ø§Ø·Ø¹ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ¹Ù…Ù„ Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ Ø­Ù‚Ù‹Ø§ Ù…Ø¹ LLMs Ø§Ù„ØªÙŠ ØªØªÙ…ØªØ¹ Ø¨Ø£Ø¯Ø§Ø¡  Ù‚ÙˆÙŠ ÙÙŠ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©.\n-\n-> [!TIP]\n-> Ø§Ù‚Ø±Ø£ Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø¹Ù† ÙˆÙƒÙŠÙ„ ReAct.\n-\n-![Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ ÙˆÙƒÙŠÙ„ ReAct](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/open-source-llms-as-agents/ReAct.png)\n-\n-Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥Ù„ÙŠÙƒ ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ ÙˆÙƒÙŠÙ„ ReAct Code Ø·Ø±ÙŠÙ‚Ù‡ Ù…Ù† Ø®Ù„Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ.\n-\n-```py3\n->>> agent.run(\n-...     \"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\",\n-... )\n-=====New task=====\n-How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\n-====Agent is executing the code below:\n-bert_blocks = search(query=\"number of blocks in BERT base encoder\")\n-print(\"BERT blocks:\", bert_blocks)\n-====\n-Print outputs:\n-BERT blocks: twelve encoder blocks\n-\n-====Agent is executing the code below:\n-attention_layer = search(query=\"number of layers in Attention is All You Need\")\n-print(\"Attention layers:\", attention_layer)\n-====\n-Print outputs:\n-Attention layers: Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- 2 Page 3 Figure 1: The Transformer - model architecture.\n-\n-====Agent is executing the code below:\n-bert_blocks = 12\n-attention_layers = 6\n-diff = bert_blocks - attention_layers\n-print(\"Difference in blocks:\", diff)\n-final_answer(diff)\n-====\n-\n-Print outputs:\n-Difference in blocks: 6\n-\n-Final answer: 6\n-```\n-\n-### ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø¨Ù†Ø§Ø¡ ÙˆÙƒÙŠÙ„ØŸ\n-\n-Ù„ØªÙ‡ÙŠØ¦Ø© ÙˆÙƒÙŠÙ„ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·:\n-\n-- Ù†Ù…ÙˆØ°Ø¬ Ù„ØºÙˆÙŠ ÙƒØ¨ÙŠØ± (LLM) ÙŠØ´ÙƒÙ„ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„ÙˆÙƒÙŠÙ„. Ø§Ù„ÙˆÙƒÙŠÙ„ Ù†ÙØ³Ù‡ Ù„ÙŠØ³ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠØŒ Ø¨Ù„ Ù‡Ùˆ Ø¨Ø±Ù†Ø§Ù…Ø¬ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ ÙƒÙ…Ø­Ø±Ùƒ Ù„Ù‡.\n-- Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… (system prompt): Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ø¹Ø·Ø§Ø¤Ù‡Ø§ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø±Ø¬Ø§ØªÙ‡.\n-- ØµÙ†Ø¯ÙˆÙ‚ Ø£Ø¯ÙˆØ§Øª (toolbox) ÙŠØ®ØªØ§Ø± Ø§Ù„ÙˆÙƒÙŠÙ„ Ù…Ù†Ù‡ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ù„ØªÙ†ÙÙŠØ°Ù‡Ø§\n-- Ù…Ø­Ù„Ù„ (parser) Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ù…Ù† Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºÙˆÙŠ LLM ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§\n-\n-Ø¹Ù†Ø¯ ØªÙ‡ÙŠØ¦Ø© Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆÙƒÙŠÙ„ØŒ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³Ù…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙˆØµÙ Ù„Ù„Ø£Ø¯Ø§Ø©ØŒ Ø«Ù… ÙŠØªÙ… Ø¯Ù…Ø¬Ù‡Ø§ ÙÙŠ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø§Øµ `system_prompt` Ù„Ù„ÙˆÙƒÙŠÙ„ Ù„Ø¥Ø¹Ù„Ø§Ù…Ù‡ Ø¨Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙˆÙ„Ù…Ø§Ø°Ø§.\n-\n-Ù„Ù„Ø¨Ø¯Ø¡ØŒ ÙŠØ±Ø¬Ù‰ ØªØ«Ø¨ÙŠØª `agents` Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ù„ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©.\n-\n-```bash\n-pip install transformers[agents]\n-```\n-\n-Ù‚Ù… Ø¨Ø¨Ù†Ø§Ø¡ Ù…Ø­Ø±Ùƒ LLM Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù…Ù† Ø®Ù„Ø§Ù„ ØªØ¹Ø±ÙŠÙ Ø·Ø±ÙŠÙ‚Ø© `llm_engine` Ø§Ù„ØªÙŠ ØªÙ‚Ø¨Ù„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† [Ø§Ù„Ø±Ø³Ø§Ø¦Ù„](./chat_templating.) ÙˆØªØ¹ÙŠØ¯ Ø§Ù„Ù†Øµ. ÙŠØ¬Ø¨ Ø£Ù† ØªÙ‚Ø¨Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø£ÙŠØ¶Ù‹Ø§ Ù…Ø¹Ø§Ù…Ù„ `stop` ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ù…ØªÙ‰ ÙŠØ¬Ø¨ Ø§Ù„ØªÙˆÙ‚Ù Ø¹Ù† Ø§Ù„ØªÙˆÙ„ÙŠØ¯.\n-\n-```python\n-from huggingface_hub import login, InferenceClient\n-\n-login(\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n-\n-client = InferenceClient(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-\n-def llm_engine(messages, stop_sequences=[\"Task\"]) -> str:\n-    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n-    answer = response.choices[0].message.content\n-    return answer\n-```\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø·Ø±ÙŠÙ‚Ø© `llm_engine` Ø·Ø§Ù„Ù…Ø§ Ø£Ù†Ù‡Ø§:\n-1. ÙŠØªØ¨Ø¹ ØªÙ†Ø³ÙŠÙ‚ [Ø±Ø³Ø§Ø¦Ù„](./chat_templating.md) Ù„Ø¥Ø¯Ø®Ø§Ù„Ù‡ (`List [Dict [strØŒ str]]`) ÙˆÙŠØ¹ÙŠØ¯ `str`\n-2. ÙŠØªÙˆÙ‚Ù Ø¹Ù† ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù…Ø®Ø±Ø§Ø¬Ø§Øª Ù…Ù† Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ ÙÙŠ Ù…Ø¹Ø§Ù…Ù„ `stop`\n-\n-Ø£Ù†Øª Ø¨Ø­Ø§Ø¬Ø© Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù…Ù„ \"Ø§Ù„Ø£Ø¯ÙˆØ§Øª\" Ø§Ù„Ø°ÙŠ ÙŠÙ‚Ø¨Ù„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† \"Ø§Ù„Ø£Ø¯ÙˆØ§Øª\". ÙŠÙ…ÙƒÙ†Ùƒ ØªÙˆÙÙŠØ± Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ© Ù„Ù€ \"Ø§Ù„Ø£Ø¯ÙˆØ§Øª\"ØŒ ÙˆÙ„ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ù… ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø¹ Ù…Ø¹Ø§Ù…Ù„ Ø§Ø®ØªÙŠØ§Ø±ÙŠ `add_base_tools=True`.\n-\n-Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ ÙˆÙƒÙŠÙ„ØŒ Ù…Ø«Ù„ [`CodeAgent`], ÙˆØªØ´ØºÙŠÙ„Ù‡. ÙˆÙ„ØªØ³Ù‡ÙŠÙ„ Ø§Ù„Ø£Ù…Ø±ØŒ Ù†Ù‚Ø¯Ù… Ø£ÙŠØ¶Ù‹Ø§ ÙØ¦Ø© [`HfEngine`] Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… `huggingface_hub.InferenceClient` Ø¨Ø´ÙƒÙ„ Ù…Ø®ÙÙ‰.\n-\n-```python\n-from transformers import CodeAgent, HfEngine\n-\n-llm_engine = HfEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-agent = CodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n-\n-agent.run(\n-    \"Could you translate this sentence from French, say it out loud and return the audio.\",\n-    sentence=\"OÃ¹ est la boulangerie la plus proche?\",\n-)\n-```\n-\n-Ù‡Ø°Ù‡ Ø§Ù„Ù…ÙŠØ²Ø© Ø³ØªÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ø¬Ø© Ø§Ù„Ù…Ù„Ø­Ø©! ÙŠÙ…ÙƒÙ†Ùƒ Ø­ØªÙ‰ ØªØ±Ùƒ Ù…Ø¹Ø§Ù…Ù„ `llm_engine` ØºÙŠØ± Ù…Ø­Ø¯Ø¯ØŒ ÙˆØ³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ [`HfEngine`] Ø¨Ø´ÙƒÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠ.\n-\n-```python\n-from transformers import CodeAgent\n-\n-agent = CodeAgent(tools=[], add_base_tools=True)\n-\n-agent.run(\n-    \"Could you translate this sentence from French, say it out loud and give me the audio.\",\n-    sentence=\"OÃ¹ est la boulangerie la plus proche?\",\n-)\n-```\n-\n-Ù„Ø§Ø­Ø¸ Ø£Ù†Ù†Ø§ Ø§Ø³ØªØ®Ø¯Ù…Ù†Ø§ Ù…Ø¹Ø§Ù…Ù„ \"sentence\" Ø¥Ø¶Ø§ÙÙŠ: ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù†Øµ ÙƒÙ…Ø¹Ø§Ù…Ù„ Ø¥Ø¶Ø§ÙÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ø£Ùˆ Ø§Ù„Ø¨Ø¹ÙŠØ¯Ø© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§:\n-\n-```py\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n-\n-agent.run(\"Why does Mike not know many people in New York?\", audio=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3\")\n-```\n-\n-\n-ØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆÙ…Ø­Ù„Ù„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ØŒ ÙˆÙ„ÙƒÙ† ÙŠÙ…ÙƒÙ†Ùƒ ÙØ­ØµÙ‡Ù…Ø§ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ `system_prompt_template` Ø¹Ù„Ù‰ ÙˆÙƒÙŠÙ„Ùƒ.\n-\n-```python\n-print(agent.system_prompt_template)\n-```\n-\n-Ù…Ù† Ø§Ù„Ù…Ù‡Ù… Ø£Ù† ØªØ´Ø±Ø­ Ø¨Ø£ÙƒØ¨Ø± Ù‚Ø¯Ø± Ù…Ù…ÙƒÙ† Ù…Ù† Ø§Ù„ÙˆØ¶ÙˆØ­ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ ØªÙ†ÙÙŠØ°Ù‡Ø§.\n-ÙƒÙ„ Ø¹Ù…Ù„ÙŠØ© [`~Agent.run`] Ù…Ø³ØªÙ‚Ù„Ø©ØŒ ÙˆØ¨Ù…Ø§ Ø£Ù† Ø§Ù„ÙˆÙƒÙŠÙ„ Ù…Ø¯Ø¹ÙˆÙ… Ù…Ù† LLMØŒ ÙÙ‚Ø¯ ØªØ¤Ø¯ÙŠ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª Ø§Ù„Ø·ÙÙŠÙØ© ÙÙŠ Ù…ÙˆØ¬Ù‡Ùƒ Ø¥Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø®ØªÙ„ÙØ© ØªÙ…Ø§Ù…Ù‹Ø§.\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ´ØºÙŠÙ„ ÙˆÙƒÙŠÙ„ Ø¨Ø´ÙƒÙ„ Ù…ØªØªØ§Ù„ÙŠ Ù„Ù…Ù‡Ø§Ù… Ù…Ø®ØªÙ„ÙØ©: ÙÙŠ ÙƒÙ„ Ù…Ø±Ø© ÙŠØªÙ… ÙÙŠÙ‡Ø§ Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦Ø© Ø³Ù…ØªÙŠ `agent.task` Ùˆ`agent.logs`.\n-\n-\n-#### ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©\n-\n-ÙŠÙ‚ÙˆÙ… Ù…ÙØ³Ø± Python Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªÙ…Ø±ÙŠØ±Ù‡Ø§ Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø£Ø¯ÙˆØ§ØªÙƒ.\n-ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ø£Ù…Ø± Ø¢Ù…Ù†Ù‹Ø§ Ù„Ø£Ù† Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ù‡ÙŠ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ Ù‚Ø¯Ù…ØªÙ‡Ø§ (Ø®Ø§ØµØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø£Ø¯ÙˆØ§Øª Ù…Ù† Hugging Face ÙÙ‚Ø·) ÙˆÙˆØ¸ÙŠÙØ© Ø§Ù„Ø·Ø¨Ø§Ø¹Ø©ØŒ Ù„Ø°Ø§ ÙØ£Ù†Øª Ù…Ù‚ÙŠØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¨Ù…Ø§ ÙŠÙ…ÙƒÙ† ØªÙ†ÙÙŠØ°Ù‡.\n-\n-Ù…ÙØ³Ø± Python Ù„Ø§ ÙŠØ³Ù…Ø­ Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯ÙˆØ§Ù„ Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø®Ø§Ø±Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ø¢Ù…Ù†Ø©ØŒ Ù„Ø°Ø§ ÙØ¥Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù‡Ø¬Ù…Ø§Øª Ø§Ù„Ø£ÙƒØ«Ø± ÙˆØ¶ÙˆØ­Ù‹Ø§ Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø´ÙƒÙ„Ø©.\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ù„Ø¥Ø°Ù† Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ø§Ù„ÙˆØ­Ø¯Ø§Øª Ø§Ù„Ù†Ù…Ø·ÙŠØ© Ø§Ù„Ù…ØµØ±Ø­ Ø¨Ù‡Ø§ ÙƒÙ‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø³Ù„Ø§Ø³Ù„ ÙÙŠ Ù…Ø¹Ø§Ù…Ù„  `additional_authorized_imports` Ø¹Ù†Ø¯ ØªÙ‡ÙŠØ¦Ø© [`ReactCodeAgent`] Ø£Ùˆ [`CodeAgent`]:\n-\n-```py\n->>> from transformers import ReactCodeAgent\n-\n->>> agent = ReactCodeAgent(tools=[], additional_authorized_imports=['requests', 'bs4'])\n->>> agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n-\n-(...)\n-'Hugging Face â€“ Blog'\n-```\n-\n-Ø³ÙŠØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªÙ†ÙÙŠØ° Ø¹Ù†Ø¯ Ø£ÙŠ Ø±Ù…Ø² ÙŠØ­Ø§ÙˆÙ„ ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ© ØºÙŠØ± Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø£Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ø®Ø·Ø£ Python Ø¹Ø§Ø¯ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙˆÙƒÙŠÙ„.\n-\n-> [!WARNING]\n-> ÙŠÙ…ÙƒÙ† Ù„Ù€ LLM ØªÙˆÙ„ÙŠØ¯ Ø´ÙØ±Ø© Ø¨Ø±Ù…Ø¬ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ: Ù„Ø§ ØªÙ‚Ù…Ø¨ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø£Ù‰ Ø¯ÙˆØ§Ù„ ØºÙŠØ± Ø¢Ù…Ù†Ø©!\n-\n-### Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù…\n-\n-ÙŠÙ†Ø´Ø¦ Ø§Ù„ÙˆÙƒÙŠÙ„ØŒ Ø£Ùˆ Ø¨Ø§Ù„Ø£Ø­Ø±Ù‰ LLM Ø§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆØ¯ Ø§Ù„ÙˆÙƒÙŠÙ„ØŒ ÙŠÙˆÙ„Ø¯ Ù…Ø®Ø±Ø¬Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù…. ÙŠÙ…ÙƒÙ† ØªØ®ØµÙŠØµ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØªØµÙ…ÙŠÙ…Ù‡ Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ù‚ØµÙˆØ¯Ø©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ù€ [`ReactCodeAgent`] (Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø£Ø¯Ù†Ø§Ù‡ Ù…Ø¨Ø³Ø· Ù‚Ù„ÙŠÙ„Ø§Ù‹).\n-\n-```text\n-You will be given a task to solve as best you can.\n-You have access to the following tools:\n-<<tool_descriptions>>\n-\n-To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n-\n-At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task, then the tools that you want to use.\n-Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '/End code' sequence.\n-During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n-These print outputs will then be available in the 'Observation:' field, for using this information as input for the next step.\n-\n-In the end you have to return a final answer using the `final_answer` tool.\n-\n-Here are a few examples using notional tools:\n----\n-{examples}\n-\n-Above example were using notional tools that might not exist for you. You only have access to those tools:\n-<<tool_names>>\n-You also can perform computations in the python code you generate.\n-\n-Always provide a 'Thought:' and a 'Code:\\n```py' sequence ending with '```<end_code>' sequence. You MUST provide at least the 'Code:' sequence to move forward.\n-\n-Remember to not perform too many operations in a single code block! You should split the task into intermediate code blocks.\n-Print results at the end of each step to save the intermediate results. Then use final_answer() to return the final result.\n-\n-Remember to make sure that variables you use are all defined.\n-\n-Now Begin!\n-```\n-\n-ÙŠØªØ¶Ù…Ù† Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù…:\n-- *Ù…Ù‚Ø¯Ù…Ø©* ØªØ´Ø±Ø­ ÙƒÙŠÙ ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªØµØ±Ù Ø§Ù„ÙˆÙƒÙŠÙ„ ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø¹Ù„ÙŠÙ‡ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§.\n-- ÙˆØµÙ Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯Ù‡Ø§ Ø¨ÙˆØ§Ø³Ø·Ø© Ø±Ù…Ø² `<<tool_descriptions>>` Ø§Ù„Ø°ÙŠ ÙŠØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ù‡ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ ÙÙŠ ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø¨Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠØ­Ø¯Ø¯Ù‡Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ ÙŠØ®ØªØ§Ø±Ù‡Ø§.\n-    - ÙŠØ£ØªÙŠ ÙˆØµÙ Ø§Ù„Ø£Ø¯Ø§Ø© Ù…Ù† Ø³Ù…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø©ØŒ `name`ØŒ Ùˆ`description`ØŒ Ùˆ`inputs` Ùˆ`output_type`ØŒ ÙˆÙ‚Ø§Ù„Ø¨ `jinja2` Ø¨Ø³ÙŠØ· ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø³ÙŠÙ†Ù‡.\n-- Ø´ÙƒÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹.\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ø³ÙŠÙ† Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù…ØŒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø¥Ø¶Ø§ÙØ© Ø´Ø±Ø­ Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª.\n-\n-Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£Ù‚ØµÙ‰ Ù‚Ø¯Ø± Ù…Ù† Ø§Ù„Ù…Ø±ÙˆÙ†Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙˆÙ‚ Ù‚Ø§Ù„Ø¨ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªÙ…Ø±ÙŠØ± Ù…ÙˆØ¬Ù‡ Ù…Ø®ØµØµ ÙƒÙ…Ø¹Ø§Ù…Ù„ Ø¥Ù„Ù‰ Ù…Ø¹Ù„Ù…Ø© `system_prompt`.\n-\n-```python\n-from transformers import ReactJsonAgent\n-from transformers.agents import PythonInterpreterTool\n-\n-agent = ReactJsonAgent(tools=[PythonInterpreterTool()], system_prompt=\"{your_custom_prompt}\")\n-```\n-\n-> [!WARNING]\n-> ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ØªØ­Ø¯ÙŠØ¯ Ø³Ù„Ø³Ù„Ø© `<<tool_descriptions>>` ÙÙŠ Ù…ÙƒØ§Ù† Ù…Ø§ ÙÙŠ `template` Ø­ØªÙ‰ ÙŠÙƒÙˆÙ† Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¹Ù„Ù‰ Ø¹Ù„Ù… \n-Ø¨Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©.\n-\n-\n-### ÙØ­Øµ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„\n-\n-ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ Ø¨Ø¹Ø¶ Ø§Ù„Ø³Ù…Ø§Øª Ø§Ù„Ù…ÙÙŠØ¯Ø© Ù„ÙØ­Øµ Ù…Ø§ Ø­Ø¯Ø« Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„:\n-- ØªØ®Ø²Ù†  `agent.logs` Ø³Ø¬Ù„Ø§Øª Ù…ÙØµÙ„Ø© Ù„Ù„ÙˆÙƒÙŠÙ„. ÙÙŠ ÙƒÙ„ Ø®Ø·ÙˆØ© Ù…Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆÙƒÙŠÙ„ØŒ ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† ÙƒÙ„ Ø´ÙŠØ¡ ÙÙŠ Ù‚Ø§Ù…ÙˆØ³ Ø¥Ù„Ø­Ø§Ù‚Ù‡ Ø¨Ù€ `agent.logs`.\n-- ØªØ´ØºÙŠÙ„ `agent.write_inner_memory_from_logs()` ÙŠØ®Ù„Ù‚ Ø°Ø§ÙƒØ±Ø© Ø¯Ø§Ø®Ù„ÙŠØ© Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ÙˆÙƒÙŠÙ„ Ù„Ù„Ù†Ø¸Ø§Ù… LLM Ù„Ø¹Ø±Ø¶Ù‡Ø§ØŒ ÙƒÙ‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¯Ø±Ø¯Ø´Ø©. ØªÙ†ØªÙ‚Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø¹Ø¨Ø± ÙƒÙ„ Ø®Ø·ÙˆØ© Ù…Ù† Ø³Ø¬Ù„ Ø§Ù„ÙˆÙƒÙŠÙ„ ÙˆÙ„Ø§ ØªØ®Ø²Ù† Ø³ÙˆÙ‰ Ù…Ø§ ÙŠÙ‡Ù…Ù‡Ø§ ÙƒØ±Ø³Ø§Ù„Ø©: Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø³ÙŠØ­ÙØ¸ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… ÙˆØ§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø±Ø³Ø§Ø¦Ù„ Ù…Ù†ÙØµÙ„Ø©ØŒ Ø«Ù… Ù„ÙƒÙ„ Ø®Ø·ÙˆØ© Ø³ÙŠØ®Ø²Ù† Ù…Ø®Ø±Ø¬ LLM ÙƒØ±Ø³Ø§Ù„Ø©ØŒ ÙˆÙ…Ø®Ø±Ø¬ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø£Ø¯Ø§Ø© ÙƒØ±Ø³Ø§Ù„Ø© Ø£Ø®Ø±Ù‰. Ø§Ø³ØªØ®Ø¯Ù… Ù‡Ø°Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø¹Ø±Ø¶Ù‹Ø§ Ø¹Ø§Ù…Ù‹Ø§ Ù„Ù…Ø§ Ø­Ø¯Ø« - ÙˆÙ„ÙƒÙ† Ù„Ù† ÙŠØªÙ… Ù†Ø³Ø® ÙƒÙ„ Ø³Ø¬Ù„ Ø¨ÙˆØ§Ø³Ø·Ø© Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø©.\n-\n-## Ø§Ù„Ø£Ø¯ÙˆØ§Øª\n-\n-Ø§Ù„Ø£Ø¯Ø§Ø© Ù‡ÙŠ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† ÙˆØ¸ÙŠÙØ© Ø£Ø³Ø§Ø³ÙŠØ© ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„ÙˆÙƒÙŠÙ„ Ù„ØªÙ†ÙÙŠØ° Ù…Ù‡Ù…Ø© Ù…Ø­Ø¯Ø¯Ø©.\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [`PythonInterpreterTool`]: Ù„Ø¯ÙŠÙ‡ Ø§Ø³Ù… ÙˆÙˆØµÙ ÙˆÙˆØµÙ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆÙ†ÙˆØ¹ Ù„Ù„Ù…Ø®Ø±Ø¬ØŒ ÙˆØ·Ø±ÙŠÙ‚Ø© `__call__` Ø§Ù„ØªÙŠ ØªÙ‚ÙˆÙ… Ø¨ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©.\n-\n-Ø¹Ù†Ø¯ ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆÙƒÙŠÙ„ØŒ ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø³Ù…Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø© Ù„ØªÙˆÙ„ÙŠØ¯ ÙˆØµÙ Ù„Ù„Ø£Ø¯Ø§Ø© ÙŠØªÙ… ØªØ¶Ù…ÙŠÙ†Ù‡ ÙÙŠ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø®Ø§Øµ Ø¨Ø§Ù„ÙˆÙƒÙŠÙ„. ÙŠØªÙŠØ­ Ù‡Ø°Ø§ Ù„Ù„ÙˆÙƒÙŠÙ„ Ù…Ø¹Ø±ÙØ© Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ù‡ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙˆÙ„Ù…Ø§Ø°Ø§.\n-\n-### ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ\n-\n-ÙŠØ£ØªÙŠ Transformers Ù…Ø¹ ØµÙ†Ø¯ÙˆÙ‚ Ø£Ø¯ÙˆØ§Øª Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„ØªÙ…ÙƒÙŠÙ† Ø§Ù„ÙˆÙƒÙ„Ø§Ø¡ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØªÙ‡ Ø¥Ù„Ù‰ ÙˆÙƒÙŠÙ„Ùƒ Ø¹Ù†Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„ `add_base_tools = True`:\n-\n-- **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯**: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ Ø§Ù„Ù…Ø³ØªÙ†Ø¯ (Ù…Ø«Ù„ Ù…Ù„Ù PDF) Ø¨ØªÙ†Ø³ÙŠÙ‚ ØµÙˆØ±Ø© ([Donut](./model_doc/donut))\n-- **Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØµÙˆØ±**: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø³Ø¤Ø§Ù„ Ø­ÙˆÙ„ ØµÙˆØ±Ø© ([VILT](./model_doc/vilt))\n-- **Ø§Ù„ØªØ­Ø¯Ø« Ø¥Ù„Ù‰ Ø§Ù„Ù†Øµ**: Ù‚Ù… Ø¨ØªÙØ±ÙŠØº Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ ([Whisper](./model_doc/whisper))\n-- **Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù…**: ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… ([SpeechT5](./model_doc/speecht5))\n-- **Ø§Ù„ØªØ±Ø¬Ù…Ø©**: ØªØ±Ø¬Ù…Ø© Ø¬Ù…Ù„Ø© Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Ù„ØºØ© Ø§Ù„Ù…ØµØ¯Ø± Ø¥Ù„Ù‰ Ù„ØºØ© Ø§Ù„Ù‡Ø¯Ù.\n-- **Ù…ÙØ³Ø± ÙƒÙˆØ¯ Python**: ØªØ´ØºÙŠÙ„ ÙƒÙˆØ¯ Python Ø§Ù„Ø°ÙŠ ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© LLM ÙÙŠ Ø¨ÙŠØ¦Ø© Ø¢Ù…Ù†Ø©. Ù„Ù† ÙŠØªÙ… Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø© Ø¥Ù„Ù‰ [`ReactJsonAgent`] Ø¥Ù„Ø§ Ø¥Ø°Ø§ Ø§Ø³ØªØ®Ø¯Ù…Øª `add_base_tools=True`ØŒ Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙŠÙ…ÙƒÙ†Ù‡Ø§ Ø¨Ø§Ù„ÙØ¹Ù„ ØªÙ†ÙÙŠØ° ÙƒÙˆØ¯ Python\n-Ù„Ø§ ØªØªØ±Ø¬Ù… Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø®Ø§ØµØ© ÙˆÙ„Ø§ Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙˆÙ„Ø§ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆÙ„Ø§ Ø±Ù…ÙˆØ² HTML ÙˆCSS:\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¯Ø§Ù„Ø© [`load_tool`] ÙˆØªØ­Ø¯ÙŠØ¯ Ù…Ù‡Ù…Ø© Ù„ØªÙ†ÙÙŠØ°Ù‡Ø§.\n-\n-```python\n-from transformers import load_tool\n-\n-tool = load_tool(\"text-to-speech\")\n-audio = tool(\"This is a text to speech tool\")\n-```\n-\n-### Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¯Ø§Ø© Ø¬Ø¯ÙŠØ¯Ø©\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¯Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ© Ù„ØªØºØ·ÙŠØ© Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªÙŠ Ù„Ø§ ØªØºØ·ÙŠÙ‡Ø§ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ù† Hugging Face.\n-Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ù‚ÙˆÙ… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¯Ø§Ø© ØªØ¹Ø±Ø¶ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ†Ø²ÙŠÙ„Ù‹Ø§ Ù„Ù…Ù‡Ù…Ø© Ù…Ø¹ÙŠÙ†Ø© Ù…Ù† Hub.\n-\n-Ø³ÙˆÙ Ù†Ø¨Ø¯Ø£ Ø¨Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ØªØ§Ù„ÙŠ.\n-\n-```python\n-from huggingface_hub import list_models\n-\n-task = \"text-classification\"\n-\n-model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-print(model.id)\n-```\n-\n-ÙŠÙ…ÙƒÙ† ØªØ­ÙˆÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„Ø´ÙŠÙØ±Ø© Ø¥Ù„Ù‰ ÙØ¦Ø© ØªØ±Ø« Ù…Ù† Ø§Ù„ÙØ¦Ø© Ø§Ù„Ø¹Ù„ÙŠØ§ [`Tool`].\n-\n-ØªØ­ØªØ§Ø¬ Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ø®ØµØµØ© Ø¥Ù„Ù‰:\n-\n-- Ø§Ø³Ù… `name`ØŒ ÙˆØ§Ù„ØªÙŠ ØªÙ…Ø«Ù„ Ø§Ø³Ù… Ø§Ù„Ø£Ø¯Ø§Ø© Ù†ÙØ³Ù‡Ø§. Ø¹Ø§Ø¯Ø©Ù‹ Ù…Ø§ ÙŠØµÙ Ø§Ù„Ø§Ø³Ù… ÙˆØ¸ÙŠÙØªÙ‡Ø§. Ø¨Ù…Ø§ Ø£Ù† Ø§Ù„ÙƒÙˆØ¯ ÙŠØ¹ÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ†Ø²ÙŠÙ„Ù‹Ø§ Ù„Ù…Ù‡Ù…Ø© Ù…Ø§ØŒ ÙÙ„Ù†Ø³Ù…Ù‡Ø§ `model_download_counter`.\n-- ØªØ³ØªØ®Ø¯Ù… Ø®Ø§ØµÙŠØ© `description` Ù„Ù…Ù„Ø¡ Ù…ÙˆØ¬Ù‡ Ù†Ø¸Ø§Ù… Ø§Ù„ÙˆÙƒÙŠÙ„.\n-- Ø®Ø§ØµÙŠØ© `inputs`ØŒ ÙˆØ§Ù„ØªÙŠ Ù‡ÙŠ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù‚Ø§Ù…ÙˆØ³ Ø¨Ù…ÙØ§ØªÙŠØ­ \"type\" Ùˆ\"description\". ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ù…ÙØ³Ø± Python Ø¹Ù„Ù‰ Ø§ØªØ®Ø§Ø° Ø®ÙŠØ§Ø±Ø§Øª Ù…Ø³ØªÙ†ÙŠØ±Ø© Ø¨Ø´Ø£Ù† Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª.\n-- Ø®Ø§ØµÙŠØ© `output_type`ØŒ ÙˆØ§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø®Ø±Ø¬.\n-- Ø·Ø±ÙŠÙ‚Ø© `forward` ÙˆØ§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©.\n-\n-```python\n-from transformers import Tool\n-from huggingface_hub import list_models\n-\n-class HFModelDownloadsTool(Tool):\n-    name = \"model_download_counter\"\n-    description = (\n-        \"This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub. \"\n-        \"It returns the name of the checkpoint.\"\n-    )\n-\n-    inputs = {\n-        \"task\": {\n-            \"type\": \"text\",\n-            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n-        }\n-    }\n-    output_type = \"text\"\n-\n-    def forward(self, task: str):\n-        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-        return model.id\n-```\n-\n-Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ø£ØµØ¨Ø­Øª ÙØ¦Ø© `HfModelDownloadsTool` Ø§Ù„Ù…Ø®ØµØµØ© Ø¬Ø§Ù‡Ø²Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸Ù‡Ø§ ÙÙŠ Ù…Ù„Ù Ø¨Ø§Ø³Ù… `model_downloads.py` ÙˆØ§Ø³ØªÙŠØ±Ø§Ø¯Ù‡Ø§ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù….\n-\n-```python\n-from model_downloads import HFModelDownloadsTool\n-\n-tool = HFModelDownloadsTool()\n-```\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ù…Ø´Ø§Ø±ÙƒØ© Ø£Ø¯Ø§ØªÙƒ Ø§Ù„Ù…Ø®ØµØµØ© ÙÙŠ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Tool.push_to_hub`] Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ø§Ø©. ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù†Ùƒ Ù‚Ù…Øª Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³ØªÙˆØ¯Ø¹ Ù„Ù‡Ø§ Ø¹Ù„Ù‰ Hub ÙˆØ£Ù†Ùƒ ØªØ³ØªØ®Ø¯Ù… Ø±Ù…Ø² ÙˆØµÙˆÙ„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©.\n-\n-```python\n-tool.push_to_hub(\"{your_username}/hf-model-downloads\")\n-```\n-\n-Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© [`~Tool.load_tool`] ÙˆÙ…Ø±Ø±Ù‡Ø§ Ø¥Ù„Ù‰ Ù…Ø¹Ù„Ù…Ø© `tools` ÙÙŠ Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.\n-\n-```python\n-from transformers import load_tool, CodeAgent\n-\n-model_download_tool = load_tool(\"m-ric/hf-model-downloads\")\n-agent = CodeAgent(tools=[model_download_tool], llm_engine=llm_engine)\n-agent.run(\n-    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\"\n-)\n-```\n-\n-Ø³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø§ ÙŠÙ„ÙŠ:\n-\n-```text\n-======== New task ========\n-Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\n-==== Agent is executing the code below:\n-most_downloaded_model = model_download_counter(task=\"text-to-video\")\n-print(f\"The most downloaded model for the 'text-to-video' task is {most_downloaded_model}.\")\n-====\n-```\n-\n-ÙˆØ§Ù„Ù†Ø§ØªØ¬:\n-\n-`\"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø£ÙƒØ«Ø± ØªÙ†Ø²ÙŠÙ„Ù‹Ø§ Ù„Ù…Ù‡Ù…Ø© `text-to-video` Ù‡Ùˆ ByteDance/AnimateDiff-Lightning.\"`\n-\n-### Ø¥Ø¯Ø§Ø±Ø© ØµÙ†Ø¯ÙˆÙ‚ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ\n-\n-Ø¥Ø°Ø§ ÙƒÙ†Øª Ù‚Ø¯ Ù‚Ù…Øª Ø¨ØªÙ‡ÙŠØ¦Ø© ÙˆÙƒÙŠÙ„ØŒ ÙÙ…Ù† ØºÙŠØ± Ø§Ù„Ù…Ù„Ø§Ø¦Ù… Ø¥Ø¹Ø§Ø¯Ø© ØªÙ‡ÙŠØ¦ØªÙ‡ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ù„Ø¥Ø¶Ø§ÙØ© Ø£Ø¯Ø§Ø© Ø¬Ø¯ÙŠØ¯Ø© ØªØ±ØºØ¨ ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§. Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© TransformersØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¯Ø§Ø±Ø© ØµÙ†Ø¯ÙˆÙ‚ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ÙˆÙƒÙŠÙ„ Ø¨Ø¥Ø¶Ø§ÙØ© Ø£Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ø¯Ø§Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©.\n-\n-Ø¯Ø¹Ù†Ø§ Ù†Ø¶ÙŠÙ Ø§Ù„Ø£Ø¯Ø§Ø© `model_download_tool` Ø¥Ù„Ù‰ ÙˆÙƒÙŠÙ„ ØªÙ… ØªÙ‡ÙŠØ¦ØªÙ‡ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ.\n-\n-```python\n-from transformers import CodeAgent\n-\n-agent = CodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n-agent.toolbox.add_tool(model_download_tool)\n-```\n-\n-Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ£Ø¯Ø§Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ø§Ù… Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©:\n-\n-```python\n-    agent.run(\n-        \"Can you read out loud the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub and return the audio?\"\n-    )\n-```\n-\n-| **Audio**                                                                                                                                            |\n-|------------------------------------------------------------------------------------------------------------------------------------------------------|\n-| <audio controls><source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/damo.wav\" type=\"audio/wav\"/> |\n-\n-> [!WARNING]\n-> Ø§Ø­ØªØ±Ø³ Ø¹Ù†Ø¯ Ø¥Ø¶Ø§ÙØ© Ø£Ø¯ÙˆØ§Øª Ø¥Ù„Ù‰ ÙˆÙƒÙŠÙ„ ÙŠØ¹Ù…Ù„ Ø¨Ø§Ù„ÙØ¹Ù„ Ù„Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ¤Ø«Ø± Ø¹Ù„Ù‰ Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£Ø¯Ø§Ø© Ù„ØµØ§Ù„Ø­ Ø£Ø¯Ø§ØªÙƒ Ø£Ùˆ Ø§Ø®ØªÙŠØ§Ø± Ø£Ø¯Ø§Ø© Ø£Ø®Ø±Ù‰ ØºÙŠØ± Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ù„ÙØ¹Ù„.\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© `agent.toolbox.update_tool()` Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø£Ø¯Ø§Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ ØµÙ†Ø¯ÙˆÙ‚ Ø£Ø¯ÙˆØ§Øª Ø§Ù„ÙˆÙƒÙŠÙ„.\n-Ù‡Ø°Ø§ Ù…ÙÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø£Ø¯Ø§ØªÙƒ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¨Ø¯ÙŠÙ„Ø§Ù‹ Ù…Ø¨Ø§Ø´Ø±Ù‹Ø§ Ù„Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù„Ø£Ù† Ø§Ù„ÙˆÙƒÙŠÙ„ ÙŠØ¹Ø±Ù Ø¨Ø§Ù„ÙØ¹Ù„ ÙƒÙŠÙÙŠØ© ØªÙ†ÙÙŠØ° ØªÙ„Ùƒ Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©.\n-ØªØ£ÙƒØ¯ ÙÙ‚Ø· Ù…Ù† Ø§ØªØ¨Ø§Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù†ÙØ³ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª (API) Ù„Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ø³ØªØ¨Ø¯Ù„Ø© Ø£Ùˆ Ù‚Ù… Ø¨ØªÙƒÙŠÙŠÙ Ù‚Ø§Ù„Ø¨ Ù…ÙˆØ¬Ù‡ Ø§Ù„Ù†Ø¸Ø§Ù… Ù„Ø¶Ù…Ø§Ù† ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ø³ØªØ¨Ø¯Ù„Ø©.\n-\n-### Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ø¯ÙˆØ§Øª\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒØ§Ø¦Ù† ToolCollectionØŒ Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§.\n-Ø«Ù… Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ±Ù‡Ø§ ÙƒÙ‚Ø§Ø¦Ù…Ø© Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨ÙƒØŒ ÙˆØ¨Ø¯Ø¡ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§!\n-\n-```py\n-from transformers import ToolCollection, ReactCodeAgent\n-\n-image_tool_collection = ToolCollection(collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\")\n-agent = ReactCodeAgent(tools=[*image_tool_collection.tools], add_base_tools=True)\n-\n-agent.run(\"Please draw me a picture of rivers and lakes.\")\n-```\n-\n-Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£Ø¯ÙˆØ§Øª ÙÙ‚Ø· Ø¥Ø°Ø§ Ø§Ø³ØªØ¯Ø¹Ø§Ù‡Ø§ Ø§Ù„ÙˆÙƒÙŠÙ„.\n-\n-Ø³ØªØ­ØµÙ„ Ø¹Ù„Ù‰ Ù‡Ø°Ù‡ Ø§Ù„ØµÙˆØ±Ø©:\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" />\n-\n-### Ø§Ø³ØªØ®Ø¯Ø§Ù… gradio-tools\n-\n-[gradio-tools](https://github.com/freddyaboulton/gradio-tools) Ù‡ÙŠ Ù…ÙƒØªØ¨Ø© Ù‚ÙˆÙŠØ© ØªØªÙŠØ­ Ø§Ø³ØªØ®Ø¯Ø§Ù… Hugging\n-Face Spaces ÙƒØ£Ø¯ÙˆØ§Øª. ØªØ¯Ø¹Ù… Ø§Ù„Ø¹Ø¯ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø­Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø­Ø§Øª Ù…Ø®ØµØµØ©.\n-\n-ØªØ¯Ø¹Ù… Ù…ÙƒØªØ¨Ø© Transformers `gradio_tools` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`Tool.from_gradio`] ÙÙŠ Ø§Ù„ÙØ¦Ø©. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ø³ØªØ®Ø¯Ù… [`StableDiffusionPromptGeneratorTool`](https://github.com/freddyaboulton/gradio-tools/blob/main/gradio_tools/tools/prompt_generator.py) Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø¯ÙˆØ§Øª `gradio-tools` Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø§Øª Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø£ÙØ¶Ù„.\n-\n-Ø§Ø³ØªÙˆØ±Ø¯ ÙˆÙ‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø¯Ø§Ø©ØŒ Ø«Ù… Ù…Ø±Ø±Ù‡Ø§ Ø¥Ù„Ù‰ Ø·Ø±ÙŠÙ‚Ø© `Tool.from_gradio`:\n-\n-```python\n-from gradio_tools import StableDiffusionPromptGeneratorTool\n-from transformers import Tool, load_tool, CodeAgent\n-\n-gradio_prompt_generator_tool = StableDiffusionPromptGeneratorTool()\n-prompt_generator_tool = Tool.from_gradio(gradio_prompt_generator_tool)\n-```\n-\n-Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù…Ø«Ù„ Ø£ÙŠ Ø£Ø¯Ø§Ø© Ø£Ø®Ø±Ù‰. Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¯Ø¹Ù†Ø§ Ù†Ø­Ø³Ù† Ø§Ù„Ù…ÙˆØ¬Ù‡ `a rabbit wearing a space suit`.\n-\n-```python\n-image_generation_tool = load_tool('huggingface-tools/text-to-image')\n-agent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)\n-\n-agent.run(\n-    \"Improve this prompt, then generate an image of it.\", prompt='A rabbit wearing a space suit'\n-)\n-```\n-\n-ÙŠØ³ØªÙÙŠØ¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ ÙƒØ§ÙÙ Ù…Ù† Ø§Ù„Ø£Ø¯Ø§Ø©:\n-\n-```text\n-======== New task ========\n-Improve this prompt, then generate an image of it.\n-You have been provided with these initial arguments: {'prompt': 'A rabbit wearing a space suit'}.\n-==== Agent is executing the code below:\n-improved_prompt = StableDiffusionPromptGenerator(query=prompt)\n-while improved_prompt == \"QUEUE_FULL\":\n-    improved_prompt = StableDiffusionPromptGenerator(query=prompt)\n-print(f\"The improved prompt is {improved_prompt}.\")\n-image = image_generator(prompt=improved_prompt)\n-====\n-```\n-\n-Ù‚Ø¨Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø© Ø£Ø®ÙŠØ±Ù‹Ø§:\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit_spacesuit_flux.webp\" />\n-\n-> [!WARNING]\n-> ØªØªØ·Ù„Ø¨ gradio-tools Ø¥Ø¯Ø®Ø§Ù„Ø§Øª ÙˆØ¥Ø®Ø±Ø§Ø¬Ø§Øª *Ù†ØµÙŠØ©* Ø­ØªÙ‰ Ø¹Ù†Ø¯ Ø§Ù„Ø¹Ù…Ù„ Ù…Ø¹ Ø·Ø±Ø§Ø¦Ù‚ Ù…Ø®ØªÙ„ÙØ© Ù…Ø«Ù„ ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ØµÙˆØª. Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„Ø§Øª ÙˆØ§Ù„Ø¥Ø®Ø±Ø§Ø¬Ø§Øª Ø§Ù„ØµÙˆØ±ÙŠØ© ÙˆØ§Ù„ØµÙˆØªÙŠØ© ØºÙŠØ± Ù…ØªÙˆØ§ÙÙ‚Ø© Ø­Ø§Ù„ÙŠÙ‹Ø§.\n-\n-### Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª LangChain\n-\n-Ù†Ø­Ù† Ù†Ø­Ø¨ Langchain ÙˆÙ†Ø¹ØªÙ‚Ø¯ Ø£Ù†Ù‡Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø¯ÙˆØ§Øª Ù‚ÙˆÙŠØ© Ù„Ù„ØºØ§ÙŠØ©.\n-Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ø¯Ø§Ø© Ù…Ù† LangChainØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© `from_langchain()`.\n-\n-ÙÙŠÙ…Ø§ ÙŠÙ„ÙŠ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© Ø¨Ø­Ø« Ø§Ù„ÙˆÙŠØ¨ LangChain.\n-\n-```python\n-from langchain.agents import load_tools\n-from transformers import Tool, ReactCodeAgent\n-\n-search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n-\n-agent = ReactCodeAgent(tools=[search_tool])\n-\n-agent.run(\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\")\n-```\n-\n-## ÙˆØ§Ø¬Ù‡Ø© Gradio\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† `gradio.Chatbot` Ù„Ø¹Ø±Ø¶ Ø£ÙÙƒØ§Ø± Ø§Ù„ÙˆÙƒÙŠÙ„ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `stream_to_gradio`ØŒ Ø¥Ù„ÙŠÙƒ Ù…Ø«Ø§Ù„:\n-\n-```py\n-import gradio as gr\n-from transformers import (\n-    load_tool,\n-    ReactCodeAgent,\n-    HfEngine,\n-    stream_to_gradio,\n-)\n-\n-# Import tool from Hub\n-image_generation_tool = load_tool(\"m-ric/text-to-image\")\n-\n-llm_engine = HfEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-\n-# Initialize the agent with the image generation tool\n-agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n-\n-\n-def interact_with_agent(task):\n-    messages = []\n-    messages.append(gr.ChatMessage(role=\"user\", content=task))\n-    yield messages\n-    for msg in stream_to_gradio(agent, task):\n-        messages.append(msg)\n-        yield messages + [\n-            gr.ChatMessage(role=\"assistant\", content=\"â³ Task not finished yet!\")\n-        ]\n-    yield messages\n-\n-\n-with gr.Blocks() as demo:\n-    text_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\n-    submit = gr.Button(\"Run illustrator agent!\")\n-    chatbot = gr.Chatbot(\n-        label=\"Agent\",\n-        type=\"messages\",\n-        avatar_images=(\n-            None,\n-            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n-        ),\n-    )\n-    submit.click(interact_with_agent, [text_input], [chatbot])\n-\n-if __name__ == \"__main__\":\n-    demo.launch()\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "df772c6c788f5ffeb9c6bd9f0a81ce3bc8ad82e4",
            "filename": "docs/source/de/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fde%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fde%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -23,8 +23,6 @@\n     title: Laden und Trainieren von Adaptern mit ğŸ¤— PEFT\n   - local: model_sharing\n     title: Ein Modell teilen\n-  - local: transformers_agents\n-    title: Agents\n   - local: llm_tutorial\n     title: Generation with LLMs\n   title: Tutorials\n@@ -39,4 +37,4 @@\n     title: Testen\n   - local: pr_checks\n     title: ÃœberprÃ¼fung einer Pull Request\n-  title: Contribute\n\\ No newline at end of file\n+  title: Contribute"
        },
        {
            "sha": "1d676c395e176964df1ea77a889749c7c96e7017",
            "filename": "docs/source/de/transformers_agents.md",
            "status": "removed",
            "additions": 0,
            "deletions": 323,
            "changes": 323,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fde%2Ftransformers_agents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fde%2Ftransformers_agents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Ftransformers_agents.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,323 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Transformers Agents\n-\n-<Tip warning={true}>\n-\n-Transformers Agents ist eine experimentelle API, die jederzeit geÃ¤ndert werden kann. Die von den Agenten zurÃ¼ckgegebenen Ergebnisse\n-zurÃ¼ckgegeben werden, kÃ¶nnen variieren, da sich die APIs oder die zugrunde liegenden Modelle Ã¤ndern kÃ¶nnen.\n-\n-</Tip>\n-\n-Transformers Version v4.29.0, die auf dem Konzept von *Tools* und *Agenten* aufbaut. Sie kÃ¶nnen damit spielen in\n-[dieses Colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj).\n-\n-Kurz gesagt, es bietet eine API fÃ¼r natÃ¼rliche Sprache auf der Grundlage von Transformers: Wir definieren eine Reihe von kuratierten Tools und entwerfen einen \n-Agenten, um natÃ¼rliche Sprache zu interpretieren und diese Werkzeuge zu verwenden. Es ist von vornherein erweiterbar; wir haben einige relevante Tools kuratiert, \n-aber wir werden Ihnen zeigen, wie das System einfach erweitert werden kann, um jedes von der Community entwickelte Tool zu verwenden.\n-\n-Beginnen wir mit einigen Beispielen dafÃ¼r, was mit dieser neuen API erreicht werden kann. Sie ist besonders leistungsfÃ¤hig, wenn es um \n-Sie ist besonders leistungsstark, wenn es um multimodale Aufgaben geht. Lassen Sie uns also eine Runde drehen, um Bilder zu erzeugen und Text vorzulesen.\n-\n-```py\n-agent.run(\"Caption the following image\", image=image)\n-```\n-\n-| **Input**                                                                                                                   | **Output**                        |\n-|-----------------------------------------------------------------------------------------------------------------------------|-----------------------------------|\n-| <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beaver.png\" width=200> | A beaver is swimming in the water |\n-\n----\n-\n-```py\n-agent.run(\"Read the following text out loud\", text=text)\n-```\n-| **Input**                                                                                                               | **Output**                                   |\n-|-------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|\n-| A beaver is swimming in the water | <audio controls><source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tts_example.wav\" type=\"audio/wav\"> your browser does not support the audio element. </audio>\n-\n----\n-\n-```py\n-agent.run(\n-    \"In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?\",\n-    document=document,\n-)\n-```\n-| **Input**                                                                                                                   | **Output**     |\n-|-----------------------------------------------------------------------------------------------------------------------------|----------------|\n-| <img src=\"https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/0/image/image.jpg\" width=200> | ballroom foyer |\n-\n-## Schnellstart\n-\n-Bevor Sie `agent.run` verwenden kÃ¶nnen, mÃ¼ssen Sie einen Agenten instanziieren, der ein groÃŸes Sprachmodell (LLM) ist. \n-Wir bieten UnterstÃ¼tzung fÃ¼r openAI-Modelle sowie fÃ¼r OpenSource-Alternativen von BigCode und OpenAssistant. Die openAI\n-Modelle sind leistungsfÃ¤higer (erfordern aber einen openAI-API-SchlÃ¼ssel, kÃ¶nnen also nicht kostenlos verwendet werden); Hugging Face\n-bietet kostenlosen Zugang zu Endpunkten fÃ¼r BigCode- und OpenAssistant-Modelle.\n-\n-To start with, please install the `agents` extras in order to install all default dependencies.\n-```bash\n-pip install transformers[agents]\n-```\n-\n-Um openAI-Modelle zu verwenden, instanziieren Sie einen [`OpenAiAgent`], nachdem Sie die `openai`-AbhÃ¤ngigkeit installiert haben:\n-\n-```bash\n-pip install openai\n-```\n-\n-\n-```py\n-from transformers import OpenAiAgent\n-\n-agent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"<your_api_key>\")\n-```\n-\n-Um BigCode oder OpenAssistant zu verwenden, melden Sie sich zunÃ¤chst an, um Zugriff auf die Inference API zu erhalten:\n-\n-```py\n-from huggingface_hub import login\n-\n-login(\"<YOUR_TOKEN>\")\n-```\n-\n-Dann instanziieren Sie den Agenten\n-\n-```py\n-from transformers import HfAgent\n-\n-# Starcoder\n-agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n-# StarcoderBase\n-# agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoderbase\")\n-# OpenAssistant\n-# agent = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n-```\n-\n-Dies geschieht mit der Inferenz-API, die Hugging Face derzeit kostenlos zur VerfÃ¼gung stellt. Wenn Sie Ihren eigenen Inferenz\n-Endpunkt fÃ¼r dieses Modell (oder einen anderen) haben, kÃ¶nnen Sie die obige URL durch Ihren URL-Endpunkt ersetzen.\n-\n-<Tip>\n-\n-StarCoder und OpenAssistant sind kostenlos und leisten bei einfachen Aufgaben bewundernswert gute Arbeit. Allerdings halten die Kontrollpunkte\n-nicht, wenn es um komplexere Aufforderungen geht. Wenn Sie mit einem solchen Problem konfrontiert sind, empfehlen wir Ihnen, das OpenAI\n-Modell auszuprobieren, das zwar leider nicht quelloffen ist, aber zur Zeit eine bessere Leistung erbringt.\n-\n-</Tip>\n-\n-Sie sind jetzt startklar! Lassen Sie uns in die beiden APIs eintauchen, die Ihnen jetzt zur VerfÃ¼gung stehen.\n-\n-### Einzelne AusfÃ¼hrung (run)\n-\n-Die Methode der einmaligen AusfÃ¼hrung ist die Verwendung der [`~Agent.run`] Methode des Agenten:\n-\n-```py\n-agent.run(\"Draw me a picture of rivers and lakes.\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" width=200>\n-\n-Es wÃ¤hlt automatisch das (oder die) Werkzeug(e) aus, das (die) fÃ¼r die von Ihnen gewÃ¼nschte Aufgabe geeignet ist (sind) und fÃ¼hrt es (sie) entsprechend aus. Es\n-kann eine oder mehrere Aufgaben in der gleichen Anweisung ausfÃ¼hren (je komplexer Ihre Anweisung ist, desto wahrscheinlicher ist ein\n-der Agent scheitern).\n-\n-```py\n-agent.run(\"Draw me a picture of the sea then transform the picture to add an island\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sea_and_island.png\" width=200>\n-\n-<br/>\n-\n-\n-Jede [`~Agent.run`] Operation ist unabhÃ¤ngig, so dass Sie sie mehrmals hintereinander mit unterschiedlichen Aufgaben ausfÃ¼hren kÃ¶nnen.\n-\n-Beachten Sie, dass Ihr `Agent` nur ein groÃŸsprachiges Modell ist, so dass kleine Variationen in Ihrer Eingabeaufforderung vÃ¶llig unterschiedliche Ergebnisse liefern kÃ¶nnen.\n-unterschiedliche Ergebnisse liefern. Es ist wichtig, dass Sie die Aufgabe, die Sie ausfÃ¼hren mÃ¶chten, so genau wie mÃ¶glich erklÃ¤ren. Wir gehen noch weiter ins Detail\n-wie man gute Prompts schreibt [hier](custom_tools#writing-good-user-inputs).\n-\n-Wenn Sie einen Status Ã¼ber AusfÃ¼hrungszeiten hinweg beibehalten oder dem Agenten Nicht-Text-Objekte Ã¼bergeben mÃ¶chten, kÃ¶nnen Sie dies tun, indem Sie\n-Variablen, die der Agent verwenden soll. Sie kÃ¶nnten zum Beispiel das erste Bild von FlÃ¼ssen und Seen erzeugen, \n-und das Modell bitten, dieses Bild zu aktualisieren und eine Insel hinzuzufÃ¼gen, indem Sie Folgendes tun:\n-\n-```python\n-picture = agent.run(\"Generate a picture of rivers and lakes.\")\n-updated_picture = agent.run(\"Transform the image in `picture` to add an island to it.\", picture=picture)\n-```\n-\n-<Tip>\n-\n-Dies kann hilfreich sein, wenn das Modell Ihre Anfrage nicht verstehen kann und die Werkzeuge verwechselt. Ein Beispiel wÃ¤re:\n-\n-```py\n-agent.run(\"Draw me the picture of a capybara swimming in the sea\")\n-```\n-\n-Hier kÃ¶nnte das Modell auf zwei Arten interpretieren:\n-- Die Funktion `Text-zu-Bild` erzeugt ein Wasserschwein, das im Meer schwimmt.\n-- Oder Sie lassen das `Text-zu-Bild` ein Wasserschwein erzeugen und verwenden dann das Werkzeug `Bildtransformation`, um es im Meer schwimmen zu lassen.\n-\n-Falls Sie das erste Szenario erzwingen mÃ¶chten, kÃ¶nnen Sie dies tun, indem Sie die Eingabeaufforderung als Argument Ã¼bergeben:\n-\n-```py\n-agent.run(\"Draw me a picture of the `prompt`\", prompt=\"a capybara swimming in the sea\")\n-```\n-\n-</Tip>\n-\n-\n-### Chat-basierte AusfÃ¼hrung (Chat)\n-\n-Der Agent verfÃ¼gt auch Ã¼ber einen Chat-basierten Ansatz, der die Methode [`~Agent.chat`] verwendet:\n-\n-```py\n-agent.chat(\"Generate a picture of rivers and lakes\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" width=200> \n-\n-```py\n-agent.chat(\"Transform the picture so that there is a rock in there\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes_and_beaver.png\" width=200>\n-\n-<br/>\n-\n-Dies ist ein interessanter Ansatz, wenn Sie den Zustand Ã¼ber Anweisungen hinweg beibehalten mÃ¶chten. Er ist besser fÃ¼r Experimente geeignet, \n-eignet sich aber eher fÃ¼r einzelne Anweisungen als fÃ¼r komplexe Anweisungen (die die [`~Agent.run`]\n-Methode besser verarbeiten kann).\n-\n-Diese Methode kann auch Argumente entgegennehmen, wenn Sie Nicht-Text-Typen oder bestimmte Aufforderungen Ã¼bergeben mÃ¶chten.\n-\n-### âš ï¸ FernausfÃ¼hrung\n-\n-Zu Demonstrationszwecken und damit es mit allen Setups verwendet werden kann, haben wir Remote-Executors fÃ¼r mehrere \n-der Standard-Tools erstellt, auf die der Agent in dieser Version Zugriff hat. Diese werden erstellt mit \n-[inference endpoints](https://huggingface.co/inference-endpoints).\n-\n-Wir haben diese vorerst deaktiviert, aber um zu sehen, wie Sie selbst Remote Executors Tools einrichten kÃ¶nnen,\n-empfehlen wir die LektÃ¼re des [custom tool guide](./custom_tools).\n-\n-### Was passiert hier? Was sind Tools und was sind Agenten?\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/diagram.png\">\n-\n-#### Agenten\n-\n-Der \"Agent\" ist hier ein groÃŸes Sprachmodell, das wir auffordern, Zugang zu einem bestimmten Satz von Tools zu erhalten.\n-\n-LLMs sind ziemlich gut darin, kleine Codeproben zu erzeugen. Diese API macht sich das zunutze, indem sie das \n-LLM ein kleines Codebeispiel gibt, das eine Aufgabe mit einer Reihe von Werkzeugen ausfÃ¼hrt. Diese Aufforderung wird dann ergÃ¤nzt durch die \n-Aufgabe, die Sie Ihrem Agenten geben, und die Beschreibung der Werkzeuge, die Sie ihm geben. Auf diese Weise erhÃ¤lt er Zugriff auf die Dokumentation der \n-Tools, insbesondere die erwarteten Eingaben und Ausgaben, und kann den entsprechenden Code generieren.\n-\n-#### Tools\n-\n-Tools sind sehr einfach: Sie bestehen aus einer einzigen Funktion mit einem Namen und einer Beschreibung. Wir verwenden dann die Beschreibungen dieser Tools \n-um den Agenten aufzufordern. Anhand der Eingabeaufforderung zeigen wir dem Agenten, wie er die Tools nutzen kann, um das zu tun, was in der \n-in der Abfrage angefordert wurde.\n-\n-Dies geschieht mit brandneuen Tools und nicht mit Pipelines, denn der Agent schreibt besseren Code mit sehr atomaren Tools. \n-Pipelines sind stÃ¤rker refaktorisiert und fassen oft mehrere Aufgaben in einer einzigen zusammen. Tools sind dafÃ¼r gedacht, sich auf\n-eine einzige, sehr einfache Aufgabe konzentrieren.\n-\n-#### Code-AusfÃ¼hrung?!\n-\n-Dieser Code wird dann mit unserem kleinen Python-Interpreter auf den mit Ihren Tools Ã¼bergebenen Eingaben ausgefÃ¼hrt. \n-Wir hÃ¶ren Sie schon schreien \"WillkÃ¼rliche CodeausfÃ¼hrung!\", aber lassen Sie uns erklÃ¤ren, warum das nicht der Fall ist.\n-\n-Die einzigen Funktionen, die aufgerufen werden kÃ¶nnen, sind die von Ihnen zur VerfÃ¼gung gestellten Tools und die Druckfunktion, so dass Sie bereits eingeschrÃ¤nkt sind \n-eingeschrÃ¤nkt, was ausgefÃ¼hrt werden kann. Sie sollten sicher sein, wenn es sich auf die Werkzeuge fÃ¼r das Umarmungsgesicht beschrÃ¤nkt. \n-\n-Dann lassen wir keine Attributsuche oder Importe zu (die ohnehin nicht benÃ¶tigt werden, um die \n-Inputs/Outputs an eine kleine Gruppe von Funktionen), so dass alle offensichtlichen Angriffe (und Sie mÃ¼ssten den LLM \n-dazu auffordern, sie auszugeben) kein Problem darstellen sollten. Wenn Sie auf Nummer sicher gehen wollen, kÃ¶nnen Sie die \n-run()-Methode mit dem zusÃ¤tzlichen Argument return_code=True ausfÃ¼hren. In diesem Fall gibt der Agent nur den auszufÃ¼hrenden Code \n-zur AusfÃ¼hrung zurÃ¼ck und Sie kÃ¶nnen entscheiden, ob Sie ihn ausfÃ¼hren mÃ¶chten oder nicht.\n-\n-Die AusfÃ¼hrung bricht bei jeder Zeile ab, in der versucht wird, eine illegale Operation auszufÃ¼hren, oder wenn ein regulÃ¤rer Python-Fehler \n-mit dem vom Agenten generierten Code.\n-\n-### Ein kuratierter Satz von Tools\n-\n-Wir haben eine Reihe von Tools identifiziert, die solche Agenten unterstÃ¼tzen kÃ¶nnen. Hier ist eine aktualisierte Liste der Tools, die wir integriert haben \n-in `transformers` integriert haben:\n-\n-- **Beantwortung von Fragen zu Dokumenten**: Beantworten Sie anhand eines Dokuments (z.B. PDF) im Bildformat eine Frage zu diesem Dokument ([Donut](./model_doc/donut))\n-- Beantworten von Textfragen**: Geben Sie einen langen Text und eine Frage an, beantworten Sie die Frage im Text ([Flan-T5](./model_doc/flan-t5))\n-- **Unbedingte Bildunterschriften**: Beschriften Sie das Bild! ([BLIP](./model_doc/blip))\n-- **Bildfragebeantwortung**: Beantworten Sie bei einem Bild eine Frage zu diesem Bild ([VILT](./model_doc/vilt))\n-- **Bildsegmentierung**: Geben Sie ein Bild und einen Prompt an und geben Sie die Segmentierungsmaske dieses Prompts aus ([CLIPSeg](./model_doc/clipseg))\n-- **Sprache in Text**: Geben Sie eine Audioaufnahme einer sprechenden Person an und transkribieren Sie die Sprache in Text ([Whisper](./model_doc/whisper))\n-- **Text in Sprache**: wandelt Text in Sprache um ([SpeechT5](./model_doc/speecht5))\n-- **Zero-Shot-Textklassifizierung**: Ermitteln Sie anhand eines Textes und einer Liste von Bezeichnungen, welcher Bezeichnung der Text am ehesten entspricht ([BART](./model_doc/bart))\n-- **Textzusammenfassung**: fassen Sie einen langen Text in einem oder wenigen SÃ¤tzen zusammen ([BART](./model_doc/bart))\n-- **Ãœbersetzung**: Ãœbersetzen des Textes in eine bestimmte Sprache ([NLLB](./model_doc/nllb))\n-\n-Diese Tools sind in Transformatoren integriert und kÃ¶nnen auch manuell verwendet werden, zum Beispiel:\n-\n-```py\n-from transformers import load_tool\n-\n-tool = load_tool(\"text-to-speech\")\n-audio = tool(\"This is a text to speech tool\")\n-```\n-\n-### Benutzerdefinierte Tools\n-\n-Wir haben zwar eine Reihe von Tools identifiziert, sind aber der festen Ãœberzeugung, dass der Hauptwert dieser Implementierung darin besteht \n-die MÃ¶glichkeit, benutzerdefinierte Tools schnell zu erstellen und weiterzugeben.\n-\n-Indem Sie den Code eines Tools in einen Hugging Face Space oder ein Modell-Repository stellen, kÃ¶nnen Sie das Tool \n-direkt mit dem Agenten nutzen. Wir haben ein paar neue Funktionen hinzugefÃ¼gt \n-**transformers-agnostic** Tools zur [`huggingface-tools` Organisation](https://huggingface.co/huggingface-tools) hinzugefÃ¼gt:\n-\n-- **Text-Downloader**: zum Herunterladen eines Textes von einer Web-URL\n-- **Text zu Bild**: erzeugt ein Bild nach einer Eingabeaufforderung und nutzt dabei stabile Diffusion\n-- **Bildtransformation**: verÃ¤ndert ein Bild anhand eines Ausgangsbildes und einer Eingabeaufforderung, unter Ausnutzung der stabilen pix2pix-Diffusion\n-- **Text zu Video**: Erzeugen eines kleinen Videos nach einer Eingabeaufforderung, unter Verwendung von damo-vilab\n-\n-Das Text-zu-Bild-Tool, das wir von Anfang an verwendet haben, ist ein Remote-Tool, das sich in \n-[*huggingface-tools/text-to-image*](https://huggingface.co/spaces/huggingface-tools/text-to-image)! Wir werden\n-weiterhin solche Tools fÃ¼r diese und andere Organisationen verÃ¶ffentlichen, um diese Implementierung weiter zu verbessern.\n-\n-Die Agenten haben standardmÃ¤ÃŸig Zugriff auf die Tools, die sich auf [*huggingface-tools*](https://huggingface.co/huggingface-tools) befinden.\n-Wie Sie Ihre eigenen Tools schreiben und freigeben kÃ¶nnen und wie Sie jedes benutzerdefinierte Tool, das sich auf dem Hub befindet, nutzen kÃ¶nnen, erklÃ¤ren wir in [folgender Anleitung](custom_tools).\n-\n-### Code-Erzeugung\n-\n-Bisher haben wir gezeigt, wie Sie die Agenten nutzen kÃ¶nnen, um Aktionen fÃ¼r Sie durchzufÃ¼hren. Der Agent generiert jedoch nur Code\n-den wir dann mit einem sehr eingeschrÃ¤nkten Python-Interpreter ausfÃ¼hren. Falls Sie den generierten Code in einer anderen Umgebung verwenden mÃ¶chten \n-einer anderen Umgebung verwenden mÃ¶chten, kÃ¶nnen Sie den Agenten auffordern, den Code zusammen mit einer Tooldefinition und genauen Importen zurÃ¼ckzugeben.\n-\n-Zum Beispiel die folgende Anweisung\n-```python\n-agent.run(\"Draw me a picture of rivers and lakes\", return_code=True)\n-```\n-\n-gibt den folgenden Code zurÃ¼ck\n-\n-```python\n-from transformers import load_tool\n-\n-image_generator = load_tool(\"huggingface-tools/text-to-image\")\n-\n-image = image_generator(prompt=\"rivers and lakes\")\n-```\n-\n-die Sie dann selbst Ã¤ndern und ausfÃ¼hren kÃ¶nnen."
        },
        {
            "sha": "5884b733862b185e93afc223fd8481928a7e6774",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -308,8 +308,6 @@\n - isExpanded: false\n   sections:\n   - sections:\n-    - local: main_classes/agent\n-      title: Agents and Tools\n     - local: model_doc/auto\n       title: Auto Classes\n     - local: main_classes/backbones"
        },
        {
            "sha": "af2e9df1e8c96afaa4daa92899ea8eeb5368967d",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 1,
            "deletions": 280,
            "changes": 281,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -15,283 +15,4 @@ rendered properly in your Markdown viewer.\n -->\n \n > [!WARNING]\n-> Agents and tools are being spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. These docs will be deprecated in the future!\n-\n-# Agents\n-\n-[[open-in-colab]]\n-\n-An agent is a system where a large language model (LLM) can execute more complex tasks through *planning* and using *tools*.\n-\n-- Planning helps a LLM reason its way through a task by breaking it down into smaller subtasks. For example, [`CodeAgent`] plans a series of actions to take and then generates Python code to execute all the actions at once.\n-\n-    Another planning method is by self-reflection and refinement of its previous actions to improve its performance. The [`ReactJsonAgent`] is an example of this type of planning, and it's based on the [ReAct](https://hf.co/papers/2210.03629) framework. This agent plans and executes actions one at a time based on the feedback it receives from each action.\n-\n-- Tools give a LLM access to external functions or APIs that it can use to help it complete a task. For example, [gradio-tools](https://github.com/freddyaboulton/gradio-tools) gives a LLM access to any of the [Gradio](https://www.gradio.app/) apps available on Hugging Face [Spaces](https://hf.co/spaces). These apps can be used for a wide range of tasks such as image generation, video generation, audio transcription, and more.\n-\n-To use agents in Transformers, make sure you have the extra `agents` dependencies installed.\n-\n-```bash\n-!pip install transformers[agents]\n-```\n-\n-Create an agent instance (refer to the [Agents](./main_classes/agent#agents) API for supported agents in Transformers) and a list of tools available for it to use, then [`~ReactAgent.run`] the agent on your task. The example below demonstrates how a ReAct agent reasons through a task.\n-\n-```py\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[])\n-agent.run(\n-    \"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\",\n-)\n-```\n-\n-```bash\n-======== New task ========\n-How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\n-==== Agent is executing the code below:\n-bert_layers = 12  # BERT base encoder has 12 layers\n-attention_layers = 6  # Encoder in Attention is All You Need has 6 layers\n-layer_diff = bert_layers - attention_layers\n-print(\"The difference in layers between BERT base encoder and Attention is All You Need is\", layer_diff)\n-====\n-Print outputs:\n-The difference in layers between BERT base encoder and Attention is All You Need is 6\n-\n-==== Agent is executing the code below:\n-final_answer(\"BERT base encoder has {} more layers than the encoder from Attention is All You Need.\".format(layer_diff))\n-====\n-Print outputs:\n-\n->>> Final answer:\n-BERT base encoder has 6 more layers than the encoder from Attention is All You Need.\n-```\n-\n-This guide will walk you through in more detail how to initialize an agent.\n-\n-## LLM\n-\n-An agent uses a LLM to plan and execute a task; it is the engine that powers the agent. To choose and build your own LLM engine, you need a method that:\n-\n-1. the input uses the [chat template](./chat_templating) format, `List[Dict[str, str]]`, and it returns a string\n-2. the LLM stops generating outputs when it encounters the sequences in `stop_sequences`\n-\n-```py\n-def llm_engine(messages, stop_sequences=[\"Task\"]) -> str:\n-    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n-    answer = response.choices[0].message.content\n-    return answer\n-```\n-\n-Next, initialize an engine to load a model. To run an agent locally, create a [`TransformersEngine`] to load a preinitialized [`Pipeline`].\n-\n-However, you could also leverage Hugging Face's powerful inference infrastructure, [Inference API](https://hf.co/docs/api-inference/index) or [Inference Endpoints](https://hf.co/docs/inference-endpoints/index), to run your model. This is useful for loading larger models that are typically required for agentic behavior. In this case, load the [`HfApiEngine`] to run the agent.\n-\n-The agent requires a list of tools it can use to complete a task. If you aren't using any additional tools, pass an empty list. The default tools provided by Transformers are loaded automatically, but you can optionally set `add_base_tools=True` to explicitly enable them.\n-\n-<hfoptions id=\"engine\">\n-<hfoption id=\"TransformersEngine\">\n-\n-```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine, CodeAgent\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n-model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\").to(\"cuda\")\n-pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n-llm_engine = TransformersEngine(pipeline)\n-agent = CodeAgent(tools=[], llm_engine=llm_engine)\n-agent.run(\n-    \"What causes bread to rise?\",\n-)\n-```\n-\n-</hfoption>\n-<hfoption id=\"HfApiEngine\">\n-\n-```py\n-from transformers import CodeAgent, HfApiEngine\n-\n-llm_engine = HfApiEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-agent = CodeAgent(tools=[], llm_engine=llm_engine)\n-agent.run(\n-    \"Could you translate this sentence from French, say it out loud and return the audio.\",\n-    sentence=\"OÃ¹ est la boulangerie la plus proche?\",\n-)\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-The agent supports [constrained generation](https://hf.co/docs/text-generation-inference/conceptual/guidance) for generating outputs according to a specific structure with the `grammar` parameter. The `grammar` parameter should be specified in the `llm_engine` method or you can set it when initializing an agent.\n-\n-Lastly, an agent accepts additional inputs such as text and audio. In the [`HfApiEngine`] example above, the agent accepted a sentence to translate. But you could also pass a path to a local or remote file for the agent to access. The example below demonstrates how to pass a path to an audio file.\n-\n-```py\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[], llm_engine=llm_engine)\n-agent.run(\"Why doesn't he know many people in New York?\", audio=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3\")\n-```\n-\n-## System prompt\n-\n-A system prompt describes how an agent should behave, a description of the available tools, and the expected output format.\n-\n-Tools are defined by the `<<tool_descriptions>>` token which is dynamically replaced during runtime with the actual tool. The tool description is derived from the tool name, description, inputs, output type, and a Jinja2 template. Refer to the [Tools](./tools) guide for more information about how to describe tools.\n-\n-The example below is the system prompt for [`ReactCodeAgent`].\n-\n-```py\n-You will be given a task to solve as best you can.\n-You have access to the following tools:\n-<<tool_descriptions>>\n-\n-To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n-\n-At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task, then the tools that you want to use.\n-Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '/End code' sequence.\n-During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n-These print outputs will then be available in the 'Observation:' field, for using this information as input for the next step.\n-\n-In the end you have to return a final answer using the `final_answer` tool.\n-\n-Here are a few examples using notional tools:\n----\n-{examples}\n-\n-Above example were using notional tools that might not exist for you. You only have access to those tools:\n-<<tool_names>>\n-You also can perform computations in the python code you generate.\n-\n-Always provide a 'Thought:' and a 'Code:\\n```py' sequence ending with '```<end_code>' sequence. You MUST provide at least the 'Code:' sequence to move forward.\n-\n-Remember to not perform too many operations in a single code block! You should split the task into intermediate code blocks.\n-Print results at the end of each step to save the intermediate results. Then use final_answer() to return the final result.\n-\n-Remember to make sure that variables you use are all defined.\n-\n-Now Begin!\n-```\n-\n-The system prompt can be tailored to the intended task. For example, you can add a better explanation of the output format or you can overwrite the system prompt template entirely with your own custom system prompt as shown below.\n-\n-> [!WARNING]\n-> If you're writing a custom system prompt, make sure to include `<<tool_descriptions>>` in the template so the agent is aware of the available tools.\n-\n-```py\n-from transformers import ReactJsonAgent\n-from transformers.agents import PythonInterpreterTool\n-\n-agent = ReactJsonAgent(tools=[PythonInterpreterTool()], system_prompt=\"{your_custom_prompt}\")\n-```\n-\n-## Code execution\n-\n-For safety, only the tools you provide (and the default Transformers tools) and the `print` function are executed. The interpreter doesn't allow importing modules that aren't on a safe list.\n-\n-To import modules that aren't on the list, add them as a list to the `additional_authorized_imports` parameter when initializing an agent.\n-\n-```py\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[], additional_authorized_imports=['requests', 'bs4'])\n-agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n-```\n-\n-Code execution stops if a tool isn't on the safe list, it isn't authorized, or if the code generated by the agent returns a Python error.\n-\n-> [!WARNING]\n-> A LLM can generate any arbitrary code that can be executed, so don't add any unsafe imports!\n-\n-## Multi-agent\n-\n-[Multi-agent](https://hf.co/papers/2308.08155) refers to multiple agents working together to solve a task. Performance is typically better because each agent is specialized for a particular subtask.\n-\n-Multi-agents are created through a [`ManagedAgent`] class, where a *manager agent* oversees how other agents work together. The manager agent requires an agent and their name and description. These are added to the manager agents system prompt which lets it know how to call and use them.\n-\n-The multi-agent example below creates a web search agent that is managed by another [`ReactCodeAgent`].\n-\n-```py\n-from transformers.agents import ReactCodeAgent, HfApiEngine, DuckDuckGoSearchTool, ManagedAgent\n-\n-llm_engine = HfApiEngine()\n-web_agent = ReactCodeAgent(tools=[DuckDuckGoSearchTool()], llm_engine=llm_engine)\n-managed_web_agent = ManagedAgent(\n-    agent=web_agent,\n-    name=\"web_search\",\n-    description=\"Runs web searches for you. Give it your query as an argument.\"\n-)\n-manager_agent = ReactCodeAgent(\n-    tools=[], llm_engine=llm_engine, managed_agents=[managed_web_agent]\n-)\n-manager_agent.run(\"Who is the CEO of Hugging Face?\")\n-```\n-\n-## Gradio integration\n-\n-[Gradio](https://www.gradio.app/) is a library for quickly creating and sharing machine learning apps. The [gradio.Chatbot](https://www.gradio.app/docs/gradio/chatbot) supports chatting with a Transformers agent with the [`stream_to_gradio`] function.\n-\n-Load a tool and LLM with an agent, and then create a Gradio app. The key is to use [`stream_to_gradio`] to stream the agents messages and display how it's reasoning through a task.\n-\n-```py\n-import gradio as gr\n-from transformers import (\n-    load_tool,\n-    ReactCodeAgent,\n-    HfApiEngine,\n-    stream_to_gradio,\n-)\n-\n-# Import tool from Hub\n-image_generation_tool = load_tool(\"m-ric/text-to-image\")\n-llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-\n-# Initialize the agent with the image generation tool\n-agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n-\n-def interact_with_agent(task):\n-    messages = []\n-    messages.append(gr.ChatMessage(role=\"user\", content=task))\n-    yield messages\n-    for msg in stream_to_gradio(agent, task):\n-        messages.append(msg)\n-        yield messages + [\n-            gr.ChatMessage(role=\"assistant\", content=\"â³ Task not finished yet!\")\n-        ]\n-    yield messages\n-\n-with gr.Blocks() as demo:\n-    text_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\n-    submit = gr.Button(\"Run illustrator agent!\")\n-    chatbot = gr.Chatbot(\n-        label=\"Agent\",\n-        type=\"messages\",\n-        avatar_images=(\n-            None,\n-            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n-        ),\n-    )\n-    submit.click(interact_with_agent, [text_input], [chatbot])\n-\n-if __name__ == \"__main__\":\n-    demo.launch()\n-```\n-\n-## Troubleshoot\n-\n-For a better idea of what is happening when you call an agent, it is always a good idea to check the system prompt template first.\n-\n-```py\n-print(agent.system_prompt_template)\n-```\n-\n-If the agent is behaving unexpectedly, remember to explain the task you want to perform as clearly as possible. Every [`~Agent.run`] is different and minor variations in your system prompt may yield completely different results.\n-\n-To find out what happened after a run, check the following agent attributes.\n-\n-- `agent.logs` stores the finegrained agent logs. At every step of the agents run, everything is stored in a dictionary and appended to `agent.logs`.\n-- `agent.write_inner_memory_from_logs` only stores a high-level overview of the agents run. For example, at each step, it stores the LLM output as a message and the tool call output as a separate message. Not every detail from a step is transcripted by `write_inner_memory_from_logs`.\n-\n-## Resources\n-\n-Learn more about ReAct agents in the [Open-source LLMs as LangChain Agents](https://hf.co/blog/open-source-llms-as-agents) blog post.\n+> Agents and tools were spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. They were removed from `transformers` in v4.52."
        },
        {
            "sha": "ed0486b60128ec84c9b99099cb4b8f5e5163508b",
            "filename": "docs/source/en/main_classes/agent.md",
            "status": "removed",
            "additions": 0,
            "deletions": 167,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fen%2Fmain_classes%2Fagent.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fen%2Fmain_classes%2Fagent.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fagent.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,167 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Agents & Tools\n-\n-<Tip warning={true}>\n-\n-Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents\n-can vary as the APIs or underlying models are prone to change.\n-\n-</Tip>\n-\n-To learn more about agents and tools make sure to read the [introductory guide](../transformers_agents). This page\n-contains the API docs for the underlying classes.\n-\n-## Agents\n-\n-We provide two types of agents, based on the main [`Agent`] class:\n-- [`CodeAgent`] acts in one shot, generating code to solve the task, then executes it at once.\n-- [`ReactAgent`] acts step by step, each step consisting of one thought, then one tool call and execution. It has two classes:\n-  - [`ReactJsonAgent`] writes its tool calls in JSON.\n-  - [`ReactCodeAgent`] writes its tool calls in Python code.\n-\n-### Agent\n-\n-[[autodoc]] Agent\n-\n-### CodeAgent\n-\n-[[autodoc]] CodeAgent\n-\n-### React agents\n-\n-[[autodoc]] ReactAgent\n-\n-[[autodoc]] ReactJsonAgent\n-\n-[[autodoc]] ReactCodeAgent\n-\n-### ManagedAgent\n-\n-[[autodoc]] ManagedAgent\n-\n-## Tools\n-\n-### load_tool\n-\n-[[autodoc]] load_tool\n-\n-### tool\n-\n-[[autodoc]] tool\n-\n-### Tool\n-\n-[[autodoc]] Tool\n-\n-### Toolbox\n-\n-[[autodoc]] Toolbox\n-\n-### PipelineTool\n-\n-[[autodoc]] PipelineTool\n-\n-### launch_gradio_demo\n-\n-[[autodoc]] launch_gradio_demo\n-\n-### stream_to_gradio\n-\n-[[autodoc]] stream_to_gradio\n-\n-### ToolCollection\n-\n-[[autodoc]] ToolCollection\n-\n-## Engines\n-\n-You're free to create and use your own engines to be usable by the Agents framework.\n-These engines have the following specification:\n-1. Follow the [messages format](../chat_templating.md) for its input (`List[Dict[str, str]]`) and return a string.\n-2. Stop generating outputs *before* the sequences passed in the argument `stop_sequences`\n-\n-### TransformersEngine\n-\n-For convenience, we have added a `TransformersEngine` that implements the points above, taking a pre-initialized `Pipeline` as input.\n-\n-```python\n->>> from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine\n-\n->>> model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n->>> model = AutoModelForCausalLM.from_pretrained(model_name)\n-\n->>> pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n-\n->>> engine = TransformersEngine(pipe)\n->>> engine([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"])\n-\n-\"What a \"\n-```\n-\n-[[autodoc]] TransformersEngine\n-\n-### HfApiEngine\n-\n-The `HfApiEngine` is an engine that wraps an [HF Inference API](https://huggingface.co/docs/api-inference/index) client for the execution of the LLM.\n-\n-```python\n->>> from transformers import HfApiEngine\n-\n->>> messages = [\n-...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n-...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n-...   {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n-... ]\n-\n->>> HfApiEngine()(messages, stop_sequences=[\"conversation\"])\n-\n-\"That's very kind of you to say! It's always nice to have a relaxed \"\n-```\n-\n-[[autodoc]] HfApiEngine\n-\n-\n-## Agent Types\n-\n-Agents can handle any type of object in-between tools; tools, being completely multimodal, can accept and return\n-text, image, audio, video, among other types. In order to increase compatibility between tools, as well as to \n-correctly render these returns in ipython (jupyter, colab, ipython notebooks, ...), we implement wrapper classes\n-around these types.\n-\n-The wrapped objects should continue behaving as initially; a text object should still behave as a string, an image\n-object should still behave as a `PIL.Image`.\n-\n-These types have three specific purposes:\n-\n-- Calling `to_raw` on the type should return the underlying object\n-- Calling `to_string` on the type should return the object as a string: that can be the string in case of an `AgentText`\n-  but will be the path of the serialized version of the object in other instances\n-- Displaying it in an ipython kernel should display the object correctly\n-\n-### AgentText\n-\n-[[autodoc]] transformers.agents.agent_types.AgentText\n-\n-### AgentImage\n-\n-[[autodoc]] transformers.agents.agent_types.AgentImage\n-\n-### AgentAudio\n-\n-[[autodoc]] transformers.agents.agent_types.AgentAudio"
        },
        {
            "sha": "af2e9df1e8c96afaa4daa92899ea8eeb5368967d",
            "filename": "docs/source/en/tools.md",
            "status": "modified",
            "additions": 1,
            "deletions": 235,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fen%2Ftools.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fen%2Ftools.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftools.md?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -15,238 +15,4 @@ rendered properly in your Markdown viewer.\n -->\n \n > [!WARNING]\n-> Agents and tools are being spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. These docs will be deprecated in the future!\n-\n-# Tools\n-\n-A tool is a function an agent can use to complete a task. Depending on your task, a tool can perform a web search, answer questions about a document, transcribe speech to text, and much more.\n-\n-Transformers provides a default set of tools for agents. These include the tools mentioned above as well as image question answering, text-to-speech, translation, and a Python code interpreter that executes the Python code generated by a LLM in a secure environment.\n-\n-Set `add_base_tools=True` to enable this default set of tools. The `tools` parameter is for adding additional tools. Leave the list empty if you aren't planning on adding any other tools to the toolbox.\n-\n-```py\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[], add_base_tools=True)\n-```\n-\n-You could also manually load a tool with [`load_tool`].\n-\n-```py\n-from transformers import load_tool, ReactCodeAgent\n-\n-tool = load_tool(\"text-to-speech\")\n-audio = tool(\"This is a text-to-speech tool\")\n-agent = ReactCodeAgent(tools=[audio])\n-```\n-\n-This guide will help you learn how to create your own tools and manage an agents toolbox.\n-\n-## Create a new tool\n-\n-You can create any tool you can dream of to empower an agent. The example in this section creates a tool that returns the most downloaded model for a task from the Hub, and the code for it is shown below.\n-\n-```py\n-from huggingface_hub import list_models\n-\n-task = \"text-classification\"\n-model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-print(model.id)\n-```\n-\n-There are two ways you can create a tool, using a decorator or a superclass.\n-\n-### Tool decorator\n-\n-A fast and simple way to create a tool is to add the `@tool` decorator.\n-\n-Convert the code above into a tool by wrapping it in a function and adding the `@tool` decorator. The function needs:\n-\n-- A clear name that describes what the tool does, `model_download_counter`.\n-- Type hints for the input and output (`str`).\n-- A description that describes the tool in more detail and its arguments. This description is incorporated in the agents system prompt. It tells the agent *how* to use the tool, so try to make it as clear as possible!\n-\n-```py\n-from transformers import tool\n-\n-@tool\n-def model_download_counter(task: str) -> str:\n-    \"\"\"\n-    This is a tool that returns the checkpoint name of the most downloaded model for a task from the Hugging Face Hub.\n-\n-    Args:\n-        task: The task to retrieve the most downloaded model from.\n-    \"\"\"\n-    model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-    return model.id\n-```\n-\n-Pass the `model_download_counter` tool to the agents `tools` parameter to use it.\n-\n-```py\n-from transformers import CodeAgent\n-\n-agent = CodeAgent(tools=[model_download_counter], add_base_tools=True)\n-agent.run(\n-    \"Can you give me the name of the model that has the most downloads on the 'text-to-video' task on the Hugging Face Hub?\"\n-)\n-```\n-\n-### Tool superclass\n-\n-Inheritance allows you to customize the [`Tool`] superclass or build a tool much more flexibly and comprehensively. This example will show you how to build the same `model_download_counter` tool as a [`Tool`] class.\n-\n-The [`Tool`] class needs:\n-\n-- A clear name that describes what the tool does, `model_download_counter`.\n-- A description that describes the tool in more detail and its arguments. This description is incorporated in the agents system prompt. It tells the agent *how* to use the tool, so try to make it as clear as possible!\n-- An `inputs` attribute that describes the input type. This is a dictionary with the keys, `type` and `description`.\n-- An `outputs` attribute that describes the output type.\n-- A `forward` method containing the code to be executed when the tool is called.\n-\n-Write the following code below to a file named `model_download.py`.\n-\n-```py\n-from transformers import Tool\n-from huggingface_hub import list_models\n-\n-class HFModelDownloadsTool(Tool):\n-    name = \"model_download_counter\"\n-    description = \"\"\"\n-    This is a tool that returns the checkpoint name of the most downloaded model for a task from the Hugging Face Hub.\"\"\"\n-\n-    inputs = {\n-        \"task\": {\n-            \"type\": \"string\",\n-            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n-        }\n-    }\n-    output_type = \"string\"\n-\n-    def forward(self, task: str):\n-        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-        return model.id\n-```\n-\n-Import the tool from `model_download.py` and use [`load_tool`] to load it into the agent.\n-\n-```py\n-from model_download import HFModelDownloadsTool\n-from transformers import load_tool, CodeAgent\n-\n-tool = HFModelDownloadsTool()\n-model_counter = load_tool(tool)\n-agent = CodeAgent(tools=[model_counter], add_base_tools=True)\n-```\n-\n-Also consider sharing your tool to the Hub with [`~Tool.push_to_hub`] so that everyone can use it!\n-\n-```py\n-from model_download import HFModelDownloadsTool\n-from transformers import load_tool, CodeAgent\n-\n-tool = HFModelDownloadsTool()\n-tool.push_to_hub(\"{your_username}/hf-model-downloads\")\n-model_counter = load_tool(\"m-ric/hf-model-downloads\")\n-agent = CodeAgent(tools=[model_counter], add_base_tools=True)\n-```\n-\n-## Add and replace tools\n-\n-Once an agent is initialized, add or replace its available tools without reinitializing the agent from scratch.\n-\n-Use [`add_tool`] to add a tool to an existing agent.\n-\n-```py\n-from transformers import CodeAgent\n-\n-agent = CodeAgent(tools=[], add_base_tools=True)\n-agent.toolbox.add_tool(model_download_counter)\n-```\n-\n-Now you can use the default text-to-speech tool to read aloud the most downloaded model for the text-to-video task.\n-\n-```py\n-agent.run(\n-    \"Can you read out loud the name of the model that has the most downloads on the 'text-to-video' task on the Hugging Face Hub and return the audio?\"\n-)\n-```\n-\n-> [!WARNING]\n-> When adding tools to an agent that already works well, it can bias the agent towards your tool or a tool other than the one currently defined.\n-\n-Use [`update_tool`] to replace an agents existing tool. This is useful if the new tool is a one-to-one replacement of the existing tool because the agent already knows how to perform the task. The new tool should follow the same API as the tool it replaced or the system prompt template should be adapted to ensure all examples using the replaced tool are updated.\n-\n-```py\n-agent.toolbox.update_tool(new_model_download_counter)\n-```\n-\n-## ToolCollection\n-\n-A [`ToolCollection`] is a collection of Hugging Face [Spaces](https://hf.co/spaces) that can be quickly loaded and used by an agent.\n-\n-> [!TIP]\n-> Learn more about creating collections on the Hub.\n-\n-Create a [`ToolCollection`] object and specify the `collection_slug` of the collection you want to use, and then pass it to the agent. To speed up the starting process, tools are only loaded if they're called by the agent.\n-\n-The example loads a collection of image generation tools.\n-\n-```py\n-from transformers import ToolCollection, ReactCodeAgent\n-\n-image_tool_collection = ToolCollection(collection_slug=\"\")\n-agent = ReactCodeAgent(tools=[*image_tool_collection], add_base_tools=True)\n-agent.run(\n-    \"Please draw me a picture of rivers and lakes.\"\n-)\n-```\n-\n-## Tool integrations\n-\n-Transformers supports tools from several other libraries, such as [gradio-tools](https://github.com/freddyaboulton/gradio-tools) and [LangChain](https://python.langchain.com/docs/introduction/).\n-\n-### gradio-tools\n-\n-gradio-tools is a library that enables [Gradio](https://www.gradio.app/) apps to be used as tools. With the wide variety of Gradio apps available, you can enhance your agent with a range of tools like generating images and videos or transcribing audio and summarizing it.\n-\n-Import and instantiate a tool from gradio-tools, for example, the [StableDiffusionPromptGeneratorTool](https://github.com/freddyaboulton/gradio-tools/blob/main/gradio_tools/tools/prompt_generator.py). This tool can help improve prompts to generate better images.\n-\n-> [!WARNING]\n-> gradio-tools require text inputs and outputs even when working with different modalities like images and audio, which are currently incompatible.\n-\n-Use [`~Tool.from_gradio`] to load the prompt generator tool.\n-\n-```py\n-from gradio_tools import StableDiffusionPromptGeneratorTool\n-from transformers import Tool, load_tool, CodeAgent\n-\n-gradio_prompt_generator_tool = StableDiffusionPromptGeneratorTool()\n-prompt_generator_tool = Tool.from_gradio(gradio_prompt_generator_tool)\n-```\n-\n-Now pass it to the agent along with a text-to-image tool.\n-\n-```py\n-image_generation_tool = load_tool(\"huggingface-tools/text-to-image\")\n-agent = CodeAgent(tools=[prompt_generator_tool, image_generation_tool], llm_engine=llm_engine)\n-agent.run(\n-    \"Improve this prompt, then generate an image of it.\", prompt=\"A rabbit wearing a space suit\"\n-)\n-```\n-\n-### LangChain\n-\n-LangChain is a library for working with LLMs which includes agents and tools. Use the [`~Tool.from_langchain`] method to load any LangChain tool into an agent.\n-\n-The example below demonstrates how to use LangChains web search tool.\n-\n-```py\n-from langchain.agents import load_tools\n-from transformers import Tool, ReactCodeAgent\n-\n-search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n-agent = ReactCodeAgent(tools=[search_tool])\n-agent.run(\"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\")\n-```\n+> Agents and tools were spun out into the standalone [smolagents](https://huggingface.co/docs/smolagents/index) library. They were removed from `transformers` in v4.52."
        },
        {
            "sha": "3b85bcab8284f6ee51190c1176521c0199c7e3d6",
            "filename": "docs/source/fr/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Ffr%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Ffr%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -23,8 +23,6 @@\n     title: Chargement et entraÃ®nement des adaptateurs avec ğŸ¤— PEFT\n   - local: in_translation\n     title: Partager un modÃ¨le\n-  - local: in_translation\n-    title: Agents\n   - local: in_translation\n     title: GÃ©nÃ©ration avec LLMs\n   title: Tutoriels\n@@ -33,4 +31,4 @@\n     title: Ce que ğŸ¤— Transformers peut faire\n   - local: tasks_explained\n     title: Comment ğŸ¤— Transformers rÃ©sout ces tÃ¢ches\n-  title: Guides conceptuels\n\\ No newline at end of file\n+  title: Guides conceptuels"
        },
        {
            "sha": "455207ccc0d06cbf9811e81ad046124a0a9e1bdb",
            "filename": "docs/source/ja/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fja%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fja%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -23,8 +23,6 @@\n     title: ğŸ¤— PEFT ã‚’ä½¿ç”¨ã—ã¦ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹\n   - local: model_sharing\n     title: ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹\n-  - local: transformers_agents\n-    title: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ\n   - local: llm_tutorial\n     title: LLM ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆ\n   title: Tutorials\n@@ -119,8 +117,6 @@\n     title: ãƒˆãƒ¼ãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n   - local: community\n     title: ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒªã‚½ãƒ¼ã‚¹\n-  - local: custom_tools\n-    title: ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ\n   - local: troubleshooting\n     title: ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°\n   title: é–‹ç™ºè€…ã‚¬ã‚¤ãƒ‰\n@@ -200,8 +196,6 @@\n   title: ã‚³ãƒ³ã‚»ãƒ—ãƒãƒ¥ã‚¢ãƒ«ã‚¬ã‚¤ãƒ‰\n - sections:\n   - sections:\n-    - local: main_classes/agent\n-      title: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ„ãƒ¼ãƒ«\n     - local: model_doc/auto\n       title: Auto Classes\n     - local: main_classes/callback"
        },
        {
            "sha": "4a9926ba2c2204ee27dd6669c47d34551573cbbb",
            "filename": "docs/source/ja/custom_tools.md",
            "status": "removed",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fja%2Fcustom_tools.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fja%2Fcustom_tools.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fcustom_tools.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,26 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Custom Tools and Prompts\n-\n-<Tip warning={true}>\n-\n-The Agents framework has significantly changed in version v4.41.0.\n-This document has been removed as it was referencing an older API.\n-\n-We eagerly welcome new contributions for the updated API.\n-\n-</Tip>"
        },
        {
            "sha": "cb7f5a992427500724a6ee00150a87ced8a823d4",
            "filename": "docs/source/ja/main_classes/agent.md",
            "status": "removed",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fja%2Fmain_classes%2Fagent.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fja%2Fmain_classes%2Fagent.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fagent.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,26 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ„ãƒ¼ãƒ«\n-\n-<Tip warning={true}>\n-\n-The Agents framework has significantly changed in version v4.41.0.\n-This document has been removed as it was referencing an older API.\n-\n-We eagerly welcome new contributions for the updated API.\n-\n-</Tip>\n\\ No newline at end of file"
        },
        {
            "sha": "572d7f290c96dc44914c01b7abf386824b3fbc89",
            "filename": "docs/source/ja/transformers_agents.md",
            "status": "removed",
            "additions": 0,
            "deletions": 282,
            "changes": 282,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fja%2Ftransformers_agents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fja%2Ftransformers_agents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftransformers_agents.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,282 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Transformers Agents\n-\n-<Tip warning={true}>\n-\n-Transformers Agentsã¯ã€ã„ã¤ã§ã‚‚å¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å®Ÿé¨“çš„ãªAPIã§ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè¿”ã™çµæœã¯ã€APIã¾ãŸã¯åŸºç¤ã¨ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€ç•°ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n-\n-</Tip>\n-\n-Transformersãƒãƒ¼ã‚¸ãƒ§ãƒ³v4.29.0ã¯ã€*ãƒ„ãƒ¼ãƒ«*ã¨*ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ*ã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’åŸºã«æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®[colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj)ã§è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-è¦ã™ã‚‹ã«ã€ã“ã‚Œã¯transformersã®ä¸Šã«è‡ªç„¶è¨€èªAPIã‚’æä¾›ã™ã‚‹ã‚‚ã®ã§ã™ï¼šç§ãŸã¡ã¯ä¸€é€£ã®å³é¸ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã€è‡ªç„¶è¨€èªã‚’è§£é‡ˆã—ã€ã“ã‚Œã‚‰ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­è¨ˆã—ã¾ã™ã€‚ã“ã‚Œã¯è¨­è¨ˆä¸Šæ‹¡å¼µå¯èƒ½ã§ã™ã€‚ç§ãŸã¡ã¯ã„ãã¤ã‹ã®é–¢é€£ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’å³é¸ã—ã¾ã—ãŸãŒã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸä»»æ„ã®ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã‚·ã‚¹ãƒ†ãƒ ã‚’ç°¡å˜ã«æ‹¡å¼µã§ãã‚‹æ–¹æ³•ã‚‚ç¤ºã—ã¾ã™ã€‚\n-\n-ã“ã®æ–°ã—ã„APIã§ä½•ãŒã§ãã‚‹ã‹ã®ã„ãã¤ã‹ã®ä¾‹ã‹ã‚‰å§‹ã‚ã¾ã—ã‚‡ã†ã€‚ç‰¹ã«å¤šãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã«é–¢ã—ã¦å¼·åŠ›ã§ã™ã®ã§ã€ç”»åƒã‚’ç”Ÿæˆã—ãŸã‚Šãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿ä¸Šã’ãŸã‚Šã™ã‚‹ã®ã«æœ€é©ã§ã™ã€‚\n-\n-ä¸Šè¨˜ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä¸Šã«ã€æ—¥æœ¬èªã®ç¿»è¨³ã‚’æä¾›ã—ã¾ã™ã€‚\n-\n-\n-```py\n-agent.run(\"Caption the following image\", image=image)\n-```\n-\n-| **Input**                                                                                                                   | **Output**                        |\n-|-----------------------------------------------------------------------------------------------------------------------------|-----------------------------------|\n-| <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beaver.png\" width=200> | A beaver is swimming in the water |\n-\n----\n-\n-```py\n-agent.run(\"Read the following text out loud\", text=text)\n-```\n-| **Input**                                                                                                               | **Output**                                   |\n-|-------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|\n-| A beaver is swimming in the water | <audio controls><source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tts_example.wav\" type=\"audio/wav\"> your browser does not support the audio element. </audio>\n-\n----\n-\n-```py\n-agent.run(\n-    \"In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?\",\n-    document=document,\n-)\n-```\n-| **Input**                                                                                                                   | **Output**     |\n-|-----------------------------------------------------------------------------------------------------------------------------|----------------|\n-| <img src=\"https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/0/image/image.jpg\" width=200> | ballroom foyer |\n-\n-## Quickstart\n-\n-`agent.run`ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã§ã™ã€‚\n-OpenAIãƒ¢ãƒ‡ãƒ«ã¨BigCodeã€OpenAssistantã‹ã‚‰ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚OpenAIãƒ¢ãƒ‡ãƒ«ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå„ªã‚Œã¦ã„ã¾ã™ãŒã€OpenAIã®APIã‚­ãƒ¼ãŒå¿…è¦ã§ã‚ã‚Šã€ç„¡æ–™ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ä¸€æ–¹ã€Hugging Faceã¯BigCodeã¨OpenAssistantãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¸ã®ç„¡æ–™ã‚¢ã‚¯ã‚»ã‚¹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n-\n-ã¾ãšã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ä¾å­˜é–¢ä¿‚ã‚’ã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã«`agents`ã®ã‚¨ã‚¯ã‚¹ãƒˆãƒ©ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚\n-\n-\n-```bash\n-pip install transformers[agents]\n-```\n-\n-OpenAIãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€`openai`ã®ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸå¾Œã€`OpenAiAgent`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚\n-\n-\n-```bash\n-pip install openai\n-```\n-\n-\n-```py\n-from transformers import OpenAiAgent\n-\n-agent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"<your_api_key>\")\n-```\n-\n-BigCodeã¾ãŸã¯OpenAssistantã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ã¾ãšãƒ­ã‚°ã‚¤ãƒ³ã—ã¦Inference APIã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n-from huggingface_hub import login\n-\n-login(\"<YOUR_TOKEN>\")\n-```\n-\n-æ¬¡ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n-from transformers import HfAgent\n-\n-# Starcoder\n-agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n-# StarcoderBase\n-# agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoderbase\")\n-# OpenAssistant\n-# agent = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n-```\n-\n-ã“ã‚Œã¯ã€Hugging FaceãŒç¾åœ¨ç„¡æ–™ã§æä¾›ã—ã¦ã„ã‚‹æ¨è«–APIã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆã¾ãŸã¯åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ï¼‰ã®ç‹¬è‡ªã®æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ãŠæŒã¡ã®å ´åˆã¯ã€ä¸Šè¨˜ã®URLã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’ã”è‡ªåˆ†ã®URLã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã§ç½®ãæ›ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-<Tip>\n-\n-StarCoderã¨OpenAssistantã¯ç„¡æ–™ã§åˆ©ç”¨ã§ãã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¿ã‚¹ã‚¯ã«ã¯éå¸¸ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚ãŸã ã—ã€ã‚ˆã‚Šè¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã™ã‚‹éš›ã«ã¯ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒååˆ†ã§ãªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãã®ã‚ˆã†ãªå ´åˆã«ã¯ã€ç¾æ™‚ç‚¹ã§ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã¯ãªã„ã‚‚ã®ã®ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹OpenAIãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ã¿ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-</Tip>\n-\n-ã“ã‚Œã§æº–å‚™ãŒæ•´ã„ã¾ã—ãŸï¼ã“ã‚Œã‹ã‚‰ã€ã‚ãªãŸãŒåˆ©ç”¨ã§ãã‚‹2ã¤ã®APIã«ã¤ã„ã¦è©³ã—ãèª¬æ˜ã—ã¾ã™ã€‚\n-\n-### Single execution (run)\n-\n-å˜ä¸€å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã® [`~Agent.run`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã§ã™ã€‚\n-\n-\n-```py\n-agent.run(\"Draw me a picture of rivers and lakes.\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" width=200>\n-\n-\n-ã“ã‚Œã¯ã€å®Ÿè¡Œã—ãŸã„ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸãƒ„ãƒ¼ãƒ«ï¼ˆã¾ãŸã¯ãƒ„ãƒ¼ãƒ«ï¼‰ã‚’è‡ªå‹•çš„ã«é¸æŠã—ã€é©åˆ‡ã«å®Ÿè¡Œã—ã¾ã™ã€‚1ã¤ã¾ãŸã¯è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’åŒã˜å‘½ä»¤ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼ˆãŸã ã—ã€å‘½ä»¤ãŒè¤‡é›‘ã§ã‚ã‚‹ã»ã©ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå¤±æ•—ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ããªã‚Šã¾ã™ï¼‰ã€‚\n-\n-\n-```py\n-agent.run(\"Draw me a picture of the sea then transform the picture to add an island\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sea_and_island.png\" width=200>\n-\n-<br/>\n-\n-[`~Agent.run`] æ“ä½œã¯ç‹¬ç«‹ã—ã¦å®Ÿè¡Œã§ãã¾ã™ã®ã§ã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã§ä½•åº¦ã‚‚å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n-\n-æ³¨æ„ç‚¹ã¨ã—ã¦ã€ã‚ãªãŸã® `agent` ã¯å˜ãªã‚‹å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ãŸã‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚ãšã‹ãªå¤‰æ›´ã§ã‚‚å®Œå…¨ã«ç•°ãªã‚‹çµæœãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€å®Ÿè¡Œã—ãŸã„ã‚¿ã‚¹ã‚¯ã‚’ã§ãã‚‹ã ã‘æ˜ç¢ºã«èª¬æ˜ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚è‰¯ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ›¸ãæ–¹ã«ã¤ã„ã¦ã¯ã€[ã“ã¡ã‚‰](custom_tools#writing-good-user-inputs) ã§è©³ã—ãèª¬æ˜ã—ã¦ã„ã¾ã™ã€‚\n-\n-å®Ÿè¡Œã”ã¨ã«çŠ¶æ…‹ã‚’ä¿æŒã—ãŸã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆä»¥å¤–ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«æ¸¡ã—ãŸã‚Šã™ã‚‹å ´åˆã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½¿ç”¨ã™ã‚‹å¤‰æ•°ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ä¾‹ãˆã°ã€æœ€åˆã®å·ã‚„æ¹–ã®ç”»åƒã‚’ç”Ÿæˆã—ã€ãã®ç”»åƒã«å³¶ã‚’è¿½åŠ ã™ã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã«æŒ‡ç¤ºã™ã‚‹ã«ã¯ã€æ¬¡ã®ã‚ˆã†ã«è¡Œã†ã“ã¨ãŒã§ãã¾ã™ï¼š\n-\n-```python\n-picture = agent.run(\"Generate a picture of rivers and lakes.\")\n-updated_picture = agent.run(\"Transform the image in `picture` to add an island to it.\", picture=picture)\n-```\n-\n-<Tip>\n-\n-ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ãªãŸã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç†è§£ã§ããªã„å ´åˆã‚„ã€ãƒ„ãƒ¼ãƒ«ã‚’æ··åŒã™ã‚‹å ´åˆã«å½¹ç«‹ã¤ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ä¾‹ãˆã°ï¼š\n-\n-```py\n-agent.run(\"Draw me the picture of a capybara swimming in the sea\")\n-```\n-\n-ã“ã“ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯2ã¤ã®æ–¹æ³•ã§è§£é‡ˆã§ãã¾ã™ï¼š\n-- `text-to-image`ã«æµ·ã§æ³³ãã‚«ãƒ”ãƒãƒ©ã‚’ç”Ÿæˆã•ã›ã‚‹\n-- ã¾ãŸã¯ã€`text-to-image`ã§ã‚«ãƒ”ãƒãƒ©ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’æµ·ã§æ³³ãŒã›ã‚‹ãŸã‚ã«`image-transformation`ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹\n-\n-æœ€åˆã®ã‚·ãƒŠãƒªã‚ªã‚’å¼·åˆ¶ã—ãŸã„å ´åˆã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¼•æ•°ã¨ã—ã¦æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼š\n-\n-\n-```py\n-agent.run(\"Draw me a picture of the `prompt`\", prompt=\"a capybara swimming in the sea\")\n-```\n-\n-</Tip>\n-\n-\n-### Chat-based execution (ãƒãƒ£ãƒƒãƒˆ)\n-\n-ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€[`~Agent.chat`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒãƒ£ãƒƒãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚‚å¯èƒ½ã§ã™ã€‚\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" width=200> \n-\n-```py\n-agent.chat(\"Transform the picture so that there is a rock in there\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes_and_beaver.png\" width=200>\n-\n-<br/>\n-\n-ã“ã‚Œã¯ã€æŒ‡ç¤ºã‚’ã¾ãŸã„ã§çŠ¶æ…‹ã‚’ä¿æŒã—ãŸã„å ´åˆã«ä¾¿åˆ©ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€å˜ä¸€ã®æŒ‡ç¤ºã«æ¯”ã¹ã¦è¤‡é›‘ãªæŒ‡ç¤ºã‚’å‡¦ç†ã™ã‚‹ã®ã¯é›£ã—ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼ˆãã®å ´åˆã¯ [`~Agent.run`] ãƒ¡ã‚½ãƒƒãƒ‰ã®æ–¹ãŒé©ã—ã¦ã„ã¾ã™ï¼‰ã€‚\n-\n-ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€éãƒ†ã‚­ã‚¹ãƒˆå‹ã®å¼•æ•°ã‚„ç‰¹å®šã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã—ãŸã„å ´åˆã«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚\n-\n-### âš ï¸ Remote execution\n-\n-ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ç›®çš„ã‚„ã™ã¹ã¦ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã§ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã€ãƒªãƒªãƒ¼ã‚¹ã®ãŸã‚ã«ã„ãã¤ã‹ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ„ãƒ¼ãƒ«ç”¨ã®ãƒªãƒ¢ãƒ¼ãƒˆå®Ÿè¡Œãƒ„ãƒ¼ãƒ«ã‚‚ä½œæˆã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã¯ [æ¨è«–ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ](https://huggingface.co/inference-endpoints) ã‚’ä½¿ç”¨ã—ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚\n-\n-ã“ã‚Œã‚‰ã¯ç¾åœ¨ã‚ªãƒ•ã«ãªã£ã¦ã„ã¾ã™ãŒã€ãƒªãƒ¢ãƒ¼ãƒˆå®Ÿè¡Œãƒ„ãƒ¼ãƒ«ã‚’è‡ªåˆ†ã§è¨­å®šã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã¯ã€[ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã‚¬ã‚¤ãƒ‰](./custom_tools) ã‚’èª­ã‚€ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-### What's happening here? What are tools, and what are agents?\n-\n-![ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ„ãƒ¼ãƒ«ã®ãƒ€ã‚¤ã‚¢ã‚°ãƒ©ãƒ ](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/diagram.png)\n-\n-#### Agents\n-\n-ã“ã“ã§ã®ã€Œã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€ã¨ã¯ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã“ã¨ã§ã‚ã‚Šã€ç‰¹å®šã®ä¸€é€£ã®ãƒ„ãƒ¼ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¨­å®šã—ã¦ã„ã¾ã™ã€‚\n-\n-LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€ã‚³ãƒ¼ãƒ‰ã®å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã®ã«ã‹ãªã‚Šå„ªã‚Œã¦ãŠã‚Šã€ã“ã®APIã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ç‰¹å®šã®ãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã®å°ã•ãªã‚µãƒ³ãƒ—ãƒ«ã‚’ç”Ÿæˆã•ã›ã‚‹ã“ã¨ã«åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚¿ã‚¹ã‚¯ã¨ãƒ„ãƒ¼ãƒ«ã®èª¬æ˜ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ„ãƒ¼ãƒ«ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-#### Tools\n-\n-ãƒ„ãƒ¼ãƒ«ã¯éå¸¸ã«å˜ç´”ã§ã€åå‰ã¨èª¬æ˜ã‹ã‚‰ãªã‚‹å˜ä¸€ã®é–¢æ•°ã§ã™ã€‚ãã‚Œã‹ã‚‰ã€ã“ã‚Œã‚‰ã®ãƒ„ãƒ¼ãƒ«ã®èª¬æ˜ã‚’ä½¿ç”¨ã—ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€šã˜ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã€ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã§è¦æ±‚ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’ã©ã®ã‚ˆã†ã«å®Ÿè¡Œã™ã‚‹ã‹ã‚’ç¤ºã—ã¾ã™ã€‚ç‰¹ã«ã€ãƒ„ãƒ¼ãƒ«ã®æœŸå¾…ã•ã‚Œã‚‹å…¥åŠ›ã¨å‡ºåŠ›ã‚’ç¤ºã—ã¾ã™ã€‚\n-\n-ã“ã‚Œã¯æ–°ã—ã„ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã¯ãªããƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ãªãœãªã‚‰ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯éå¸¸ã«åŸå­çš„ãªãƒ„ãƒ¼ãƒ«ã§ã‚ˆã‚Šè‰¯ã„ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã‹ã‚‰ã§ã™ã€‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¯ã‚ˆã‚Šãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œã€ã—ã°ã—ã°è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã¾ã™ã€‚ãƒ„ãƒ¼ãƒ«ã¯éå¸¸ã«å˜ç´”ãªã‚¿ã‚¹ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ã‚’æ„å›³ã—ã¦ã„ã¾ã™ã€‚\n-\n-#### Code-execution?!\n-\n-ã“ã®ã‚³ãƒ¼ãƒ‰ã¯ã€ãƒ„ãƒ¼ãƒ«ã¨ãƒ„ãƒ¼ãƒ«ã¨ä¸€ç·’ã«æ¸¡ã•ã‚Œã‚‹å…¥åŠ›ã®ã‚»ãƒƒãƒˆã§ã€å½“ç¤¾ã®å°è¦æ¨¡ãªPythonã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ã§å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚ã™ã§ã«æä¾›ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã¨printé–¢æ•°ã—ã‹å‘¼ã³å‡ºã™ã“ã¨ãŒã§ããªã„ãŸã‚ã€å®Ÿè¡Œã§ãã‚‹ã“ã¨ã¯ã™ã§ã«åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ã€‚Hugging Faceã®ãƒ„ãƒ¼ãƒ«ã«åˆ¶é™ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€å®‰å…¨ã ã¨è€ƒãˆã¦ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚\n-\n-ã•ã‚‰ã«ã€å±æ€§ã®æ¤œç´¢ã‚„ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯è¨±å¯ã—ã¦ãŠã‚‰ãšï¼ˆãã‚Œã‚‰ã¯æ¸¡ã•ã‚ŒãŸå…¥åŠ›/å‡ºåŠ›ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã«ã¯å¿…è¦ãªã„ã¯ãšã§ã™ï¼‰ã€æœ€ã‚‚æ˜ã‚‰ã‹ãªæ”»æ’ƒã¯å•é¡Œã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãã‚Œã‚‰ã‚’å‡ºåŠ›ã™ã‚‹ã‚ˆã†ã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚è¶…å®‰å…¨ãªå´ã«ç«‹ã¡ãŸã„å ´åˆã¯ã€è¿½åŠ ã®å¼•æ•° return_code=True ã‚’æŒ‡å®šã—ã¦ run() ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚ãã®å ´åˆã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯å®Ÿè¡Œã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™ã ã‘ã§ã€å®Ÿè¡Œã™ã‚‹ã‹ã©ã†ã‹ã¯ã‚ãªãŸæ¬¡ç¬¬ã§ã™ã€‚\n-\n-å®Ÿè¡Œã¯ã€é•æ³•ãªæ“ä½œã‚’è©¦ã¿ã‚‹è¡Œã¾ãŸã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç”Ÿæˆã—ãŸã‚³ãƒ¼ãƒ‰ã«é€šå¸¸ã®Pythonã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã«åœæ­¢ã—ã¾ã™ã€‚\n-\n-### A curated set of tools\n-\n-ç§ãŸã¡ã¯ã€ã“ã®ã‚ˆã†ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å¼·åŒ–ã§ãã‚‹ãƒ„ãƒ¼ãƒ«ã®ã‚»ãƒƒãƒˆã‚’ç‰¹å®šã—ã¾ã™ã€‚ä»¥ä¸‹ã¯ã€`transformers`ã«çµ±åˆã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã®æ›´æ–°ã•ã‚ŒãŸãƒªã‚¹ãƒˆã§ã™ï¼š\n-\n-- **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè³ªå•å¿œç­”**: ç”»åƒå½¢å¼ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆPDFãªã©ï¼‰ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã—ã¾ã™ï¼ˆ[Donut](./model_doc/donut)ï¼‰\n-- **ãƒ†ã‚­ã‚¹ãƒˆè³ªå•å¿œç­”**: é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¨è³ªå•ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆå†…ã®è³ªå•ã«å›ç­”ã—ã¾ã™ï¼ˆ[Flan-T5](./model_doc/flan-t5)ï¼‰\n-- **ç„¡æ¡ä»¶ã®ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³**: ç”»åƒã«ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ã‘ã¾ã™ï¼ï¼ˆ[BLIP](./model_doc/blip)ï¼‰\n-- **ç”»åƒè³ªå•å¿œç­”**: ç”»åƒãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãã®ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”ã—ã¾ã™ï¼ˆ[VILT](./model_doc/vilt)ï¼‰\n-- **ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**: ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯ã‚’å‡ºåŠ›ã—ã¾ã™ï¼ˆ[CLIPSeg](./model_doc/clipseg)ï¼‰\n-- **éŸ³å£°ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®å¤‰æ›**: äººã®è©±ã—å£°ã®ã‚ªãƒ¼ãƒ‡ã‚£ã‚ªéŒ²éŸ³ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãã®éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«è»¢è¨˜ã—ã¾ã™ï¼ˆ[Whisper](./model_doc/whisper)ï¼‰\n-- **ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›**: ãƒ†ã‚­ã‚¹ãƒˆã‚’éŸ³å£°ã«å¤‰æ›ã—ã¾ã™ï¼ˆ[SpeechT5](./model_doc/speecht5)ï¼‰\n-- **ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡**: ãƒ†ã‚­ã‚¹ãƒˆã¨ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆãŒæœ€ã‚‚å¯¾å¿œã™ã‚‹ãƒ©ãƒ™ãƒ«ã‚’è­˜åˆ¥ã—ã¾ã™ï¼ˆ[BART](./model_doc/bart)ï¼‰\n-- **ãƒ†ã‚­ã‚¹ãƒˆè¦ç´„**: é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’1ã¤ã¾ãŸã¯æ•°æ–‡ã«è¦ç´„ã—ã¾ã™ï¼ˆ[BART](./model_doc/bart)ï¼‰\n-- **ç¿»è¨³**: ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ‡å®šã•ã‚ŒãŸè¨€èªã«ç¿»è¨³ã—ã¾ã™ï¼ˆ[NLLB](./model_doc/nllb)ï¼‰\n-\n-ã“ã‚Œã‚‰ã®ãƒ„ãƒ¼ãƒ«ã¯transformersã«çµ±åˆã•ã‚Œã¦ãŠã‚Šã€æ‰‹å‹•ã§ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€æ¬¡ã®ã‚ˆã†ã«ä½¿ç”¨ã§ãã¾ã™ï¼š\n-\n-```py\n-from transformers import load_tool\n-\n-tool = load_tool(\"text-to-speech\")\n-audio = tool(\"This is a text to speech tool\")\n-```\n-\n-### Custom tools\n-\n-ç§ãŸã¡ã¯ã€å³é¸ã•ã‚ŒãŸãƒ„ãƒ¼ãƒ«ã®ã‚»ãƒƒãƒˆã‚’ç‰¹å®šã™ã‚‹ä¸€æ–¹ã€ã“ã®å®Ÿè£…ãŒæä¾›ã™ã‚‹ä¸»è¦ãªä¾¡å€¤ã¯ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã‚’è¿…é€Ÿã«ä½œæˆã—ã¦å…±æœ‰ã§ãã‚‹èƒ½åŠ›ã ã¨å¼·ãä¿¡ã˜ã¦ã„ã¾ã™ã€‚\n-\n-ãƒ„ãƒ¼ãƒ«ã®ã‚³ãƒ¼ãƒ‰ã‚’Hugging Face Spaceã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ç›´æ¥é€£æºã—ã¦ãƒ„ãƒ¼ãƒ«ã‚’æ´»ç”¨ã§ãã¾ã™ã€‚[`huggingface-tools` organization](https://huggingface.co/huggingface-tools)ã«ã¯ã€**transformerséä¾å­˜**ã®ã„ãã¤ã‹ã®ãƒ„ãƒ¼ãƒ«ãŒè¿½åŠ ã•ã‚Œã¾ã—ãŸï¼š\n-\n-- **ãƒ†ã‚­ã‚¹ãƒˆãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼**: ã‚¦ã‚§ãƒ–URLã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«\n-- **ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã€‚å®‰å®šã—ãŸæ‹¡æ•£ã‚’æ´»ç”¨ã—ã¾ã™\n-- **ç”»åƒå¤‰æ›**: åˆæœŸç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŒ‡å®šã—ã¦ç”»åƒã‚’å¤‰æ›´ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã€‚instruct pix2pixã®å®‰å®šã—ãŸæ‹¡æ•£ã‚’æ´»ç”¨ã—ã¾ã™\n-- **ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒ“ãƒ‡ã‚ªã¸**: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¾“ã£ã¦å°ã•ãªãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã€‚damo-vilabã‚’æ´»ç”¨ã—ã¾ã™\n-\n-æœ€åˆã‹ã‚‰ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ãƒ„ãƒ¼ãƒ«ã¯ã€[*huggingface-tools/text-to-image*](https://huggingface.co/spaces/huggingface-tools/text-to-image)ã«ã‚ã‚‹ãƒªãƒ¢ãƒ¼ãƒˆãƒ„ãƒ¼ãƒ«ã§ã™ï¼ä»Šå¾Œã‚‚ã€ã“ã®çµ„ç¹”ãŠã‚ˆã³ä»–ã®çµ„ç¹”ã«ã•ã‚‰ã«ã“ã®ã‚ˆã†ãªãƒ„ãƒ¼ãƒ«ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã€ã“ã®å®Ÿè£…ã‚’ã•ã‚‰ã«å¼·åŒ–ã—ã¦ã„ãã¾ã™ã€‚\n-\n-ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§[`huggingface-tools`](https://huggingface.co/huggingface-tools)ã«ã‚ã‚‹ãƒ„ãƒ¼ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã™ã€‚\n-ãƒ„ãƒ¼ãƒ«ã®ä½œæˆã¨å…±æœ‰æ–¹æ³•ã€ã¾ãŸHubã«å­˜åœ¨ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ„ãƒ¼ãƒ«ã‚’æ´»ç”¨ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦ã®è©³ç´°ã¯ã€[æ¬¡ã®ã‚¬ã‚¤ãƒ‰](custom_tools)ã§èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚\n-\n-### Code generation\n-\n-ã“ã‚Œã¾ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚ãªãŸã®ãŸã‚ã«ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã—ãŸã€‚ãŸã ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã™ã‚‹ã ã‘ã§ã€éå¸¸ã«åˆ¶é™ã•ã‚ŒãŸPythonã‚¤ãƒ³ã‚¿ãƒ¼ãƒ—ãƒªã‚¿ã‚’ä½¿ç”¨ã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸã‚³ãƒ¼ãƒ‰ã‚’ç•°ãªã‚‹ç’°å¢ƒã§ä½¿ç”¨ã—ãŸã„å ´åˆã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™ã‚ˆã†ã«æŒ‡ç¤ºã§ãã¾ã™ã€‚ãƒ„ãƒ¼ãƒ«ã®å®šç¾©ã¨æ­£ç¢ºãªã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚‚å«ã‚ã¦ã€‚\n-\n-ä¾‹ãˆã°ã€ä»¥ä¸‹ã®å‘½ä»¤ï¼š\n-```python\n-agent.run(\"Draw me a picture of rivers and lakes\", return_code=True)\n-```\n-\n-æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã—ã¾ã™\n-```python\n-from transformers import load_tool\n-\n-image_generator = load_tool(\"huggingface-tools/text-to-image\")\n-\n-image = image_generator(prompt=\"rivers and lakes\")\n-```\n-\n-ãã®å¾Œã€è‡ªåˆ†ã§å¤‰æ›´ã—ã¦å®Ÿè¡Œã§ãã¾ã™ã€‚"
        },
        {
            "sha": "29518d9a07233578e066d7e44cea65283273d58c",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -23,8 +23,6 @@\n     title: ğŸ¤— PEFTë¡œ ì–´ëŒ‘í„° ë¡œë“œ ë° í•™ìŠµí•˜ê¸°\n   - local: model_sharing\n     title: ë§Œë“  ëª¨ë¸ ê³µìœ í•˜ê¸°\n-  - local: transformers_agents\n-    title: ì—ì´ì „íŠ¸\n   - local: llm_tutorial\n     title: ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ ìƒì„±í•˜ê¸°\n   - local: conversations\n@@ -248,8 +246,6 @@\n   title: (ë²ˆì—­ì¤‘) ê°œë… ê°€ì´ë“œ\n - sections:\n   - sections:\n-    - local: main_classes/agent\n-      title: ì—ì´ì „íŠ¸ì™€ ë„êµ¬\n     - local: model_doc/auto\n       title: ìë™ í´ë˜ìŠ¤\n     - local: in_translation"
        },
        {
            "sha": "d0ef630e2cdf77f4d5755acdc06666c1bd78e88d",
            "filename": "docs/source/ko/main_classes/agent.md",
            "status": "removed",
            "additions": 0,
            "deletions": 134,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fko%2Fmain_classes%2Fagent.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fko%2Fmain_classes%2Fagent.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fagent.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,134 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# ì—ì´ì „íŠ¸ & ë„êµ¬ [[agents-tools]]\n-\n-<Tip warning={true}>\n-\n-Transformers AgentëŠ” ì‹¤í—˜ ì¤‘ì¸ APIì´ë¯€ë¡œ ì–¸ì œë“ ì§€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n-APIë‚˜ ê¸°ë°˜ ëª¨ë¸ì´ ìì£¼ ì—…ë°ì´íŠ¸ë˜ë¯€ë¡œ, ì—ì´ì „íŠ¸ê°€ ì œê³µí•˜ëŠ” ê²°ê³¼ë¬¼ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-</Tip>\n-\n-ì—ì´ì „íŠ¸ì™€ ë„êµ¬ì— ëŒ€í•´ ë” ì•Œì•„ë³´ë ¤ë©´ [ì†Œê°œ ê°€ì´ë“œ](../transformers_agents)ë¥¼ ê¼­ ì½ì–´ë³´ì„¸ìš”. \n-ì´ í˜ì´ì§€ì—ëŠ” ê¸°ë³¸ í´ë˜ìŠ¤ì— ëŒ€í•œ API ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n-\n-## ì—ì´ì „íŠ¸ [[agents]]\n-\n-ìš°ë¦¬ëŠ” ê¸°ë³¸ [`Agent`] í´ë˜ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‘ ê°€ì§€ ìœ í˜•ì˜ ì—ì´ì „íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤:\n-- [`CodeAgent`]ëŠ” í•œ ë²ˆì— ë™ì‘í•©ë‹ˆë‹¤. ì‘ì—…ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì½”ë“œë¥¼ ìƒì„±í•œ ë‹¤ìŒ, ë°”ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.\n-- [`ReactAgent`]ëŠ” ë‹¨ê³„ë³„ë¡œ ë™ì‘í•˜ë©°, ê° ë‹¨ê³„ëŠ” í•˜ë‚˜ì˜ ìƒê°, í•˜ë‚˜ì˜ ë„êµ¬ í˜¸ì¶œ ë° ì‹¤í–‰ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì´ ì—ì´ì „íŠ¸ì—ëŠ” ë‘ ê°€ì§€ í´ë˜ìŠ¤ê°€ ìˆìŠµë‹ˆë‹¤:\n-  - [`ReactJsonAgent`]ëŠ” ë„êµ¬ í˜¸ì¶œì„ JSONìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\n-  - [`ReactCodeAgent`]ëŠ” ë„êµ¬ í˜¸ì¶œì„ Python ì½”ë“œë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\n-\n-### Agent [[agent]]\n-\n-[[autodoc]] Agent\n-\n-### CodeAgent [[codeagent]]\n-\n-[[autodoc]] CodeAgent\n-\n-### React agents [[react-agents]]\n-\n-[[autodoc]] ReactAgent\n-\n-[[autodoc]] ReactJsonAgent\n-\n-[[autodoc]] ReactCodeAgent\n-\n-## Tools [[tools]]\n-\n-### load_tool [[loadtool]]\n-\n-[[autodoc]] load_tool\n-\n-### Tool [[tool]]\n-\n-[[autodoc]] Tool\n-\n-### Toolbox [[toolbox]]\n-\n-[[autodoc]] Toolbox\n-\n-### PipelineTool [[pipelinetool]]\n-\n-[[autodoc]] PipelineTool\n-\n-### launch_gradio_demo [[launchgradiodemo]]\n-\n-[[autodoc]] launch_gradio_demo\n-\n-### ToolCollection [[toolcollection]]\n-\n-[[autodoc]] ToolCollection\n-\n-## ì—”ì§„ [[engines]]\n-\n-ì—ì´ì „íŠ¸ í”„ë ˆì„ì›Œí¬ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—”ì§„ì„ ììœ ë¡­ê²Œ ë§Œë“¤ê³  ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-ì´ ì—”ì§„ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ì–‘ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤:\n-1. ì…ë ¥(`List[Dict[str, str]]`)ì— ëŒ€í•œ [ë©”ì‹œì§€ í˜•ì‹](../chat_templating.md)ì„ ë”°ë¥´ê³  ë¬¸ìì—´ì„ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n-2. ì¸ìˆ˜ `stop_sequences`ì— ì‹œí€€ìŠ¤ê°€ ì „ë‹¬ë˜ê¸° *ì „ì—* ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ê²ƒì„ ì¤‘ì§€í•´ì•¼ í•©ë‹ˆë‹¤.\n-\n-### HfApiEngine [[HfApiEngine]]\n-\n-í¸ì˜ë¥¼ ìœ„í•´, ìœ„ì˜ ì‚¬í•­ì„ êµ¬í˜„í•˜ê³  ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì‹¤í–‰ì„ ìœ„í•´ ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” `HfApiEngine`ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.\n-\n-```python\n->>> from transformers import HfApiEngine\n-\n->>> messages = [\n-...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n-...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n-...   {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n-... ]\n-\n->>> HfApiEngine()(messages, stop_sequences=[\"conversation\"])\n-\n-\"That's very kind of you to say! It's always nice to have a relaxed \"\n-```\n-\n-[[autodoc]] HfApiEngine\n-\n-\n-## ì—ì´ì „íŠ¸ ìœ í˜• [[agent-types]]\n-\n-ì—ì´ì „íŠ¸ëŠ” ë„êµ¬ ê°„ì˜ ëª¨ë“  ìœ í˜•ì˜ ê°ì²´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤; ë„êµ¬ëŠ” ì™„ì „íˆ ë©€í‹°ëª¨ë‹¬ì´ë¯€ë¡œ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤, ë¹„ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ìœ í˜•ì„ ìˆ˜ë½í•˜ê³  ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n-ë„êµ¬ ê°„ì˜ í˜¸í™˜ì„±ì„ ë†’ì´ê³  ipython (jupyter, colab, ipython ë…¸íŠ¸ë¶, ...)ì—ì„œ ì´ëŸ¬í•œ \n-ë°˜í™˜ ê°’ì„ ì˜¬ë°”ë¥´ê²Œ ë Œë”ë§í•˜ê¸° ìœ„í•´ ì´ëŸ¬í•œ ìœ í˜•ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë˜í¼ í´ë˜ìŠ¤ë¥¼ \n-êµ¬í˜„í•©ë‹ˆë‹¤.\n-\n-ë˜í•‘ëœ ê°ì²´ëŠ” ì²˜ìŒê³¼ ë™ì¼í•˜ê²Œ ì‘ë™í•´ì•¼ í•©ë‹ˆë‹¤; í…ìŠ¤íŠ¸ ê°ì²´ëŠ” ì—¬ì „íˆ ë¬¸ìì—´ë¡œ ì‘ë™í•´ì•¼ í•˜ë©°, \n-ì´ë¯¸ì§€ ê°ì²´ëŠ” ì—¬ì „íˆ `PIL.Image`ë¡œ ì‘ë™í•´ì•¼ í•©ë‹ˆë‹¤.\n-\n-ì´ëŸ¬í•œ ìœ í˜•ì—ëŠ” ì„¸ ê°€ì§€ íŠ¹ì • ëª©ì ì´ ìˆìŠµë‹ˆë‹¤:\n-\n-- `to_raw`ë¥¼ í˜¸ì¶œí•˜ë©´ ê¸°ë³¸ ê°ì²´ê°€ ë°˜í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n-- `to_string`ì„ í˜¸ì¶œí•˜ë©´ ê°ì²´ê°€ ë¬¸ìì—´ë¡œ ë°˜í™˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤: \n-`AgentText`ì˜ ê²½ìš° ë¬¸ìì—´ì´ ë  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ë¥¸ ê²½ìš°ì—ëŠ” ê°ì²´ì˜ ì§ë ¬í™”ëœ ë²„ì „ì˜ ê²½ë¡œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-- ipython ì»¤ë„ì—ì„œ í‘œì‹œí•  ë•Œ ê°ì²´ê°€ ì˜¬ë°”ë¥´ê²Œ í‘œì‹œë˜ì–´ì•¼ í•©ë‹ˆë‹¤.\n-\n-### AgentText [[agenttext]]\n-\n-[[autodoc]] transformers.agents.agent_types.AgentText\n-\n-### AgentImage [[agentimage]]\n-\n-[[autodoc]] transformers.agents.agent_types.AgentImage\n-\n-### AgentAudio [[agentaudio]]\n-\n-[[autodoc]] transformers.agents.agent_types.AgentAudio"
        },
        {
            "sha": "eeb00761e9a7775e9d17315772680478b3aa99dc",
            "filename": "docs/source/ko/transformers_agents.md",
            "status": "removed",
            "additions": 0,
            "deletions": 328,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fko%2Ftransformers_agents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fko%2Ftransformers_agents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftransformers_agents.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,328 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Transformers Agent [[transformers-agent]]\n-\n-<Tip warning={true}>\n-\n-Transformers AgentëŠ” ì‹¤í—˜ ì¤‘ì¸ APIë¡œ ì–¸ì œë“ ì§€ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n-API ë˜ëŠ” ê¸°ë°˜ ëª¨ë¸ì´ ë³€ê²½ë˜ê¸° ì‰½ê¸° ë•Œë¬¸ì— ì—ì´ì „íŠ¸ê°€ ë°˜í™˜í•˜ëŠ” ê²°ê³¼ë„ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-</Tip>\n-\n-Transformers ë²„ì „ 4.29.0.ì—ì„œ *ë„êµ¬*ì™€ *ì—ì´ì „íŠ¸*ë¼ëŠ” ì»¨ì…‰ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. [ì´ colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj)ì—ì„œ ì‚¬ìš©í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-ê°„ë‹¨íˆ ë§í•˜ë©´, AgentëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ ìœ„ì— ìì—°ì–´ APIë¥¼ ì œê³µí•©ë‹ˆë‹¤. \n-ì—„ì„ ëœ ë„êµ¬ ì„¸íŠ¸ë¥¼ ì •ì˜í•˜ê³ , ìì—°ì–´ë¥¼ í•´ì„í•˜ì—¬ ì´ëŸ¬í•œ ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ë¥¼ ì„¤ê³„í–ˆìŠµë‹ˆë‹¤. \n-ì´ APIëŠ” í™•ì¥ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ ë˜ì—ˆìŠµë‹ˆë‹¤. \n-ì£¼ìš” ë„êµ¬ë¥¼ ì„ ë³„í•´ë‘ì—ˆì§€ë§Œ, ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ê°œë°œí•œ ëª¨ë“  ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì‹œìŠ¤í…œì„ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë„ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n-\n-ëª‡ ê°€ì§€ ì˜ˆë¥¼ í†µí•´ ìƒˆë¡œìš´ APIë¡œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. \n-ì´ APIëŠ” íŠ¹íˆ ë©€í‹°ëª¨ë‹¬ ì‘ì—…ì—ì„œ ê°•ë ¥í•˜ë¯€ë¡œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì†Œë¦¬ë‚´ì–´ ì½ì–´ë³´ê² ìŠµë‹ˆë‹¤.\n-\n-```py\n-agent.run(\"Caption the following image\", image=image)\n-```\n-\n-| **Input**                                                                                                                   | **Output**                        |\n-|-----------------------------------------------------------------------------------------------------------------------------|-----------------------------------|\n-| <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/beaver.png\" width=200> | A beaver is swimming in the water |\n-\n----\n-\n-```py\n-agent.run(\"Read the following text out loud\", text=text)\n-```\n-| **Input**                                                                                                               | **Output**                                   |\n-|-------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|\n-| A beaver is swimming in the water | <audio controls><source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tts_example.wav\" type=\"audio/wav\"> your browser does not support the audio element. </audio>\n-\n----\n-\n-```py\n-agent.run(\n-    \"In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?\",\n-    document=document,\n-)\n-```\n-| **Input**                                                                                                                   | **Output**     |\n-|-----------------------------------------------------------------------------------------------------------------------------|----------------|\n-| <img src=\"https://datasets-server.huggingface.co/assets/hf-internal-testing/example-documents/--/hf-internal-testing--example-documents/test/0/image/image.jpg\" width=200> | ballroom foyer |\n-\n-## ë°”ë¡œ ì‹œì‘í•˜ê¸° [[quickstart]]\n-\n-`agent.run`ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì¸ ì—ì´ì „íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•´ì•¼ í•©ë‹ˆë‹¤. \n-ì €í¬ëŠ” openAI ëª¨ë¸ë¿ë§Œ ì•„ë‹ˆë¼ BigCode ë° OpenAssistantì˜ ì˜¤í”ˆì†ŒìŠ¤ ëŒ€ì²´ ëª¨ë¸ë„ ì§€ì›í•©ë‹ˆë‹¤. \n-openAI ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ë” ìš°ìˆ˜í•˜ì§€ë§Œ(ë‹¨, openAI API í‚¤ê°€ í•„ìš”í•˜ë¯€ë¡œ ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ), \n-Hugging FaceëŠ” BigCodeì™€ OpenAssistant ëª¨ë¸ì˜ ì—”ë“œí¬ì¸íŠ¸ì— ëŒ€í•œ ë¬´ë£Œ ì•¡ì„¸ìŠ¤ë¥¼ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.\n-\n-ìš°ì„  ëª¨ë“  ê¸°ë³¸ ì¢…ì†ì„±ì„ ì„¤ì¹˜í•˜ë ¤ë©´ `agents`ë¥¼ ì¶”ê°€ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.\n-```bash\n-pip install transformers[agents]\n-```\n-\n-openAI ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ `openai` ì¢…ì†ì„±ì„ ì„¤ì¹˜í•œ í›„ [`OpenAiAgent`]ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤:\n-\n-```bash\n-pip install openai\n-```\n-\n-\n-```py\n-from transformers import OpenAiAgent\n-\n-agent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"<your_api_key>\")\n-```\n-\n-BigCode ë˜ëŠ” OpenAssistantë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € ë¡œê·¸ì¸í•˜ì—¬ Inference APIì— ì•¡ì„¸ìŠ¤í•˜ì„¸ìš”:\n-\n-```py\n-from huggingface_hub import login\n-\n-login(\"<YOUR_TOKEN>\")\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ì—ì´ì „íŠ¸ë¥¼ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.\n-\n-```py\n-from transformers import HfAgent\n-\n-# Starcoder\n-agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n-# StarcoderBase\n-# agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoderbase\")\n-# OpenAssistant\n-# agent = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\")\n-```\n-\n-í˜„ì¬ Hugging Faceì—ì„œ ë¬´ë£Œë¡œ ì œê³µí•˜ëŠ” ì¶”ë¡  APIë¥¼ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. \n-ì´ ëª¨ë¸ì— ëŒ€í•œ ìì²´ ì¶”ë¡  ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°(ë˜ëŠ” ë‹¤ë¥¸ ì—”ë“œí¬ì¸íŠ¸ê°€ ìˆëŠ” ê²½ìš°) ìœ„ì˜ URLì„ í•´ë‹¹ URL ì—”ë“œí¬ì¸íŠ¸ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-<Tip>\n-\n-StarCoderì™€ OpenAssistantëŠ” ë¬´ë£Œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©° ê°„ë‹¨í•œ ì‘ì—…ì—ì„œ ë†€ë¼ìš¸ ì •ë„ë¡œ ì˜ ì‘ë™í•©ë‹ˆë‹¤. \n-ê·¸ëŸ¬ë‚˜ ë” ë³µì¡í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•  ë•ŒëŠ” ì²´í¬í¬ì¸íŠ¸ê°€ ì˜ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n-ì´ëŸ¬í•œ ë¬¸ì œê°€ ë°œìƒí•˜ë©´ OpenAI ëª¨ë¸ì„ ì‚¬ìš©í•´ ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤. ì•„ì‰½ê²Œë„ ì˜¤í”ˆì†ŒìŠ¤ëŠ” ì•„ë‹ˆì§€ë§Œ í˜„ì¬ë¡œì„œëŠ” ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n-\n-</Tip>\n-\n-ì´ì œ ì¤€ë¹„ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì´ì œ ììœ ë¡­ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë‘ ê°€ì§€ APIì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n-\n-### ë‹¨ì¼ ì‹¤í–‰ (run) [[single-execution-(run)]] \n-\n-ë‹¨ì¼ ì‹¤í–‰ ë°©ë²•ì€ ì—ì´ì „íŠ¸ì˜ [`~Agent.run`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì…ë‹ˆë‹¤:\n-\n-```py\n-agent.run(\"Draw me a picture of rivers and lakes.\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" width=200>\n-\n-ìˆ˜í–‰í•˜ë ¤ëŠ” ì‘ì—…ì— ì í•©í•œ ë„êµ¬ë¥¼ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ì ì ˆí•˜ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤. \n-ë™ì¼í•œ ëª…ë ¹ì–´ì—ì„œ í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ ê°œì˜ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n-(ë‹¤ë§Œ, ëª…ë ¹ì–´ê°€ ë³µì¡í• ìˆ˜ë¡ ì—ì´ì „íŠ¸ê°€ ì‹¤íŒ¨í•  ê°€ëŠ¥ì„±ì´ ë†’ì•„ì§‘ë‹ˆë‹¤).\n-\n-```py\n-agent.run(\"Draw me a picture of the sea then transform the picture to add an island\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sea_and_island.png\" width=200>\n-\n-<br/>\n-\n-\n-ëª¨ë“  [`~Agent.run`] ì‘ì—…ì€ ë…ë¦½ì ì´ë¯€ë¡œ ë‹¤ë¥¸ ì‘ì—…ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ì—°ì†í•´ì„œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-`agent`ëŠ” í° ì–¸ì–´ ëª¨ë¸ì¼ ë¿ì´ë¯€ë¡œ í”„ë¡¬í”„íŠ¸ì— ì•½ê°„ì˜ ë³€í™”ë¥¼ ì£¼ë©´ ì™„ì „íˆ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì„¸ìš”. \n-ìˆ˜í–‰í•˜ë ¤ëŠ” ì‘ì—…ì„ ìµœëŒ€í•œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. \n-ì¢‹ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ëŠ” ë°©ë²•ì€ [ì—¬ê¸°](custom_tools#writing-good-user-inputs)ì—ì„œ ìì„¸íˆ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-ì—¬ëŸ¬ ì‹¤í–‰ì— ê±¸ì³ ìƒíƒœë¥¼ ìœ ì§€í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê°œì²´ë¥¼ ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•˜ë ¤ëŠ” ê²½ìš°ì—ëŠ” ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  ë³€ìˆ˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n-ì˜ˆë¥¼ ë“¤ì–´ ê°•ê³¼ í˜¸ìˆ˜ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•œ ë’¤, \n-ëª¨ë¸ì´ í•´ë‹¹ ê·¸ë¦¼ì— ì„¬ì„ ì¶”ê°€í•˜ë„ë¡ ë‹¤ìŒê³¼ ê°™ì´ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```python\n-picture = agent.run(\"Generate a picture of rivers and lakes.\")\n-updated_picture = agent.run(\"Transform the image in `picture` to add an island to it.\", picture=picture)\n-```\n-\n-<Tip>\n-\n-ì´ ë°©ë²•ì€ ëª¨ë¸ì´ ìš”ì²­ì„ ì´í•´í•˜ì§€ ëª»í•˜ê³  ë„êµ¬ë¥¼ í˜¼í•©í•  ë•Œ ìœ ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n-\n-```py\n-agent.run(\"Draw me the picture of a capybara swimming in the sea\")\n-```\n-\n-ì—¬ê¸°ì„œ ëª¨ë¸ì€ ë‘ ê°€ì§€ ë°©ì‹ìœ¼ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-- `text-to-image`ì´ ë°”ë‹¤ì—ì„œ í—¤ì—„ì¹˜ëŠ” ì¹´í”¼ë°”ë¼ë¥¼ ìƒì„±í•˜ë„ë¡ í•©ë‹ˆë‹¤.\n-- ë˜ëŠ” `text-to-image`ì´ ì¹´í”¼ë°”ë¼ë¥¼ ìƒì„±í•œ ë‹¤ìŒ `image-transformation` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ë‹¤ì—ì„œ í—¤ì—„ì¹˜ë„ë¡ í•©ë‹ˆë‹¤.\n-\n-ì²« ë²ˆì§¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê°•ì œë¡œ ì‹¤í–‰í•˜ë ¤ë©´ í”„ë¡¬í”„íŠ¸ë¥¼ ì¸ìˆ˜ë¡œ ì „ë‹¬í•˜ì—¬ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n-agent.run(\"Draw me a picture of the `prompt`\", prompt=\"a capybara swimming in the sea\")\n-```\n-\n-</Tip>\n-\n-\n-### ëŒ€í™” ê¸°ë°˜ ì‹¤í–‰ (chat) [[chat-based-execution-(chat)]]\n-\n-ì—ì´ì „íŠ¸ëŠ” [`~Agent.chat`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€í™” ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ë„ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n-agent.chat(\"Generate a picture of rivers and lakes\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\" width=200> \n-\n-```py\n-agent.chat(\"Transform the picture so that there is a rock in there\")\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes_and_beaver.png\" width=200>\n-\n-<br/>\n-\n-ì´ ë°©ì‹ì€ ì—¬ëŸ¬ ëª…ë ¹ì–´ì— ê±¸ì³ ìƒíƒœë¥¼ ìœ ì§€í•˜ê³ ì í•  ë•Œ í¥ë¯¸ë¡œìš´ ì ‘ê·¼ ë°©ì‹ì…ë‹ˆë‹¤. \n-ì‹¤í—˜ìš©ìœ¼ë¡œ ë” ì¢‹ì§€ë§Œ ë³µì¡í•œ ëª…ë ¹ì–´ë³´ë‹¤ëŠ” \n-ë‹¨ì¼ ëª…ë ¹ì–´([`~Agent.run`] ë©”ì†Œë“œê°€ ë” ì˜ ì²˜ë¦¬í•˜ëŠ” ëª…ë ¹ì–´)ì— í›¨ì”¬ ë” ì˜ ì‘ë™í•˜ëŠ” ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤.\n-\n-ì´ ë©”ì†Œë“œëŠ” í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ìœ í˜•ì´ë‚˜ íŠ¹ì • í”„ë¡¬í”„íŠ¸ë¥¼ ì „ë‹¬í•˜ë ¤ëŠ” ê²½ìš° ì¸ìˆ˜ë¥¼ ë°›ì„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n-\n-### âš ï¸ ì›ê²© ì‹¤í–‰ [[remote-execution]]\n-\n-ë°ëª¨ ëª©ì ê³¼ ëª¨ë“  ì„¤ì •ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ \n-ì—ì´ì „íŠ¸ê°€ ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ëª‡ ê°€ì§€ ê¸°ë³¸ ë„êµ¬ì— ëŒ€í•œ ì›ê²© ì‹¤í–‰ê¸°ë¥¼ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. \n-ì´ëŸ¬í•œ ë„êµ¬ëŠ” [inference endpoints](https://huggingface.co/inference-endpoints)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë§Œë“¤ì–´ì¡ŒìŠµë‹ˆë‹¤. \n-ì›ê²© ì‹¤í–‰ê¸° ë„êµ¬ë¥¼ ì§ì ‘ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ë³´ë ¤ë©´ [ì‚¬ìš©ì ì •ì˜ ë„êµ¬ ê°€ì´ë“œ](./custom_tools)ë¥¼ ì½ì–´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n-\n-ì›ê²© ë„êµ¬ë¡œ ì‹¤í–‰í•˜ë ¤ë©´ [`~Agent.run`] ë˜ëŠ” [`~Agent.chat`] ì¤‘ í•˜ë‚˜ì— `remote=True`ë¥¼ ì§€ì •í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.\n-\n-ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ëª…ë ¹ì€ ë§ì€ RAMì´ë‚˜ GPU ì—†ì´ë„ ëª¨ë“  ì¥ì¹˜ì—ì„œ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n-agent.run(\"Draw me a picture of rivers and lakes\", remote=True)\n-```\n-\n-[`~Agent.chat`]ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤:\n-\n-```py\n-agent.chat(\"Draw me a picture of rivers and lakes\", remote=True)\n-```\n-\n-### ì—¬ê¸°ì„œ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ” ê±°ì£ ? ë„êµ¬ë€ ë¬´ì—‡ì´ê³ , ì—ì´ì „íŠ¸ë€ ë¬´ì—‡ì¸ê°€ìš”? [[whats-happening-here-what-are-tools-and-what-are-agents]]\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/diagram.png\">\n-\n-#### ì—ì´ì „íŠ¸ [[agents]]\n-\n-ì—¬ê¸°ì„œ \"ì—ì´ì „íŠ¸\"ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ë©°, íŠ¹ì • ë„êµ¬ ëª¨ìŒì— ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡ í”„ë¡¬í”„íŠ¸í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n-\n-LLMì€ ì‘ì€ ì½”ë“œ ìƒ˜í”Œì„ ìƒì„±í•˜ëŠ” ë° ìƒë‹¹íˆ ëŠ¥ìˆ™í•˜ë¯€ë¡œ,\n-ì´ ì¥ì ì„ í™œìš©í•´ ë„êµ¬ ëª¨ìŒì„ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‘ì€ ì½”ë“œ ìƒ˜í”Œì„ ì œê³µí•˜ë¼ëŠ” ë©”ì‹œì§€ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. \n-ê·¸ëŸ° ë‹¤ìŒ ì—ì´ì „íŠ¸ì—ê²Œ ì œê³µí•˜ëŠ” ì‘ì—…ê³¼ ì œê³µí•˜ëŠ” ë„êµ¬ì— ëŒ€í•œ ì„¤ëª…ìœ¼ë¡œ ì´ í”„ë¡¬í”„íŠ¸ê°€ ì™„ë£Œë©ë‹ˆë‹¤. \n-ì´ë ‡ê²Œ í•˜ë©´ ì‚¬ìš© ì¤‘ì¸ ë„êµ¬ë“¤ì˜ ë¬¸ì„œì— ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë©°, í•´ë‹¹ ë„êµ¬ë“¤ì˜ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì˜ˆìƒí•˜ê³ , ê´€ë ¨ëœ ì½”ë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-#### ë„êµ¬ [[tools]]\n-\n-ë„êµ¬ëŠ” ë§¤ìš° ê°„ë‹¨í•©ë‹ˆë‹¤. ì´ë¦„ê³¼ ì„¤ëª…ì´ ìˆëŠ” ë‹¨ì¼ ê¸°ëŠ¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n-ê·¸ëŸ° ë‹¤ìŒ ì´ëŸ¬í•œ ë„êµ¬ì˜ ì„¤ëª…ì„ ì‚¬ìš©í•˜ì—¬ ìƒë‹´ì›ì—ê²Œ í”„ë¡¬í”„íŠ¸ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. \n-ì´ í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ìƒë‹´ì›ì—ê²Œ ì¿¼ë¦¬ì—ì„œ ìš”ì²­ëœ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n-\n-ì—ì´ì „íŠ¸ê°€ ë§¤ìš° ì›ìì ì¸ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë” ë‚˜ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ê¸° ë•Œë¬¸ì— íŒŒì´í”„ë¼ì¸ì´ ì•„ë‹Œ ì™„ì „íˆ ìƒˆë¡œìš´ ë„êµ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. \n-íŒŒì´í”„ë¼ì¸ì€ ë” ë§ì´ ë¦¬íŒ©í„°ë§ë˜ë©° ì¢…ì¢… ì—¬ëŸ¬ ì‘ì—…ì„ í•˜ë‚˜ë¡œ ê²°í•©í•©ë‹ˆë‹¤. \n-ë„êµ¬ëŠ” í•˜ë‚˜ì˜ ë§¤ìš° ê°„ë‹¨í•œ ì‘ì—…ì—ë§Œ ì§‘ì¤‘í•˜ë„ë¡ ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n-\n-#### ì½”ë“œ ì‹¤í–‰?! [[code-execution]]\n-\n-ê·¸ëŸ° ë‹¤ìŒ ì´ ì½”ë“œëŠ” ë„êµ¬ì™€ í•¨ê»˜ ì „ë‹¬ëœ ì…ë ¥ ì„¸íŠ¸ì— ëŒ€í•´ ì‘ì€ Python ì¸í„°í”„ë¦¬í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰ë©ë‹ˆë‹¤. \n-\"ì„ì˜ ì½”ë“œ ì‹¤í–‰ì´ë¼ë‹ˆ!\"ì´ë¼ê³  ë¹„ëª…ì„ ì§€ë¥´ëŠ” ì†Œë¦¬ê°€ ë“¤ë¦¬ê² ì§€ë§Œ, ê·¸ë ‡ì§€ ì•Šì€ ì´ìœ ë¥¼ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.\n-\n-í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ëŠ” ì œê³µí•œ ë„êµ¬ì™€ ì¸ì‡„ ê¸°ëŠ¥ë¿ì´ë¯€ë¡œ ì´ë¯¸ ì‹¤í–‰í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ ì œí•œë˜ì–´ ìˆìŠµë‹ˆë‹¤. \n-Hugging Face ë„êµ¬ë¡œ ì œí•œë˜ì–´ ìˆë‹¤ë©´ ì•ˆì „í•  ê²ƒì…ë‹ˆë‹¤. \n-\n-ê·¸ë¦¬ê³  ì–´íŠ¸ë¦¬ë·°íŠ¸ ì¡°íšŒë‚˜ ê°€ì ¸ì˜¤ê¸°ë¥¼ í—ˆìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ\n-(ì–´ì°¨í”¼ ì‘ì€ í•¨ìˆ˜ ì§‘í•©ì— ì…/ì¶œë ¥ì„ ì „ë‹¬í•  ë•ŒëŠ” í•„ìš”í•˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤) \n-ê°€ì¥ ëª…ë°±í•œ ê³µê²©(ì–´ì°¨í”¼ LLMì— ì¶œë ¥í•˜ë¼ëŠ” ë©”ì‹œì§€ë¥¼ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤)ì€ ë¬¸ì œê°€ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n-ë§¤ìš° ì•ˆì „í•˜ê²Œ í•˜ê³  ì‹¶ë‹¤ë©´ ì¶”ê°€ ì¸ìˆ˜ return_code=Trueë¥¼ ì‚¬ìš©í•˜ì—¬ run() ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.\n-ì´ ê²½ìš° ì—ì´ì „íŠ¸ê°€ ì‹¤í–‰í•  ì½”ë“œë¥¼ ë°˜í™˜í•˜ê³  ì‹¤í–‰í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-ë¶ˆë²•ì ì¸ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë ¤ê³  í•˜ê±°ë‚˜ ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ì½”ë“œì— ì¼ë°˜ì ì¸ íŒŒì´ì¬ ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° \n-ì‹¤í–‰ì´ ì¤‘ì§€ë©ë‹ˆë‹¤.\n-\n-### ì—„ì„ ëœ ë„êµ¬ ëª¨ìŒ [[a-curated-set-of-tools]]\n-\n-ì €í¬ëŠ” ì´ëŸ¬í•œ ì—ì´ì „íŠ¸ë“¤ì˜ ì—­ëŸ‰ì„ ê°•í™”í•  ìˆ˜ ìˆëŠ” ì¼ë ¨ì˜ ë„êµ¬ë¥¼ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤. \n-ë‹¤ìŒì€ ì—°ë™ëœ ë„êµ¬ì˜ ìµœì‹  ëª©ë¡ì…ë‹ˆë‹¤:\n-\n-- **ë¬¸ì„œ ì§ˆë¬¸ ë‹µë³€**: ì´ë¯¸ì§€ í˜•ì‹ì˜ ë¬¸ì„œ(ì˜ˆ: PDF)ê°€ ì£¼ì–´ì§€ë©´ ì´ ë¬¸ì„œì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. ([Donut](./model_doc/donut))\n-- **í…ìŠ¤íŠ¸ ì§ˆë¬¸ ë‹µë³€**: ê¸´ í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì´ ì£¼ì–´ì§€ë©´ í…ìŠ¤íŠ¸ì—ì„œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤. ([Flan-T5](./model_doc/flan-t5))\n-- **ë¬´ì¡°ê±´ ì´ë¯¸ì§€ ìº¡ì…”ë‹**: ì´ë¯¸ì§€ì— ìº¡ì…˜ì„ ë‹µë‹ˆë‹¤! ([BLIP](./model_doc/blip))\n-- **ì´ë¯¸ì§€ ì§ˆë¬¸ ë‹µë³€**: ì´ë¯¸ì§€ê°€ ì£¼ì–´ì§€ë©´ ì´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸°. ([VILT](./model_doc/vilt))\n-- **ì´ë¯¸ì§€ ë¶„í• **: ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì§€ë©´ í•´ë‹¹ í”„ë¡¬í”„íŠ¸ì˜ ë¶„í•  ë§ˆìŠ¤í¬ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ([CLIPSeg](./model_doc/clipseg))\n-- **ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜**: ì‚¬ëŒì´ ë§í•˜ëŠ” ì˜¤ë””ì˜¤ ë…¹ìŒì´ ì£¼ì–´ì§€ë©´ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ([Whisper](./model_doc/whisper))\n-- **í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜**: í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ([SpeechT5](./model_doc/speecht5))\n-- **ì œë¡œ ìƒ·(zero-shot) í…ìŠ¤íŠ¸ ë¶„ë¥˜**: í…ìŠ¤íŠ¸ì™€ ë ˆì´ë¸” ëª©ë¡ì´ ì£¼ì–´ì§€ë©´ í…ìŠ¤íŠ¸ì™€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” ë ˆì´ë¸”ì„ ì‹ë³„í•©ë‹ˆë‹¤. ([BART](./model_doc/bart))\n-- **í…ìŠ¤íŠ¸ ìš”ì•½**: ê¸´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ ë˜ëŠ” ëª‡ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•©ë‹ˆë‹¤. ([BART](./model_doc/bart))\n-- **ë²ˆì—­**: í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ì–¸ì–´ë¡œ ë²ˆì—­í•©ë‹ˆë‹¤. ([NLLB](./model_doc/nllb))\n-\n-ì´ëŸ¬í•œ ë„êµ¬ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©ë˜ì–´ ìˆìœ¼ë©°, ì˜ˆë¥¼ ë“¤ì–´ ìˆ˜ë™ìœ¼ë¡œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n-from transformers import load_tool\n-\n-tool = load_tool(\"text-to-speech\")\n-audio = tool(\"This is a text to speech tool\")\n-```\n-\n-### ì‚¬ìš©ì ì •ì˜ ë„êµ¬ [[custom-tools]]\n-\n-ì—„ì„ ëœ ë„êµ¬ ì„¸íŠ¸ë„ ìˆì§€ë§Œ, ì´ êµ¬í˜„ì´ ì œê³µí•˜ëŠ” ê°€ì¥ í° ê°€ì¹˜ëŠ” ì‚¬ìš©ì ì§€ì • ë„êµ¬ë¥¼ ë¹ ë¥´ê²Œ ë§Œë“¤ê³  ê³µìœ í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n-\n-ë„êµ¬ì˜ ì½”ë“œë¥¼ Hugging Face Spaceë‚˜ ëª¨ë¸ ì €ì¥ì†Œì— í‘¸ì‹œí•˜ë©´ ì—ì´ì „íŠ¸ì—ê²Œ ì§ì ‘ ë„êµ¬ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  [`huggingface-tools` organization](https://huggingface.co/huggingface-tools)ì— ëª‡ ê°€ì§€ **íŠ¸ëœìŠ¤í¬ë¨¸ì— êµ¬ì• ë°›ì§€ ì•ŠëŠ”** íˆ´ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤:\n-\n-- **í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë”**: ì›¹ URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.\n-- **í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ë³€í™˜**: í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì—¬ ì•ˆì •ì ì¸ í™•ì‚°ì„ í™œìš©í•©ë‹ˆë‹¤.\n-- **ì´ë¯¸ì§€ ë³€í™˜**: ì´ˆê¸° ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì§„ ì´ë¯¸ì§€ë¥¼ ìˆ˜ì •í•˜ê³ , ì•ˆì •ì ì¸ í™•ì‚°ì„ í™œìš©í•˜ëŠ” ì§€ì‹œ í”½ì…€ 2 í”½ì…€ì„ í™œìš©í•©ë‹ˆë‹¤.\n-- **í…ìŠ¤íŠ¸ ë¹„ë””ì˜¤ ë³€í™˜**: í”„ë¡¬í”„íŠ¸ì— ë”°ë¼ ì‘ì€ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•˜ë©°, damo-vilabì„ í™œìš©í•©ë‹ˆë‹¤.\n-\n-ì €í¬ê°€ ì²˜ìŒë¶€í„° ì‚¬ìš©í•˜ê³  ìˆëŠ” í…ìŠ¤íŠ¸-ì´ë¯¸ì§€ ë³€í™˜ ë„êµ¬ëŠ” [*huggingface-tools/text-to-image*](https://huggingface.co/spaces/huggingface-tools/text-to-image)ì— ìˆëŠ” ì›ê²© ë„êµ¬ì…ë‹ˆë‹¤! ì €í¬ëŠ” ì´ ë„êµ¬ì™€ ë‹¤ë¥¸ ì¡°ì§ì— ì´ëŸ¬í•œ ë„êµ¬ë¥¼ ê³„ì† ì¶œì‹œí•˜ì—¬ ì´ êµ¬í˜„ì„ ë”ìš± ê°•í™”í•  ê²ƒì…ë‹ˆë‹¤.\n-\n-ì—ì´ì „íŠ¸ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ [`huggingface-tools`](https://huggingface.co/huggingface-tools)ì— ìˆëŠ” ë„êµ¬ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-[ë‹¤ìŒ ê°€ì´ë“œ](custom_tools)ì—ì„œ ë„êµ¬ë¥¼ ì‘ì„±í•˜ê³  ê³µìœ í•˜ëŠ” ë°©ë²•ê³¼ Hubì— ìˆëŠ” ì‚¬ìš©ì ì§€ì • ë„êµ¬ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.\n-\n-### ì½”ë“œ ìƒì„±[[code-generation]]\n-\n-ì§€ê¸ˆê¹Œì§€ ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ë“œë ¸ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì—ì´ì „íŠ¸ëŠ” ë§¤ìš° ì œí•œëœ Python ì¸í„°í”„ë¦¬í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤í–‰í•  ì½”ë“œë§Œ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì„¤ì •ì—ì„œ ìƒì„±ëœ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ë ¤ëŠ” ê²½ìš° ì—ì´ì „íŠ¸ì—ê²Œ ë„êµ¬ ì •ì˜ ë° ì •í™•í•œ ê°€ì ¸ì˜¤ê¸°ì™€ í•¨ê»˜ ì½”ë“œë¥¼ ë°˜í™˜í•˜ë¼ëŠ” ë©”ì‹œì§€ë¥¼ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒ ëª…ë ¹ì–´ëŠ” \n-```python\n-agent.run(\"Draw me a picture of rivers and lakes\", return_code=True)\n-```\n-\n-ë‹¤ìŒ ì½”ë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n-\n-```python\n-from transformers import load_tool\n-\n-image_generator = load_tool(\"huggingface-tools/text-to-image\")\n-\n-image = image_generator(prompt=\"rivers and lakes\")\n-```\n-\n-ì´ ì½”ë“œëŠ” ì§ì ‘ ìˆ˜ì •í•˜ê³  ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\ No newline at end of file"
        },
        {
            "sha": "56a4744b8b86857e2c293879153c453eea2be906",
            "filename": "docs/source/ms/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fms%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fms%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fms%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -21,8 +21,6 @@\n       title: Sediakan latihan yang diedarkan dengan ğŸ¤— Accelerate\n     - local: model_sharing\n       title: Kongsi model anda\n-    - local: transformers_agents\n-      title: Ejen\n   title: Tutorials\n - sections:\n     - sections:\n@@ -179,8 +177,6 @@\n   title: Panduan konsep\n - sections:\n     - sections:\n-        - local: main_classes/agent\n-          title: Ejen dan Alat\n         - local: model_doc/auto\n           title: Kelas Auto\n         - local: main_classes/callback"
        },
        {
            "sha": "66816a5309863bd25b0f3cc85d8ca659021a9c4b",
            "filename": "docs/source/zh/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fzh%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/docs%2Fsource%2Fzh%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2F_toctree.yml?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -23,10 +23,6 @@\n     title: ä½¿ç”¨ğŸ¤— PEFTåŠ è½½å’Œè®­ç»ƒadapters\n   - local: model_sharing\n     title: åˆ†äº«æ‚¨çš„æ¨¡å‹\n-  - local: agents\n-    title: æ™ºèƒ½ä½“å’Œå·¥å…·\n-  - local: agents_advanced\n-    title: æ™ºèƒ½ä½“ï¼Œè¶…å¼ºç‰ˆ - å¤šæ™ºèƒ½ä½“ã€å¤–éƒ¨å·¥å…·ç­‰\n   - local: llm_tutorial\n     title: ä½¿ç”¨LLMsè¿›è¡Œç”Ÿæˆ\n   title: æ•™ç¨‹\n@@ -105,8 +101,6 @@\n   title: æ¦‚å¿µæŒ‡å—\n - sections:\n   - sections:\n-    - local: main_classes/agent\n-      title: æ™ºèƒ½ä½“å’Œå·¥å…·\n     - local: main_classes/callback\n       title: Callbacks\n     - local: main_classes/configuration"
        },
        {
            "sha": "b10fe43608599836dbed81524986ac27181d1acf",
            "filename": "docs/source/zh/agents.md",
            "status": "removed",
            "additions": 0,
            "deletions": 427,
            "changes": 427,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fzh%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fzh%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fagents.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,427 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-# æ™ºèƒ½ä½“å’Œå·¥å…·\n-\n-[[åœ¨colabé‡Œæ‰“å¼€]]\n-\n-### ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ (Agent)ï¼Ÿ\n-\n-å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç»è¿‡ [å› æœè¯­è¨€å»ºæ¨¡è®­ç»ƒ](./tasks/language_modeling) å¯ä»¥åº”å¯¹å„ç§ä»»åŠ¡ï¼Œä½†åœ¨ä¸€äº›åŸºæœ¬ä»»åŠ¡ï¼ˆå¦‚é€»è¾‘æ¨ç†ã€è®¡ç®—å’Œæœç´¢ï¼‰ä¸Šå¸¸å¸¸è¡¨ç°ä¸ä½³ã€‚å½“å®ƒä»¬è¢«ç”¨åœ¨è‡ªå·±ä¸æ“…é•¿çš„é¢†åŸŸæ—¶ï¼Œå¾€å¾€æ— æ³•ç”Ÿæˆæˆ‘ä»¬æœŸæœ›çš„ç­”æ¡ˆã€‚\n-\n-ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥åˆ›å»º**æ™ºèƒ½ä½“**.\n-\n-æ™ºèƒ½ä½“æ˜¯ä¸€ä¸ªç³»ç»Ÿï¼Œå®ƒä½¿ç”¨ LLM ä½œä¸ºå¼•æ“ï¼Œå¹¶ä¸”èƒ½å¤Ÿè®¿é—®ç§°ä¸º**å·¥å…·**çš„åŠŸèƒ½ã€‚\n-\n-è¿™äº›**å·¥å…·**æ˜¯æ‰§è¡Œä»»åŠ¡çš„å‡½æ•°ï¼ŒåŒ…å«æ‰€æœ‰å¿…è¦çš„æè¿°ä¿¡æ¯ï¼Œå¸®åŠ©æ™ºèƒ½ä½“æ­£ç¡®ä½¿ç”¨å®ƒä»¬ã€‚\n-\n-æ™ºèƒ½ä½“å¯ä»¥è¢«ç¼–ç¨‹ä¸ºï¼š\n-- ä¸€æ¬¡æ€§è®¾è®¡ä¸€ç³»åˆ—å·¥å…·å¹¶åŒæ—¶æ‰§è¡Œå®ƒä»¬ï¼Œåƒ  [`CodeAgent`]\n-- ä¸€æ¬¡æ‰§è¡Œä¸€ä¸ªå·¥å…·ï¼Œå¹¶ç­‰å¾…æ¯ä¸ªå·¥å…·çš„ç»“æœåå†å¯åŠ¨ä¸‹ä¸€ä¸ªï¼Œåƒ [`ReactJsonAgent`]\n-\n-### æ™ºèƒ½ä½“ç±»å‹\n-\n-#### ä»£ç æ™ºèƒ½ä½“\n-\n-æ­¤æ™ºèƒ½ä½“åŒ…å«ä¸€ä¸ªè§„åˆ’æ­¥éª¤ï¼Œç„¶åç”Ÿæˆ Python ä»£ç ä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ã€‚å®ƒåŸç”Ÿæ”¯æŒå¤„ç†ä¸åŒè¾“å…¥å’Œè¾“å‡ºç±»å‹ï¼Œå› æ­¤æ¨èç”¨äºå¤šæ¨¡æ€ä»»åŠ¡ã€‚\n-\n-#### æ¨ç†æ™ºèƒ½ä½“\n-\n-è¿™æ˜¯è§£å†³æ¨ç†ä»»åŠ¡çš„é¦–é€‰ä»£ç†ï¼Œå› ä¸º ReAct æ¡†æ¶ ([Yao et al., 2022](https://huggingface.co/papers/2210.03629)) ä½¿å…¶åœ¨åŸºäºä¹‹å‰è§‚å¯Ÿè¿›è¡Œæ¨ç†æ—¶éå¸¸é«˜æ•ˆã€‚\n-\n-æˆ‘ä»¬å®ç°äº†ä¸¤ç§ç‰ˆæœ¬çš„ ReactJsonAgentï¼š\n-- [`ReactJsonAgent`] å°†å·¥å…·è°ƒç”¨ä½œä¸º JSON æ ¼å¼è¾“å‡ºã€‚\n-- [`ReactCodeAgent`] æ˜¯ ReactJsonAgent çš„ä¸€ç§æ–°å‹ï¼Œç”Ÿæˆå·¥å…·è°ƒç”¨çš„ä»£ç å—ï¼Œå¯¹äºå…·å¤‡å¼ºå¤§ç¼–ç¨‹èƒ½åŠ›çš„ LLM éå¸¸é€‚ç”¨ã€‚\n-\n-> [TIP]\n-> é˜…è¯» [Open-source LLMs as LangChain Agents](https://huggingface.co/blog/open-source-llms-as-agents) åšæ–‡ï¼Œäº†è§£æ›´å¤šå…³äºæ¨ç†æ™ºèƒ½ä½“çš„ä¿¡æ¯ã€‚\n-\n-<div class=\"flex justify-center\">\n-    <img\n-        class=\"block dark:hidden\"\n-        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif\"\n-    />\n-    <img\n-        class=\"hidden dark:block\"\n-        src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Agent_ManimCE.gif\"\n-    />\n-</div>\n-\n-![æ¨ç†æ™ºèƒ½ä½“çš„æ¡†æ¶](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/open-source-llms-as-agents/ReAct.png)\n-\n-ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ¨ç†ä»£ç æ™ºèƒ½ä½“å¦‚ä½•å¤„ç†ä»¥ä¸‹é—®é¢˜çš„ç¤ºä¾‹ï¼š\n-\n-```py3\n->>> agent.run(\n-...     \"How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\",\n-... )\n-=====New task=====\n-How many more blocks (also denoted as layers) in BERT base encoder than the encoder from the architecture proposed in Attention is All You Need?\n-====Agent is executing the code below:\n-bert_blocks = search(query=\"number of blocks in BERT base encoder\")\n-print(\"BERT blocks:\", bert_blocks)\n-====\n-Print outputs:\n-BERT blocks: twelve encoder blocks\n-\n-====Agent is executing the code below:\n-attention_layer = search(query=\"number of layers in Attention is All You Need\")\n-print(\"Attention layers:\", attention_layer)\n-====\n-Print outputs:\n-Attention layers: Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- 2 Page 3 Figure 1: The Transformer - model architecture.\n-\n-====Agent is executing the code below:\n-bert_blocks = 12\n-attention_layers = 6\n-diff = bert_blocks - attention_layers\n-print(\"Difference in blocks:\", diff)\n-final_answer(diff)\n-====\n-\n-Print outputs:\n-Difference in blocks: 6\n-\n-Final answer: 6\n-```\n-\n-### å¦‚ä½•æ„å»ºæ™ºèƒ½ä½“ï¼Ÿ\n-\n-è¦åˆå§‹åŒ–ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œæ‚¨éœ€è¦ä»¥ä¸‹å‚æ•°ï¼š\n-\n-- **ä¸€ä¸ª LLM** æ¥é©±åŠ¨æ™ºèƒ½ä½“â€”â€”æ™ºèƒ½ä½“æœ¬èº«å¹¶ä¸æ˜¯ LLMï¼Œè€Œæ˜¯ä¸€ä¸ªä½¿ç”¨ LLM ä½œä¸ºå¼•æ“çš„ç¨‹åºã€‚\n-- **ä¸€ä¸ªç³»ç»Ÿæç¤º**ï¼šå‘Šè¯‰ LLM å¼•æ“åº”è¯¥å¦‚ä½•ç”Ÿæˆè¾“å‡ºã€‚\n-- **ä¸€ä¸ªå·¥å…·ç®±**ï¼Œæ™ºèƒ½ä½“å¯ä»¥ä»ä¸­é€‰æ‹©å·¥å…·æ‰§è¡Œã€‚\n-- **ä¸€ä¸ªè§£æå™¨**ï¼Œä» LLM è¾“å‡ºä¸­æå–å‡ºå“ªäº›å·¥å…·éœ€è¦è°ƒç”¨ï¼Œä»¥åŠä½¿ç”¨å“ªäº›å‚æ•°ã€‚\n-\n-åœ¨æ™ºèƒ½ä½“ç³»ç»Ÿåˆå§‹åŒ–æ—¶ï¼Œå·¥å…·å±æ€§å°†ç”Ÿæˆå·¥å…·æè¿°ï¼Œå¹¶åµŒå…¥åˆ°æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºä¸­ï¼Œå‘ŠçŸ¥æ™ºèƒ½ä½“å¯ä»¥ä½¿ç”¨å“ªäº›å·¥å…·ï¼Œå¹¶ä¸”ä¸ºä»€ä¹ˆä½¿ç”¨å®ƒä»¬ã€‚\n-\n-**å®‰è£…ä¾èµ–**\n-\n-é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£…**æ™ºèƒ½ä½“**æ‰€éœ€çš„é¢å¤–ä¾èµ–ï¼š\n-\n-```bash\n-pip install transformers[agents]\n-```\n-**åˆ›å»ºLLMå¼•æ“**\n-\n-å®šä¹‰ä¸€ä¸ª `llm_engine` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æ¥å—ä¸€ç³»åˆ—[æ¶ˆæ¯](./chat_templating)å¹¶è¿”å›æ–‡æœ¬ã€‚è¯¥ `callable` è¿˜éœ€è¦æ¥å—ä¸€ä¸ª `stop` å‚æ•°ï¼Œç”¨äºæŒ‡ç¤ºä½•æ—¶åœæ­¢ç”Ÿæˆè¾“å‡ºã€‚ \n-\n-```python\n-from huggingface_hub import login, InferenceClient\n-\n-login(\"<YOUR_HUGGINGFACEHUB_API_TOKEN>\")\n-\n-client = InferenceClient(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-\n-def llm_engine(messages, stop_sequences=[\"Task\"]) -> str:\n-    response = client.chat_completion(messages, stop=stop_sequences, max_tokens=1000)\n-    answer = response.choices[0].message.content\n-    return answer\n-```\n-\n-æ‚¨å¯ä»¥ä½¿ç”¨ä»»ä½•ç¬¦åˆä»¥ä¸‹è¦æ±‚çš„ `llm_engine` æ–¹æ³•ï¼š\n-1. [è¾“å…¥æ ¼å¼](./chat_templating)ä¸º (`List[Dict[str, str]]`)ï¼Œå¹¶ä¸”è¿”å›ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚\n-2. å®ƒåœ¨ `stop_sequences` å‚æ•°ä¼ é€’çš„åºåˆ—å¤„åœæ­¢ç”Ÿæˆè¾“å‡ºã€‚\n-\n-æ­¤å¤–ï¼Œ`llm_engine` è¿˜å¯ä»¥æ¥å—ä¸€ä¸ª `grammar` å‚æ•°ã€‚å¦‚æœåœ¨æ™ºèƒ½ä½“åˆå§‹åŒ–æ—¶æŒ‡å®šäº† `grammar`ï¼Œåˆ™è¯¥å‚æ•°å°†ä¼ é€’ç»™ `llm_engine` çš„è°ƒç”¨ï¼Œä»¥å…è®¸[å—é™ç”Ÿæˆ](https://huggingface.co/docs/text-generation-inference/conceptual/guidance)ï¼Œä»¥å¼ºåˆ¶ç”Ÿæˆæ ¼å¼æ­£ç¡®çš„æ™ºèƒ½ä½“è¾“å‡ºã€‚\n-\n-æ‚¨è¿˜éœ€è¦ä¸€ä¸ª `tools` å‚æ•°ï¼Œå®ƒæ¥å—ä¸€ä¸ª `Tools` åˆ—è¡¨ â€”â€” å¯ä»¥æ˜¯ç©ºåˆ—è¡¨ã€‚æ‚¨ä¹Ÿå¯ä»¥é€šè¿‡å®šä¹‰å¯é€‰å‚æ•° `add_base_tools=True` æ¥å°†é»˜è®¤å·¥å…·ç®±æ·»åŠ åˆ°å·¥å…·åˆ—è¡¨ä¸­ã€‚\n-\n-ç°åœ¨ï¼Œæ‚¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œä¾‹å¦‚ [`CodeAgent`]ï¼Œå¹¶è¿è¡Œå®ƒã€‚æ‚¨è¿˜å¯ä»¥åˆ›å»ºä¸€ä¸ª [`TransformersEngine`]ï¼Œä½¿ç”¨ `transformers` åœ¨æœ¬åœ°æœºå™¨ä¸Šè¿è¡Œé¢„åˆå§‹åŒ–çš„æ¨ç†ç®¡é“ã€‚ ä¸ºäº†æ–¹ä¾¿èµ·è§ï¼Œç”±äºæ™ºèƒ½ä½“è¡Œä¸ºé€šå¸¸éœ€è¦æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œä¾‹å¦‚ `Llama-3.1-70B-Instruct`ï¼Œå®ƒä»¬ç›®å‰è¾ƒéš¾åœ¨æœ¬åœ°è¿è¡Œï¼Œæˆ‘ä»¬è¿˜æä¾›äº† [`HfApiEngine`] ç±»ï¼Œå®ƒåœ¨åº•å±‚åˆå§‹åŒ–äº†ä¸€ä¸ª `huggingface_hub.InferenceClient`ã€‚\n-\n-```python\n-from transformers import CodeAgent, HfApiEngine\n-\n-llm_engine = HfApiEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-agent = CodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n-\n-agent.run(\n-    \"Could you translate this sentence from French, say it out loud and return the audio.\",\n-    sentence=\"OÃ¹ est la boulangerie la plus proche?\",\n-)\n-```\n-\n-å½“ä½ æ€¥éœ€æŸä¸ªä¸œè¥¿æ—¶è¿™å°†ä¼šå¾ˆæœ‰ç”¨!\n-æ‚¨ç”šè‡³å¯ä»¥å°† `llm_engine` å‚æ•°ç•™ç©ºï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šåˆ›å»ºä¸€ä¸ª [`HfApiEngine`]ã€‚\n-\n-```python\n-from transformers import CodeAgent\n-\n-agent = CodeAgent(tools=[], add_base_tools=True)\n-\n-agent.run(\n-    \"Could you translate this sentence from French, say it out loud and give me the audio.\",\n-    sentence=\"OÃ¹ est la boulangerie la plus proche?\",\n-)\n-```\n-\n-è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†é¢å¤–çš„ `sentence` å‚æ•°ï¼šæ‚¨å¯ä»¥å°†æ–‡æœ¬ä½œä¸ºé™„åŠ å‚æ•°ä¼ é€’ç»™æ¨¡å‹ã€‚\n-\n-æ‚¨è¿˜å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ¥æŒ‡å®šæœ¬åœ°æˆ–è¿œç¨‹æ–‡ä»¶çš„è·¯å¾„ä¾›æ¨¡å‹ä½¿ç”¨ï¼š\n-\n-```py\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n-\n-agent.run(\"Why does Mike not know many people in New York?\", audio=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/recording.mp3\")\n-```\n-\n-ç³»ç»Ÿæç¤ºå’Œè¾“å‡ºè§£æå™¨ä¼šè‡ªåŠ¨å®šä¹‰ï¼Œä½†æ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨æ™ºèƒ½ä½“çš„ `system_prompt_template` æ¥è½»æ¾æŸ¥çœ‹å®ƒä»¬ã€‚\n-\n-```python\n-print(agent.system_prompt_template)\n-```\n-\n-å°½å¯èƒ½æ¸…æ¥šåœ°è§£é‡Šæ‚¨è¦æ‰§è¡Œçš„ä»»åŠ¡éå¸¸é‡è¦ã€‚ æ¯æ¬¡ [`~Agent.run`] æ“ä½œéƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œå¹¶ä¸”ç”±äºæ™ºèƒ½ä½“æ˜¯ç”± LLM é©±åŠ¨çš„ï¼Œæç¤ºä¸­çš„ç»†å¾®å˜åŒ–å¯èƒ½ä¼šå¯¼è‡´å®Œå…¨ä¸åŒçš„ç»“æœã€‚ \n-æ‚¨è¿˜å¯ä»¥è¿ç»­è¿è¡Œå¤šä¸ªä»»åŠ¡ï¼Œæ¯æ¬¡éƒ½ä¼šé‡æ–°åˆå§‹åŒ–æ™ºèƒ½ä½“çš„ `agent.task` å’Œ `agent.logs` å±æ€§ã€‚\n-\n-\n-#### ä»£ç æ‰§è¡Œ\n-\n-Python è§£é‡Šå™¨åœ¨ä¸€ç»„è¾“å…¥å’Œå·¥å…·ä¸Šæ‰§è¡Œä»£ç ã€‚ è¿™åº”è¯¥æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºåªèƒ½è°ƒç”¨æ‚¨æä¾›çš„å·¥å…·ï¼ˆç‰¹åˆ«æ˜¯ Hugging Face çš„å·¥å…·ï¼‰å’Œ print å‡½æ•°ï¼Œå› æ­¤æ‚¨å·²ç»é™åˆ¶äº†å¯ä»¥æ‰§è¡Œçš„æ“ä½œã€‚\n-\n-Python è§£é‡Šå™¨é»˜è®¤ä¸å…è®¸å¯¼å…¥ä¸åœ¨å®‰å…¨åˆ—è¡¨ä¸­çš„æ¨¡å—ï¼Œå› æ­¤å¤§å¤šæ•°æ˜æ˜¾çš„æ”»å‡»é—®é¢˜åº”è¯¥ä¸æˆé—®é¢˜ã€‚ æ‚¨ä»ç„¶å¯ä»¥é€šè¿‡åœ¨ [`ReactCodeAgent`] æˆ– [`CodeAgent`] åˆå§‹åŒ–æ—¶é€šè¿‡ `additional_authorized_imports` å‚æ•°ä¼ é€’ä¸€ä¸ªæˆæƒçš„æ¨¡å—åˆ—è¡¨æ¥æˆæƒé¢å¤–çš„å¯¼å…¥ï¼š\n-\n-```py\n->>> from transformers import ReactCodeAgent\n-\n->>> agent = ReactCodeAgent(tools=[], additional_authorized_imports=['requests', 'bs4'])\n->>> agent.run(\"Could you get me the title of the page at url 'https://huggingface.co/blog'?\")\n-\n-(...)\n-'Hugging Face â€“ Blog'\n-```\n-\n-å¦‚æœæœ‰ä»»ä½•ä»£ç å°è¯•æ‰§è¡Œéæ³•æ“ä½œï¼Œæˆ–è€…ç”Ÿæˆçš„ä»£ç å‡ºç°å¸¸è§„ Python é”™è¯¯ï¼Œæ‰§è¡Œå°†åœæ­¢ã€‚\n-\n-> [!WARNING]\n-> åœ¨ä½¿ç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ç”Ÿæˆä»£ç æ—¶ï¼Œç”Ÿæˆçš„ä»£ç ä¼šè¢«æ‰§è¡Œï¼Œé¿å…å¯¼å…¥æˆ–ä½¿ç”¨ä»»ä½•ä¸å®‰å…¨çš„åº“æˆ–æ¨¡å—ã€‚\n-\n-### ç³»ç»Ÿæç¤º\n-\n-æ™ºèƒ½ä½“ï¼Œæˆ–è€…è¯´é©±åŠ¨æ™ºèƒ½ä½“çš„ LLMï¼Œæ ¹æ®ç³»ç»Ÿæç¤ºç”Ÿæˆè¾“å‡ºã€‚ç³»ç»Ÿæç¤ºå¯ä»¥å®šåˆ¶å¹¶æ ¹æ®ç›®æ ‡ä»»åŠ¡è¿›è¡Œè°ƒæ•´ã€‚ä¾‹å¦‚ï¼Œæ£€æŸ¥ [`ReactCodeAgent`] çš„ç³»ç»Ÿæç¤ºï¼ˆä»¥ä¸‹ç‰ˆæœ¬ç»è¿‡ç®€åŒ–ï¼‰ã€‚\n-\n-```text\n-You will be given a task to solve as best you can.\n-You have access to the following tools:\n-<<tool_descriptions>>\n-\n-To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n-\n-At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task, then the tools that you want to use.\n-Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '/End code' sequence.\n-During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n-These print outputs will then be available in the 'Observation:' field, for using this information as input for the next step.\n-\n-In the end you have to return a final answer using the `final_answer` tool.\n-\n-Here are a few examples using notional tools:\n----\n-{examples}\n-\n-Above example were using notional tools that might not exist for you. You only have access to those tools:\n-<<tool_names>>\n-You also can perform computations in the python code you generate.\n-\n-Always provide a 'Thought:' and a 'Code:\\n```py' sequence ending with '```<end_code>' sequence. You MUST provide at least the 'Code:' sequence to move forward.\n-\n-Remember to not perform too many operations in a single code block! You should split the task into intermediate code blocks.\n-Print results at the end of each step to save the intermediate results. Then use final_answer() to return the final result.\n-\n-Remember to make sure that variables you use are all defined.\n-\n-Now Begin!\n-```\n-\n-ç³»ç»Ÿæç¤ºåŒ…æ‹¬ï¼š\n-- è§£é‡Šæ™ºèƒ½ä½“åº”è¯¥å¦‚ä½•å·¥ä½œä»¥åŠå·¥å…·çš„**ä»‹ç»**ã€‚\n-- æ‰€æœ‰å·¥å…·çš„æè¿°ç”± `<<tool_descriptions>>` æ ‡è®°åœ¨è¿è¡Œæ—¶åŠ¨æ€æ›¿æ¢ï¼Œè¿™æ ·æ™ºèƒ½ä½“å°±çŸ¥é“å¯ä»¥ä½¿ç”¨å“ªäº›å·¥å…·åŠå…¶ç”¨é€”ã€‚\n-    - å·¥å…·çš„æè¿°æ¥è‡ªå·¥å…·çš„å±æ€§,`name`ã€`description`ã€`inputs` å’Œ `output_type`ï¼Œä»¥åŠä¸€ä¸ªç®€å•çš„ `jinja2` æ¨¡æ¿ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œè°ƒæ•´ã€‚\n-- æœŸæœ›çš„è¾“å‡ºæ ¼å¼ã€‚\n-\n-æ‚¨å¯ä»¥é€šè¿‡å‘ `system_prompt` å‚æ•°ä¼ é€’è‡ªå®šä¹‰æç¤ºæ¥æœ€å¤§ç¨‹åº¦åœ°æé«˜çµæ´»æ€§ï¼Œä»è€Œè¦†ç›–æ•´ä¸ªç³»ç»Ÿæç¤ºæ¨¡æ¿ã€‚\n-\n-```python\n-from transformers import ReactJsonAgent\n-from transformers.agents import PythonInterpreterTool\n-\n-agent = ReactJsonAgent(tools=[PythonInterpreterTool()], system_prompt=\"{your_custom_prompt}\")\n-```\n-\n-> [WARNING]\n-> å¿…é¡»åœ¨`template`ä¸­å®šä¹‰ `<<tool_descriptions>>` è¿™ä¸ªå˜é‡ï¼Œä»¥ä¾¿æ™ºèƒ½ä½“èƒ½å¤Ÿæ­£ç¡®åœ°è¯†åˆ«å¹¶ä½¿ç”¨å¯ç”¨çš„å·¥å…·\n-\n-\n-### æ£€æŸ¥æ™ºèƒ½ä½“çš„è¿è¡Œ\n-\n-ä»¥ä¸‹æ˜¯æ£€æŸ¥è¿è¡Œåå‘ç”Ÿäº†ä»€ä¹ˆçš„ä¸€äº›æœ‰ç”¨å±æ€§ï¼š\n-- `agent.logs` å­˜å‚¨äº†æ™ºèƒ½ä½“çš„è¯¦ç»†æ—¥å¿—ã€‚æ¯ä¸€æ­¥çš„æ‰€æœ‰å†…å®¹éƒ½ä¼šå­˜å‚¨åœ¨ä¸€ä¸ªå­—å…¸ä¸­ï¼Œç„¶åé™„åŠ åˆ° `agent.logs`ã€‚\n-- è¿è¡Œ `agent.write_inner_memory_from_logs()` ä¼šä»æ—¥å¿—ä¸­åˆ›å»ºæ™ºèƒ½ä½“çš„å†…å­˜ï¼Œä»¥ä¾¿ LLM æŸ¥çœ‹ï¼Œä½œä¸ºä¸€ç³»åˆ—èŠå¤©æ¶ˆæ¯ã€‚æ­¤æ–¹æ³•ä¼šéå†æ—¥å¿—çš„æ¯ä¸ªæ­¥éª¤ï¼Œåªä¿å­˜å…¶æ„Ÿå…´è¶£çš„æ¶ˆæ¯ï¼šä¾‹å¦‚ï¼Œå®ƒä¼šå•ç‹¬ä¿å­˜ç³»ç»Ÿæç¤ºå’Œä»»åŠ¡ï¼Œç„¶åä¸ºæ¯ä¸ªæ­¥éª¤ä¿å­˜ LLM è¾“å‡ºçš„æ¶ˆæ¯ï¼Œä»¥åŠå·¥å…·è°ƒç”¨è¾“å‡ºçš„æ¶ˆæ¯ã€‚å¦‚æœæ‚¨æƒ³è¦æ›´é«˜å±‚æ¬¡çš„æŸ¥çœ‹å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¯ä»¥ä½¿ç”¨æ­¤æ–¹æ³• â€”â€” ä½†å¹¶ä¸æ˜¯æ¯ä¸ªæ—¥å¿—éƒ½ä¼šè¢«æ­¤æ–¹æ³•è½¬å½•ã€‚\n-\n-## å·¥å…·\n-\n-å·¥å…·æ˜¯æ™ºèƒ½ä½“ä½¿ç”¨çš„åŸºæœ¬åŠŸèƒ½ã€‚\n-\n-ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æ£€æŸ¥ [`PythonInterpreterTool`]ï¼šå®ƒæœ‰ä¸€ä¸ªåç§°ã€æè¿°ã€è¾“å…¥æè¿°ã€è¾“å‡ºç±»å‹å’Œ `__call__` æ–¹æ³•æ¥æ‰§è¡Œè¯¥æ“ä½œã€‚\n-\n-å½“æ™ºèƒ½ä½“åˆå§‹åŒ–æ—¶ï¼Œå·¥å…·å±æ€§ä¼šç”¨æ¥ç”Ÿæˆå·¥å…·æè¿°ï¼Œç„¶åå°†å…¶åµŒå…¥åˆ°æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºä¸­ï¼Œè¿™è®©æ™ºèƒ½ä½“çŸ¥é“å¯ä»¥ä½¿ç”¨å“ªäº›å·¥å…·ä»¥åŠä¸ºä»€ä¹ˆä½¿ç”¨å®ƒä»¬ã€‚\n-\n-### é»˜è®¤å·¥å…·ç®±\n-\n-Transformers æä¾›äº†ä¸€ä¸ªé»˜è®¤å·¥å…·ç®±ï¼Œç”¨äºå¢å¼ºæ™ºèƒ½ä½“ï¼Œæ‚¨å¯ä»¥åœ¨åˆå§‹åŒ–æ—¶é€šè¿‡ `add_base_tools=True` å‚æ•°å°†å…¶æ·»åŠ åˆ°æ™ºèƒ½ä½“ä¸­ï¼š\n-\n-- **æ–‡æ¡£é—®ç­”**ï¼šç»™å®šä¸€ä¸ªæ–‡æ¡£ï¼ˆå¦‚å›¾åƒæ ¼å¼çš„ PDFï¼‰ï¼Œå›ç­”å…³äºè¯¥æ–‡æ¡£çš„é—®é¢˜([Donut](./model_doc/donut))\n-- **å›¾åƒé—®ç­”**ï¼šç»™å®šä¸€å¼ å›¾ç‰‡ï¼Œå›ç­”å…³äºè¯¥å›¾åƒçš„é—®é¢˜([VILT](./model_doc/vilt))\n-- **è¯­éŸ³è½¬æ–‡æœ¬**ï¼šç»™å®šä¸€ä¸ªäººè®²è¿°çš„éŸ³é¢‘å½•éŸ³ï¼Œå°†å…¶è½¬å½•ä¸ºæ–‡æœ¬ï¼ˆWhisperï¼‰\n-- **æ–‡æœ¬è½¬è¯­éŸ³**ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³([SpeechT5](./model_doc/speecht5))\n-- **ç¿»è¯‘**ï¼šå°†ç»™å®šçš„å¥å­ä»æºè¯­è¨€ç¿»è¯‘ä¸ºç›®æ ‡è¯­è¨€\n-- **DuckDuckGo æœç´¢**ï¼šä½¿ç”¨ `DuckDuckGo` æµè§ˆå™¨è¿›è¡Œç½‘ç»œæœç´¢\n-- **Python ä»£ç è§£é‡Šå™¨**ï¼šåœ¨å®‰å…¨ç¯å¢ƒä¸­è¿è¡Œ LLM ç”Ÿæˆçš„ Python ä»£ç ã€‚åªæœ‰åœ¨åˆå§‹åŒ– [`ReactJsonAgent`] æ—¶å°† `add_base_tools=True` æ—¶ï¼Œä»£ç æ™ºèƒ½ä½“æ‰ä¼šæ·»åŠ æ­¤å·¥å…·ï¼Œå› ä¸ºåŸºäºä»£ç çš„æ™ºèƒ½ä½“å·²ç»èƒ½å¤ŸåŸç”Ÿæ‰§è¡Œ Python ä»£ç \n-\n-\n-æ‚¨å¯ä»¥é€šè¿‡è°ƒç”¨ [`load_tool`] å‡½æ•°æ¥æ‰‹åŠ¨ä½¿ç”¨æŸä¸ªå·¥å…·å¹¶æ‰§è¡Œä»»åŠ¡ã€‚\n-\n-\n-```python\n-from transformers import load_tool\n-\n-tool = load_tool(\"text-to-speech\")\n-audio = tool(\"This is a text to speech tool\")\n-```\n-\n-\n-### åˆ›å»ºæ–°å·¥å…·\n-\n-æ‚¨å¯ä»¥ä¸º `Hugging Face` é»˜è®¤å·¥å…·æ— æ³•æ¶µç›–çš„ç”¨ä¾‹åˆ›å»ºè‡ªå·±çš„å·¥å…·ã€‚ \n-ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬è¦åˆ›å»ºä¸€ä¸ªè¿”å›åœ¨ `Hugging Face Hub` ä¸ŠæŸä¸ªä»»åŠ¡ä¸­ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„æ¨¡å‹çš„å·¥å…·ã€‚\n-\n-æ‚¨å°†ä»ä»¥ä¸‹ä»£ç å¼€å§‹ï¼š\n-\n-```python\n-from huggingface_hub import list_models\n-\n-task = \"text-classification\"\n-\n-model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-print(model.id)\n-```\n-\n-è¿™æ®µä»£ç å¯ä»¥å¾ˆå¿«è½¬æ¢ä¸ºå·¥å…·ï¼Œåªéœ€å°†å…¶åŒ…è£…æˆä¸€ä¸ªå‡½æ•°ï¼Œå¹¶æ·»åŠ  `tool` è£…é¥°å™¨ï¼š\n-\n-\n-```py\n-from transformers import tool\n-\n-@tool\n-def model_download_tool(task: str) -> str:\n-    \"\"\"\n-    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n-    It returns the name of the checkpoint.\n-\n-    Args:\n-        task: The task for which\n-    \"\"\"\n-    model = next(iter(list_models(filter=\"text-classification\", sort=\"downloads\", direction=-1)))\n-    return model.id\n-```\n-\n-è¯¥å‡½æ•°éœ€è¦ï¼š\n-- ä¸€ä¸ªæ¸…æ™°çš„åç§°ã€‚åç§°é€šå¸¸æè¿°å·¥å…·çš„åŠŸèƒ½ã€‚ç”±äºä»£ç è¿”å›æŸä¸ªä»»åŠ¡ä¸­ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬å°†å…¶å‘½åä¸º `model_download_tool`ã€‚\n-- å¯¹è¾“å…¥å’Œè¾“å‡ºè¿›è¡Œç±»å‹æç¤º\n-- æè¿°ï¼Œå…¶ä¸­åŒ…æ‹¬ \"`Args`:\" éƒ¨åˆ†ï¼Œæè¿°æ¯ä¸ªå‚æ•°ï¼ˆè¿™æ¬¡ä¸éœ€è¦ç±»å‹æŒ‡ç¤ºï¼Œå®ƒä¼šä»ç±»å‹æç¤ºä¸­è·å–ï¼‰ã€‚ \n-\n-æ‰€æœ‰è¿™äº›å°†è‡ªåŠ¨åµŒå…¥åˆ°æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºä¸­ï¼Œå› æ­¤è¯·å°½é‡ä½¿å®ƒä»¬å°½å¯èƒ½æ¸…æ™°ï¼\n-\n-> [TIP]\n-> è¿™ä¸ªå®šä¹‰æ ¼å¼ä¸ apply_chat_template ä¸­ä½¿ç”¨çš„å·¥å…·æ¨¡å¼ç›¸åŒï¼Œå”¯ä¸€çš„åŒºåˆ«æ˜¯æ·»åŠ äº† tool è£…é¥°å™¨ï¼šå¯ä»¥åœ¨æˆ‘ä»¬çš„å·¥å…·ä½¿ç”¨ API ä¸­[äº†è§£æ›´å¤š](https://huggingface.co/blog/unified-tool-use#passing-tools-to-a-chat-template).\n-\n-ç„¶åï¼Œæ‚¨å¯ä»¥ç›´æ¥åˆå§‹åŒ–æ‚¨çš„æ™ºèƒ½ä½“ï¼š\n-```py\n-from transformers import CodeAgent\n-agent = CodeAgent(tools=[model_download_tool], llm_engine=llm_engine)\n-agent.run(\n-    \"Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\"\n-)\n-```\n-\n-æ‚¨å°†å¾—åˆ°ä»¥ä¸‹è¾“å‡ºï¼š\n-```text\n-======== New task ========\n-Can you give me the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub?\n-==== Agent is executing the code below:\n-most_downloaded_model = model_download_tool(task=\"text-to-video\")\n-print(f\"The most downloaded model for the 'text-to-video' task is {most_downloaded_model}.\")\n-====\n-```\n-\n-è¾“å‡ºï¼š\n-`\"The most downloaded model for the 'text-to-video' task is ByteDance/AnimateDiff-Lightning.\"`\n-\n-### ç®¡ç†æ™ºèƒ½ä½“çš„å·¥å…·ç®±\n-\n-å¦‚æœæ‚¨å·²ç»åˆå§‹åŒ–äº†ä¸€ä¸ªæ™ºèƒ½ä½“ï¼Œä½†æƒ³æ·»åŠ ä¸€ä¸ªæ–°çš„å·¥å…·ï¼Œé‡æ–°åˆå§‹åŒ–æ™ºèƒ½ä½“ä¼šå¾ˆéº»çƒ¦ã€‚å€ŸåŠ© Transformersï¼Œæ‚¨å¯ä»¥é€šè¿‡æ·»åŠ æˆ–æ›¿æ¢å·¥å…·æ¥ç®¡ç†æ™ºèƒ½ä½“çš„å·¥å…·ç®±ã€‚\n-\n-è®©æˆ‘ä»¬å°† `model_download_tool` æ·»åŠ åˆ°ä¸€ä¸ªä»…åˆå§‹åŒ–äº†é»˜è®¤å·¥å…·ç®±çš„ç°æœ‰æ™ºèƒ½ä½“ä¸­ã€‚\n-\n-```python\n-from transformers import CodeAgent\n-\n-agent = CodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n-agent.toolbox.add_tool(model_download_tool)\n-```\n-ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥åŒæ—¶ä½¿ç”¨æ–°å·¥å…·å’Œä¹‹å‰çš„æ–‡æœ¬åˆ°è¯­éŸ³å·¥å…·ï¼š\n-\n-```python\n-agent.run(\n-    \"Can you read out loud the name of the model that has the most downloads in the 'text-to-video' task on the Hugging Face Hub and return the audio?\"\n-)\n-```\n-\n-\n-| **Audio**                                                                                                                                            |\n-|------------------------------------------------------------------------------------------------------------------------------------------------------|\n-| <audio controls><source src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/damo.wav\" type=\"audio/wav\"/> |\n-\n-\n-> [WARNING]\n-> å½“å‘ä¸€ä¸ªå·²ç»è¿è¡Œè‰¯å¥½çš„ä»£ç†æ·»åŠ å·¥å…·æ—¶è¦å°å¿ƒï¼Œå› ä¸ºè¿™å¯èƒ½ä¼šå¯¼è‡´é€‰æ‹©åå‘ä½ çš„å·¥å…·ï¼Œæˆ–è€…é€‰æ‹©å·²ç»å®šä¹‰çš„å·¥å…·ä¹‹å¤–çš„å…¶ä»–å·¥å…·ã€‚\n-\n-\n-ä½¿ç”¨ agent.toolbox.update_tool() æ–¹æ³•å¯ä»¥æ›¿æ¢æ™ºèƒ½ä½“å·¥å…·ç®±ä¸­çš„ç°æœ‰å·¥å…·ã€‚\n-å¦‚æœæ‚¨çš„æ–°å·¥å…·å®Œå…¨æ›¿ä»£äº†ç°æœ‰å·¥å…·ï¼Œè¿™éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºæ™ºèƒ½ä½“å·²ç»çŸ¥é“å¦‚ä½•æ‰§è¡Œè¯¥ç‰¹å®šä»»åŠ¡ã€‚\n-åªéœ€ç¡®ä¿æ–°å·¥å…·éµå¾ªä¸æ›¿æ¢å·¥å…·ç›¸åŒçš„ APIï¼Œæˆ–è€…è°ƒæ•´ç³»ç»Ÿæç¤ºæ¨¡æ¿ï¼Œä»¥ç¡®ä¿æ‰€æœ‰ä½¿ç”¨æ›¿æ¢å·¥å…·çš„ç¤ºä¾‹éƒ½å¾—åˆ°æ›´æ–°ã€‚\n-\n-\n-### ä½¿ç”¨å·¥å…·é›†åˆ\n-\n-æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨ ToolCollection å¯¹è±¡æ¥åˆ©ç”¨å·¥å…·é›†åˆï¼ŒæŒ‡å®šæ‚¨æƒ³è¦ä½¿ç”¨çš„å·¥å…·é›†åˆçš„ slugã€‚\n-ç„¶åå°†è¿™äº›å·¥å…·ä½œä¸ºåˆ—è¡¨ä¼ é€’ç»™æ™ºèƒ½ä½“è¿›è¡Œåˆå§‹åŒ–ï¼Œå¹¶å¼€å§‹ä½¿ç”¨å®ƒä»¬ï¼\n-\n-```py\n-from transformers import ToolCollection, ReactCodeAgent\n-\n-image_tool_collection = ToolCollection(collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\")\n-agent = ReactCodeAgent(tools=[*image_tool_collection.tools], add_base_tools=True)\n-\n-agent.run(\"Please draw me a picture of rivers and lakes.\")\n-```\n-\n-ä¸ºäº†åŠ é€Ÿå¯åŠ¨ï¼Œå·¥å…·ä»…åœ¨æ™ºèƒ½ä½“è°ƒç”¨æ—¶åŠ è½½ã€‚\n-\n-è¿™å°†ç”Ÿæˆå¦‚ä¸‹å›¾åƒï¼š\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rivers_and_lakes.png\">"
        },
        {
            "sha": "9eb4dcf5124c820e485f52eb662990a34f412fc3",
            "filename": "docs/source/zh/agents_advanced.md",
            "status": "removed",
            "additions": 0,
            "deletions": 250,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fzh%2Fagents_advanced.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fzh%2Fagents_advanced.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fagents_advanced.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,250 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-http://www.apache.org/licenses/LICENSE-2.0\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n--->\n-# æ™ºèƒ½ä½“ï¼Œè¶…å¼ºç‰ˆ - å¤šæ™ºèƒ½ä½“ã€å¤–éƒ¨å·¥å…·ç­‰\n-\n-[[open-in-colab]]\n-\n-### ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼Ÿ\n-\n-> [!TIP]\n-> å¦‚æœä½ æ˜¯ `transformers.agents` çš„æ–°æ‰‹ï¼Œè¯·å…ˆé˜…è¯»ä¸»æ–‡æ¡£ [æ™ºèƒ½ä½“æ–‡æ¡£ ](./agents).\n-åœ¨æœ¬é¡µé¢ä¸­ï¼Œæˆ‘ä»¬å°†é‡ç‚¹ä»‹ç» `transformers.agents` çš„å‡ ç§é«˜çº§ç”¨æ³•ã€‚\n-\n-## å¤šæ™ºèƒ½ä½“\n-\n-å¤šæ™ºèƒ½ä½“åŠŸèƒ½æ˜¯å¾®è½¯æ¡†æ¶ [Autogen](https://huggingface.co/papers/2308.08155) ä¸­å¼•å…¥çš„ã€‚\n-å®ƒçš„æ„æ€æ˜¯è®©å¤šä¸ªæ™ºèƒ½ä½“ä¸€èµ·å·¥ä½œæ¥è§£å†³ä»»åŠ¡ï¼Œè€Œä¸æ˜¯åªæœ‰ä¸€ä¸ªæ™ºèƒ½ä½“ã€‚\n-ç»éªŒè¡¨æ˜ï¼Œåœ¨å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­ï¼Œè¿™ç§æ–¹æ³•èƒ½å¸¦æ¥æ›´å¥½çš„æ€§èƒ½ã€‚ä¹‹æ‰€ä»¥æœ‰æ›´å¥½çš„æ€§èƒ½ï¼ŒåŸå› å¾ˆç®€å•ï¼šå¯¹äºè®¸å¤šä»»åŠ¡ï¼Œé€šå¸¸æˆ‘ä»¬æ›´æ„¿æ„è®©å¤šä¸ªå•ç‹¬çš„å•å…ƒä¸“æ³¨äºå­ä»»åŠ¡ï¼Œè€Œä¸æ˜¯è®©ä¸€ä¸ªç³»ç»Ÿåšæ‰€æœ‰äº‹æƒ…ã€‚è¿™é‡Œï¼Œæ‹¥æœ‰ä¸åŒå·¥å…·é›†å’Œè®°å¿†çš„å¤šä¸ªæ™ºèƒ½ä½“å¯ä»¥å®ç°é«˜æ•ˆçš„ä¸“ä¸šåŒ–ã€‚\n-\n-ä½ å¯ä»¥è½»æ¾åœ°ç”¨ `transformers.agents` æ„å»ºå±‚æ¬¡åŒ–çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿã€‚\n-\n-ä¸ºæ­¤ï¼Œéœ€è¦å°†æ™ºèƒ½ä½“å°è£…åœ¨ [`ManagedAgent`] å¯¹è±¡ä¸­ã€‚è¿™ä¸ªå¯¹è±¡éœ€è¦ `agent`ã€`name` å’Œ `description` è¿™å‡ ä¸ªå‚æ•°ï¼Œè¿™äº›ä¿¡æ¯ä¼šåµŒå…¥åˆ°ç®¡ç†æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºä¸­ï¼Œå¸®åŠ©å®ƒçŸ¥é“å¦‚ä½•è°ƒç”¨è¿™ä¸ªç®¡ç†çš„æ™ºèƒ½ä½“ï¼Œå°±åƒæˆ‘ä»¬å¯¹å·¥å…·æ‰€åšçš„é‚£æ ·ã€‚\n-\n-ä¸‹é¢æ˜¯ä¸€ä¸ªé€šè¿‡ä½¿ç”¨æˆ‘ä»¬çš„ [`DuckDuckGoSearchTool`] åˆ›å»ºä¸€ä¸ªç®¡ç†ç‰¹å®šç½‘ç»œæœç´¢æ™ºèƒ½ä½“çš„ç¤ºä¾‹ï¼š\n-\n-\n-```py\n-from transformers.agents import ReactCodeAgent, HfApiEngine, DuckDuckGoSearchTool, ManagedAgent\n-\n-llm_engine = HfApiEngine()\n-\n-web_agent = ReactCodeAgent(tools=[DuckDuckGoSearchTool()], llm_engine=llm_engine)\n-\n-managed_web_agent = ManagedAgent(\n-    agent=web_agent,\n-    name=\"web_search\",\n-    description=\"Runs web searches for you. Give it your query as an argument.\"\n-)\n-\n-manager_agent = ReactCodeAgent(\n-    tools=[], llm_engine=llm_engine, managed_agents=[managed_web_agent]\n-)\n-\n-manager_agent.run(\"Who is the CEO of Hugging Face?\")\n-```\n-\n-> [!TIP]\n-> å¦‚æœä½ æƒ³æ·±å…¥äº†è§£å¦‚ä½•é«˜æ•ˆåœ°å®ç°å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œè¯·æŸ¥çœ‹ [how we pushed our multi-agent system to the top of the GAIA leaderboard](https://huggingface.co/blog/beating-gaia).\n-\n-## é«˜çº§å·¥å…·ä½¿ç”¨\n-\n-### é€šè¿‡å­ç±»åŒ– Tool æ¥ç›´æ¥å®šä¹‰å·¥å…·ï¼Œå¹¶å°†å…¶å…±äº«åˆ° Hub\n-\n-è®©æˆ‘ä»¬å†æ¬¡ä½¿ç”¨ä¸»æ–‡æ¡£ä¸­çš„å·¥å…·ç¤ºä¾‹ï¼Œæˆ‘ä»¬å·²ç»å®ç°äº†ä¸€ä¸ª `tool` è£…é¥°å™¨ã€‚\n-\n-å¦‚æœä½ éœ€è¦æ·»åŠ ä¸€äº›å˜åŒ–ï¼Œæ¯”å¦‚ä¸ºå·¥å…·è‡ªå®šä¹‰å±æ€§ï¼Œå¯ä»¥æŒ‰ç…§æ›´ç»†ç²’åº¦çš„æ–¹æ³•æ„å»ºå·¥å…·ï¼šæ„å»ºä¸€ä¸ªç»§æ‰¿è‡ª [`Tool`] è¶…ç±»çš„ç±»ã€‚\n-\n-è‡ªå®šä¹‰å·¥å…·éœ€è¦ï¼š\n-- `name` å±æ€§ï¼šè¡¨ç¤ºå·¥å…·æœ¬èº«çš„åç§°ï¼Œé€šå¸¸æè¿°å·¥å…·çš„ä½œç”¨ã€‚ç”±äºä»£ç è¿”å›äº†é’ˆå¯¹ä»»åŠ¡ä¸‹è½½é‡æœ€å¤šçš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†å…¶å‘½åä¸º model_download_counterã€‚\n-- `description` å±æ€§ï¼šç”¨äºå¡«å……æ™ºèƒ½ä½“çš„ç³»ç»Ÿæç¤ºã€‚\n-- `inputs` å±æ€§ï¼šè¿™æ˜¯ä¸€ä¸ªåŒ…å« \"type\" å’Œ \"description\" é”®çš„å­—å…¸ã€‚å®ƒåŒ…å«äº†æœ‰åŠ©äº Python è§£é‡Šå™¨åšå‡ºé€‰æ‹©çš„è¾“å…¥ä¿¡æ¯ã€‚\n-- `output_type` å±æ€§ï¼šæŒ‡å®šè¾“å‡ºç±»å‹ã€‚\n-- `forward` æ–¹æ³•ï¼šå…¶ä¸­åŒ…å«æ‰§è¡Œæ¨ç†ä»£ç ã€‚\n-\n-`inputs` å’Œ `output_type` çš„ç±»å‹åº”å½“æ˜¯ [Pydantic æ ¼å¼](https://docs.pydantic.dev/latest/concepts/json_schema/#generating-json-schema)ã€‚\n-\n-```python\n-from transformers import Tool\n-from huggingface_hub import list_models\n-\n-class HFModelDownloadsTool(Tool):\n-    name = \"model_download_counter\"\n-    description = \"\"\"\n-    This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n-    It returns the name of the checkpoint.\"\"\"\n-\n-    inputs = {\n-        \"task\": {\n-            \"type\": \"string\",\n-            \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n-        }\n-    }\n-    output_type = \"string\"\n-\n-    def forward(self, task: str):\n-        model = next(iter(list_models(filter=task, sort=\"downloads\", direction=-1)))\n-        return model.id\n-```\n-\n-ç°åœ¨ï¼Œè‡ªå®šä¹‰çš„ `HfModelDownloadsTool` ç±»å·²ç»å‡†å¤‡å¥½ï¼Œå¯ä»¥å°†å…¶ä¿å­˜åˆ°åä¸º `model_downloads.py` çš„æ–‡ä»¶ä¸­ï¼Œå¹¶å¯¼å…¥ä½¿ç”¨ã€‚\n-\n-\n-```python\n-from model_downloads import HFModelDownloadsTool\n-\n-tool = HFModelDownloadsTool()\n-```\n-\n-ä½ è¿˜å¯ä»¥é€šè¿‡è°ƒç”¨ [`~Tool.push_to_hub`] å°†è‡ªå®šä¹‰å·¥å…·æ¨é€åˆ° Hubã€‚ç¡®ä¿ä½ å·²ç»ä¸ºè¯¥å·¥å…·åˆ›å»ºäº†ä¸€ä¸ªä»“åº“ï¼Œå¹¶ä½¿ç”¨å…·æœ‰è¯»å–è®¿é—®æƒé™çš„è®¸å¯ã€‚\n-\n-```python\n-tool.push_to_hub(\"{your_username}/hf-model-downloads\")\n-```\n-\n-é€šè¿‡ [`~Tool.load_tool`] å‡½æ•°åŠ è½½å·¥å…·ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™æ™ºèƒ½ä½“çš„ tools å‚æ•°ã€‚\n-\n-```python\n-from transformers import load_tool, CodeAgent\n-\n-model_download_tool = load_tool(\"m-ric/hf-model-downloads\")\n-```\n-\n-### å°† Space å¯¼å…¥ä¸ºå·¥å…· ğŸš€\n-\n-ä½ å¯ä»¥ç›´æ¥é€šè¿‡ [`Tool.from_space`] æ–¹æ³•å°† Hub ä¸Šçš„ Space å¯¼å…¥ä¸ºå·¥å…·ï¼\n-\n-åªéœ€è¦æä¾› Space åœ¨ Hub ä¸Šçš„ IDã€åç§°å’Œæè¿°ï¼Œå¸®åŠ©æ™ºèƒ½ä½“ç†è§£å·¥å…·çš„ä½œç”¨ã€‚åœ¨å¹•åï¼Œè¿™å°†ä½¿ç”¨ [`gradio-client`](https://pypi.org/project/gradio-client/) åº“æ¥è°ƒç”¨ Spaceã€‚\n-\n-ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯ä» Hub å¯¼å…¥ `FLUX.1-dev` Space å¹¶ç”¨å…¶ç”Ÿæˆå›¾åƒçš„ç¤ºä¾‹ï¼š\n-\n-```\n-from transformers import Tool\n-image_generation_tool = Tool.from_space(\n-    \"black-forest-labs/FLUX.1-dev\",\n-    name=\"image_generator\",\n-    description=\"Generate an image from a prompt\")\n-image_generation_tool(\"A sunny beach\")\n-```\n-çœ‹ï¼è¿™å°±æ˜¯ä½ ç”Ÿæˆçš„å›¾åƒï¼ğŸ–ï¸\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/sunny_beach.webp\">\n-\n-ç„¶åï¼Œä½ å¯ä»¥åƒä½¿ç”¨å…¶ä»–å·¥å…·ä¸€æ ·ä½¿ç”¨è¿™ä¸ªå·¥å…·ã€‚ä¾‹å¦‚ï¼Œæ”¹è¿›æç¤º `ç©¿å®‡èˆªæœçš„å…”å­` å¹¶ç”Ÿæˆå…¶å›¾åƒï¼š\n-\n-```python\n-from transformers import ReactCodeAgent\n-\n-agent = ReactCodeAgent(tools=[image_generation_tool])\n-\n-agent.run(\n-    \"Improve this prompt, then generate an image of it.\", prompt='A rabbit wearing a space suit'\n-)\n-```\n-\n-```text\n-=== Agent thoughts:\n-improved_prompt could be \"A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background\"\n-Now that I have improved the prompt, I can use the image generator tool to generate an image based on this prompt.\n->>> Agent is executing the code below:\n-image = image_generator(prompt=\"A bright blue space suit wearing rabbit, on the surface of the moon, under a bright orange sunset, with the Earth visible in the background\")\n-final_answer(image)\n-```\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit_spacesuit_flux.webp\">\n-\n-è¿™çœŸé…·å§ï¼ŸğŸ¤©\n-\n-### ä½¿ç”¨ gradio-tools\n-\n-[gradio-tools](https://github.com/freddyaboulton/gradio-tools) æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åº“ï¼Œå…è®¸ä½¿ç”¨ Hugging Face Spaces ä½œä¸ºå·¥å…·ã€‚å®ƒæ”¯æŒè®¸å¤šç°æœ‰çš„ Spacesï¼Œä¹Ÿæ”¯æŒè‡ªå®šä¹‰ Spacesã€‚\n-\n-transformers æ”¯æŒé€šè¿‡ [`Tool.from_gradio`] æ–¹æ³•ä½¿ç”¨ `gradio_tools`ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯å¦‚ä½•ä½¿ç”¨æ¥è‡ª `gradio-tools` å·¥å…·åŒ…çš„ [`StableDiffusionPromptGeneratorTool`](https://github.com/freddyaboulton/gradio-tools/blob/main/gradio_tools/tools/prompt_generator.py) æ¥æ”¹è¿›æç¤ºï¼Œä»¥ç”Ÿæˆæ›´å¥½çš„å›¾åƒï¼š\n-\n-å¯¼å…¥å’Œå®ä¾‹åŒ–å·¥å…·ï¼Œå¹¶å°†å…¶ä¼ é€’ç»™ `Tool.from_gradio` æ–¹æ³•:\n-\n-```python\n-from gradio_tools import StableDiffusionPromptGeneratorTool\n-from transformers import Tool, load_tool, CodeAgent\n-\n-gradio_prompt_generator_tool = StableDiffusionPromptGeneratorTool()\n-prompt_generator_tool = Tool.from_gradio(gradio_prompt_generator_tool)\n-```\n-\n-> [!WARNING]\n-> gradio-tools éœ€è¦ **æ–‡æœ¬** è¾“å…¥å’Œè¾“å‡ºï¼Œå³ä½¿åœ¨å¤„ç†åƒå›¾åƒå’ŒéŸ³é¢‘è¿™æ ·çš„ä¸åŒæ¨¡æ€æ—¶ä¹Ÿæ˜¯å¦‚æ­¤ã€‚ç›®å‰ï¼Œå›¾åƒå’ŒéŸ³é¢‘çš„è¾“å…¥è¾“å‡ºä¸æ­¤ä¸å…¼å®¹ã€‚\n-### ä½¿ç”¨ LangChain å·¥å…·\n-\n-æˆ‘ä»¬å¾ˆå–œæ¬¢ LangChainï¼Œå¹¶è®¤ä¸ºå®ƒæœ‰ä¸€å¥—éå¸¸æœ‰å¸å¼•åŠ›çš„å·¥å…·ã€‚\n-è¦ä» LangChain å¯¼å…¥å·¥å…·ï¼Œå¯ä»¥ä½¿ç”¨ `from_langchain()` æ–¹æ³•ã€‚\n-\n-ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯å¦‚ä½•ä½¿ç”¨å®ƒæ¥é‡æ–°åˆ›å»ºä¸Šé¢ä»‹ç»çš„æœç´¢ç»“æœï¼Œä½¿ç”¨ä¸€ä¸ª LangChain ç½‘ç»œæœç´¢å·¥å…·ã€‚è¯¥å·¥å…·éœ€è¦ `pip install google-search-results` æ¥æ­£å¸¸å·¥ä½œã€‚\n-\n-```python\n-from langchain.agents import load_tools\n-from transformers import Tool, ReactCodeAgent\n-\n-search_tool = Tool.from_langchain(load_tools([\"serpapi\"])[0])\n-\n-agent = ReactCodeAgent(tools=[search_tool])\n-\n-agent.run(\"How many more blocks (also denoted as layers) are in BERT base encoder compared to the encoder from the architecture proposed in Attention is All You Need?\")\n-```\n-\n-## åœ¨é…·ç‚«çš„ Gradio ç•Œé¢ä¸­å±•ç¤ºæ™ºèƒ½ä½“è¿è¡Œ\n-\n-ä½ å¯ä»¥åˆ©ç”¨ `gradio.Chatbot` æ¥å±•ç¤ºæ™ºèƒ½ä½“çš„æ€è€ƒè¿‡ç¨‹ï¼Œé€šè¿‡ `stream_to_gradio`ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼š\n-\n-```py\n-import gradio as gr\n-from transformers import (\n-    load_tool,\n-    ReactCodeAgent,\n-    HfApiEngine,\n-    stream_to_gradio,\n-)\n-\n-# Import tool from Hub\n-image_generation_tool = load_tool(\"m-ric/text-to-image\")\n-\n-llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n-\n-# Initialize the agent with the image generation tool\n-agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n-\n-\n-def interact_with_agent(task):\n-    messages = []\n-    messages.append(gr.ChatMessage(role=\"user\", content=task))\n-    yield messages\n-    for msg in stream_to_gradio(agent, task):\n-        messages.append(msg)\n-        yield messages + [\n-            gr.ChatMessage(role=\"assistant\", content=\"â³ Task not finished yet!\")\n-        ]\n-    yield messages\n-\n-\n-with gr.Blocks() as demo:\n-    text_input = gr.Textbox(lines=1, label=\"Chat Message\", value=\"Make me a picture of the Statue of Liberty.\")\n-    submit = gr.Button(\"Run illustrator agent!\")\n-    chatbot = gr.Chatbot(\n-        label=\"Agent\",\n-        type=\"messages\",\n-        avatar_images=(\n-            None,\n-            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n-        ),\n-    )\n-    submit.click(interact_with_agent, [text_input], [chatbot])\n-\n-if __name__ == \"__main__\":\n-    demo.launch()\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "b35a382feb9797ffe4870bde9465a021952530ff",
            "filename": "docs/source/zh/main_classes/agent.md",
            "status": "removed",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fzh%2Fmain_classes%2Fagent.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/docs%2Fsource%2Fzh%2Fmain_classes%2Fagent.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fagent.md?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,26 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Agentså’Œå·¥å…·\n-\n-<Tip warning={true}>\n-\n-The Agents framework has significantly changed in version v4.41.0.\n-This document has been removed as it was referencing an older API.\n-\n-We eagerly welcome new contributions for the updated API.\n-\n-</Tip>"
        },
        {
            "sha": "504bfb708fb1420cda4cfe9c3c9c8d3e0cea0273",
            "filename": "setup.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -431,10 +431,6 @@ def run(self):\n     \"tqdm\",\n )\n \n-extras[\"agents\"] = deps_list(\n-    \"diffusers\", \"accelerate\", \"datasets\", \"torch\", \"sentencepiece\", \"opencv-python\", \"Pillow\"\n-)\n-\n extras[\"benchmark\"] = deps_list(\"optimum-benchmark\")\n \n # when modifying the following list, make sure to update src/transformers/dependency_versions_check.py"
        },
        {
            "sha": "75f03315f9c06515bd5485d29763aa4c72a5397d",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -55,24 +55,6 @@\n \n # Base objects, independent of any specific backend\n _import_structure = {\n-    \"agents\": [\n-        \"Agent\",\n-        \"CodeAgent\",\n-        \"HfApiEngine\",\n-        \"ManagedAgent\",\n-        \"PipelineTool\",\n-        \"ReactAgent\",\n-        \"ReactCodeAgent\",\n-        \"ReactJsonAgent\",\n-        \"Tool\",\n-        \"Toolbox\",\n-        \"ToolCollection\",\n-        \"TransformersEngine\",\n-        \"launch_gradio_demo\",\n-        \"load_tool\",\n-        \"stream_to_gradio\",\n-        \"tool\",\n-    ],\n     \"audio_utils\": [],\n     \"commands\": [],\n     \"configuration_utils\": [\"PretrainedConfig\"],\n@@ -565,25 +547,6 @@\n # Direct imports for type-checking\n if TYPE_CHECKING:\n     # All modeling imports\n-    # Agents\n-    from .agents import (\n-        Agent,\n-        CodeAgent,\n-        HfApiEngine,\n-        ManagedAgent,\n-        PipelineTool,\n-        ReactAgent,\n-        ReactCodeAgent,\n-        ReactJsonAgent,\n-        Tool,\n-        Toolbox,\n-        ToolCollection,\n-        TransformersEngine,\n-        launch_gradio_demo,\n-        load_tool,\n-        stream_to_gradio,\n-        tool,\n-    )\n     from .configuration_utils import PretrainedConfig\n \n     # Data"
        },
        {
            "sha": "70762c252a832882a53c1baf515c58356dffe1a1",
            "filename": "src/transformers/agents/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 69,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2F__init__.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,69 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from typing import TYPE_CHECKING\n-\n-from ..utils import (\n-    OptionalDependencyNotAvailable,\n-    _LazyModule,\n-    is_torch_available,\n-)\n-\n-\n-_import_structure = {\n-    \"agents\": [\"Agent\", \"CodeAgent\", \"ManagedAgent\", \"ReactAgent\", \"ReactCodeAgent\", \"ReactJsonAgent\", \"Toolbox\"],\n-    \"llm_engine\": [\"HfApiEngine\", \"TransformersEngine\"],\n-    \"monitoring\": [\"stream_to_gradio\"],\n-    \"tools\": [\"PipelineTool\", \"Tool\", \"ToolCollection\", \"launch_gradio_demo\", \"load_tool\", \"tool\"],\n-}\n-\n-try:\n-    if not is_torch_available():\n-        raise OptionalDependencyNotAvailable()\n-except OptionalDependencyNotAvailable:\n-    pass\n-else:\n-    _import_structure[\"default_tools\"] = [\"FinalAnswerTool\", \"PythonInterpreterTool\"]\n-    _import_structure[\"document_question_answering\"] = [\"DocumentQuestionAnsweringTool\"]\n-    _import_structure[\"image_question_answering\"] = [\"ImageQuestionAnsweringTool\"]\n-    _import_structure[\"search\"] = [\"DuckDuckGoSearchTool\", \"VisitWebpageTool\"]\n-    _import_structure[\"speech_to_text\"] = [\"SpeechToTextTool\"]\n-    _import_structure[\"text_to_speech\"] = [\"TextToSpeechTool\"]\n-    _import_structure[\"translation\"] = [\"TranslationTool\"]\n-\n-if TYPE_CHECKING:\n-    from .agents import Agent, CodeAgent, ManagedAgent, ReactAgent, ReactCodeAgent, ReactJsonAgent, Toolbox\n-    from .llm_engine import HfApiEngine, TransformersEngine\n-    from .monitoring import stream_to_gradio\n-    from .tools import PipelineTool, Tool, ToolCollection, launch_gradio_demo, load_tool, tool\n-\n-    try:\n-        if not is_torch_available():\n-            raise OptionalDependencyNotAvailable()\n-    except OptionalDependencyNotAvailable:\n-        pass\n-    else:\n-        from .default_tools import FinalAnswerTool, PythonInterpreterTool\n-        from .document_question_answering import DocumentQuestionAnsweringTool\n-        from .image_question_answering import ImageQuestionAnsweringTool\n-        from .search import DuckDuckGoSearchTool, VisitWebpageTool\n-        from .speech_to_text import SpeechToTextTool\n-        from .text_to_speech import TextToSpeechTool\n-        from .translation import TranslationTool\n-else:\n-    import sys\n-\n-    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure, module_spec=__spec__)"
        },
        {
            "sha": "2da60745fae091efaad4cbbf322040a6ecd54af2",
            "filename": "src/transformers/agents/agent_types.py",
            "status": "removed",
            "additions": 0,
            "deletions": 260,
            "changes": 260,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fagent_types.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fagent_types.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagent_types.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,260 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import os\n-import pathlib\n-import tempfile\n-import uuid\n-\n-import numpy as np\n-\n-from ..utils import is_soundfile_available, is_torch_available, is_vision_available, logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-if is_vision_available():\n-    from PIL import Image\n-    from PIL.Image import Image as ImageType\n-else:\n-    ImageType = object\n-\n-if is_torch_available():\n-    import torch\n-    from torch import Tensor\n-else:\n-    Tensor = object\n-\n-if is_soundfile_available():\n-    import soundfile as sf\n-\n-\n-class AgentType:\n-    \"\"\"\n-    Abstract class to be reimplemented to define types that can be returned by agents.\n-\n-    These objects serve three purposes:\n-\n-    - They behave as they were the type they're meant to be, e.g., a string for text, a PIL.Image for images\n-    - They can be stringified: str(object) in order to return a string defining the object\n-    - They should be displayed correctly in ipython notebooks/colab/jupyter\n-    \"\"\"\n-\n-    def __init__(self, value):\n-        self._value = value\n-\n-    def __str__(self):\n-        return self.to_string()\n-\n-    def to_raw(self):\n-        logger.error(\n-            \"This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable\"\n-        )\n-        return self._value\n-\n-    def to_string(self) -> str:\n-        logger.error(\n-            \"This is a raw AgentType of unknown type. Display in notebooks and string conversion will be unreliable\"\n-        )\n-        return str(self._value)\n-\n-\n-class AgentText(AgentType, str):\n-    \"\"\"\n-    Text type returned by the agent. Behaves as a string.\n-    \"\"\"\n-\n-    def to_raw(self):\n-        return self._value\n-\n-    def to_string(self):\n-        return str(self._value)\n-\n-\n-class AgentImage(AgentType, ImageType):\n-    \"\"\"\n-    Image type returned by the agent. Behaves as a PIL.Image.\n-    \"\"\"\n-\n-    def __init__(self, value):\n-        AgentType.__init__(self, value)\n-        ImageType.__init__(self)\n-\n-        if not is_vision_available():\n-            raise ImportError(\"PIL must be installed in order to handle images.\")\n-\n-        self._path = None\n-        self._raw = None\n-        self._tensor = None\n-\n-        if isinstance(value, ImageType):\n-            self._raw = value\n-        elif isinstance(value, (str, pathlib.Path)):\n-            self._path = value\n-        elif isinstance(value, torch.Tensor):\n-            self._tensor = value\n-        elif isinstance(value, np.ndarray):\n-            self._tensor = torch.from_numpy(value)\n-        else:\n-            raise TypeError(f\"Unsupported type for {self.__class__.__name__}: {type(value)}\")\n-\n-    def _ipython_display_(self, include=None, exclude=None):\n-        \"\"\"\n-        Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)\n-        \"\"\"\n-        from IPython.display import Image, display\n-\n-        display(Image(self.to_string()))\n-\n-    def to_raw(self):\n-        \"\"\"\n-        Returns the \"raw\" version of that object. In the case of an AgentImage, it is a PIL.Image.\n-        \"\"\"\n-        if self._raw is not None:\n-            return self._raw\n-\n-        if self._path is not None:\n-            self._raw = Image.open(self._path)\n-            return self._raw\n-\n-        if self._tensor is not None:\n-            array = self._tensor.detach().cpu().numpy()\n-            return Image.fromarray((255 - array * 255).astype(np.uint8))\n-\n-    def to_string(self):\n-        \"\"\"\n-        Returns the stringified version of that object. In the case of an AgentImage, it is a path to the serialized\n-        version of the image.\n-        \"\"\"\n-        if self._path is not None:\n-            return self._path\n-\n-        if self._raw is not None:\n-            directory = tempfile.mkdtemp()\n-            self._path = os.path.join(directory, str(uuid.uuid4()) + \".png\")\n-            self._raw.save(self._path)\n-            return self._path\n-\n-        if self._tensor is not None:\n-            array = self._tensor.detach().cpu().numpy()\n-\n-            # There is likely simpler than load into image into save\n-            img = Image.fromarray((255 - array * 255).astype(np.uint8))\n-\n-            directory = tempfile.mkdtemp()\n-            self._path = os.path.join(directory, str(uuid.uuid4()) + \".png\")\n-\n-            img.save(self._path)\n-\n-            return self._path\n-\n-    def save(self, output_bytes, format, **params):\n-        \"\"\"\n-        Saves the image to a file.\n-        Args:\n-            output_bytes (bytes): The output bytes to save the image to.\n-            format (str): The format to use for the output image. The format is the same as in PIL.Image.save.\n-            **params: Additional parameters to pass to PIL.Image.save.\n-        \"\"\"\n-        img = self.to_raw()\n-        img.save(output_bytes, format, **params)\n-\n-\n-class AgentAudio(AgentType, str):\n-    \"\"\"\n-    Audio type returned by the agent.\n-    \"\"\"\n-\n-    def __init__(self, value, samplerate=16_000):\n-        super().__init__(value)\n-\n-        if not is_soundfile_available():\n-            raise ImportError(\"soundfile must be installed in order to handle audio.\")\n-\n-        self._path = None\n-        self._tensor = None\n-\n-        self.samplerate = samplerate\n-        if isinstance(value, (str, pathlib.Path)):\n-            self._path = value\n-        elif is_torch_available() and isinstance(value, torch.Tensor):\n-            self._tensor = value\n-        elif isinstance(value, tuple):\n-            self.samplerate = value[0]\n-            if isinstance(value[1], np.ndarray):\n-                self._tensor = torch.from_numpy(value[1])\n-            else:\n-                self._tensor = torch.tensor(value[1])\n-        else:\n-            raise ValueError(f\"Unsupported audio type: {type(value)}\")\n-\n-    def _ipython_display_(self, include=None, exclude=None):\n-        \"\"\"\n-        Displays correctly this type in an ipython notebook (ipython, colab, jupyter, ...)\n-        \"\"\"\n-        from IPython.display import Audio, display\n-\n-        display(Audio(self.to_string(), rate=self.samplerate))\n-\n-    def to_raw(self):\n-        \"\"\"\n-        Returns the \"raw\" version of that object. It is a `torch.Tensor` object.\n-        \"\"\"\n-        if self._tensor is not None:\n-            return self._tensor\n-\n-        if self._path is not None:\n-            tensor, self.samplerate = sf.read(self._path)\n-            self._tensor = torch.tensor(tensor)\n-            return self._tensor\n-\n-    def to_string(self):\n-        \"\"\"\n-        Returns the stringified version of that object. In the case of an AgentAudio, it is a path to the serialized\n-        version of the audio.\n-        \"\"\"\n-        if self._path is not None:\n-            return self._path\n-\n-        if self._tensor is not None:\n-            directory = tempfile.mkdtemp()\n-            self._path = os.path.join(directory, str(uuid.uuid4()) + \".wav\")\n-            sf.write(self._path, self._tensor, samplerate=self.samplerate)\n-            return self._path\n-\n-\n-AGENT_TYPE_MAPPING = {\"string\": AgentText, \"image\": AgentImage, \"audio\": AgentAudio}\n-INSTANCE_TYPE_MAPPING = {str: AgentText, ImageType: AgentImage}\n-\n-if is_torch_available():\n-    INSTANCE_TYPE_MAPPING[Tensor] = AgentAudio\n-\n-\n-def handle_agent_inputs(*args, **kwargs):\n-    args = [(arg.to_raw() if isinstance(arg, AgentType) else arg) for arg in args]\n-    kwargs = {k: (v.to_raw() if isinstance(v, AgentType) else v) for k, v in kwargs.items()}\n-    return args, kwargs\n-\n-\n-def handle_agent_outputs(output, output_type=None):\n-    if output_type in AGENT_TYPE_MAPPING:\n-        # If the class has defined outputs, we can map directly according to the class definition\n-        decoded_outputs = AGENT_TYPE_MAPPING[output_type](output)\n-        return decoded_outputs\n-    else:\n-        # If the class does not have defined output, then we map according to the type\n-        for _k, _v in INSTANCE_TYPE_MAPPING.items():\n-            if isinstance(output, _k):\n-                return _v(output)\n-        return output"
        },
        {
            "sha": "f1c1d58ff217026e9e58b51d2b4cac29c92bb322",
            "filename": "src/transformers/agents/agents.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1292,
            "changes": 1292,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fagents.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fagents.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagents.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,1292 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import json\n-import logging\n-import re\n-import time\n-from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n-\n-from huggingface_hub.utils._deprecation import _deprecate_method\n-\n-from .. import is_torch_available\n-from ..utils import logging as transformers_logging\n-from ..utils.import_utils import is_pygments_available\n-from .agent_types import AgentAudio, AgentImage\n-from .default_tools import BASE_PYTHON_TOOLS, FinalAnswerTool, setup_default_tools\n-from .llm_engine import HfApiEngine, MessageRole\n-from .monitoring import Monitor\n-from .prompts import (\n-    DEFAULT_CODE_SYSTEM_PROMPT,\n-    DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n-    DEFAULT_REACT_JSON_SYSTEM_PROMPT,\n-    PLAN_UPDATE_FINAL_PLAN_REDACTION,\n-    PROMPTS_FOR_INITIAL_PLAN,\n-    PROMPTS_FOR_PLAN_UPDATE,\n-    SUPPORTED_PLAN_TYPES,\n-    SYSTEM_PROMPT_FACTS,\n-    SYSTEM_PROMPT_FACTS_UPDATE,\n-    USER_PROMPT_FACTS_UPDATE,\n-)\n-from .python_interpreter import LIST_SAFE_MODULES, evaluate_python_code\n-from .tools import (\n-    DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n-    Tool,\n-    get_tool_description_with_args,\n-    load_tool,\n-)\n-\n-\n-if is_pygments_available():\n-    from pygments import highlight\n-    from pygments.formatters import Terminal256Formatter\n-    from pygments.lexers import PythonLexer\n-\n-\n-class CustomFormatter(logging.Formatter):\n-    grey = \"\\x1b[38;20m\"\n-    bold_yellow = \"\\x1b[33;1m\"\n-    red = \"\\x1b[31;20m\"\n-    green = \"\\x1b[32;20m\"\n-    bold_green = \"\\x1b[32;20;1m\"\n-    bold_red = \"\\x1b[31;1m\"\n-    bold_white = \"\\x1b[37;1m\"\n-    orange = \"\\x1b[38;5;214m\"\n-    bold_orange = \"\\x1b[38;5;214;1m\"\n-    reset = \"\\x1b[0m\"\n-    format = \"%(message)s\"\n-\n-    FORMATS = {\n-        logging.DEBUG: grey + format + reset,\n-        logging.INFO: format,\n-        logging.WARNING: bold_yellow + format + reset,\n-        logging.ERROR: red + format + reset,\n-        logging.CRITICAL: bold_red + format + reset,\n-        31: reset + format + reset,\n-        32: green + format + reset,\n-        33: bold_green + format + reset,\n-        34: bold_white + format + reset,\n-        35: orange + format + reset,\n-        36: bold_orange + format + reset,\n-    }\n-\n-    def format(self, record):\n-        log_fmt = self.FORMATS.get(record.levelno)\n-        formatter = logging.Formatter(log_fmt)\n-        return formatter.format(record)\n-\n-\n-logger = transformers_logging.get_logger(__name__)\n-logger.propagate = False\n-ch = logging.StreamHandler()\n-ch.setFormatter(CustomFormatter())\n-logger.addHandler(ch)\n-\n-\n-def parse_json_blob(json_blob: str) -> Dict[str, str]:\n-    try:\n-        first_accolade_index = json_blob.find(\"{\")\n-        last_accolade_index = [a.start() for a in list(re.finditer(\"}\", json_blob))][-1]\n-        json_blob = json_blob[first_accolade_index : last_accolade_index + 1].replace('\\\\\"', \"'\")\n-        json_data = json.loads(json_blob, strict=False)\n-        return json_data\n-    except json.JSONDecodeError as e:\n-        place = e.pos\n-        if json_blob[place - 1 : place + 2] == \"},\\n\":\n-            raise ValueError(\n-                \"JSON is invalid: you probably tried to provide multiple tool calls in one action. PROVIDE ONLY ONE TOOL CALL.\"\n-            )\n-        raise ValueError(\n-            f\"The JSON blob you used is invalid due to the following error: {e}.\\n\"\n-            f\"JSON blob was: {json_blob}, decoding failed on that specific part of the blob:\\n\"\n-            f\"'{json_blob[place - 4 : place + 5]}'.\"\n-        )\n-    except Exception as e:\n-        raise ValueError(f\"Error in parsing the JSON blob: {e}\")\n-\n-\n-def parse_code_blob(code_blob: str) -> str:\n-    try:\n-        pattern = r\"```(?:py|python)?\\n(.*?)\\n```\"\n-        match = re.search(pattern, code_blob, re.DOTALL)\n-        return match.group(1).strip()\n-    except Exception as e:\n-        raise ValueError(\n-            f\"\"\"\n-The code blob you used is invalid: due to the following error: {e}\n-This means that the regex pattern {pattern} was not respected: make sure to include code with the correct pattern, for instance:\n-Thoughts: Your thoughts\n-Code:\n-```py\n-# Your python code here\n-```<end_action>\"\"\"\n-        )\n-\n-\n-def parse_json_tool_call(json_blob: str) -> Tuple[str, Dict[str, str]]:\n-    json_blob = json_blob.replace(\"```json\", \"\").replace(\"```\", \"\")\n-    tool_call = parse_json_blob(json_blob)\n-    if \"action\" in tool_call and \"action_input\" in tool_call:\n-        return tool_call[\"action\"], tool_call[\"action_input\"]\n-    elif \"action\" in tool_call:\n-        return tool_call[\"action\"], None\n-    else:\n-        raise ValueError(\n-            f\"Missing keys: {[key for key in ['action', 'action_input'] if key not in tool_call]} in blob {tool_call}\"\n-        )\n-\n-\n-def parse_text_tool_call(text: str) -> Tuple[str, Union[str, Dict[str, str]]]:\n-    \"\"\"\n-    Expects a text in the format: 'Action:', 'Action input:', 'Observation:'. 'Action input:' contains a json string with input arguments.\n-    \"\"\"\n-    try:\n-        if \"Observation:\" in text:\n-            text = text.split(\"Observation:\")[0]\n-        if \"Action:\" in text:\n-            text = text.split(\"Action:\")[1]\n-        tool_name, tool_input = text.split(\"Action input:\")\n-        if \"{\" in tool_input:\n-            tool_input = parse_json_blob(tool_input)\n-        else:\n-            tool_input = tool_input.strip().replace('\"', \"\")\n-        return tool_name.strip().replace('\"', \"\").replace(\"\\\\\", \"\"), tool_input\n-    except Exception as e:\n-        raise ValueError(\n-            f\"Error in parsing the text tool call: {e}. Be sure to provide the correct format. DO NOT repeat your previous incorrect tool call.\"\n-        )\n-\n-\n-def to_text(input: Union[List[Dict[str, str]], Dict[str, str], str]) -> str:\n-    if isinstance(input, list):\n-        return \"\\n\".join([m[\"content\"] for m in input])\n-    elif isinstance(input, dict):\n-        return input[\"content\"]\n-    else:\n-        return input\n-\n-\n-HUGGINGFACE_DEFAULT_TOOLS = {}\n-_tools_are_initialized = False\n-\n-\n-class Toolbox:\n-    \"\"\"\n-    The toolbox contains all tools that the agent can perform operations with, as well as a few methods to\n-    manage them.\n-\n-    Args:\n-        tools (`List[Tool]`):\n-            The list of tools to instantiate the toolbox with\n-        add_base_tools (`bool`, defaults to `False`, *optional*, defaults to `False`):\n-            Whether to add the tools available within `transformers` to the toolbox.\n-    \"\"\"\n-\n-    def __init__(self, tools: List[Tool], add_base_tools: bool = False):\n-        self._tools = {tool.name: tool for tool in tools}\n-        if add_base_tools:\n-            self.add_base_tools()\n-        self._load_tools_if_needed()\n-\n-    def add_base_tools(self, add_python_interpreter: bool = False):\n-        global _tools_are_initialized\n-        global HUGGINGFACE_DEFAULT_TOOLS\n-        if not _tools_are_initialized:\n-            HUGGINGFACE_DEFAULT_TOOLS = setup_default_tools(logger)\n-            _tools_are_initialized = True\n-        for tool in HUGGINGFACE_DEFAULT_TOOLS.values():\n-            if tool.name != \"python_interpreter\" or add_python_interpreter:\n-                self.add_tool(tool)\n-        self._load_tools_if_needed()\n-\n-    @property\n-    def tools(self) -> Dict[str, Tool]:\n-        \"\"\"Get all tools currently in the toolbox\"\"\"\n-        return self._tools\n-\n-    def show_tool_descriptions(self, tool_description_template: Optional[str] = None) -> str:\n-        \"\"\"\n-        Returns the description of all tools in the toolbox\n-\n-        Args:\n-            tool_description_template (`str`, *optional*):\n-                The template to use to describe the tools. If not provided, the default template will be used.\n-        \"\"\"\n-        return \"\\n\".join(\n-            [get_tool_description_with_args(tool, tool_description_template) for tool in self._tools.values()]\n-        )\n-\n-    def add_tool(self, tool: Tool):\n-        \"\"\"\n-        Adds a tool to the toolbox\n-\n-        Args:\n-            tool (`Tool`):\n-                The tool to add to the toolbox.\n-        \"\"\"\n-        if tool.name in self._tools:\n-            raise KeyError(f\"Error: tool '{tool.name}' already exists in the toolbox.\")\n-        self._tools[tool.name] = tool\n-\n-    def remove_tool(self, tool_name: str):\n-        \"\"\"\n-        Removes a tool from the toolbox\n-\n-        Args:\n-            tool_name (`str`):\n-                The tool to remove from the toolbox.\n-        \"\"\"\n-        if tool_name not in self._tools:\n-            raise KeyError(\n-                f\"Error: tool {tool_name} not found in toolbox for removal, should be instead one of {list(self._tools.keys())}.\"\n-            )\n-        del self._tools[tool_name]\n-\n-    def update_tool(self, tool: Tool):\n-        \"\"\"\n-        Updates a tool in the toolbox according to its name.\n-\n-        Args:\n-            tool (`Tool`):\n-                The tool to update to the toolbox.\n-        \"\"\"\n-        if tool.name not in self._tools:\n-            raise KeyError(\n-                f\"Error: tool {tool.name} not found in toolbox for update, should be instead one of {list(self._tools.keys())}.\"\n-            )\n-        self._tools[tool.name] = tool\n-\n-    def clear_toolbox(self):\n-        \"\"\"Clears the toolbox\"\"\"\n-        self._tools = {}\n-\n-    def _load_tools_if_needed(self):\n-        for name, tool in self._tools.items():\n-            if not isinstance(tool, Tool):\n-                task_or_repo_id = tool.task if tool.repo_id is None else tool.repo_id\n-                self._tools[name] = load_tool(task_or_repo_id)\n-\n-    def __repr__(self):\n-        toolbox_description = \"Toolbox contents:\\n\"\n-        for tool in self._tools.values():\n-            toolbox_description += f\"\\t{tool.name}: {tool.description}\\n\"\n-        return toolbox_description\n-\n-\n-class AgentError(Exception):\n-    \"\"\"Base class for other agent-related exceptions\"\"\"\n-\n-    def __init__(self, message):\n-        super().__init__(message)\n-        self.message = message\n-\n-\n-class AgentParsingError(AgentError):\n-    \"\"\"Exception raised for errors in parsing in the agent\"\"\"\n-\n-    pass\n-\n-\n-class AgentExecutionError(AgentError):\n-    \"\"\"Exception raised for errors in execution in the agent\"\"\"\n-\n-    pass\n-\n-\n-class AgentMaxIterationsError(AgentError):\n-    \"\"\"Exception raised for errors in execution in the agent\"\"\"\n-\n-    pass\n-\n-\n-class AgentGenerationError(AgentError):\n-    \"\"\"Exception raised for errors in generation in the agent\"\"\"\n-\n-    pass\n-\n-\n-def format_prompt_with_tools(toolbox: Toolbox, prompt_template: str, tool_description_template: str) -> str:\n-    tool_descriptions = toolbox.show_tool_descriptions(tool_description_template)\n-    prompt = prompt_template.replace(\"<<tool_descriptions>>\", tool_descriptions)\n-\n-    if \"<<tool_names>>\" in prompt:\n-        tool_names = [f\"'{tool_name}'\" for tool_name in toolbox.tools.keys()]\n-        prompt = prompt.replace(\"<<tool_names>>\", \", \".join(tool_names))\n-\n-    return prompt\n-\n-\n-def show_agents_descriptions(managed_agents: list):\n-    managed_agents_descriptions = \"\"\"\n-You can also give requests to team members.\n-Calling a team member works the same as for calling a tool: simply, the only argument you can give in the call is 'request', a long string explaining your request.\n-Given that this team member is a real human, you should be very verbose in your request.\n-Here is a list of the team members that you can call:\"\"\"\n-    for agent in managed_agents.values():\n-        managed_agents_descriptions += f\"\\n- {agent.name}: {agent.description}\"\n-    return managed_agents_descriptions\n-\n-\n-def format_prompt_with_managed_agents_descriptions(prompt_template, managed_agents=None) -> str:\n-    if managed_agents is not None:\n-        return prompt_template.replace(\"<<managed_agents_descriptions>>\", show_agents_descriptions(managed_agents))\n-    else:\n-        return prompt_template.replace(\"<<managed_agents_descriptions>>\", \"\")\n-\n-\n-def format_prompt_with_imports(prompt_template: str, authorized_imports: List[str]) -> str:\n-    if \"<<authorized_imports>>\" not in prompt_template:\n-        raise AgentError(\"Tag '<<authorized_imports>>' should be provided in the prompt.\")\n-    return prompt_template.replace(\"<<authorized_imports>>\", str(authorized_imports))\n-\n-\n-class Agent:\n-    def __init__(\n-        self,\n-        tools: Union[List[Tool], Toolbox],\n-        llm_engine: Callable = None,\n-        system_prompt: Optional[str] = None,\n-        tool_description_template: Optional[str] = None,\n-        additional_args: Dict = {},\n-        max_iterations: int = 6,\n-        tool_parser: Optional[Callable] = None,\n-        add_base_tools: bool = False,\n-        verbose: int = 0,\n-        grammar: Optional[Dict[str, str]] = None,\n-        managed_agents: Optional[List] = None,\n-        step_callbacks: Optional[List[Callable]] = None,\n-        monitor_metrics: bool = True,\n-    ):\n-        if system_prompt is None:\n-            system_prompt = DEFAULT_REACT_CODE_SYSTEM_PROMPT\n-        if tool_parser is None:\n-            tool_parser = parse_json_tool_call\n-        self.agent_name = self.__class__.__name__\n-        self.llm_engine = llm_engine\n-        self.system_prompt_template = system_prompt\n-        self.tool_description_template = (\n-            tool_description_template if tool_description_template else DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n-        )\n-        self.additional_args = additional_args\n-        self.max_iterations = max_iterations\n-        self.logger = logger\n-        self.tool_parser = tool_parser\n-        self.grammar = grammar\n-\n-        self.managed_agents = None\n-        if managed_agents is not None:\n-            self.managed_agents = {agent.name: agent for agent in managed_agents}\n-\n-        if isinstance(tools, Toolbox):\n-            self._toolbox = tools\n-            if add_base_tools:\n-                if not is_torch_available():\n-                    raise ImportError(\"Using the base tools requires torch to be installed.\")\n-\n-                self._toolbox.add_base_tools(add_python_interpreter=(self.__class__ == ReactJsonAgent))\n-        else:\n-            self._toolbox = Toolbox(tools, add_base_tools=add_base_tools)\n-        self._toolbox.add_tool(FinalAnswerTool())\n-\n-        self.system_prompt = format_prompt_with_tools(\n-            self._toolbox, self.system_prompt_template, self.tool_description_template\n-        )\n-        self.system_prompt = format_prompt_with_managed_agents_descriptions(self.system_prompt, self.managed_agents)\n-        self.prompt = None\n-        self.logs = []\n-        self.task = None\n-\n-        if verbose == 0:\n-            logger.setLevel(logging.WARNING)\n-        elif verbose == 1:\n-            logger.setLevel(logging.INFO)\n-        elif verbose == 2:\n-            logger.setLevel(logging.DEBUG)\n-\n-        # Initialize step callbacks\n-        self.step_callbacks = step_callbacks if step_callbacks is not None else []\n-\n-        # Initialize Monitor if monitor_metrics is True\n-        self.monitor = None\n-        if monitor_metrics:\n-            self.monitor = Monitor(self.llm_engine)\n-            self.step_callbacks.append(self.monitor.update_metrics)\n-\n-    @property\n-    def toolbox(self) -> Toolbox:\n-        \"\"\"Get the toolbox currently available to the agent\"\"\"\n-        return self._toolbox\n-\n-    def initialize_for_run(self):\n-        self.token_count = 0\n-        self.system_prompt = format_prompt_with_tools(\n-            self._toolbox,\n-            self.system_prompt_template,\n-            self.tool_description_template,\n-        )\n-        self.system_prompt = format_prompt_with_managed_agents_descriptions(self.system_prompt, self.managed_agents)\n-        if hasattr(self, \"authorized_imports\"):\n-            self.system_prompt = format_prompt_with_imports(\n-                self.system_prompt, list(set(LIST_SAFE_MODULES) | set(self.authorized_imports))\n-            )\n-        self.logs = [{\"system_prompt\": self.system_prompt, \"task\": self.task}]\n-        self.logger.log(33, \"======== New task ========\")\n-        self.logger.log(34, self.task)\n-        self.logger.debug(\"System prompt is as follows:\")\n-        self.logger.debug(self.system_prompt)\n-\n-    def write_inner_memory_from_logs(self, summary_mode: Optional[bool] = False) -> List[Dict[str, str]]:\n-        \"\"\"\n-        Reads past llm_outputs, actions, and observations or errors from the logs into a series of messages\n-        that can be used as input to the LLM.\n-        \"\"\"\n-        prompt_message = {\"role\": MessageRole.SYSTEM, \"content\": self.logs[0][\"system_prompt\"]}\n-        task_message = {\n-            \"role\": MessageRole.USER,\n-            \"content\": \"Task: \" + self.logs[0][\"task\"],\n-        }\n-        if summary_mode:\n-            memory = [task_message]\n-        else:\n-            memory = [prompt_message, task_message]\n-        for i, step_log in enumerate(self.logs[1:]):\n-            if \"llm_output\" in step_log and not summary_mode:\n-                thought_message = {\"role\": MessageRole.ASSISTANT, \"content\": step_log[\"llm_output\"].strip()}\n-                memory.append(thought_message)\n-            if \"facts\" in step_log:\n-                thought_message = {\n-                    \"role\": MessageRole.ASSISTANT,\n-                    \"content\": \"[FACTS LIST]:\\n\" + step_log[\"facts\"].strip(),\n-                }\n-                memory.append(thought_message)\n-\n-            if \"plan\" in step_log and not summary_mode:\n-                thought_message = {\"role\": MessageRole.ASSISTANT, \"content\": \"[PLAN]:\\n\" + step_log[\"plan\"].strip()}\n-                memory.append(thought_message)\n-\n-            if \"tool_call\" in step_log and summary_mode:\n-                tool_call_message = {\n-                    \"role\": MessageRole.ASSISTANT,\n-                    \"content\": f\"[STEP {i} TOOL CALL]: \" + str(step_log[\"tool_call\"]).strip(),\n-                }\n-                memory.append(tool_call_message)\n-\n-            if \"task\" in step_log:\n-                tool_call_message = {\n-                    \"role\": MessageRole.USER,\n-                    \"content\": \"New task:\\n\" + step_log[\"task\"],\n-                }\n-                memory.append(tool_call_message)\n-\n-            if \"error\" in step_log or \"observation\" in step_log:\n-                if \"error\" in step_log:\n-                    message_content = (\n-                        f\"[OUTPUT OF STEP {i}] -> Error:\\n\"\n-                        + str(step_log[\"error\"])\n-                        + \"\\nNow let's retry: take care not to repeat previous errors! If you have retried several times, try a completely different approach.\\n\"\n-                    )\n-                elif \"observation\" in step_log:\n-                    message_content = f\"[OUTPUT OF STEP {i}] -> Observation:\\n{step_log['observation']}\"\n-                tool_response_message = {\"role\": MessageRole.TOOL_RESPONSE, \"content\": message_content}\n-                memory.append(tool_response_message)\n-\n-        return memory\n-\n-    def get_succinct_logs(self):\n-        return [{key: value for key, value in log.items() if key != \"agent_memory\"} for log in self.logs]\n-\n-    def extract_action(self, llm_output: str, split_token: str) -> str:\n-        \"\"\"\n-        Parse action from the LLM output\n-\n-        Args:\n-            llm_output (`str`): Output of the LLM\n-            split_token (`str`): Separator for the action. Should match the example in the system prompt.\n-        \"\"\"\n-        try:\n-            split = llm_output.split(split_token)\n-            rationale, action = (\n-                split[-2],\n-                split[-1],\n-            )  # NOTE: using indexes starting from the end solves for when you have more than one split_token in the output\n-        except Exception as e:\n-            self.logger.error(e, exc_info=1)\n-            raise AgentParsingError(\n-                f\"Error: No '{split_token}' token provided in your output.\\nYour output:\\n{llm_output}\\n. Be sure to include an action, prefaced with '{split_token}'!\"\n-            )\n-        return rationale.strip(), action.strip()\n-\n-    def execute_tool_call(self, tool_name: str, arguments: Dict[str, str]) -> Any:\n-        \"\"\"\n-        Execute tool with the provided input and returns the result.\n-        This method replaces arguments with the actual values from the state if they refer to state variables.\n-\n-        Args:\n-            tool_name (`str`): Name of the Tool to execute (should be one from self.toolbox).\n-            arguments (Dict[str, str]): Arguments passed to the Tool.\n-        \"\"\"\n-        available_tools = self.toolbox.tools\n-        if self.managed_agents is not None:\n-            available_tools = {**available_tools, **self.managed_agents}\n-        if tool_name not in available_tools:\n-            error_msg = f\"Error: unknown tool {tool_name}, should be instead one of {list(available_tools.keys())}.\"\n-            self.logger.error(error_msg, exc_info=1)\n-            raise AgentExecutionError(error_msg)\n-\n-        try:\n-            if isinstance(arguments, str):\n-                observation = available_tools[tool_name](arguments)\n-            elif isinstance(arguments, dict):\n-                for key, value in arguments.items():\n-                    # if the value is the name of a state variable like \"image.png\", replace it with the actual value\n-                    if isinstance(value, str) and value in self.state:\n-                        arguments[key] = self.state[value]\n-                observation = available_tools[tool_name](**arguments)\n-            else:\n-                raise AgentExecutionError(\n-                    f\"Arguments passed to tool should be a dict or string: got a {type(arguments)}.\"\n-                )\n-            return observation\n-        except Exception as e:\n-            if tool_name in self.toolbox.tools:\n-                raise AgentExecutionError(\n-                    f\"Error in tool call execution: {e}\\nYou should only use this tool with a correct input.\\n\"\n-                    f\"As a reminder, this tool's description is the following:\\n{get_tool_description_with_args(available_tools[tool_name])}\"\n-                )\n-            elif tool_name in self.managed_agents:\n-                raise AgentExecutionError(\n-                    f\"Error in calling team member: {e}\\nYou should only ask this team member with a correct request.\\n\"\n-                    f\"As a reminder, this team member's description is the following:\\n{available_tools[tool_name]}\"\n-                )\n-\n-    def log_rationale_code_action(self, rationale: str, code_action: str) -> None:\n-        self.logger.warning(\"=== Agent thoughts:\")\n-        self.logger.log(31, rationale)\n-        self.logger.warning(\">>> Agent is executing the code below:\")\n-        if is_pygments_available():\n-            self.logger.log(\n-                31, highlight(code_action, PythonLexer(ensurenl=False), Terminal256Formatter(style=\"nord\"))\n-            )\n-        else:\n-            self.logger.log(31, code_action)\n-        self.logger.warning(\"====\")\n-\n-    def run(self, **kwargs):\n-        \"\"\"To be implemented in the child class\"\"\"\n-        raise NotImplementedError\n-\n-\n-class CodeAgent(Agent):\n-    \"\"\"\n-    A class for an agent that solves the given task using a single block of code. It plans all its actions, then executes all in one shot.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        tools: List[Tool],\n-        llm_engine: Optional[Callable] = None,\n-        system_prompt: Optional[str] = None,\n-        tool_description_template: Optional[str] = None,\n-        grammar: Optional[Dict[str, str]] = None,\n-        additional_authorized_imports: Optional[List[str]] = None,\n-        **kwargs,\n-    ):\n-        if llm_engine is None:\n-            llm_engine = HfApiEngine()\n-        if system_prompt is None:\n-            system_prompt = DEFAULT_CODE_SYSTEM_PROMPT\n-        if tool_description_template is None:\n-            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n-        super().__init__(\n-            tools=tools,\n-            llm_engine=llm_engine,\n-            system_prompt=system_prompt,\n-            tool_description_template=tool_description_template,\n-            grammar=grammar,\n-            **kwargs,\n-        )\n-\n-        if not is_pygments_available():\n-            transformers_logging.warning_once(\n-                logger,\n-                \"pygments isn't installed. Installing pygments will enable color syntax highlighting in the \"\n-                \"CodeAgent.\",\n-            )\n-\n-        self.python_evaluator = evaluate_python_code\n-        self.additional_authorized_imports = additional_authorized_imports if additional_authorized_imports else []\n-        self.authorized_imports = list(set(LIST_SAFE_MODULES) | set(self.additional_authorized_imports))\n-        self.system_prompt = self.system_prompt.replace(\"<<authorized_imports>>\", str(self.authorized_imports))\n-\n-    def parse_code_blob(self, result: str) -> str:\n-        \"\"\"\n-        Override this method if you want to change the way the code is\n-        cleaned in the `run` method.\n-        \"\"\"\n-        return parse_code_blob(result)\n-\n-    def run(self, task: str, return_generated_code: bool = False, **kwargs):\n-        \"\"\"\n-        Runs the agent for the given task.\n-\n-        Args:\n-            task (`str`): The task to perform\n-            return_generated_code (`bool`, *optional*, defaults to `False`): Whether to return the generated code instead of running it\n-            kwargs (additional keyword arguments, *optional*):\n-                Any keyword argument to send to the agent when evaluating the code.\n-\n-        Example:\n-\n-        ```py\n-        from transformers.agents import CodeAgent\n-\n-        agent = CodeAgent(tools=[])\n-        agent.run(\"What is the result of 2 power 3.7384?\")\n-        ```\n-        \"\"\"\n-        self.task = task\n-        if len(kwargs) > 0:\n-            self.task += f\"\\nYou have been provided with these initial arguments: {str(kwargs)}.\"\n-        self.state = kwargs.copy()\n-        self.initialize_for_run()\n-\n-        # Run LLM\n-        prompt_message = {\"role\": MessageRole.SYSTEM, \"content\": self.system_prompt}\n-        task_message = {\n-            \"role\": MessageRole.USER,\n-            \"content\": \"Task: \" + self.task,\n-        }\n-\n-        self.prompt = [prompt_message, task_message]\n-        self.logger.info(\"====Executing with this prompt====\")\n-        self.logger.info(self.prompt)\n-\n-        additional_args = {\"grammar\": self.grammar} if self.grammar is not None else {}\n-        llm_output = self.llm_engine(self.prompt, stop_sequences=[\"<end_action>\"], **additional_args)\n-\n-        if return_generated_code:\n-            return llm_output\n-\n-        # Parse\n-        try:\n-            rationale, code_action = self.extract_action(llm_output=llm_output, split_token=\"Code:\")\n-        except Exception as e:\n-            self.logger.debug(\n-                f\"Error in extracting action, trying to parse the whole output as code. Error trace: {e}\"\n-            )\n-            rationale, code_action = \"\", llm_output\n-\n-        try:\n-            code_action = self.parse_code_blob(code_action)\n-        except Exception as e:\n-            error_msg = f\"Error in code parsing: {e}. Be sure to provide correct code\"\n-            self.logger.error(error_msg, exc_info=1)\n-            return error_msg\n-\n-        # Execute\n-        self.log_rationale_code_action(rationale, code_action)\n-        try:\n-            available_tools = {**BASE_PYTHON_TOOLS.copy(), **self.toolbox.tools}\n-            output = self.python_evaluator(\n-                code_action,\n-                static_tools=available_tools,\n-                custom_tools={},\n-                state=self.state,\n-                authorized_imports=self.authorized_imports,\n-            )\n-            self.logger.info(self.state[\"print_outputs\"])\n-            return output\n-        except Exception as e:\n-            error_msg = f\"Error in execution: {e}. Be sure to provide correct code.\"\n-            self.logger.error(error_msg, exc_info=1)\n-            return error_msg\n-\n-\n-class ReactAgent(Agent):\n-    \"\"\"\n-    This agent that solves the given task step by step, using the ReAct framework:\n-    While the objective is not reached, the agent will perform a cycle of thinking and acting.\n-    The action will be parsed from the LLM output: it consists in calls to tools from the toolbox, with arguments chosen by the LLM engine.\n-    \"\"\"\n-\n-    @_deprecate_method(\n-        version=\"4.51.0\",\n-        message=\"Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\",\n-    )\n-    def __init__(\n-        self,\n-        tools: List[Tool],\n-        llm_engine: Optional[Callable] = None,\n-        system_prompt: Optional[str] = None,\n-        tool_description_template: Optional[str] = None,\n-        grammar: Optional[Dict[str, str]] = None,\n-        plan_type: Optional[str] = None,\n-        planning_interval: Optional[int] = None,\n-        **kwargs,\n-    ):\n-        if llm_engine is None:\n-            llm_engine = HfApiEngine()\n-        if system_prompt is None:\n-            system_prompt = DEFAULT_REACT_CODE_SYSTEM_PROMPT\n-        if tool_description_template is None:\n-            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n-        if plan_type is None:\n-            plan_type = SUPPORTED_PLAN_TYPES[0]\n-        else:\n-            assert plan_type in SUPPORTED_PLAN_TYPES, f\"plan type {plan_type} is not supported\"\n-        super().__init__(\n-            tools=tools,\n-            llm_engine=llm_engine,\n-            system_prompt=system_prompt,\n-            tool_description_template=tool_description_template,\n-            grammar=grammar,\n-            **kwargs,\n-        )\n-        self.planning_interval = planning_interval\n-        self.plan_type = plan_type\n-\n-    def provide_final_answer(self, task) -> str:\n-        \"\"\"\n-        This method provides a final answer to the task, based on the logs of the agent's interactions.\n-        \"\"\"\n-        self.prompt = [\n-            {\n-                \"role\": MessageRole.SYSTEM,\n-                \"content\": \"An agent tried to answer an user query but it got stuck and failed to do so. You are tasked with providing an answer instead. Here is the agent's memory:\",\n-            }\n-        ]\n-        self.prompt += self.write_inner_memory_from_logs()[1:]\n-        self.prompt += [\n-            {\n-                \"role\": MessageRole.USER,\n-                \"content\": f\"Based on the above, please provide an answer to the following user request:\\n{task}\",\n-            }\n-        ]\n-        try:\n-            return self.llm_engine(self.prompt)\n-        except Exception as e:\n-            return f\"Error in generating final llm output: {e}.\"\n-\n-    @_deprecate_method(\n-        version=\"4.51.0\",\n-        message=\"Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\",\n-    )\n-    def run(self, task: str, stream: bool = False, reset: bool = True, **kwargs):\n-        \"\"\"\n-        Runs the agent for the given task.\n-\n-        Args:\n-            task (`str`): The task to perform\n-\n-        Example:\n-        ```py\n-        from transformers.agents import ReactCodeAgent\n-        agent = ReactCodeAgent(tools=[])\n-        agent.run(\"What is the result of 2 power 3.7384?\")\n-        ```\n-        \"\"\"\n-        self.task = task\n-        if len(kwargs) > 0:\n-            self.task += f\"\\nYou have been provided with these initial arguments: {str(kwargs)}.\"\n-        self.state = kwargs.copy()\n-        if reset:\n-            self.initialize_for_run()\n-        else:\n-            self.logs.append({\"task\": task})\n-        if stream:\n-            return self.stream_run(task)\n-        else:\n-            return self.direct_run(task)\n-\n-    def stream_run(self, task: str):\n-        \"\"\"\n-        Runs the agent in streaming mode, yielding steps as they are executed: should be launched only in the `run` method.\n-        \"\"\"\n-        final_answer = None\n-        iteration = 0\n-        while final_answer is None and iteration < self.max_iterations:\n-            step_start_time = time.time()\n-            step_log_entry = {\"iteration\": iteration, \"start_time\": step_start_time}\n-            try:\n-                self.step(step_log_entry)\n-                if \"final_answer\" in step_log_entry:\n-                    final_answer = step_log_entry[\"final_answer\"]\n-            except AgentError as e:\n-                self.logger.error(e, exc_info=1)\n-                step_log_entry[\"error\"] = e\n-            finally:\n-                step_end_time = time.time()\n-                step_log_entry[\"step_end_time\"] = step_end_time\n-                step_log_entry[\"step_duration\"] = step_end_time - step_start_time\n-                self.logs.append(step_log_entry)\n-                for callback in self.step_callbacks:\n-                    callback(step_log_entry)\n-                iteration += 1\n-                yield step_log_entry\n-\n-        if final_answer is None and iteration == self.max_iterations:\n-            error_message = \"Reached max iterations.\"\n-            final_step_log = {\"error\": AgentMaxIterationsError(error_message)}\n-            self.logs.append(final_step_log)\n-            self.logger.error(error_message, exc_info=1)\n-            final_answer = self.provide_final_answer(task)\n-            final_step_log[\"final_answer\"] = final_answer\n-            final_step_log[\"step_duration\"] = 0\n-            for callback in self.step_callbacks:\n-                callback(final_step_log)\n-            yield final_step_log\n-\n-        yield final_answer\n-\n-    def direct_run(self, task: str):\n-        \"\"\"\n-        Runs the agent in direct mode, returning outputs only at the end: should be launched only in the `run` method.\n-        \"\"\"\n-        final_answer = None\n-        iteration = 0\n-        while final_answer is None and iteration < self.max_iterations:\n-            step_start_time = time.time()\n-            step_log_entry = {\"iteration\": iteration, \"start_time\": step_start_time}\n-            try:\n-                if self.planning_interval is not None and iteration % self.planning_interval == 0:\n-                    self.planning_step(task, is_first_step=(iteration == 0), iteration=iteration)\n-                self.step(step_log_entry)\n-                if \"final_answer\" in step_log_entry:\n-                    final_answer = step_log_entry[\"final_answer\"]\n-            except AgentError as e:\n-                self.logger.error(e, exc_info=1)\n-                step_log_entry[\"error\"] = e\n-            finally:\n-                step_end_time = time.time()\n-                step_log_entry[\"step_end_time\"] = step_end_time\n-                step_log_entry[\"step_duration\"] = step_end_time - step_start_time\n-                self.logs.append(step_log_entry)\n-                for callback in self.step_callbacks:\n-                    callback(step_log_entry)\n-                iteration += 1\n-\n-        if final_answer is None and iteration == self.max_iterations:\n-            error_message = \"Reached max iterations.\"\n-            final_step_log = {\"error\": AgentMaxIterationsError(error_message)}\n-            self.logs.append(final_step_log)\n-            self.logger.error(error_message, exc_info=1)\n-            final_answer = self.provide_final_answer(task)\n-            final_step_log[\"final_answer\"] = final_answer\n-            final_step_log[\"step_duration\"] = 0\n-            for callback in self.step_callbacks:\n-                callback(final_step_log)\n-\n-        return final_answer\n-\n-    def planning_step(self, task, is_first_step: bool = False, iteration: Optional[int] = None):\n-        \"\"\"\n-        Used periodically by the agent to plan the next steps to reach the objective.\n-\n-        Args:\n-            task (`str`): The task to perform\n-            is_first_step (`bool`): If this step is not the first one, the plan should be an update over a previous plan.\n-            iteration (`int`): The number of the current step, used as an indication for the LLM.\n-        \"\"\"\n-        if is_first_step:\n-            message_prompt_facts = {\"role\": MessageRole.SYSTEM, \"content\": SYSTEM_PROMPT_FACTS}\n-            message_prompt_task = {\n-                \"role\": MessageRole.USER,\n-                \"content\": f\"\"\"Here is the task:\n-```\n-{task}\n-```\n-Now begin!\"\"\",\n-            }\n-\n-            answer_facts = self.llm_engine([message_prompt_facts, message_prompt_task])\n-\n-            message_system_prompt_plan = {\n-                \"role\": MessageRole.SYSTEM,\n-                \"content\": PROMPTS_FOR_INITIAL_PLAN[self.plan_type][\"system\"],\n-            }\n-            message_user_prompt_plan = {\n-                \"role\": MessageRole.USER,\n-                \"content\": PROMPTS_FOR_INITIAL_PLAN[self.plan_type][\"user\"].format(\n-                    task=task,\n-                    tool_descriptions=self._toolbox.show_tool_descriptions(self.tool_description_template),\n-                    managed_agents_descriptions=(\n-                        show_agents_descriptions(self.managed_agents) if self.managed_agents is not None else \"\"\n-                    ),\n-                    answer_facts=answer_facts,\n-                ),\n-            }\n-            answer_plan = self.llm_engine(\n-                [message_system_prompt_plan, message_user_prompt_plan], stop_sequences=[\"<end_plan>\"]\n-            )\n-\n-            final_plan_redaction = f\"\"\"Here is the plan of action that I will follow to solve the task:\n-```\n-{answer_plan}\n-```\"\"\"\n-            final_facts_redaction = f\"\"\"Here are the facts that I know so far:\n-```\n-{answer_facts}\n-```\"\"\".strip()\n-            self.logs.append({\"plan\": final_plan_redaction, \"facts\": final_facts_redaction})\n-            self.logger.log(36, \"===== Initial plan =====\")\n-            self.logger.log(35, final_plan_redaction)\n-        else:  # update plan\n-            agent_memory = self.write_inner_memory_from_logs(\n-                summary_mode=False\n-            )  # This will not log the plan but will log facts\n-\n-            # Redact updated facts\n-            facts_update_system_prompt = {\n-                \"role\": MessageRole.SYSTEM,\n-                \"content\": SYSTEM_PROMPT_FACTS_UPDATE,\n-            }\n-            facts_update_message = {\n-                \"role\": MessageRole.USER,\n-                \"content\": USER_PROMPT_FACTS_UPDATE,\n-            }\n-            facts_update = self.llm_engine([facts_update_system_prompt] + agent_memory + [facts_update_message])\n-\n-            # Redact updated plan\n-            plan_update_message = {\n-                \"role\": MessageRole.SYSTEM,\n-                \"content\": PROMPTS_FOR_PLAN_UPDATE[self.plan_type][\"system\"].format(task=task),\n-            }\n-            plan_update_message_user = {\n-                \"role\": MessageRole.USER,\n-                \"content\": PROMPTS_FOR_PLAN_UPDATE[self.plan_type][\"user\"].format(\n-                    task=task,\n-                    tool_descriptions=self._toolbox.show_tool_descriptions(self.tool_description_template),\n-                    managed_agents_descriptions=(\n-                        show_agents_descriptions(self.managed_agents) if self.managed_agents is not None else \"\"\n-                    ),\n-                    facts_update=facts_update,\n-                    remaining_steps=(self.max_iterations - iteration),\n-                ),\n-            }\n-            plan_update = self.llm_engine(\n-                [plan_update_message] + agent_memory + [plan_update_message_user], stop_sequences=[\"<end_plan>\"]\n-            )\n-\n-            # Log final facts and plan\n-            final_plan_redaction = PLAN_UPDATE_FINAL_PLAN_REDACTION.format(task=task, plan_update=plan_update)\n-            final_facts_redaction = f\"\"\"Here is the updated list of the facts that I know:\n-```\n-{facts_update}\n-```\"\"\"\n-            self.logs.append({\"plan\": final_plan_redaction, \"facts\": final_facts_redaction})\n-            self.logger.log(36, \"===== Updated plan =====\")\n-            self.logger.log(35, final_plan_redaction)\n-\n-\n-class ReactJsonAgent(ReactAgent):\n-    \"\"\"\n-    This agent that solves the given task step by step, using the ReAct framework:\n-    While the objective is not reached, the agent will perform a cycle of thinking and acting.\n-    The tool calls will be formulated by the LLM in JSON format, then parsed and executed.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        tools: List[Tool],\n-        llm_engine: Optional[Callable] = None,\n-        system_prompt: Optional[str] = None,\n-        tool_description_template: Optional[str] = None,\n-        grammar: Optional[Dict[str, str]] = None,\n-        planning_interval: Optional[int] = None,\n-        **kwargs,\n-    ):\n-        if llm_engine is None:\n-            llm_engine = HfApiEngine()\n-        if system_prompt is None:\n-            system_prompt = DEFAULT_REACT_JSON_SYSTEM_PROMPT\n-        if tool_description_template is None:\n-            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n-        super().__init__(\n-            tools=tools,\n-            llm_engine=llm_engine,\n-            system_prompt=system_prompt,\n-            tool_description_template=tool_description_template,\n-            grammar=grammar,\n-            planning_interval=planning_interval,\n-            **kwargs,\n-        )\n-\n-    def step(self, log_entry: Dict[str, Any]):\n-        \"\"\"\n-        Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.\n-        The errors are raised here, they are caught and logged in the run() method.\n-        \"\"\"\n-        agent_memory = self.write_inner_memory_from_logs()\n-\n-        self.prompt = agent_memory\n-        self.logger.debug(\"===== New step =====\")\n-\n-        # Add new step in logs\n-        log_entry[\"agent_memory\"] = agent_memory.copy()\n-\n-        self.logger.info(\"===== Calling LLM with this last message: =====\")\n-        self.logger.info(self.prompt[-1])\n-\n-        try:\n-            additional_args = {\"grammar\": self.grammar} if self.grammar is not None else {}\n-            llm_output = self.llm_engine(\n-                self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"], **additional_args\n-            )\n-        except Exception as e:\n-            raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n-        self.logger.debug(\"===== Output message of the LLM: =====\")\n-        self.logger.debug(llm_output)\n-        log_entry[\"llm_output\"] = llm_output\n-\n-        # Parse\n-        self.logger.debug(\"===== Extracting action =====\")\n-        rationale, action = self.extract_action(llm_output=llm_output, split_token=\"Action:\")\n-\n-        try:\n-            tool_name, arguments = self.tool_parser(action)\n-        except Exception as e:\n-            raise AgentParsingError(f\"Could not parse the given action: {e}.\")\n-\n-        log_entry[\"rationale\"] = rationale\n-        log_entry[\"tool_call\"] = {\"tool_name\": tool_name, \"tool_arguments\": arguments}\n-\n-        # Execute\n-        self.logger.warning(\"=== Agent thoughts:\")\n-        self.logger.log(31, rationale)\n-        self.logger.warning(f\">>> Calling tool: '{tool_name}' with arguments: {arguments}\")\n-        if tool_name == \"final_answer\":\n-            if isinstance(arguments, dict):\n-                if \"answer\" in arguments:\n-                    answer = arguments[\"answer\"]\n-                    if (\n-                        isinstance(answer, str) and answer in self.state.keys()\n-                    ):  # if the answer is a state variable, return the value\n-                        answer = self.state[answer]\n-                else:\n-                    answer = arguments\n-            else:\n-                answer = arguments\n-            log_entry[\"final_answer\"] = answer\n-            return answer\n-        else:\n-            if arguments is None:\n-                arguments = {}\n-            observation = self.execute_tool_call(tool_name, arguments)\n-            observation_type = type(observation)\n-            if observation_type in [AgentImage, AgentAudio]:\n-                if observation_type == AgentImage:\n-                    observation_name = \"image.png\"\n-                elif observation_type == AgentAudio:\n-                    observation_name = \"audio.mp3\"\n-                # TODO: observation naming could allow for different names of same type\n-\n-                self.state[observation_name] = observation\n-                updated_information = f\"Stored '{observation_name}' in memory.\"\n-            else:\n-                updated_information = str(observation).strip()\n-            self.logger.info(updated_information)\n-            log_entry[\"observation\"] = updated_information\n-            return log_entry\n-\n-\n-class ReactCodeAgent(ReactAgent):\n-    \"\"\"\n-    This agent that solves the given task step by step, using the ReAct framework:\n-    While the objective is not reached, the agent will perform a cycle of thinking and acting.\n-    The tool calls will be formulated by the LLM in code format, then parsed and executed.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        tools: List[Tool],\n-        llm_engine: Optional[Callable] = None,\n-        system_prompt: Optional[str] = None,\n-        tool_description_template: Optional[str] = None,\n-        grammar: Optional[Dict[str, str]] = None,\n-        additional_authorized_imports: Optional[List[str]] = None,\n-        planning_interval: Optional[int] = None,\n-        **kwargs,\n-    ):\n-        if llm_engine is None:\n-            llm_engine = HfApiEngine()\n-        if system_prompt is None:\n-            system_prompt = DEFAULT_REACT_CODE_SYSTEM_PROMPT\n-        if tool_description_template is None:\n-            tool_description_template = DEFAULT_TOOL_DESCRIPTION_TEMPLATE\n-        super().__init__(\n-            tools=tools,\n-            llm_engine=llm_engine,\n-            system_prompt=system_prompt,\n-            tool_description_template=tool_description_template,\n-            grammar=grammar,\n-            planning_interval=planning_interval,\n-            **kwargs,\n-        )\n-\n-        if not is_pygments_available():\n-            transformers_logging.warning_once(\n-                logger,\n-                \"pygments isn't installed. Installing pygments will enable color syntax highlighting in the \"\n-                \"ReactCodeAgent.\",\n-            )\n-\n-        self.python_evaluator = evaluate_python_code\n-        self.additional_authorized_imports = additional_authorized_imports if additional_authorized_imports else []\n-        self.authorized_imports = list(set(LIST_SAFE_MODULES) | set(self.additional_authorized_imports))\n-        self.system_prompt = self.system_prompt.replace(\"<<authorized_imports>>\", str(self.authorized_imports))\n-        self.custom_tools = {}\n-\n-    def step(self, log_entry: Dict[str, Any]):\n-        \"\"\"\n-        Perform one step in the ReAct framework: the agent thinks, acts, and observes the result.\n-        The errors are raised here, they are caught and logged in the run() method.\n-        \"\"\"\n-        agent_memory = self.write_inner_memory_from_logs()\n-\n-        self.prompt = agent_memory.copy()\n-        self.logger.debug(\"===== New step =====\")\n-\n-        # Add new step in logs\n-        log_entry[\"agent_memory\"] = agent_memory.copy()\n-\n-        self.logger.info(\"===== Calling LLM with these last messages: =====\")\n-        self.logger.info(self.prompt[-2:])\n-\n-        try:\n-            additional_args = {\"grammar\": self.grammar} if self.grammar is not None else {}\n-            llm_output = self.llm_engine(\n-                self.prompt, stop_sequences=[\"<end_action>\", \"Observation:\"], **additional_args\n-            )\n-        except Exception as e:\n-            raise AgentGenerationError(f\"Error in generating llm output: {e}.\")\n-\n-        self.logger.debug(\"=== Output message of the LLM:\")\n-        self.logger.debug(llm_output)\n-        log_entry[\"llm_output\"] = llm_output\n-\n-        # Parse\n-        self.logger.debug(\"=== Extracting action ===\")\n-        try:\n-            rationale, raw_code_action = self.extract_action(llm_output=llm_output, split_token=\"Code:\")\n-        except Exception as e:\n-            self.logger.debug(f\"Error in extracting action, trying to parse the whole output. Error trace: {e}\")\n-            rationale, raw_code_action = llm_output, llm_output\n-\n-        try:\n-            code_action = parse_code_blob(raw_code_action)\n-        except Exception as e:\n-            error_msg = f\"Error in code parsing: {e}. Make sure to provide correct code\"\n-            raise AgentParsingError(error_msg)\n-\n-        log_entry[\"rationale\"] = rationale\n-        log_entry[\"tool_call\"] = {\"tool_name\": \"code interpreter\", \"tool_arguments\": code_action}\n-\n-        # Execute\n-        self.log_rationale_code_action(rationale, code_action)\n-        try:\n-            static_tools = {\n-                **BASE_PYTHON_TOOLS.copy(),\n-                **self.toolbox.tools,\n-            }\n-            if self.managed_agents is not None:\n-                static_tools = {**static_tools, **self.managed_agents}\n-            result = self.python_evaluator(\n-                code_action,\n-                static_tools=static_tools,\n-                custom_tools=self.custom_tools,\n-                state=self.state,\n-                authorized_imports=self.authorized_imports,\n-            )\n-            self.logger.warning(\"Print outputs:\")\n-            self.logger.log(32, self.state[\"print_outputs\"])\n-            observation = \"Print outputs:\\n\" + self.state[\"print_outputs\"]\n-            if result is not None:\n-                self.logger.warning(\"Last output from code snippet:\")\n-                self.logger.log(32, str(result))\n-                observation += \"Last output from code snippet:\\n\" + str(result)[:100000]\n-            log_entry[\"observation\"] = observation\n-        except Exception as e:\n-            error_msg = f\"Code execution failed due to the following error:\\n{str(e)}\"\n-            if \"'dict' object has no attribute 'read'\" in str(e):\n-                error_msg += \"\\nYou get this error because you passed a dict as input for one of the arguments instead of a string.\"\n-            raise AgentExecutionError(error_msg)\n-        for line in code_action.split(\"\\n\"):\n-            if line[: len(\"final_answer\")] == \"final_answer\":\n-                self.logger.log(33, \"Final answer:\")\n-                self.logger.log(32, result)\n-                log_entry[\"final_answer\"] = result\n-        return result\n-\n-\n-LENGTH_TRUNCATE_REPORTS = 1000\n-\n-\n-class ManagedAgent:\n-    @_deprecate_method(\n-        version=\"4.51.0\",\n-        message=\"Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\",\n-    )\n-    def __init__(self, agent, name, description, additional_prompting=None, provide_run_summary=False):\n-        self.agent = agent\n-        self.name = name\n-        self.description = description\n-        self.additional_prompting = additional_prompting\n-        self.provide_run_summary = provide_run_summary\n-\n-    def write_full_task(self, task):\n-        full_task = f\"\"\"You're a helpful agent named '{self.name}'.\n-You have been submitted this task by your manager.\n----\n-Task:\n-{task}\n----\n-You're helping your manager solve a wider task: so make sure to not provide a one-line answer, but give as much information as possible so that they have a clear understanding of the answer.\n-\n-Your final_answer WILL HAVE to contain these parts:\n-### 1. Task outcome (short version):\n-### 2. Task outcome (extremely detailed version):\n-### 3. Additional context (if relevant):\n-\n-Put all these in your final_answer tool, everything that you do not pass as an argument to final_answer will be lost.\n-And even if your task resolution is not successful, please return as much context as possible, so that your manager can act upon this feedback.\n-<<additional_prompting>>\"\"\"\n-        if self.additional_prompting:\n-            full_task = full_task.replace(\"\\n<<additional_prompting>>\", self.additional_prompting).strip()\n-        else:\n-            full_task = full_task.replace(\"\\n<<additional_prompting>>\", \"\").strip()\n-        return full_task\n-\n-    def __call__(self, request, **kwargs):\n-        full_task = self.write_full_task(request)\n-        output = self.agent.run(full_task, **kwargs)\n-        if self.provide_run_summary:\n-            answer = f\"Here is the final answer from your managed agent '{self.name}':\\n\"\n-            answer += str(output)\n-            answer += f\"\\n\\nFor more detail, find below a summary of this agent's work:\\nSUMMARY OF WORK FROM AGENT '{self.name}':\\n\"\n-            for message in self.agent.write_inner_memory_from_logs(summary_mode=True):\n-                content = message[\"content\"]\n-                if len(str(content)) < LENGTH_TRUNCATE_REPORTS or \"[FACTS LIST]\" in str(content):\n-                    answer += \"\\n\" + str(content) + \"\\n---\"\n-                else:\n-                    answer += (\n-                        \"\\n\"\n-                        + str(content)[:LENGTH_TRUNCATE_REPORTS]\n-                        + \"\\n(...Step was truncated because too long)...\\n---\"\n-                    )\n-            answer += f\"\\nEND OF SUMMARY OF WORK FROM AGENT '{self.name}'.\"\n-            return answer\n-        else:\n-            return output"
        },
        {
            "sha": "3946aa9f873503c9a9564da2d5e0dbba7773db89",
            "filename": "src/transformers/agents/default_tools.py",
            "status": "removed",
            "additions": 0,
            "deletions": 187,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fdefault_tools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fdefault_tools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fdefault_tools.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,187 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import importlib.util\n-import json\n-import math\n-from dataclasses import dataclass\n-from math import sqrt\n-from typing import Dict\n-\n-from huggingface_hub import hf_hub_download, list_spaces\n-\n-from ..utils import is_offline_mode\n-from .python_interpreter import LIST_SAFE_MODULES, evaluate_python_code\n-from .tools import TOOL_CONFIG_FILE, TOOL_MAPPING, Tool\n-\n-\n-def custom_print(*args):\n-    return None\n-\n-\n-BASE_PYTHON_TOOLS = {\n-    \"print\": custom_print,\n-    \"isinstance\": isinstance,\n-    \"range\": range,\n-    \"float\": float,\n-    \"int\": int,\n-    \"bool\": bool,\n-    \"str\": str,\n-    \"set\": set,\n-    \"list\": list,\n-    \"dict\": dict,\n-    \"tuple\": tuple,\n-    \"round\": round,\n-    \"ceil\": math.ceil,\n-    \"floor\": math.floor,\n-    \"log\": math.log,\n-    \"exp\": math.exp,\n-    \"sin\": math.sin,\n-    \"cos\": math.cos,\n-    \"tan\": math.tan,\n-    \"asin\": math.asin,\n-    \"acos\": math.acos,\n-    \"atan\": math.atan,\n-    \"atan2\": math.atan2,\n-    \"degrees\": math.degrees,\n-    \"radians\": math.radians,\n-    \"pow\": math.pow,\n-    \"sqrt\": sqrt,\n-    \"len\": len,\n-    \"sum\": sum,\n-    \"max\": max,\n-    \"min\": min,\n-    \"abs\": abs,\n-    \"enumerate\": enumerate,\n-    \"zip\": zip,\n-    \"reversed\": reversed,\n-    \"sorted\": sorted,\n-    \"all\": all,\n-    \"any\": any,\n-    \"map\": map,\n-    \"filter\": filter,\n-    \"ord\": ord,\n-    \"chr\": chr,\n-    \"next\": next,\n-    \"iter\": iter,\n-    \"divmod\": divmod,\n-    \"callable\": callable,\n-    \"getattr\": getattr,\n-    \"hasattr\": hasattr,\n-    \"setattr\": setattr,\n-    \"issubclass\": issubclass,\n-    \"type\": type,\n-}\n-\n-\n-@dataclass\n-class PreTool:\n-    name: str\n-    inputs: Dict[str, str]\n-    output_type: type\n-    task: str\n-    description: str\n-    repo_id: str\n-\n-\n-HUGGINGFACE_DEFAULT_TOOLS_FROM_HUB = [\n-    \"image-transformation\",\n-    \"text-to-image\",\n-]\n-\n-\n-def get_remote_tools(logger, organization=\"huggingface-tools\"):\n-    if is_offline_mode():\n-        logger.info(\"You are in offline mode, so remote tools are not available.\")\n-        return {}\n-\n-    spaces = list_spaces(author=organization)\n-    tools = {}\n-    for space_info in spaces:\n-        repo_id = space_info.id\n-        resolved_config_file = hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type=\"space\")\n-        with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-            config = json.load(reader)\n-        task = repo_id.split(\"/\")[-1]\n-        tools[config[\"name\"]] = PreTool(\n-            task=task,\n-            description=config[\"description\"],\n-            repo_id=repo_id,\n-            name=task,\n-            inputs=config[\"inputs\"],\n-            output_type=config[\"output_type\"],\n-        )\n-\n-    return tools\n-\n-\n-def setup_default_tools(logger):\n-    default_tools = {}\n-    main_module = importlib.import_module(\"transformers\")\n-    tools_module = main_module.agents\n-\n-    for task_name, tool_class_name in TOOL_MAPPING.items():\n-        tool_class = getattr(tools_module, tool_class_name)\n-        tool_instance = tool_class()\n-        default_tools[tool_class.name] = PreTool(\n-            name=tool_instance.name,\n-            inputs=tool_instance.inputs,\n-            output_type=tool_instance.output_type,\n-            task=task_name,\n-            description=tool_instance.description,\n-            repo_id=None,\n-        )\n-\n-    return default_tools\n-\n-\n-class PythonInterpreterTool(Tool):\n-    name = \"python_interpreter\"\n-    description = \"This is a tool that evaluates python code. It can be used to perform calculations.\"\n-\n-    output_type = \"string\"\n-\n-    def __init__(self, *args, authorized_imports=None, **kwargs):\n-        if authorized_imports is None:\n-            self.authorized_imports = list(set(LIST_SAFE_MODULES))\n-        else:\n-            self.authorized_imports = list(set(LIST_SAFE_MODULES) | set(authorized_imports))\n-        self.inputs = {\n-            \"code\": {\n-                \"type\": \"string\",\n-                \"description\": (\n-                    \"The code snippet to evaluate. All variables used in this snippet must be defined in this same snippet, \"\n-                    f\"else you will get an error. This code can only import the following python libraries: {authorized_imports}.\"\n-                ),\n-            }\n-        }\n-        super().__init__(*args, **kwargs)\n-\n-    def forward(self, code):\n-        output = str(\n-            evaluate_python_code(code, static_tools=BASE_PYTHON_TOOLS, authorized_imports=self.authorized_imports)\n-        )\n-        return output\n-\n-\n-class FinalAnswerTool(Tool):\n-    name = \"final_answer\"\n-    description = \"Provides a final answer to the given problem.\"\n-    inputs = {\"answer\": {\"type\": \"any\", \"description\": \"The final answer to the problem\"}}\n-    output_type = \"any\"\n-\n-    def forward(self, answer):\n-        return answer"
        },
        {
            "sha": "23ae5b0429120dd6dd9c2eb7f454959c92625012",
            "filename": "src/transformers/agents/document_question_answering.py",
            "status": "removed",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fdocument_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fdocument_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fdocument_question_answering.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,89 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import re\n-\n-import numpy as np\n-import torch\n-\n-from ..models.auto import AutoProcessor\n-from ..models.vision_encoder_decoder import VisionEncoderDecoderModel\n-from ..utils import is_vision_available\n-from .tools import PipelineTool\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-class DocumentQuestionAnsweringTool(PipelineTool):\n-    default_checkpoint = \"naver-clova-ix/donut-base-finetuned-docvqa\"\n-    description = \"This is a tool that answers a question about an document (pdf). It returns a string that contains the answer to the question.\"\n-    name = \"document_qa\"\n-    pre_processor_class = AutoProcessor\n-    model_class = VisionEncoderDecoderModel\n-\n-    inputs = {\n-        \"document\": {\n-            \"type\": \"image\",\n-            \"description\": \"The image containing the information. Can be a PIL Image or a string path to the image.\",\n-        },\n-        \"question\": {\"type\": \"string\", \"description\": \"The question in English\"},\n-    }\n-    output_type = \"string\"\n-\n-    def __init__(self, *args, **kwargs):\n-        if not is_vision_available():\n-            raise ValueError(\"Pillow must be installed to use the DocumentQuestionAnsweringTool.\")\n-\n-        super().__init__(*args, **kwargs)\n-\n-    def encode(self, document: \"Image\", question: str):\n-        task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n-        prompt = task_prompt.replace(\"{user_input}\", question)\n-        decoder_input_ids = self.pre_processor.tokenizer(\n-            prompt, add_special_tokens=False, return_tensors=\"pt\"\n-        ).input_ids\n-        if isinstance(document, str):\n-            img = Image.open(document).convert(\"RGB\")\n-            img_array = np.array(img).transpose(2, 0, 1)\n-            document = torch.from_numpy(img_array)\n-        pixel_values = self.pre_processor(document, return_tensors=\"pt\").pixel_values\n-\n-        return {\"decoder_input_ids\": decoder_input_ids, \"pixel_values\": pixel_values}\n-\n-    def forward(self, inputs):\n-        return self.model.generate(\n-            inputs[\"pixel_values\"].to(self.device),\n-            decoder_input_ids=inputs[\"decoder_input_ids\"].to(self.device),\n-            max_length=self.model.decoder.config.max_position_embeddings,\n-            early_stopping=True,\n-            pad_token_id=self.pre_processor.tokenizer.pad_token_id,\n-            eos_token_id=self.pre_processor.tokenizer.eos_token_id,\n-            use_cache=True,\n-            num_beams=1,\n-            bad_words_ids=[[self.pre_processor.tokenizer.unk_token_id]],\n-            return_dict_in_generate=True,\n-        ).sequences\n-\n-    def decode(self, outputs):\n-        sequence = self.pre_processor.batch_decode(outputs)[0]\n-        sequence = sequence.replace(self.pre_processor.tokenizer.eos_token, \"\")\n-        sequence = sequence.replace(self.pre_processor.tokenizer.pad_token, \"\")\n-        sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n-        sequence = self.pre_processor.token2json(sequence)\n-\n-        return sequence[\"answer\"]"
        },
        {
            "sha": "555acef28c8514f0d06c35284aa86e43329fac94",
            "filename": "src/transformers/agents/evaluate_agent.py",
            "status": "removed",
            "additions": 0,
            "deletions": 414,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fevaluate_agent.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fevaluate_agent.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fevaluate_agent.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,414 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from .agents import BASE_PYTHON_TOOLS\n-from .python_interpreter import InterpreterError, evaluate\n-\n-\n-### Fake tools for test\n-def classifier(text, labels):\n-    return f\"This is the classification of {text} along {labels}.\"\n-\n-\n-def translator(text, src_lang, tgt_lang):\n-    return f\"This is the translation of {text} from {src_lang} to {tgt_lang}.\"\n-\n-\n-def speaker(text):\n-    return f\"This is actually a sound reading {text}.\"\n-\n-\n-def transcriber(audio):\n-    if \"sound\" not in audio:\n-        raise ValueError(f\"`audio` ({audio}) is not a sound.\")\n-    return f\"This is the transcribed text from {audio}.\"\n-\n-\n-def image_generator(prompt):\n-    return f\"This is actually an image representing {prompt}.\"\n-\n-\n-def image_captioner(image):\n-    if \"image\" not in image:\n-        raise ValueError(f\"`image` ({image}) is not an image.\")\n-    return f\"This is a description of {image}.\"\n-\n-\n-def image_transformer(image, prompt):\n-    if \"image\" not in image:\n-        raise ValueError(f\"`image` ({image}) is not an image.\")\n-    return f\"This is a transformation of {image} according to {prompt}.\"\n-\n-\n-def question_answerer(text, question):\n-    return f\"This is the answer to {question} from {text}.\"\n-\n-\n-def image_qa(image, question):\n-    if \"image\" not in image:\n-        raise ValueError(f\"`image` ({image}) is not an image.\")\n-    return f\"This is the answer to {question} from {image}.\"\n-\n-\n-def text_downloader(url):\n-    return f\"This is the content of {url}.\"\n-\n-\n-def summarizer(text):\n-    return f\"This is a summary of {text}.\"\n-\n-\n-def video_generator(prompt, seconds=2):\n-    return f\"A video of {prompt}\"\n-\n-\n-def document_qa(image, question):\n-    return f\"This is the answer to {question} from the document {image}.\"\n-\n-\n-def image_segmenter(image, prompt):\n-    return f\"This is the mask of {prompt} in {image}\"\n-\n-\n-TEST_TOOLS = {\n-    \"text_classifier\": classifier,\n-    \"translator\": translator,\n-    \"text_reader\": speaker,\n-    \"summarizer\": summarizer,\n-    \"transcriber\": transcriber,\n-    \"image_generator\": image_generator,\n-    \"image_captioner\": image_captioner,\n-    \"image_transformer\": image_transformer,\n-    \"text_qa\": question_answerer,\n-    \"text_downloader\": text_downloader,\n-    \"image_qa\": image_qa,\n-    \"video_generator\": video_generator,\n-    \"document_qa\": document_qa,\n-    \"image_segmenter\": image_segmenter,\n-}\n-\n-\n-class Problem:\n-    \"\"\"\n-    A class regrouping all the information to solve a problem on which we will evaluate agents.\n-\n-    Args:\n-        task (`str` ou `list[str]`):\n-            One or several descriptions of the task to perform. If a list, it should contain variations on the\n-            phrasing, but for the same task.\n-        inputs (`list[str]` or `dict[str, str]`):\n-            The inputs that will be fed to the tools. For this testing environment, only strings are accepted as\n-            values. Pass along a dictionary when you want to specify the values of each inputs, or just the list of\n-            inputs expected (the value used will be `<<input_name>>` in this case).\n-        answer (`str` or `list[str]`):\n-            The theoretical answer (or list of possible valid answers) to the problem, as code.\n-    \"\"\"\n-\n-    def __init__(self, task, inputs, answer):\n-        self.task = task\n-        self.inputs = inputs\n-        self.answer = answer\n-\n-\n-### The list of problems the agent will be evaluated on.\n-EVALUATION_TASKS = [\n-    Problem(\n-        task=[\n-            \"Is the following `text` (in Spanish) positive or negative?\",\n-            \"Is the text in the variable `text` (in Spanish) positive or negative?\",\n-            \"Translate the following `text` from Spanish to English then tell me if its positive or negative.\",\n-        ],\n-        inputs=[\"text\"],\n-        answer=\"\"\"text_classifier(translator(text, src_lang=\"Spanish\", tgt_lang=\"English\"), labels=[\"positive\", \"negative\"])\"\"\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Tell me out loud what the `image` contains.\",\n-            \"Describe the following `image` out loud.\",\n-            \"Find what is in the picture stored in `image` then read it out loud.\",\n-        ],\n-        inputs=[\"image\"],\n-        answer=[\n-            \"text_reader(image_captioner(image))\",\n-            \"text_reader(image_qa(image, question='What is in the image?'))\",\n-        ],\n-    ),\n-    Problem(\n-        task=[\n-            \"Generate an image from the text given in `text_input`. Then transform it according to the text in `prompt`.\",\n-            \"Use the following `text_input` to generate an image, then transform it by using the text in `prompt`.\",\n-        ],\n-        inputs=[\"text_input\", \"prompt\"],\n-        answer=\"image_transformer(image_generator(text_input), prompt)\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Download the content of `url`, summarize it then generate an image from its content.\",\n-            \"Use a summary of the web page at `url` to generate an image.\",\n-            \"Summarize the content of the web page at `url`, and use the result to generate an image.\",\n-        ],\n-        inputs=[\"url\"],\n-        answer=\"image_generator(summarizer(text_downloader(url)))\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Transform the following `image` using the prompt in `text`. The prompt is in Spanish.\",\n-            \"Use the text prompt in `text` (in Spanish) to transform the following `image`.\",\n-            \"Translate the `text` from Spanish to English then use it to transform the picture in `image`.\",\n-        ],\n-        inputs=[\"text\", \"image\"],\n-        answer=\"image_transformer(image, translator(text, src_lang='Spanish', tgt_lang='English'))\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Download the content of `url`, summarize it then read it out loud to me.\",\n-            \"Read me a summary of the web page at `url`.\",\n-        ],\n-        inputs=[\"url\"],\n-        answer=\"text_reader(summarizer(text_downloader(url)))\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Generate an image from the text given in `text_input`.\",\n-        ],\n-        inputs=[\"text_input\"],\n-        answer=\"image_generator(text_input)\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Replace the beaver in the `image` by the `prompt`.\",\n-            \"Transform the `image` so that it contains the `prompt`.\",\n-            \"Use `prompt` to transform this `image`.\",\n-        ],\n-        inputs=[\"image\", \"prompt\"],\n-        answer=\"image_transformer(image, prompt)\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Provide me the summary of the `text`, then read it to me before transcribing it and translating it in French.\",\n-            \"Summarize `text`, read it out loud then transcribe the audio and translate it in French.\",\n-            \"Read me a summary of the `text` out loud. Transcribe this and translate it in French.\",\n-        ],\n-        inputs=[\"text\"],\n-        answer=\"translator(transcriber(text_reader(summarizer(text))), src_lang='English', tgt_lang='French')\",\n-    ),\n-    Problem(\n-        task=[\"Generate a video of the `prompt`\", \"Animate a `prompt`\", \"Make me a short video using `prompt`.\"],\n-        inputs={\"prompt\": \"A lobster swimming\"},\n-        answer=\"video_generator('A lobster swimming')\",\n-    ),\n-    Problem(\n-        task=[\n-            \"Download the following file `url`, summarize it in a few words and generate a video from it.\"\n-            \"Fetch the file at this `url`, summarize it, and create an animation out of it.\"\n-        ],\n-        inputs=[\"url\"],\n-        answer=\"video_generator(summarizer(text_downloader(url)))\",\n-    ),\n-]\n-\n-\n-def get_theoretical_tools(agent_answer, theoretical_answer, code_answer):\n-    if not isinstance(theoretical_answer, list):\n-        return {name for name in TEST_TOOLS if name in code_answer}\n-\n-    if isinstance(agent_answer, dict):\n-        for one_answer, one_code in zip(theoretical_answer, code_answer):\n-            if one_answer in agent_answer.values():\n-                return {name for name in TEST_TOOLS if name in one_code}\n-\n-    for one_answer, one_code in zip(theoretical_answer, code_answer):\n-        if agent_answer == one_answer:\n-            return {name for name in TEST_TOOLS if name in one_code}\n-\n-    return {name for name in TEST_TOOLS if name in code_answer[0]}\n-\n-\n-def evaluate_code(code, inputs=None, state=None, verbose=False, return_interpretor_error=False):\n-    tools = BASE_PYTHON_TOOLS.copy()\n-    for name, tool in TEST_TOOLS.items():\n-        if name not in code:\n-            continue\n-        tools[name] = tool\n-\n-    if isinstance(inputs, dict):\n-        inputs = inputs.copy()\n-    elif inputs is not None:\n-        inputs = {inp: f\"<<{inp}>>\" for inp in inputs}\n-\n-    if state is not None:\n-        state.update(inputs)\n-    else:\n-        state = inputs\n-\n-    try:\n-        return evaluate(code, tools, state)\n-    except InterpreterError as e:\n-        return str(e)\n-    except Exception as e:\n-        if verbose:\n-            print(e)\n-        return None\n-\n-\n-def score_code(agent_answer, theoretical_answer, verbose: bool = False):\n-    if verbose:\n-        print(agent_answer, theoretical_answer)\n-    theoretical_answer = theoretical_answer if isinstance(theoretical_answer, list) else [theoretical_answer]\n-\n-    if agent_answer in theoretical_answer:\n-        if verbose:\n-            print(\"Perfect!\")\n-        return 1\n-    elif isinstance(agent_answer, dict) and any(v in theoretical_answer for v in agent_answer.values()):\n-        if verbose:\n-            print(\"Almost perfect, result in state!\")\n-        return 0.75\n-    else:\n-        if verbose:\n-            print(\"Result is not the right one but code executed.\")\n-        return 0.3\n-\n-\n-def evaluate_one_result(code, agent_answer, theoretical_answer, answer, verbose=False):\n-    tools_in_code = {name for name in TEST_TOOLS if f\"`{name}`\" in code}\n-    theoretical_tools = get_theoretical_tools(agent_answer, theoretical_answer, answer)\n-    if tools_in_code == theoretical_tools:\n-        tool_selection_score = 1.0\n-        tool_selection_errors = None\n-    else:\n-        missing_tools = len(theoretical_tools - tools_in_code)\n-        unexpected_tools = len(tools_in_code - theoretical_tools)\n-        tool_selection_score = max(0, 1.0 - 0.25 * missing_tools - 0.25 * unexpected_tools)\n-\n-        tool_selection_errors = {\n-            \"selected_tools\": tools_in_code,\n-            \"theoretical_tools\": theoretical_tools,\n-        }\n-\n-    tools_in_code = {name for name in TEST_TOOLS if name in code}\n-    if tools_in_code == theoretical_tools:\n-        tool_used_score = 1.0\n-        tool_used_errors = None\n-    else:\n-        missing_tools = len(theoretical_tools - tools_in_code)\n-        unexpected_tools = len(tools_in_code - theoretical_tools)\n-        tool_used_score = max(0, 1.0 - 0.25 * missing_tools - 0.25 * unexpected_tools)\n-\n-        tool_used_errors = {\n-            \"selected_tools\": tools_in_code,\n-            \"theoretical_tools\": theoretical_tools,\n-        }\n-\n-    score = score_code(agent_answer, theoretical_answer, verbose=verbose)\n-    if score < 1.0:\n-        code_errors = {\n-            \"code_produced\": code,\n-            \"evaluation\": agent_answer,\n-            \"theoretical_answer\": theoretical_answer,\n-        }\n-    else:\n-        code_errors = None\n-\n-    return (tool_selection_score, tool_used_score, score), (tool_selection_errors, tool_used_errors, code_errors)\n-\n-\n-def evaluate_agent(agent, batch_size=8, verbose=False, return_errors=False):\n-    \"\"\"\n-    Evaluates a new agent on all `EVALUATION_TASKS`.\n-\n-    Example:\n-\n-    ```py\n-    agent = NewOpenAiAgent(model=\"text-davinci-003\", api_key=your_api_key)\n-    bads = new_evaluate_agent(agent)\n-    for bad in bads:\n-        print(bad)\n-    ```\n-    \"\"\"\n-    # Sanity check\n-    agent_tools = set(agent.toolbox.keys())\n-    if agent_tools != set(TEST_TOOLS):\n-        missing_tools = set(TEST_TOOLS) - agent_tools\n-        unexpected_tools = set(agent_tools) - TEST_TOOLS\n-        raise ValueError(\n-            f\"Fix the test tools in the evaluate_agent module. Tools missing: {missing_tools}. Extra tools: {unexpected_tools}.\"\n-        )\n-\n-    eval_tasks = []\n-    eval_idx = []\n-    for idx, pb in enumerate(EVALUATION_TASKS):\n-        if isinstance(pb.task, list):\n-            eval_tasks.extend(pb.task)\n-            eval_idx.extend([idx] * len(pb.task))\n-        else:\n-            eval_tasks.append(pb.task)\n-            eval_idx.append(idx)\n-\n-    tool_selection_score = 0\n-    tool_used_score = 0\n-    code_score = 0\n-\n-    if return_errors:\n-        tool_selection_errors = {}\n-        tool_used_errors = {}\n-        code_errors = {}\n-\n-    for start_idx in range(0, len(eval_tasks), batch_size):\n-        end_idx = min(start_idx + batch_size, len(eval_tasks))\n-        batch_tasks = eval_tasks[start_idx:end_idx]\n-\n-        results = [agent.run(task, return_generated_code=True) for task in batch_tasks]\n-\n-        for idx, result in enumerate(results):\n-            problem = EVALUATION_TASKS[eval_idx[start_idx + idx]]\n-            if verbose:\n-                print(f\"====Task {start_idx + idx}====\\n{batch_tasks[idx]}\\n\")\n-            code = agent.extract_action(result, split_token=\"Answer:\")\n-\n-            # Evaluate agent answer and code answer\n-            agent_answer = evaluate_code(code, problem.inputs, verbose=verbose)\n-            if isinstance(problem.answer, list):\n-                theoretical_answer = [evaluate_code(answer, problem.inputs) for answer in problem.answer]\n-            else:\n-                theoretical_answer = evaluate_code(problem.answer, problem.inputs)\n-\n-            scores, errors = evaluate_one_result(\n-                code, agent_answer, theoretical_answer, problem.answer, verbose=verbose\n-            )\n-\n-            tool_selection_score += scores[0]\n-            tool_used_score += scores[1]\n-            code_score += scores[2]\n-\n-            if return_errors:\n-                if errors[0] is not None:\n-                    tool_selection_errors[batch_tasks[idx]] = errors[0]\n-                if errors[1] is not None:\n-                    tool_used_errors[batch_tasks[idx]] = errors[1]\n-                if errors[2] is not None:\n-                    code_errors[batch_tasks[idx]] = errors[2]\n-\n-    scores = {\n-        \"tool selection score\": 100 * (tool_selection_score / len(eval_tasks)),\n-        \"tool used score\": 100 * (tool_used_score / len(eval_tasks)),\n-        \"code score\": 100 * (code_score / len(eval_tasks)),\n-    }\n-\n-    if return_errors:\n-        return scores, tool_selection_errors, tool_used_errors, code_errors\n-    else:\n-        return scores"
        },
        {
            "sha": "832f77a4ebe72597543a52be8b38e63508bab310",
            "filename": "src/transformers/agents/image_question_answering.py",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fimage_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fimage_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fimage_question_answering.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,57 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import torch\n-from PIL import Image\n-\n-from ..models.auto import AutoModelForVisualQuestionAnswering, AutoProcessor\n-from ..utils import requires_backends\n-from .tools import PipelineTool\n-\n-\n-class ImageQuestionAnsweringTool(PipelineTool):\n-    default_checkpoint = \"dandelin/vilt-b32-finetuned-vqa\"\n-    description = (\n-        \"This is a tool that answers a question about an image. It returns a text that is the answer to the question.\"\n-    )\n-    name = \"image_qa\"\n-    pre_processor_class = AutoProcessor\n-    model_class = AutoModelForVisualQuestionAnswering\n-\n-    inputs = {\n-        \"image\": {\n-            \"type\": \"image\",\n-            \"description\": \"The image containing the information. Can be a PIL Image or a string path to the image.\",\n-        },\n-        \"question\": {\"type\": \"string\", \"description\": \"The question in English\"},\n-    }\n-    output_type = \"string\"\n-\n-    def __init__(self, *args, **kwargs):\n-        requires_backends(self, [\"vision\"])\n-        super().__init__(*args, **kwargs)\n-\n-    def encode(self, image: \"Image\", question: str):\n-        return self.pre_processor(image, question, return_tensors=\"pt\")\n-\n-    def forward(self, inputs):\n-        with torch.no_grad():\n-            return self.model(**inputs).logits\n-\n-    def decode(self, outputs):\n-        idx = outputs.argmax(-1).item()\n-        return self.model.config.id2label[idx]"
        },
        {
            "sha": "49c96b95797144b683779a47ef4722593e093a38",
            "filename": "src/transformers/agents/llm_engine.py",
            "status": "removed",
            "additions": 0,
            "deletions": 243,
            "changes": 243,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fllm_engine.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,243 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from copy import deepcopy\n-from enum import Enum\n-from typing import Dict, List, Optional\n-\n-from huggingface_hub import InferenceClient\n-from huggingface_hub.utils._deprecation import _deprecate_method\n-\n-from .. import AutoTokenizer\n-from ..pipelines.base import Pipeline\n-from ..utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class MessageRole(str, Enum):\n-    USER = \"user\"\n-    ASSISTANT = \"assistant\"\n-    SYSTEM = \"system\"\n-    TOOL_CALL = \"tool-call\"\n-    TOOL_RESPONSE = \"tool-response\"\n-\n-    @classmethod\n-    def roles(cls):\n-        return [r.value for r in cls]\n-\n-\n-def get_clean_message_list(message_list: List[Dict[str, str]], role_conversions: Dict[str, str] = {}):\n-    \"\"\"\n-    Subsequent messages with the same role will be concatenated to a single message.\n-\n-    Args:\n-        message_list (`List[Dict[str, str]]`): List of chat messages.\n-    \"\"\"\n-    final_message_list = []\n-    message_list = deepcopy(message_list)  # Avoid modifying the original list\n-    for message in message_list:\n-        if not set(message.keys()) == {\"role\", \"content\"}:\n-            raise ValueError(\"Message should contain only 'role' and 'content' keys!\")\n-\n-        role = message[\"role\"]\n-        if role not in MessageRole.roles():\n-            raise ValueError(f\"Incorrect role {role}, only {MessageRole.roles()} are supported for now.\")\n-\n-        if role in role_conversions:\n-            message[\"role\"] = role_conversions[role]\n-\n-        if len(final_message_list) > 0 and message[\"role\"] == final_message_list[-1][\"role\"]:\n-            final_message_list[-1][\"content\"] += \"\\n=======\\n\" + message[\"content\"]\n-        else:\n-            final_message_list.append(message)\n-    return final_message_list\n-\n-\n-llama_role_conversions = {\n-    MessageRole.TOOL_RESPONSE: MessageRole.USER,\n-}\n-\n-\n-class HfEngine:\n-    @_deprecate_method(\n-        version=\"4.51.0\",\n-        message=\"Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\",\n-    )\n-    def __init__(self, model_id: Optional[str] = None):\n-        self.last_input_token_count = None\n-        self.last_output_token_count = None\n-        if model_id is None:\n-            model_id = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n-            logger.warning(f\"Using default model for token counting: '{model_id}'\")\n-        try:\n-            self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n-        except Exception as e:\n-            logger.warning(f\"Failed to load tokenizer for model {model_id}: {e}. Loading default tokenizer instead.\")\n-            self.tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\n-\n-    def get_token_counts(self):\n-        return {\n-            \"input_token_count\": self.last_input_token_count,\n-            \"output_token_count\": self.last_output_token_count,\n-        }\n-\n-    def generate(\n-        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n-    ):\n-        raise NotImplementedError\n-\n-    def __call__(\n-        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n-    ) -> str:\n-        \"\"\"Process the input messages and return the model's response.\n-\n-        This method sends a list of messages to the Hugging Face Inference API, optionally with stop sequences and grammar customization.\n-\n-        Parameters:\n-            messages (`List[Dict[str, str]]`):\n-                A list of message dictionaries to be processed. Each dictionary should have the structure `{\"role\": \"user/system\", \"content\": \"message content\"}`.\n-            stop_sequences (`List[str]`, *optional*):\n-                A list of strings that will stop the generation if encountered in the model's output.\n-            grammar (`str`, *optional*):\n-                The grammar or formatting structure to use in the model's response.\n-\n-        Returns:\n-            `str`: The text content of the model's response.\n-\n-        Example:\n-            ```python\n-            >>> engine = HfApiEngine(\n-            ...     model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-            ...     token=\"your_hf_token_here\",\n-            ...     max_tokens=2000\n-            ... )\n-            >>> messages = [{\"role\": \"user\", \"content\": \"Explain quantum mechanics in simple terms.\"}]\n-            >>> response = engine(messages, stop_sequences=[\"END\"])\n-            >>> print(response)\n-            \"Quantum mechanics is the branch of physics that studies...\"\n-            ```\n-        \"\"\"\n-        if not isinstance(messages, List):\n-            raise ValueError(\"Messages should be a list of dictionaries with 'role' and 'content' keys.\")\n-        if stop_sequences is None:\n-            stop_sequences = []\n-        response = self.generate(messages, stop_sequences, grammar)\n-        self.last_input_token_count = len(self.tokenizer.apply_chat_template(messages, tokenize=True))\n-        self.last_output_token_count = len(self.tokenizer.encode(response))\n-\n-        # Remove stop sequences from LLM output\n-        for stop_seq in stop_sequences:\n-            if response[-len(stop_seq) :] == stop_seq:\n-                response = response[: -len(stop_seq)]\n-        return response\n-\n-\n-class HfApiEngine(HfEngine):\n-    \"\"\"A class to interact with Hugging Face's Inference API for language model interaction.\n-\n-    This engine allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.\n-\n-    Parameters:\n-        model (`str`, *optional*, defaults to `\"meta-llama/Meta-Llama-3.1-8B-Instruct\"`):\n-            The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.\n-        token (`str`, *optional*):\n-            Token used by the Hugging Face API for authentication.\n-            If not provided, the class will use the token stored in the Hugging Face CLI configuration.\n-        max_tokens (`int`, *optional*, defaults to 1500):\n-            The maximum number of tokens allowed in the output.\n-        timeout (`int`, *optional*, defaults to 120):\n-            Timeout for the API request, in seconds.\n-\n-    Raises:\n-        ValueError:\n-            If the model name is not provided.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n-        token: Optional[str] = None,\n-        max_tokens: Optional[int] = 1500,\n-        timeout: Optional[int] = 120,\n-    ):\n-        super().__init__(model_id=model)\n-        self.model = model\n-        self.client = InferenceClient(self.model, token=token, timeout=timeout)\n-        self.max_tokens = max_tokens\n-\n-    def generate(\n-        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n-    ) -> str:\n-        # Get clean message list\n-        messages = get_clean_message_list(messages, role_conversions=llama_role_conversions)\n-\n-        # Send messages to the Hugging Face Inference API\n-        if grammar is not None:\n-            response = self.client.chat_completion(\n-                messages, stop=stop_sequences, max_tokens=self.max_tokens, response_format=grammar\n-            )\n-        else:\n-            response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n-\n-        response = response.choices[0].message.content\n-        return response\n-\n-\n-class TransformersEngine(HfEngine):\n-    \"\"\"This engine uses a pre-initialized local text-generation pipeline.\"\"\"\n-\n-    def __init__(self, pipeline: Pipeline, model_id: Optional[str] = None):\n-        super().__init__(model_id)\n-        self.pipeline = pipeline\n-\n-    def generate(\n-        self,\n-        messages: List[Dict[str, str]],\n-        stop_sequences: Optional[List[str]] = None,\n-        grammar: Optional[str] = None,\n-        max_length: int = 1500,\n-    ) -> str:\n-        # Get clean message list\n-        messages = get_clean_message_list(messages, role_conversions=llama_role_conversions)\n-\n-        # Get LLM output\n-        if stop_sequences is not None and len(stop_sequences) > 0:\n-            stop_strings = stop_sequences\n-        else:\n-            stop_strings = None\n-\n-        output = self.pipeline(\n-            messages,\n-            stop_strings=stop_strings,\n-            max_length=max_length,\n-            tokenizer=self.pipeline.tokenizer,\n-        )\n-\n-        response = output[0][\"generated_text\"][-1][\"content\"]\n-        return response\n-\n-\n-DEFAULT_JSONAGENT_REGEX_GRAMMAR = {\n-    \"type\": \"regex\",\n-    \"value\": 'Thought: .+?\\\\nAction:\\\\n\\\\{\\\\n\\\\s{4}\"action\":\\\\s\"[^\"\\\\n]+\",\\\\n\\\\s{4}\"action_input\":\\\\s\"[^\"\\\\n]+\"\\\\n\\\\}\\\\n<end_action>',\n-}\n-\n-DEFAULT_CODEAGENT_REGEX_GRAMMAR = {\n-    \"type\": \"regex\",\n-    \"value\": \"Thought: .+?\\\\nCode:\\\\n```(?:py|python)?\\\\n(?:.|\\\\s)+?\\\\n```<end_action>\",\n-}"
        },
        {
            "sha": "7126e72b5fd0606fbb4bb6f7eff46e0ad0992abd",
            "filename": "src/transformers/agents/monitoring.py",
            "status": "removed",
            "additions": 0,
            "deletions": 117,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fmonitoring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fmonitoring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fmonitoring.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,117 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from ..utils import logging\n-from .agent_types import AgentAudio, AgentImage, AgentText\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-def pull_message(step_log: dict, test_mode: bool = True):\n-    try:\n-        from gradio import ChatMessage\n-    except ImportError:\n-        if test_mode:\n-\n-            class ChatMessage:\n-                def __init__(self, role, content, metadata=None):\n-                    self.role = role\n-                    self.content = content\n-                    self.metadata = metadata\n-        else:\n-            raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n-\n-    if step_log.get(\"rationale\"):\n-        yield ChatMessage(role=\"assistant\", content=step_log[\"rationale\"])\n-    if step_log.get(\"tool_call\"):\n-        used_code = step_log[\"tool_call\"][\"tool_name\"] == \"code interpreter\"\n-        content = step_log[\"tool_call\"][\"tool_arguments\"]\n-        if used_code:\n-            content = f\"```py\\n{content}\\n```\"\n-        yield ChatMessage(\n-            role=\"assistant\",\n-            metadata={\"title\": f\"ğŸ› ï¸ Used tool {step_log['tool_call']['tool_name']}\"},\n-            content=str(content),\n-        )\n-    if step_log.get(\"observation\"):\n-        yield ChatMessage(role=\"assistant\", content=f\"```\\n{step_log['observation']}\\n```\")\n-    if step_log.get(\"error\"):\n-        yield ChatMessage(\n-            role=\"assistant\",\n-            content=str(step_log[\"error\"]),\n-            metadata={\"title\": \"ğŸ’¥ Error\"},\n-        )\n-\n-\n-def stream_to_gradio(agent, task: str, test_mode: bool = False, **kwargs):\n-    \"\"\"Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages.\"\"\"\n-\n-    try:\n-        from gradio import ChatMessage\n-    except ImportError:\n-        if test_mode:\n-\n-            class ChatMessage:\n-                def __init__(self, role, content, metadata=None):\n-                    self.role = role\n-                    self.content = content\n-                    self.metadata = metadata\n-        else:\n-            raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n-\n-    for step_log in agent.run(task, stream=True, **kwargs):\n-        if isinstance(step_log, dict):\n-            for message in pull_message(step_log, test_mode=test_mode):\n-                yield message\n-\n-    final_answer = step_log  # Last log is the run's final_answer\n-\n-    if isinstance(final_answer, AgentText):\n-        yield ChatMessage(role=\"assistant\", content=f\"**Final answer:**\\n```\\n{final_answer.to_string()}\\n```\")\n-    elif isinstance(final_answer, AgentImage):\n-        yield ChatMessage(\n-            role=\"assistant\",\n-            content={\"path\": final_answer.to_string(), \"mime_type\": \"image/png\"},\n-        )\n-    elif isinstance(final_answer, AgentAudio):\n-        yield ChatMessage(\n-            role=\"assistant\",\n-            content={\"path\": final_answer.to_string(), \"mime_type\": \"audio/wav\"},\n-        )\n-    else:\n-        yield ChatMessage(role=\"assistant\", content=str(final_answer))\n-\n-\n-class Monitor:\n-    def __init__(self, tracked_llm_engine):\n-        self.step_durations = []\n-        self.tracked_llm_engine = tracked_llm_engine\n-        if getattr(self.tracked_llm_engine, \"last_input_token_count\", \"Not found\") != \"Not found\":\n-            self.total_input_token_count = 0\n-            self.total_output_token_count = 0\n-\n-    def update_metrics(self, step_log):\n-        step_duration = step_log[\"step_duration\"]\n-        self.step_durations.append(step_duration)\n-        logger.info(f\"Step {len(self.step_durations)}:\")\n-        logger.info(f\"- Time taken: {step_duration:.2f} seconds (valid only if step succeeded)\")\n-\n-        if getattr(self.tracked_llm_engine, \"last_input_token_count\", None) is not None:\n-            self.total_input_token_count += self.tracked_llm_engine.last_input_token_count\n-            self.total_output_token_count += self.tracked_llm_engine.last_output_token_count\n-            logger.info(f\"- Input tokens: {self.total_input_token_count}\")\n-            logger.info(f\"- Output tokens: {self.total_output_token_count}\")"
        },
        {
            "sha": "0cf8beb144f8ba222125ee23ae1c206d1a84189b",
            "filename": "src/transformers/agents/prompts.py",
            "status": "removed",
            "additions": 0,
            "deletions": 789,
            "changes": 789,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fprompts.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fprompts.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fprompts.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,789 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import re\n-\n-from ..utils import cached_file\n-\n-\n-# docstyle-ignore\n-CHAT_MESSAGE_PROMPT = \"\"\"\n-Human: <<task>>\n-\n-Assistant: \"\"\"\n-\n-\n-DEFAULT_PROMPTS_REPO = \"huggingface-tools/default-prompts\"\n-PROMPT_FILES = {\"chat\": \"chat_prompt_template.txt\", \"run\": \"run_prompt_template.txt\"}\n-\n-\n-def download_prompt(prompt_or_repo_id, agent_name, mode=\"run\"):\n-    \"\"\"\n-    Downloads and caches the prompt from a repo and returns it contents (if necessary).\n-    \"\"\"\n-    if prompt_or_repo_id is None:\n-        prompt_or_repo_id = DEFAULT_PROMPTS_REPO\n-\n-    # prompt is considered a repo ID when it does not contain any kind of space\n-    if re.search(\"\\\\s\", prompt_or_repo_id) is not None:\n-        return prompt_or_repo_id\n-\n-    prompt_file = cached_file(\n-        prompt_or_repo_id, PROMPT_FILES[mode], repo_type=\"dataset\", user_agent={\"agent\": agent_name}\n-    )\n-    with open(prompt_file, \"r\", encoding=\"utf-8\") as f:\n-        return f.read()\n-\n-\n-DEFAULT_CODE_SYSTEM_PROMPT = \"\"\"You will be given a task to solve, your job is to come up with a series of simple commands in Python that will perform the task.\n-To help you, I will give you access to a set of tools that you can use. Each tool is a Python function and has a description explaining the task it performs, the inputs it expects and the outputs it returns.\n-You should first explain which tool you will use to perform the task and for what reason, then write the code in Python.\n-Each instruction in Python should be a simple assignment. You can print intermediate results if it makes sense to do so.\n-In the end, use tool 'final_answer' to return your answer, its argument will be what gets returned.\n-You can use imports in your code, but only from the following list of modules: <<authorized_imports>>\n-Be sure to provide a 'Code:' token, else the run will fail.\n-\n-Tools:\n-<<tool_descriptions>>\n-\n-Examples:\n----\n-Task: \"Answer the question in the variable `question` about the image stored in the variable `image`. The question is in French.\"\n-\n-Thought: I will use the following tools: `translator` to translate the question into English and then `image_qa` to answer the question on the input image.\n-Code:\n-```py\n-translated_question = translator(question=question, src_lang=\"French\", tgt_lang=\"English\")\n-print(f\"The translated question is {translated_question}.\")\n-answer = image_qa(image=image, question=translated_question)\n-final_answer(f\"The answer is {answer}\")\n-```<end_action>\n-\n----\n-Task: \"Identify the oldest person in the `document` and create an image showcasing the result.\"\n-\n-Thought: I will use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\n-Code:\n-```py\n-answer = document_qa(document, question=\"What is the oldest person?\")\n-print(f\"The answer is {answer}.\")\n-image = image_generator(answer)\n-final_answer(image)\n-```<end_action>\n-\n----\n-Task: \"Generate an image using the text given in the variable `caption`.\"\n-\n-Thought: I will use the following tool: `image_generator` to generate an image.\n-Code:\n-```py\n-image = image_generator(prompt=caption)\n-final_answer(image)\n-```<end_action>\n-\n----\n-Task: \"Summarize the text given in the variable `text` and read it out loud.\"\n-\n-Thought: I will use the following tools: `summarizer` to create a summary of the input text, then `text_reader` to read it out loud.\n-Code:\n-```py\n-summarized_text = summarizer(text)\n-print(f\"Summary: {summarized_text}\")\n-audio_summary = text_reader(summarized_text)\n-final_answer(audio_summary)\n-```<end_action>\n-\n----\n-Task: \"Answer the question in the variable `question` about the text in the variable `text`. Use the answer to generate an image.\"\n-\n-Thought: I will use the following tools: `text_qa` to create the answer, then `image_generator` to generate an image according to the answer.\n-Code:\n-```py\n-answer = text_qa(text=text, question=question)\n-print(f\"The answer is {answer}.\")\n-image = image_generator(answer)\n-final_answer(image)\n-```<end_action>\n-\n----\n-Task: \"Caption the following `image`.\"\n-\n-Thought: I will use the following tool: `image_captioner` to generate a caption for the image.\n-Code:\n-```py\n-caption = image_captioner(image)\n-final_answer(caption)\n-```<end_action>\n-\n----\n-Above example were using tools that might not exist for you. You only have access to these Tools:\n-<<tool_names>>\n-\n-Remember to make sure that variables you use are all defined.\n-Be sure to provide a 'Code:\\n```' sequence before the code and '```<end_action>' after, else you will get an error.\n-DO NOT pass the arguments as a dict as in 'answer = ask_search_agent({'query': \"What is the place where James Bond lives?\"})', but use the arguments directly as in 'answer = ask_search_agent(query=\"What is the place where James Bond lives?\")'.\n-\n-Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n-\"\"\"\n-\n-\n-DEFAULT_REACT_JSON_SYSTEM_PROMPT = \"\"\"You are an expert assistant who can solve any task using JSON tool calls. You will be given a task to solve as best you can.\n-To do so, you have been given access to the following tools: <<tool_names>>\n-The way you use the tools is by specifying a json blob, ending with '<end_action>'.\n-Specifically, this json should have an `action` key (name of the tool to use) and an `action_input` key (input to the tool).\n-\n-The $ACTION_JSON_BLOB should only contain a SINGLE action, do NOT return a list of multiple actions. It should be formatted in json. Do not try to escape special characters. Here is the template of a valid $ACTION_JSON_BLOB:\n-{\n-  \"action\": $TOOL_NAME,\n-  \"action_input\": $INPUT\n-}<end_action>\n-\n-Make sure to have the $INPUT as a dictionary in the right format for the tool you are using, and do not put variable names as input if you can find the right values.\n-\n-You should ALWAYS use the following format:\n-\n-Thought: you should always think about one action to take. Then use the action as follows:\n-Action:\n-$ACTION_JSON_BLOB\n-Observation: the result of the action\n-... (this Thought/Action/Observation can repeat N times, you should take several steps when needed. The $ACTION_JSON_BLOB must only use a SINGLE action at a time.)\n-\n-You can use the result of the previous action as input for the next action.\n-The observation will always be a string: it can represent a file, like \"image_1.jpg\".\n-Then you can use it as input for the next action. You can do it for instance as follows:\n-\n-Observation: \"image_1.jpg\"\n-\n-Thought: I need to transform the image that I received in the previous observation to make it green.\n-Action:\n-{\n-  \"action\": \"image_transformer\",\n-  \"action_input\": {\"image\": \"image_1.jpg\"}\n-}<end_action>\n-\n-To provide the final answer to the task, use an action blob with \"action\": \"final_answer\" tool. It is the only way to complete the task, else you will be stuck on a loop. So your final output should look like this:\n-Action:\n-{\n-  \"action\": \"final_answer\",\n-  \"action_input\": {\"answer\": \"insert your final answer here\"}\n-}<end_action>\n-\n-\n-Here are a few examples using notional tools:\n----\n-Task: \"Generate an image of the oldest person in this document.\"\n-\n-Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\n-Action:\n-{\n-  \"action\": \"document_qa\",\n-  \"action_input\": {\"document\": \"document.pdf\", \"question\": \"Who is the oldest person mentioned?\"}\n-}<end_action>\n-Observation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\n-\n-\n-Thought: I will now generate an image showcasing the oldest person.\n-Action:\n-{\n-  \"action\": \"image_generator\",\n-  \"action_input\": {\"prompt\": \"A portrait of John Doe, a 55-year-old man living in Canada.\"}\n-}<end_action>\n-Observation: \"image.png\"\n-\n-Thought: I will now return the generated image.\n-Action:\n-{\n-  \"action\": \"final_answer\",\n-  \"action_input\": \"image.png\"\n-}<end_action>\n-\n----\n-Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\n-\n-Thought: I will use python code evaluator to compute the result of the operation and then return the final answer using the `final_answer` tool\n-Action:\n-{\n-    \"action\": \"python_interpreter\",\n-    \"action_input\": {\"code\": \"5 + 3 + 1294.678\"}\n-}<end_action>\n-Observation: 1302.678\n-\n-Thought: Now that I know the result, I will now return it.\n-Action:\n-{\n-  \"action\": \"final_answer\",\n-  \"action_input\": \"1302.678\"\n-}<end_action>\n-\n----\n-Task: \"Which city has the highest population , Guangzhou or Shanghai?\"\n-\n-Thought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\n-Action:\n-{\n-    \"action\": \"search\",\n-    \"action_input\": \"Population Guangzhou\"\n-}<end_action>\n-Observation: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\n-\n-\n-Thought: Now let's get the population of Shanghai using the tool 'search'.\n-Action:\n-{\n-    \"action\": \"search\",\n-    \"action_input\": \"Population Shanghai\"\n-}\n-Observation: '26 million (2019)'\n-\n-Thought: Now I know that Shanghai has a larger population. Let's return the result.\n-Action:\n-{\n-  \"action\": \"final_answer\",\n-  \"action_input\": \"Shanghai\"\n-}<end_action>\n-\n-\n-Above example were using notional tools that might not exist for you. You only have access to these tools:\n-<<tool_descriptions>>\n-\n-Here are the rules you should always follow to solve your task:\n-1. ALWAYS provide a 'Thought:' sequence, and an 'Action:' sequence that ends with <end_action>, else you will fail.\n-2. Always use the right arguments for the tools. Never use variable names in the 'action_input' field, use the value instead.\n-3. Call a tool only when needed: do not call the search agent if you do not need information, try to solve the task yourself.\n-4. Never re-do a tool call that you previously did with the exact same parameters.\n-\n-Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n-\"\"\"\n-\n-\n-DEFAULT_REACT_CODE_SYSTEM_PROMPT = \"\"\"You are an expert assistant who can solve any task using code blobs. You will be given a task to solve as best you can.\n-To do so, you have been given access to a list of tools: these tools are basically Python functions which you can call with code.\n-To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n-\n-At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task and the tools that you want to use.\n-Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '<end_action>' sequence.\n-During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n-These print outputs will then appear in the 'Observation:' field, which will be available as input for the next step.\n-In the end you have to return a final answer using the `final_answer` tool.\n-\n-Here are a few examples using notional tools:\n----\n-Task: \"Generate an image of the oldest person in this document.\"\n-\n-Thought: I will proceed step by step and use the following tools: `document_qa` to find the oldest person in the document, then `image_generator` to generate an image according to the answer.\n-Code:\n-```py\n-answer = document_qa(document=document, question=\"Who is the oldest person mentioned?\")\n-print(answer)\n-```<end_action>\n-Observation: \"The oldest person in the document is John Doe, a 55 year old lumberjack living in Newfoundland.\"\n-\n-Thought: I will now generate an image showcasing the oldest person.\n-Code:\n-```py\n-image = image_generator(\"A portrait of John Doe, a 55-year-old man living in Canada.\")\n-final_answer(image)\n-```<end_action>\n-\n----\n-Task: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\n-\n-Thought: I will use python code to compute the result of the operation and then return the final answer using the `final_answer` tool\n-Code:\n-```py\n-result = 5 + 3 + 1294.678\n-final_answer(result)\n-```<end_action>\n-\n----\n-Task: \"Which city has the highest population: Guangzhou or Shanghai?\"\n-\n-Thought: I need to get the populations for both cities and compare them: I will use the tool `search` to get the population of both cities.\n-Code:\n-```py\n-population_guangzhou = search(\"Guangzhou population\")\n-print(\"Population Guangzhou:\", population_guangzhou)\n-population_shanghai = search(\"Shanghai population\")\n-print(\"Population Shanghai:\", population_shanghai)\n-```<end_action>\n-Observation:\n-Population Guangzhou: ['Guangzhou has a population of 15 million inhabitants as of 2021.']\n-Population Shanghai: '26 million (2019)'\n-\n-Thought: Now I know that Shanghai has the highest population.\n-Code:\n-```py\n-final_answer(\"Shanghai\")\n-```<end_action>\n-\n----\n-Task: \"What is the current age of the pope, raised to the power 0.36?\"\n-\n-Thought: I will use the tool `wiki` to get the age of the pope, then raise it to the power 0.36.\n-Code:\n-```py\n-pope_age = wiki(query=\"current pope age\")\n-print(\"Pope age:\", pope_age)\n-```<end_action>\n-Observation:\n-Pope age: \"The pope Francis is currently 85 years old.\"\n-\n-Thought: I know that the pope is 85 years old. Let's compute the result using python code.\n-Code:\n-```py\n-pope_current_age = 85 ** 0.36\n-final_answer(pope_current_age)\n-```<end_action>\n-\n-Above example were using notional tools that might not exist for you. On top of performing computations in the Python code snippets that you create, you have access to these tools (and no other tool):\n-\n-<<tool_descriptions>>\n-\n-<<managed_agents_descriptions>>\n-\n-Here are the rules you should always follow to solve your task:\n-1. Always provide a 'Thought:' sequence, and a 'Code:\\n```py' sequence ending with '```<end_action>' sequence, else you will fail.\n-2. Use only variables that you have defined!\n-3. Always use the right arguments for the tools. DO NOT pass the arguments as a dict as in 'answer = wiki({'query': \"What is the place where James Bond lives?\"})', but use the arguments directly as in 'answer = wiki(query=\"What is the place where James Bond lives?\")'.\n-4. Take care to not chain too many sequential tool calls in the same code block, especially when the output format is unpredictable. For instance, a call to search has an unpredictable return format, so do not have another tool call that depends on its output in the same block: rather output results with print() to use them in the next block.\n-5. Call a tool only when needed, and never re-do a tool call that you previously did with the exact same parameters.\n-6. Don't name any new variable with the same name as a tool: for instance don't name a variable 'final_answer'.\n-7. Never create any notional variables in our code, as having these in your logs might derail you from the true variables.\n-8. You can use imports in your code, but only from the following list of modules: <<authorized_imports>>\n-9. The state persists between code executions: so if in one step you've created variables or imported modules, these will all persist.\n-10. Don't give up! You're in charge of solving the task, not providing directions to solve it.\n-\n-Now Begin! If you solve the task correctly, you will receive a reward of $1,000,000.\n-\"\"\"\n-\n-SYSTEM_PROMPT_FACTS = \"\"\"Below I will present you a task.\n-\n-You will now build a comprehensive preparatory survey of which facts we have at our disposal and which ones we still need.\n-To do so, you will have to read the task and identify things that must be discovered in order to successfully complete it.\n-Don't make any assumptions. For each item, provide a thorough reasoning. Here is how you will structure this survey:\n-\n----\n-### 1. Facts given in the task\n-List here the specific facts given in the task that could help you (there might be nothing here).\n-\n-### 2. Facts to look up\n-List here any facts that we may need to look up.\n-Also list where to find each of these, for instance a website, a file... - maybe the task contains some sources that you should re-use here.\n-\n-### 3. Facts to derive\n-List here anything that we want to derive from the above by logical reasoning, for instance computation or simulation.\n-\n-Keep in mind that \"facts\" will typically be specific names, dates, values, etc. Your answer should use the below headings:\n-### 1. Facts given in the task\n-### 2. Facts to look up\n-### 3. Facts to derive\n-Do not add anything else.\"\"\"\n-\n-SYSTEM_PROMPT_PLAN = \"\"\"You are a world expert at making efficient plans to solve any task using a set of carefully crafted tools.\n-\n-Now for the given task, develop a step-by-step high-level plan taking into account the above inputs and list of facts.\n-This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.\n-Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.\n-After writing the final step of the plan, write the '\\n<end_plan>' tag and stop there.\"\"\"\n-\n-USER_PROMPT_PLAN = \"\"\"\n-Here is your task:\n-\n-Task:\n-```\n-{task}\n-```\n-\n-Your plan can leverage any of these tools:\n-{tool_descriptions}\n-\n-{managed_agents_descriptions}\n-\n-List of facts that you know:\n-```\n-{answer_facts}\n-```\n-\n-Now begin! Write your plan below.\"\"\"\n-\n-SYSTEM_PROMPT_FACTS_UPDATE = \"\"\"\n-You are a world expert at gathering known and unknown facts based on a conversation.\n-Below you will find a task, and ahistory of attempts made to solve the task. You will have to produce a list of these:\n-### 1. Facts given in the task\n-### 2. Facts that we have learned\n-### 3. Facts still to look up\n-### 4. Facts still to derive\n-Find the task and history below.\"\"\"\n-\n-USER_PROMPT_FACTS_UPDATE = \"\"\"Earlier we've built a list of facts.\n-But since in your previous steps you may have learned useful new facts or invalidated some false ones.\n-Please update your list of facts based on the previous history, and provide these headings:\n-### 1. Facts given in the task\n-### 2. Facts that we have learned\n-### 3. Facts still to look up\n-### 4. Facts still to derive\n-\n-Now write your new list of facts below.\"\"\"\n-\n-SYSTEM_PROMPT_PLAN_UPDATE = \"\"\"You are a world expert at making efficient plans to solve any task using a set of carefully crafted tools.\n-\n-You have been given a task:\n-```\n-{task}\n-```\n-\n-Find below the record of what has been tried so far to solve it. Then you will be asked to make an updated plan to solve the task.\n-If the previous tries so far have met some success, you can make an updated plan based on these actions.\n-If you are stalled, you can make a completely new plan starting from scratch.\n-\"\"\"\n-\n-USER_PROMPT_PLAN_UPDATE = \"\"\"You're still working towards solving this task:\n-```\n-{task}\n-```\n-\n-You have access to these tools and only these:\n-{tool_descriptions}\n-\n-{managed_agents_descriptions}\n-\n-Here is the up to date list of facts that you know:\n-```\n-{facts_update}\n-```\n-\n-Now for the given task, develop a step-by-step high-level plan taking into account the above inputs and list of facts.\n-This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer.\n-Beware that you have {remaining_steps} steps remaining.\n-Do not skip steps, do not add any superfluous steps. Only write the high-level plan, DO NOT DETAIL INDIVIDUAL TOOL CALLS.\n-After writing the final step of the plan, write the '\\n<end_plan>' tag and stop there.\n-\n-Now write your new plan below.\"\"\"\n-\n-SYSTEM_PROMPT_PLAN_STRUCTURED = \"\"\"Output a step-by-step plan to solve the task using the given tools.\n-This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer. Each step should be structured as follows:\n-Step #n: {\n-  \"description\": <description of what the step does and its output>\n-  \"tool\": <tool to use>,\n-  \"params\": {\n-      <parameters to pass to the tool as a valid dict>\n-  }\n-  \"output_var\": <output variable name>\n-}\n-Each step must be necessary to reach the final answer. Steps should reuse outputs produced by earlier steps. The last step must be the final answer.\n-\n-Below are some examples:\n-\n-Example 1:\n-------\n-Inputs:\n----\n-Task:\n-How many encoder blocks were in the first attention-only ML architecture published?\n-\n-[FACTS LIST]:\n-### 1. Facts given in the task\n-- The paper first introduced an attention-only ML architecture.\n-- The specific information required is the page number where the number of encoder blocks is stated.\n-- No local files are provided for access.\n-\n-### 2. Facts to look up\n-- The title and authors of the paper that first introduced an attention-only ML architecture.\n-  - Source: Online search (e.g., Google Scholar, arXiv, or other academic databases)\n-- The full text of the identified paper.\n-  - Source: Online academic repositories (e.g., arXiv, journal websites)\n-- The specific page number in the paper where the number of encoder blocks is mentioned.\n-  - Source: The content of the identified paper\n-\n-### 3. Facts to derive\n-- By identifying the correct paper and locating the specific page, we will derive the page number where the number of encoder blocks is stated.\n-  - Logical steps: Identify the correct paper, access its content, search for the term \"encoder blocks,\" and note the page number where this information is found.\n-```\n-\n-[STEP 1 TOOL CALL]: {'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Identify the title and authors of the paper that first introduced an attention-only ML architecture.\\nanswer = ask_search_agent(query=\"Can you find the title and authors of the paper that first introduced an attention-only machine learning architecture? Please provide the full citation.\")\\nprint(answer)'}\n-[OUTPUT OF STEP 1] Observation: **Title**: Attention Is All You Need\n-**Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n-[STEP 2 TOOL CALL]: {'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Find the full text of the identified paper on arXiv\\\\npaper_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\\\\nprint(paper_url)'}\n-[OUTPUT OF STEP 2] Observation: https://arxiv.org/pdf/1706.03762.pdf\n----\n-\n-Output plan:\n----\n-Step #1: {\n-  \"description\": \"Open the PDF of the paper from the provided URL and search within the text of the paper for the  mention of \"encoder blocks\"\",\n-  \"tool\": \"inspect_file_as_text\",\n-  \"params\": {\n-    \"file_path\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n-    \"question\": \"On which page is the number of encoder blocks mentioned?\"\n-  },\n-  \"output_var\": \"page_number\"\n-}\n-\n-Step #2: {\n-  \"description\": \"Provide the final answer\",\n-  \"tool\": \"final_answer\",\n-  \"params\": {\n-      \"answer\": \"{page_number}\"\n-  },\n-  \"output_var\": \"\"\n-}\n-------\n-\n-Example 2:\n-------\n-Inputs:\n----\n-Task:\n-How many golf balls fits into a Boeing-747?\n-\n-[FACTS LIST]:\n-### 1. Facts given in the task\n-- The task requires calculating the number of golf balls that fir into a Boeing-747\n-### 2. Facts to look up\n-- The volume of a golf ball\n-- The volume of a Boeing-747\n-### 3. Facts to derive\n-- Once the volumes are known the final answer can be calculated\n----\n-Output plan:\n----\n-Step #1: {\n-  \"description\": \"Find the volume of a Boeing-747\",\n-  \"tool\": \"web_search\",\n-  \"params\": {\n-      \"query\": \"What is the internal volume of a Boeing-747 in cubic meters?\"\n-  },\n-  \"output_var\": \"boeing_volume\"\n-}\n-\n-Step #2: {\n-  \"description\": \"Find the volume of a standard golf ball\",\n-  \"tool\": \"ask_search_agent\",\n-  \"params\": {\n-      \"query\": \"What is the volume of a standard golf ball in cubic centimeters?\"\n-  },\n-  \"output_var\": \"golf_ball_volume\"\n-}\n-\n-Step #3: {\n-  \"description\": \"Convert the volume of a golf ball from cubic centimeters to cubic meters. Calculate the number of golf balls that fit into the Boeing-747 by dividing the internal volume of the Boeing-747 by the volume of a golf ball.\",\n-  \"tool\": \"python_code\",\n-  \"params\": {\n-      \"code\": \"golf_ball_volume_m3 = golf_ball_volume / 1e6\\nnumber_of_golf_balls = boeing_volume / golf_ball_volume_m3\"\n-  },\n-  \"output_var\": \"number_of_golf_balls\"\n-}\n-\n-Step #4: {\n-  \"description\": \"Provide the final answer\",\n-  \"tool\": \"final_answer\",\n-  \"params\": {\n-      \"answer\": \"{number_of_golf_balls}\"\n-  },\n-  \"output_var\": \"\"\n-}\n-------\n-Above example were using tools that might not exist for you.\n-Your goal is to create a plan to solve the task.\"\"\"\n-\n-USER_PROMPT_PLAN_STRUCTURED = \"\"\"\n-Here are your inputs:\n-\n-Task:\n-```\n-{task}\n-```\n-\n-Your plan can leverage any of these tools:\n-{tool_descriptions}\n-These tools are Python functions which you can call with code. You also have access to a Python interpreter so you can run Python code.\n-\n-List of facts that you know:\n-```\n-{answer_facts}\n-```\n-\n-Now for the given task, create a plan taking into account the list of facts.\n-After writing the final step of the plan, write the '\\n<end_plan>' tag and stop there. Output the plan only and nothing else.\"\"\"\n-\n-SYSTEM_PROMPT_PLAN_UPDATE_STRUCTURED = \"\"\"Output a step-by-step plan to solve the task using the given tools.\n-This plan should involve individual tasks based on the available tools, that if executed correctly will yield the correct answer. Each step should be structured as follows:\n-Step #n: {{\n-  \"description\": <description of what the step does and its output>\n-  \"tool\": <tool to use>,\n-  \"params\": {{\n-      <parameters to pass to the tool as a valid dict>\n-  }}\n-  \"output_var\": <output variable name>\n-}}\n-Each step must be necessary to reach the final answer. Steps should reuse outputs produced by earlier steps. The last step must be the final answer.\n-\n-Below are some examples:\n-\n-Example 1:\n-------\n-Inputs:\n----\n-Task:\n-How many encoder blocks were in the first attention-only ML architecture published?\n-\n-[FACTS LIST]:\n-### 1. Facts given in the task\n-- The paper first introduced an attention-only ML architecture.\n-- The specific information required is the page number where the number of encoder blocks is stated.\n-- No local files are provided for access.\n-\n-### 2. Facts to look up\n-- The title and authors of the paper that first introduced an attention-only ML architecture.\n-  - Source: Online search (e.g., Google Scholar, arXiv, or other academic databases)\n-- The full text of the identified paper.\n-  - Source: Online academic repositories (e.g., arXiv, journal websites)\n-- The specific page number in the paper where the number of encoder blocks is mentioned.\n-  - Source: The content of the identified paper\n-\n-### 3. Facts to derive\n-- By identifying the correct paper and locating the specific page, we will derive the page number where the number of encoder blocks is stated.\n-  - Logical steps: Identify the correct paper, access its content, search for the term \"encoder blocks,\" and note the page number where this information is found.\n-```\n-\n-[STEP 1 TOOL CALL]: {{'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Identify the title and authors of the paper that first introduced an attention-only ML architecture.\\nanswer = ask_search_agent(query=\"Can you find the title and authors of the paper that first introduced an attention-only machine learning architecture? Please provide the full citation.\")\\nprint(answer)'}}\n-[OUTPUT OF STEP 1] Observation: **Title**: Attention Is All You Need\n-**Authors**: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n-[STEP 2 TOOL CALL]: {{'tool_name': 'code interpreter', 'tool_arguments': '# Step 1: Find the full text of the identified paper on arXiv\\\\npaper_url = \"https://arxiv.org/pdf/1706.03762.pdf\"\\\\nprint(paper_url)'}}\n-[OUTPUT OF STEP 2] Observation: https://arxiv.org/pdf/1706.03762.pdf\n----\n-\n-Output plan:\n----\n-Step #1: {{\n-  \"description\": \"Open the PDF of the paper from the provided URL and search within the text of the paper for the  mention of \"encoder blocks\"\",\n-  \"tool\": \"inspect_file_as_text\",\n-  \"params\": {{\n-    \"file_path\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n-    \"question\": \"On which page is the number of encoder blocks mentioned?\"\n-  }},\n-  \"output_var\": \"page_number\"\n-}}\n-\n-Step #2: {{\n-  \"description\": \"Provide the final answer\",\n-  \"tool\": \"final_answer\",\n-  \"params\": {{\n-      \"answer\": \"{{page_number}}\"\n-  }},\n-  \"output_var\": \"\"\n-}}\n-------\n-\n-Example 2:\n-------\n-Inputs:\n----\n-Task:\n-How many golf balls fits into a Boeing-747?\n-\n-[FACTS LIST]:\n-### 1. Facts given in the task\n-- The task requires calculating the number of golf balls that fir into a Boeing-747\n-### 2. Facts to look up\n-- The volume of a golf ball\n-- The volume of a Boeing-747\n-### 3. Facts to derive\n-- Once the volumes are known the final answer can be calculated\n----\n-Output plan:\n----\n-Step #1: {{\n-  \"description\": \"Find the volume of a Boeing-747\",\n-  \"tool\": \"web_search\",\n-  \"params\": {{\n-      \"query\": \"What is the internal volume of a Boeing-747 in cubic meters?\"\n-  }},\n-  \"output_var\": \"boeing_volume\"\n-}}\n-\n-Step #2: {{\n-  \"description\": \"Find the volume of a standard golf ball\",\n-  \"tool\": \"ask_search_agent\",\n-  \"params\": {{\n-      \"query\": \"What is the volume of a standard golf ball in cubic centimeters?\"\n-  }},\n-  \"output_var\": \"golf_ball_volume\"\n-}}\n-\n-Step #3: {{\n-  \"description\": \"Convert the volume of a golf ball from cubic centimeters to cubic meters. Calculate the number of golf balls that fit into the Boeing-747 by dividing the internal volume of the Boeing-747 by the volume of a golf ball.\",\n-  \"tool\": \"python_code\",\n-  \"params\": {{\n-      \"code\": \"golf_ball_volume_m3 = golf_ball_volume / 1e6\\nnumber_of_golf_balls = boeing_volume / golf_ball_volume_m3\"\n-  }},\n-  \"output_var\": \"number_of_golf_balls\"\n-}}\n-\n-Step #4: {{\n-  \"description\": \"Provide the final answer\",\n-  \"tool\": \"final_answer\",\n-  \"params\": {{\n-      \"answer\": \"{{number_of_golf_balls}}\"\n-  }},\n-  \"output_var\": \"\"\n-}}\n-------\n-Above example were using tools that might not exist for you.\n-Find below the record of what has been tried so far to solve it. Your goal is to create an updated plan to solve the task.\"\"\"\n-\n-USER_PROMPT_PLAN_UPDATE_STRUCTURED = \"\"\"\n-Here are your inputs:\n-\n-Task:\n-```\n-{task}\n-```\n-\n-Your plan can leverage any of these tools:\n-{tool_descriptions}\n-These tools are Python functions which you can call with code. You also have access to a Python interpreter so you can run Python code.\n-\n-List of facts that you know:\n-```\n-{facts_update}\n-```\n-\n-Now for the given task, create a plan taking into account the above inputs and list of facts.\n-Beware that you have {remaining_steps} steps remaining.\n-After writing the final step of the plan, write the '\\n<end_plan>' tag and stop there. Output the plan only and nothing else.\"\"\"\n-\n-PLAN_UPDATE_FINAL_PLAN_REDACTION = \"\"\"I still need to solve the task I was given:\n-```\n-{task}\n-```\n-\n-Here is my new/updated plan of action to solve the task:\n-```\n-{plan_update}\n-```\"\"\"\n-\n-SUPPORTED_PLAN_TYPES = [\"default\", \"structured\"]\n-\n-PROMPTS_FOR_INITIAL_PLAN = {\n-    \"default\": {\"system\": SYSTEM_PROMPT_PLAN, \"user\": USER_PROMPT_PLAN},\n-    \"structured\": {\"system\": SYSTEM_PROMPT_PLAN_STRUCTURED, \"user\": USER_PROMPT_PLAN_STRUCTURED},\n-}\n-\n-PROMPTS_FOR_PLAN_UPDATE = {\n-    \"default\": {\"system\": SYSTEM_PROMPT_PLAN_UPDATE, \"user\": USER_PROMPT_PLAN_UPDATE},\n-    \"structured\": {\"system\": SYSTEM_PROMPT_PLAN_UPDATE_STRUCTURED, \"user\": USER_PROMPT_PLAN_UPDATE_STRUCTURED},\n-}"
        },
        {
            "sha": "109461ba5f6af75a8a41403a3305d115721792ce",
            "filename": "src/transformers/agents/python_interpreter.py",
            "status": "removed",
            "additions": 0,
            "deletions": 908,
            "changes": 908,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fpython_interpreter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fpython_interpreter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fpython_interpreter.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,908 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import ast\n-import builtins\n-import difflib\n-from collections.abc import Mapping\n-from importlib import import_module\n-from typing import Any, Callable, Dict, List, Optional\n-\n-import numpy as np\n-\n-from ..utils import is_pandas_available\n-\n-\n-if is_pandas_available():\n-    import pandas as pd\n-\n-\n-class InterpreterError(ValueError):\n-    \"\"\"\n-    An error raised when the interpreter cannot evaluate a Python expression, due to syntax error or unsupported\n-    operations.\n-    \"\"\"\n-\n-    pass\n-\n-\n-ERRORS = {\n-    name: getattr(builtins, name)\n-    for name in dir(builtins)\n-    if isinstance(getattr(builtins, name), type) and issubclass(getattr(builtins, name), BaseException)\n-}\n-\n-\n-LIST_SAFE_MODULES = [\n-    \"random\",\n-    \"collections\",\n-    \"math\",\n-    \"time\",\n-    \"queue\",\n-    \"itertools\",\n-    \"re\",\n-    \"stat\",\n-    \"statistics\",\n-    \"unicodedata\",\n-]\n-\n-PRINT_OUTPUTS, MAX_LEN_OUTPUT = \"\", 50000\n-OPERATIONS_COUNT, MAX_OPERATIONS = 0, 10000000\n-\n-\n-class BreakException(Exception):\n-    pass\n-\n-\n-class ContinueException(Exception):\n-    pass\n-\n-\n-class ReturnException(Exception):\n-    def __init__(self, value):\n-        self.value = value\n-\n-\n-def get_iterable(obj):\n-    if isinstance(obj, list):\n-        return obj\n-    elif hasattr(obj, \"__iter__\"):\n-        return list(obj)\n-    else:\n-        raise InterpreterError(\"Object is not iterable\")\n-\n-\n-def evaluate_unaryop(expression, state, static_tools, custom_tools):\n-    operand = evaluate_ast(expression.operand, state, static_tools, custom_tools)\n-    if isinstance(expression.op, ast.USub):\n-        return -operand\n-    elif isinstance(expression.op, ast.UAdd):\n-        return operand\n-    elif isinstance(expression.op, ast.Not):\n-        return not operand\n-    elif isinstance(expression.op, ast.Invert):\n-        return ~operand\n-    else:\n-        raise InterpreterError(f\"Unary operation {expression.op.__class__.__name__} is not supported.\")\n-\n-\n-def evaluate_lambda(lambda_expression, state, static_tools, custom_tools):\n-    args = [arg.arg for arg in lambda_expression.args.args]\n-\n-    def lambda_func(*values):\n-        new_state = state.copy()\n-        for arg, value in zip(args, values):\n-            new_state[arg] = value\n-        return evaluate_ast(lambda_expression.body, new_state, static_tools, custom_tools)\n-\n-    return lambda_func\n-\n-\n-def evaluate_while(while_loop, state, static_tools, custom_tools):\n-    max_iterations = 1000\n-    iterations = 0\n-    while evaluate_ast(while_loop.test, state, static_tools, custom_tools):\n-        for node in while_loop.body:\n-            try:\n-                evaluate_ast(node, state, static_tools, custom_tools)\n-            except BreakException:\n-                return None\n-            except ContinueException:\n-                break\n-        iterations += 1\n-        if iterations > max_iterations:\n-            raise InterpreterError(f\"Maximum number of {max_iterations} iterations in While loop exceeded\")\n-    return None\n-\n-\n-def create_function(func_def, state, static_tools, custom_tools):\n-    def new_func(*args, **kwargs):\n-        func_state = state.copy()\n-        arg_names = [arg.arg for arg in func_def.args.args]\n-        default_values = [evaluate_ast(d, state, static_tools, custom_tools) for d in func_def.args.defaults]\n-\n-        # Apply default values\n-        defaults = dict(zip(arg_names[-len(default_values) :], default_values))\n-\n-        # Set positional arguments\n-        for name, value in zip(arg_names, args):\n-            func_state[name] = value\n-\n-        # # Set keyword arguments\n-        for name, value in kwargs.items():\n-            func_state[name] = value\n-\n-        # Handle variable arguments\n-        if func_def.args.vararg:\n-            vararg_name = func_def.args.vararg.arg\n-            func_state[vararg_name] = args\n-\n-        if func_def.args.kwarg:\n-            kwarg_name = func_def.args.kwarg.arg\n-            func_state[kwarg_name] = kwargs\n-\n-        # Set default values for arguments that were not provided\n-        for name, value in defaults.items():\n-            if name not in func_state:\n-                func_state[name] = value\n-\n-        # Update function state with self and __class__\n-        if func_def.args.args and func_def.args.args[0].arg == \"self\":\n-            if args:\n-                func_state[\"self\"] = args[0]\n-                func_state[\"__class__\"] = args[0].__class__\n-\n-        result = None\n-        try:\n-            for stmt in func_def.body:\n-                result = evaluate_ast(stmt, func_state, static_tools, custom_tools)\n-        except ReturnException as e:\n-            result = e.value\n-        return result\n-\n-    return new_func\n-\n-\n-def create_class(class_name, class_bases, class_body):\n-    class_dict = {}\n-    for key, value in class_body.items():\n-        class_dict[key] = value\n-    return type(class_name, tuple(class_bases), class_dict)\n-\n-\n-def evaluate_function_def(func_def, state, static_tools, custom_tools):\n-    custom_tools[func_def.name] = create_function(func_def, state, static_tools, custom_tools)\n-    return custom_tools[func_def.name]\n-\n-\n-def evaluate_class_def(class_def, state, static_tools, custom_tools):\n-    class_name = class_def.name\n-    bases = [evaluate_ast(base, state, static_tools, custom_tools) for base in class_def.bases]\n-    class_dict = {}\n-\n-    for stmt in class_def.body:\n-        if isinstance(stmt, ast.FunctionDef):\n-            class_dict[stmt.name] = evaluate_function_def(stmt, state, static_tools, custom_tools)\n-        elif isinstance(stmt, ast.Assign):\n-            for target in stmt.targets:\n-                if isinstance(target, ast.Name):\n-                    class_dict[target.id] = evaluate_ast(stmt.value, state, static_tools, custom_tools)\n-                elif isinstance(target, ast.Attribute):\n-                    class_dict[target.attr] = evaluate_ast(stmt.value, state, static_tools, custom_tools)\n-        else:\n-            raise InterpreterError(f\"Unsupported statement in class body: {stmt.__class__.__name__}\")\n-\n-    new_class = type(class_name, tuple(bases), class_dict)\n-    state[class_name] = new_class\n-    return new_class\n-\n-\n-def evaluate_augassign(expression, state, static_tools, custom_tools):\n-    # Helper function to get current value and set new value based on the target type\n-    def get_current_value(target):\n-        if isinstance(target, ast.Name):\n-            return state.get(target.id, 0)\n-        elif isinstance(target, ast.Subscript):\n-            obj = evaluate_ast(target.value, state, static_tools, custom_tools)\n-            key = evaluate_ast(target.slice, state, static_tools, custom_tools)\n-            return obj[key]\n-        elif isinstance(target, ast.Attribute):\n-            obj = evaluate_ast(target.value, state, static_tools, custom_tools)\n-            return getattr(obj, target.attr)\n-        elif isinstance(target, ast.Tuple):\n-            return tuple(get_current_value(elt) for elt in target.elts)\n-        elif isinstance(target, ast.List):\n-            return [get_current_value(elt) for elt in target.elts]\n-        else:\n-            raise InterpreterError(\"AugAssign not supported for {type(target)} targets.\")\n-\n-    current_value = get_current_value(expression.target)\n-    value_to_add = evaluate_ast(expression.value, state, static_tools, custom_tools)\n-\n-    # Determine the operation and apply it\n-    if isinstance(expression.op, ast.Add):\n-        if isinstance(current_value, list):\n-            if not isinstance(value_to_add, list):\n-                raise InterpreterError(f\"Cannot add non-list value {value_to_add} to a list.\")\n-            updated_value = current_value + value_to_add\n-        else:\n-            updated_value = current_value + value_to_add\n-    elif isinstance(expression.op, ast.Sub):\n-        updated_value = current_value - value_to_add\n-    elif isinstance(expression.op, ast.Mult):\n-        updated_value = current_value * value_to_add\n-    elif isinstance(expression.op, ast.Div):\n-        updated_value = current_value / value_to_add\n-    elif isinstance(expression.op, ast.Mod):\n-        updated_value = current_value % value_to_add\n-    elif isinstance(expression.op, ast.Pow):\n-        updated_value = current_value**value_to_add\n-    elif isinstance(expression.op, ast.FloorDiv):\n-        updated_value = current_value // value_to_add\n-    elif isinstance(expression.op, ast.BitAnd):\n-        updated_value = current_value & value_to_add\n-    elif isinstance(expression.op, ast.BitOr):\n-        updated_value = current_value | value_to_add\n-    elif isinstance(expression.op, ast.BitXor):\n-        updated_value = current_value ^ value_to_add\n-    elif isinstance(expression.op, ast.LShift):\n-        updated_value = current_value << value_to_add\n-    elif isinstance(expression.op, ast.RShift):\n-        updated_value = current_value >> value_to_add\n-    else:\n-        raise InterpreterError(f\"Operation {type(expression.op).__name__} is not supported.\")\n-\n-    # Update the state\n-    set_value(expression.target, updated_value, state, static_tools, custom_tools)\n-\n-    return updated_value\n-\n-\n-def evaluate_boolop(node, state, static_tools, custom_tools):\n-    if isinstance(node.op, ast.And):\n-        for value in node.values:\n-            if not evaluate_ast(value, state, static_tools, custom_tools):\n-                return False\n-        return True\n-    elif isinstance(node.op, ast.Or):\n-        for value in node.values:\n-            if evaluate_ast(value, state, static_tools, custom_tools):\n-                return True\n-        return False\n-\n-\n-def evaluate_binop(binop, state, static_tools, custom_tools):\n-    # Recursively evaluate the left and right operands\n-    left_val = evaluate_ast(binop.left, state, static_tools, custom_tools)\n-    right_val = evaluate_ast(binop.right, state, static_tools, custom_tools)\n-\n-    # Determine the operation based on the type of the operator in the BinOp\n-    if isinstance(binop.op, ast.Add):\n-        return left_val + right_val\n-    elif isinstance(binop.op, ast.Sub):\n-        return left_val - right_val\n-    elif isinstance(binop.op, ast.Mult):\n-        return left_val * right_val\n-    elif isinstance(binop.op, ast.Div):\n-        return left_val / right_val\n-    elif isinstance(binop.op, ast.Mod):\n-        return left_val % right_val\n-    elif isinstance(binop.op, ast.Pow):\n-        return left_val**right_val\n-    elif isinstance(binop.op, ast.FloorDiv):\n-        return left_val // right_val\n-    elif isinstance(binop.op, ast.BitAnd):\n-        return left_val & right_val\n-    elif isinstance(binop.op, ast.BitOr):\n-        return left_val | right_val\n-    elif isinstance(binop.op, ast.BitXor):\n-        return left_val ^ right_val\n-    elif isinstance(binop.op, ast.LShift):\n-        return left_val << right_val\n-    elif isinstance(binop.op, ast.RShift):\n-        return left_val >> right_val\n-    else:\n-        raise NotImplementedError(f\"Binary operation {type(binop.op).__name__} is not implemented.\")\n-\n-\n-def evaluate_assign(assign, state, static_tools, custom_tools):\n-    result = evaluate_ast(assign.value, state, static_tools, custom_tools)\n-    if len(assign.targets) == 1:\n-        target = assign.targets[0]\n-        set_value(target, result, state, static_tools, custom_tools)\n-    else:\n-        if len(assign.targets) != len(result):\n-            raise InterpreterError(f\"Assign failed: expected {len(result)} values but got {len(assign.targets)}.\")\n-        expanded_values = []\n-        for tgt in assign.targets:\n-            if isinstance(tgt, ast.Starred):\n-                expanded_values.extend(result)\n-            else:\n-                expanded_values.append(result)\n-        for tgt, val in zip(assign.targets, expanded_values):\n-            set_value(tgt, val, state, static_tools, custom_tools)\n-    return result\n-\n-\n-def set_value(target, value, state, static_tools, custom_tools):\n-    if isinstance(target, ast.Name):\n-        if target.id in static_tools:\n-            raise InterpreterError(f\"Cannot assign to name '{target.id}': doing this would erase the existing tool!\")\n-        state[target.id] = value\n-    elif isinstance(target, ast.Tuple):\n-        if not isinstance(value, tuple):\n-            if hasattr(value, \"__iter__\") and not isinstance(value, (str, bytes)):\n-                value = tuple(value)\n-            else:\n-                raise InterpreterError(\"Cannot unpack non-tuple value\")\n-        if len(target.elts) != len(value):\n-            raise InterpreterError(\"Cannot unpack tuple of wrong size\")\n-        for i, elem in enumerate(target.elts):\n-            set_value(elem, value[i], state, static_tools, custom_tools)\n-    elif isinstance(target, ast.Subscript):\n-        obj = evaluate_ast(target.value, state, static_tools, custom_tools)\n-        key = evaluate_ast(target.slice, state, static_tools, custom_tools)\n-        obj[key] = value\n-    elif isinstance(target, ast.Attribute):\n-        obj = evaluate_ast(target.value, state, static_tools, custom_tools)\n-        setattr(obj, target.attr, value)\n-\n-\n-def evaluate_call(call, state, static_tools, custom_tools):\n-    if not (isinstance(call.func, ast.Attribute) or isinstance(call.func, ast.Name)):\n-        raise InterpreterError(f\"This is not a correct function: {call.func}).\")\n-    if isinstance(call.func, ast.Attribute):\n-        obj = evaluate_ast(call.func.value, state, static_tools, custom_tools)\n-        func_name = call.func.attr\n-        if not hasattr(obj, func_name):\n-            raise InterpreterError(f\"Object {obj} has no attribute {func_name}\")\n-        func = getattr(obj, func_name)\n-\n-    elif isinstance(call.func, ast.Name):\n-        func_name = call.func.id\n-        if func_name in state:\n-            func = state[func_name]\n-        elif func_name in static_tools:\n-            func = static_tools[func_name]\n-        elif func_name in custom_tools:\n-            func = custom_tools[func_name]\n-        elif func_name in ERRORS:\n-            func = ERRORS[func_name]\n-        else:\n-            raise InterpreterError(\n-                f\"It is not permitted to evaluate other functions than the provided tools or functions defined in previous code (tried to execute {call.func.id}).\"\n-            )\n-\n-    args = []\n-    for arg in call.args:\n-        if isinstance(arg, ast.Starred):\n-            args.extend(evaluate_ast(arg.value, state, static_tools, custom_tools))\n-        else:\n-            args.append(evaluate_ast(arg, state, static_tools, custom_tools))\n-\n-    args = []\n-    for arg in call.args:\n-        if isinstance(arg, ast.Starred):\n-            unpacked = evaluate_ast(arg.value, state, static_tools, custom_tools)\n-            if not hasattr(unpacked, \"__iter__\") or isinstance(unpacked, (str, bytes)):\n-                raise InterpreterError(f\"Cannot unpack non-iterable value {unpacked}\")\n-            args.extend(unpacked)\n-        else:\n-            args.append(evaluate_ast(arg, state, static_tools, custom_tools))\n-\n-    kwargs = {keyword.arg: evaluate_ast(keyword.value, state, static_tools, custom_tools) for keyword in call.keywords}\n-\n-    if isinstance(func, type) and len(func.__module__.split(\".\")) > 1:  # Check for user-defined classes\n-        # Instantiate the class using its constructor\n-        obj = func.__new__(func)  # Create a new instance of the class\n-        if hasattr(obj, \"__init__\"):  # Check if the class has an __init__ method\n-            obj.__init__(*args, **kwargs)  # Call the __init__ method correctly\n-        return obj\n-    else:\n-        if func_name == \"super\":\n-            if not args:\n-                if \"__class__\" in state and \"self\" in state:\n-                    return super(state[\"__class__\"], state[\"self\"])\n-                else:\n-                    raise InterpreterError(\"super() needs at least one argument\")\n-            cls = args[0]\n-            if not isinstance(cls, type):\n-                raise InterpreterError(\"super() argument 1 must be type\")\n-            if len(args) == 1:\n-                return super(cls)\n-            elif len(args) == 2:\n-                instance = args[1]\n-                return super(cls, instance)\n-            else:\n-                raise InterpreterError(\"super() takes at most 2 arguments\")\n-        else:\n-            if func_name == \"print\":\n-                output = \" \".join(map(str, args))\n-                global PRINT_OUTPUTS\n-                PRINT_OUTPUTS += output + \"\\n\"\n-                # cap the number of lines\n-                return None\n-            else:  # Assume it's a callable object\n-                output = func(*args, **kwargs)\n-                return output\n-\n-\n-def evaluate_subscript(subscript, state, static_tools, custom_tools):\n-    index = evaluate_ast(subscript.slice, state, static_tools, custom_tools)\n-    value = evaluate_ast(subscript.value, state, static_tools, custom_tools)\n-\n-    if isinstance(value, str) and isinstance(index, str):\n-        raise InterpreterError(\"You're trying to subscript a string with a string index, which is impossible\")\n-    if isinstance(value, pd.core.indexing._LocIndexer):\n-        parent_object = value.obj\n-        return parent_object.loc[index]\n-    if isinstance(value, (pd.DataFrame, pd.Series, np.ndarray)):\n-        return value[index]\n-    elif isinstance(value, pd.core.groupby.generic.DataFrameGroupBy):\n-        return value[index]\n-    elif isinstance(index, slice):\n-        return value[index]\n-    elif isinstance(value, (list, tuple)):\n-        if not (-len(value) <= index < len(value)):\n-            raise InterpreterError(f\"Index {index} out of bounds for list of length {len(value)}\")\n-        return value[int(index)]\n-    elif isinstance(value, str):\n-        if not (-len(value) <= index < len(value)):\n-            raise InterpreterError(f\"Index {index} out of bounds for string of length {len(value)}\")\n-        return value[index]\n-    elif index in value:\n-        return value[index]\n-    elif isinstance(index, str) and isinstance(value, Mapping):\n-        close_matches = difflib.get_close_matches(index, list(value.keys()))\n-        if len(close_matches) > 0:\n-            return value[close_matches[0]]\n-    raise InterpreterError(f\"Could not index {value} with '{index}'.\")\n-\n-\n-def evaluate_name(name, state, static_tools, custom_tools):\n-    if name.id in state:\n-        return state[name.id]\n-    elif name.id in static_tools:\n-        return static_tools[name.id]\n-    elif name.id in ERRORS:\n-        return ERRORS[name.id]\n-    close_matches = difflib.get_close_matches(name.id, list(state.keys()))\n-    if len(close_matches) > 0:\n-        return state[close_matches[0]]\n-    raise InterpreterError(f\"The variable `{name.id}` is not defined.\")\n-\n-\n-def evaluate_condition(condition, state, static_tools, custom_tools):\n-    left = evaluate_ast(condition.left, state, static_tools, custom_tools)\n-    comparators = [evaluate_ast(c, state, static_tools, custom_tools) for c in condition.comparators]\n-    ops = [type(op) for op in condition.ops]\n-\n-    result = True\n-    current_left = left\n-\n-    for op, comparator in zip(ops, comparators):\n-        if op == ast.Eq:\n-            current_result = current_left == comparator\n-        elif op == ast.NotEq:\n-            current_result = current_left != comparator\n-        elif op == ast.Lt:\n-            current_result = current_left < comparator\n-        elif op == ast.LtE:\n-            current_result = current_left <= comparator\n-        elif op == ast.Gt:\n-            current_result = current_left > comparator\n-        elif op == ast.GtE:\n-            current_result = current_left >= comparator\n-        elif op == ast.Is:\n-            current_result = current_left is comparator\n-        elif op == ast.IsNot:\n-            current_result = current_left is not comparator\n-        elif op == ast.In:\n-            current_result = current_left in comparator\n-        elif op == ast.NotIn:\n-            current_result = current_left not in comparator\n-        else:\n-            raise InterpreterError(f\"Operator not supported: {op}\")\n-\n-        result = result & current_result\n-        current_left = comparator\n-\n-        if isinstance(result, bool) and not result:\n-            break\n-\n-    return result if isinstance(result, (bool, pd.Series)) else result.all()\n-\n-\n-def evaluate_if(if_statement, state, static_tools, custom_tools):\n-    result = None\n-    test_result = evaluate_ast(if_statement.test, state, static_tools, custom_tools)\n-    if test_result:\n-        for line in if_statement.body:\n-            line_result = evaluate_ast(line, state, static_tools, custom_tools)\n-            if line_result is not None:\n-                result = line_result\n-    else:\n-        for line in if_statement.orelse:\n-            line_result = evaluate_ast(line, state, static_tools, custom_tools)\n-            if line_result is not None:\n-                result = line_result\n-    return result\n-\n-\n-def evaluate_for(for_loop, state, static_tools, custom_tools):\n-    result = None\n-    iterator = evaluate_ast(for_loop.iter, state, static_tools, custom_tools)\n-    for counter in iterator:\n-        set_value(for_loop.target, counter, state, static_tools, custom_tools)\n-        for node in for_loop.body:\n-            try:\n-                line_result = evaluate_ast(node, state, static_tools, custom_tools)\n-                if line_result is not None:\n-                    result = line_result\n-            except BreakException:\n-                break\n-            except ContinueException:\n-                continue\n-        else:\n-            continue\n-        break\n-    return result\n-\n-\n-def evaluate_listcomp(listcomp, state, static_tools, custom_tools):\n-    def inner_evaluate(generators, index, current_state):\n-        if index >= len(generators):\n-            return [evaluate_ast(listcomp.elt, current_state, static_tools, custom_tools)]\n-        generator = generators[index]\n-        iter_value = evaluate_ast(generator.iter, current_state, static_tools, custom_tools)\n-        result = []\n-        for value in iter_value:\n-            new_state = current_state.copy()\n-            if isinstance(generator.target, ast.Tuple):\n-                for idx, elem in enumerate(generator.target.elts):\n-                    new_state[elem.id] = value[idx]\n-            else:\n-                new_state[generator.target.id] = value\n-            if all(evaluate_ast(if_clause, new_state, static_tools, custom_tools) for if_clause in generator.ifs):\n-                result.extend(inner_evaluate(generators, index + 1, new_state))\n-        return result\n-\n-    return inner_evaluate(listcomp.generators, 0, state)\n-\n-\n-def evaluate_try(try_node, state, static_tools, custom_tools):\n-    try:\n-        for stmt in try_node.body:\n-            evaluate_ast(stmt, state, static_tools, custom_tools)\n-    except Exception as e:\n-        matched = False\n-        for handler in try_node.handlers:\n-            if handler.type is None or isinstance(e, evaluate_ast(handler.type, state, static_tools, custom_tools)):\n-                matched = True\n-                if handler.name:\n-                    state[handler.name] = e\n-                for stmt in handler.body:\n-                    evaluate_ast(stmt, state, static_tools, custom_tools)\n-                break\n-        if not matched:\n-            raise e\n-    else:\n-        if try_node.orelse:\n-            for stmt in try_node.orelse:\n-                evaluate_ast(stmt, state, static_tools, custom_tools)\n-    finally:\n-        if try_node.finalbody:\n-            for stmt in try_node.finalbody:\n-                evaluate_ast(stmt, state, static_tools, custom_tools)\n-\n-\n-def evaluate_raise(raise_node, state, static_tools, custom_tools):\n-    if raise_node.exc is not None:\n-        exc = evaluate_ast(raise_node.exc, state, static_tools, custom_tools)\n-    else:\n-        exc = None\n-    if raise_node.cause is not None:\n-        cause = evaluate_ast(raise_node.cause, state, static_tools, custom_tools)\n-    else:\n-        cause = None\n-    if exc is not None:\n-        if cause is not None:\n-            raise exc from cause\n-        else:\n-            raise exc\n-    else:\n-        raise InterpreterError(\"Re-raise is not supported without an active exception\")\n-\n-\n-def evaluate_assert(assert_node, state, static_tools, custom_tools):\n-    test_result = evaluate_ast(assert_node.test, state, static_tools, custom_tools)\n-    if not test_result:\n-        if assert_node.msg:\n-            msg = evaluate_ast(assert_node.msg, state, static_tools, custom_tools)\n-            raise AssertionError(msg)\n-        else:\n-            # Include the failing condition in the assertion message\n-            test_code = ast.unparse(assert_node.test)\n-            raise AssertionError(f\"Assertion failed: {test_code}\")\n-\n-\n-def evaluate_with(with_node, state, static_tools, custom_tools):\n-    contexts = []\n-    for item in with_node.items:\n-        context_expr = evaluate_ast(item.context_expr, state, static_tools, custom_tools)\n-        if item.optional_vars:\n-            state[item.optional_vars.id] = context_expr.__enter__()\n-            contexts.append(state[item.optional_vars.id])\n-        else:\n-            context_var = context_expr.__enter__()\n-            contexts.append(context_var)\n-\n-    try:\n-        for stmt in with_node.body:\n-            evaluate_ast(stmt, state, static_tools, custom_tools)\n-    except Exception as e:\n-        for context in reversed(contexts):\n-            context.__exit__(type(e), e, e.__traceback__)\n-        raise\n-    else:\n-        for context in reversed(contexts):\n-            context.__exit__(None, None, None)\n-\n-\n-def import_modules(expression, state, authorized_imports):\n-    def check_module_authorized(module_name):\n-        module_path = module_name.split(\".\")\n-        module_subpaths = [\".\".join(module_path[:i]) for i in range(1, len(module_path) + 1)]\n-        return any(subpath in authorized_imports for subpath in module_subpaths)\n-\n-    if isinstance(expression, ast.Import):\n-        for alias in expression.names:\n-            if check_module_authorized(alias.name):\n-                module = import_module(alias.name)\n-                state[alias.asname or alias.name] = module\n-            else:\n-                raise InterpreterError(\n-                    f\"Import of {alias.name} is not allowed. Authorized imports are: {str(authorized_imports)}\"\n-                )\n-        return None\n-    elif isinstance(expression, ast.ImportFrom):\n-        if check_module_authorized(expression.module):\n-            module = __import__(expression.module, fromlist=[alias.name for alias in expression.names])\n-            for alias in expression.names:\n-                state[alias.asname or alias.name] = getattr(module, alias.name)\n-        else:\n-            raise InterpreterError(f\"Import from {expression.module} is not allowed.\")\n-        return None\n-\n-\n-def evaluate_dictcomp(dictcomp, state, static_tools, custom_tools):\n-    result = {}\n-    for gen in dictcomp.generators:\n-        iter_value = evaluate_ast(gen.iter, state, static_tools, custom_tools)\n-        for value in iter_value:\n-            new_state = state.copy()\n-            set_value(gen.target, value, new_state, static_tools, custom_tools)\n-            if all(evaluate_ast(if_clause, new_state, static_tools, custom_tools) for if_clause in gen.ifs):\n-                key = evaluate_ast(dictcomp.key, new_state, static_tools, custom_tools)\n-                val = evaluate_ast(dictcomp.value, new_state, static_tools, custom_tools)\n-                result[key] = val\n-    return result\n-\n-\n-def evaluate_ast(\n-    expression: ast.AST,\n-    state: Dict[str, Any],\n-    static_tools: Dict[str, Callable],\n-    custom_tools: Dict[str, Callable],\n-    authorized_imports: List[str] = LIST_SAFE_MODULES,\n-):\n-    \"\"\"\n-    Evaluate an abstract syntax tree using the content of the variables stored in a state and only evaluating a given\n-    set of functions.\n-\n-    This function will recurse through the nodes of the tree provided.\n-\n-    Args:\n-        expression (`ast.AST`):\n-            The code to evaluate, as an abstract syntax tree.\n-        state (`Dict[str, Any]`):\n-            A dictionary mapping variable names to values. The `state` is updated if need be when the evaluation\n-            encounters assignments.\n-        static_tools (`Dict[str, Callable]`):\n-            Functions that may be called during the evaluation. Trying to change one of these static_tools will raise an error.\n-        custom_tools (`Dict[str, Callable]`):\n-            Functions that may be called during the evaluation. These static_tools can be overwritten.\n-        authorized_imports (`List[str]`):\n-            The list of modules that can be imported by the code. By default, only a few safe modules are allowed.\n-            Add more at your own risk!\n-    \"\"\"\n-    global OPERATIONS_COUNT\n-    if OPERATIONS_COUNT >= MAX_OPERATIONS:\n-        raise InterpreterError(\n-            f\"Reached the max number of operations of {MAX_OPERATIONS}. Maybe there is an infinite loop somewhere in the code, or you're just asking too many calculations.\"\n-        )\n-    OPERATIONS_COUNT += 1\n-    if isinstance(expression, ast.Assign):\n-        # Assignment -> we evaluate the assignment which should update the state\n-        # We return the variable assigned as it may be used to determine the final result.\n-        return evaluate_assign(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.AugAssign):\n-        return evaluate_augassign(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Call):\n-        # Function call -> we return the value of the function call\n-        return evaluate_call(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Constant):\n-        # Constant -> just return the value\n-        return expression.value\n-    elif isinstance(expression, ast.Tuple):\n-        return tuple(evaluate_ast(elt, state, static_tools, custom_tools) for elt in expression.elts)\n-    elif isinstance(expression, (ast.ListComp, ast.GeneratorExp)):\n-        return evaluate_listcomp(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.UnaryOp):\n-        return evaluate_unaryop(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Starred):\n-        return evaluate_ast(expression.value, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.BoolOp):\n-        # Boolean operation -> evaluate the operation\n-        return evaluate_boolop(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Break):\n-        raise BreakException()\n-    elif isinstance(expression, ast.Continue):\n-        raise ContinueException()\n-    elif isinstance(expression, ast.BinOp):\n-        # Binary operation -> execute operation\n-        return evaluate_binop(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Compare):\n-        # Comparison -> evaluate the comparison\n-        return evaluate_condition(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Lambda):\n-        return evaluate_lambda(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.FunctionDef):\n-        return evaluate_function_def(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Dict):\n-        # Dict -> evaluate all keys and values\n-        keys = [evaluate_ast(k, state, static_tools, custom_tools) for k in expression.keys]\n-        values = [evaluate_ast(v, state, static_tools, custom_tools) for v in expression.values]\n-        return dict(zip(keys, values))\n-    elif isinstance(expression, ast.Expr):\n-        # Expression -> evaluate the content\n-        return evaluate_ast(expression.value, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.For):\n-        # For loop -> execute the loop\n-        return evaluate_for(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.FormattedValue):\n-        # Formatted value (part of f-string) -> evaluate the content and return\n-        return evaluate_ast(expression.value, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.If):\n-        # If -> execute the right branch\n-        return evaluate_if(expression, state, static_tools, custom_tools)\n-    elif hasattr(ast, \"Index\") and isinstance(expression, ast.Index):\n-        return evaluate_ast(expression.value, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.JoinedStr):\n-        return \"\".join([str(evaluate_ast(v, state, static_tools, custom_tools)) for v in expression.values])\n-    elif isinstance(expression, ast.List):\n-        # List -> evaluate all elements\n-        return [evaluate_ast(elt, state, static_tools, custom_tools) for elt in expression.elts]\n-    elif isinstance(expression, ast.Name):\n-        # Name -> pick up the value in the state\n-        return evaluate_name(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Subscript):\n-        # Subscript -> return the value of the indexing\n-        return evaluate_subscript(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.IfExp):\n-        test_val = evaluate_ast(expression.test, state, static_tools, custom_tools)\n-        if test_val:\n-            return evaluate_ast(expression.body, state, static_tools, custom_tools)\n-        else:\n-            return evaluate_ast(expression.orelse, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Attribute):\n-        value = evaluate_ast(expression.value, state, static_tools, custom_tools)\n-        return getattr(value, expression.attr)\n-    elif isinstance(expression, ast.Slice):\n-        return slice(\n-            evaluate_ast(expression.lower, state, static_tools, custom_tools)\n-            if expression.lower is not None\n-            else None,\n-            evaluate_ast(expression.upper, state, static_tools, custom_tools)\n-            if expression.upper is not None\n-            else None,\n-            evaluate_ast(expression.step, state, static_tools, custom_tools) if expression.step is not None else None,\n-        )\n-    elif isinstance(expression, ast.DictComp):\n-        return evaluate_dictcomp(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.While):\n-        return evaluate_while(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, (ast.Import, ast.ImportFrom)):\n-        return import_modules(expression, state, authorized_imports)\n-    elif isinstance(expression, ast.ClassDef):\n-        return evaluate_class_def(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Try):\n-        return evaluate_try(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Raise):\n-        return evaluate_raise(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Assert):\n-        return evaluate_assert(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.With):\n-        return evaluate_with(expression, state, static_tools, custom_tools)\n-    elif isinstance(expression, ast.Set):\n-        return {evaluate_ast(elt, state, static_tools, custom_tools) for elt in expression.elts}\n-    elif isinstance(expression, ast.Return):\n-        raise ReturnException(\n-            evaluate_ast(expression.value, state, static_tools, custom_tools) if expression.value else None\n-        )\n-    else:\n-        # For now we refuse anything else. Let's add things as we need them.\n-        raise InterpreterError(f\"{expression.__class__.__name__} is not supported.\")\n-\n-\n-def truncate_print_outputs(print_outputs: str, max_len_outputs: int = MAX_LEN_OUTPUT) -> str:\n-    if len(print_outputs) < max_len_outputs:\n-        return print_outputs\n-    else:\n-        return f\"Print outputs:\\n{print_outputs[:max_len_outputs]}\\n_Print outputs have been truncated over the limit of {max_len_outputs} characters._\\n\"\n-\n-\n-def evaluate_python_code(\n-    code: str,\n-    static_tools: Optional[Dict[str, Callable]] = None,\n-    custom_tools: Optional[Dict[str, Callable]] = None,\n-    state: Optional[Dict[str, Any]] = None,\n-    authorized_imports: List[str] = LIST_SAFE_MODULES,\n-):\n-    \"\"\"\n-    Evaluate a python expression using the content of the variables stored in a state and only evaluating a given set\n-    of functions.\n-\n-    This function will recurse through the nodes of the tree provided.\n-\n-    Args:\n-        code (`str`):\n-            The code to evaluate.\n-        static_tools (`Dict[str, Callable]`):\n-            The functions that may be called during the evaluation.\n-            These tools cannot be overwritten in the code: any assignment to their name will raise an error.\n-        custom_tools (`Dict[str, Callable]`):\n-            The functions that may be called during the evaluation.\n-            These tools can be overwritten in the code: any assignment to their name will overwrite them.\n-        state (`Dict[str, Any]`):\n-            A dictionary mapping variable names to values. The `state` should contain the initial inputs but will be\n-            updated by this function to contain all variables as they are evaluated.\n-            The print outputs will be stored in the state under the key 'print_outputs'.\n-    \"\"\"\n-    try:\n-        expression = ast.parse(code)\n-    except SyntaxError as e:\n-        raise SyntaxError(f\"The code generated by the agent is not valid.\\n{e}\")\n-    if state is None:\n-        state = {}\n-    if static_tools is None:\n-        static_tools = {}\n-    if custom_tools is None:\n-        custom_tools = {}\n-    result = None\n-    global PRINT_OUTPUTS\n-    PRINT_OUTPUTS = \"\"\n-    global OPERATIONS_COUNT\n-    OPERATIONS_COUNT = 0\n-    try:\n-        for node in expression.body:\n-            result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)\n-        state[\"print_outputs\"] = truncate_print_outputs(PRINT_OUTPUTS, max_len_outputs=MAX_LEN_OUTPUT)\n-        return result\n-    except InterpreterError as e:\n-        msg = truncate_print_outputs(PRINT_OUTPUTS, max_len_outputs=MAX_LEN_OUTPUT)\n-        msg += f\"EXECUTION FAILED:\\nEvaluation stopped at line '{ast.get_source_segment(code, node)}' because of the following error:\\n{e}\"\n-        raise InterpreterError(msg)"
        },
        {
            "sha": "1c2c33913c97fb06f33c0d6f6946aeb6308efbe5",
            "filename": "src/transformers/agents/search.py",
            "status": "removed",
            "additions": 0,
            "deletions": 77,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fsearch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fsearch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fsearch.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,77 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import re\n-\n-import requests\n-from requests.exceptions import RequestException\n-\n-from .tools import Tool\n-\n-\n-class DuckDuckGoSearchTool(Tool):\n-    name = \"web_search\"\n-    description = \"\"\"Perform a web search based on your query (think a Google search) then returns the top search results as a list of dict elements.\n-    Each result has keys 'title', 'href' and 'body'.\"\"\"\n-    inputs = {\"query\": {\"type\": \"string\", \"description\": \"The search query to perform.\"}}\n-    output_type = \"any\"\n-\n-    def forward(self, query: str) -> str:\n-        try:\n-            from duckduckgo_search import DDGS\n-        except ImportError:\n-            raise ImportError(\n-                \"You must install package `duckduckgo_search` to run this tool: for instance run `pip install duckduckgo-search`.\"\n-            )\n-        results = DDGS().text(query, max_results=7)\n-        return results\n-\n-\n-class VisitWebpageTool(Tool):\n-    name = \"visit_webpage\"\n-    description = \"Visits a webpage at the given url and returns its content as a markdown string.\"\n-    inputs = {\n-        \"url\": {\n-            \"type\": \"string\",\n-            \"description\": \"The url of the webpage to visit.\",\n-        }\n-    }\n-    output_type = \"string\"\n-\n-    def forward(self, url: str) -> str:\n-        try:\n-            from markdownify import markdownify\n-        except ImportError:\n-            raise ImportError(\n-                \"You must install package `markdownify` to run this tool: for instance run `pip install markdownify`.\"\n-            )\n-        try:\n-            # Send a GET request to the URL\n-            response = requests.get(url)\n-            response.raise_for_status()  # Raise an exception for bad status codes\n-\n-            # Convert the HTML content to Markdown\n-            markdown_content = markdownify(response.text).strip()\n-\n-            # Remove multiple line breaks\n-            markdown_content = re.sub(r\"\\n{3,}\", \"\\n\\n\", markdown_content)\n-\n-            return markdown_content\n-\n-        except RequestException as e:\n-            return f\"Error fetching the webpage: {str(e)}\"\n-        except Exception as e:\n-            return f\"An unexpected error occurred: {str(e)}\""
        },
        {
            "sha": "8061651a086479475a7096541b9552f0409d36f5",
            "filename": "src/transformers/agents/speech_to_text.py",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fspeech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Fspeech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fspeech_to_text.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,39 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from ..models.whisper import WhisperForConditionalGeneration, WhisperProcessor\n-from .tools import PipelineTool\n-\n-\n-class SpeechToTextTool(PipelineTool):\n-    default_checkpoint = \"distil-whisper/distil-large-v3\"\n-    description = \"This is a tool that transcribes an audio into text. It returns the transcribed text.\"\n-    name = \"transcriber\"\n-    pre_processor_class = WhisperProcessor\n-    model_class = WhisperForConditionalGeneration\n-\n-    inputs = {\"audio\": {\"type\": \"audio\", \"description\": \"The audio to transcribe\"}}\n-    output_type = \"string\"\n-\n-    def encode(self, audio):\n-        return self.pre_processor(audio, return_tensors=\"pt\")\n-\n-    def forward(self, inputs):\n-        return self.model.generate(inputs[\"input_features\"])\n-\n-    def decode(self, outputs):\n-        return self.pre_processor.batch_decode(outputs, skip_special_tokens=True)[0]"
        },
        {
            "sha": "65f47ad6ddb50e29b220529ca45428ebf994c94b",
            "filename": "src/transformers/agents/text_to_speech.py",
            "status": "removed",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Ftext_to_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Ftext_to_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftext_to_speech.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,67 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import torch\n-\n-from ..models.speecht5 import SpeechT5ForTextToSpeech, SpeechT5HifiGan, SpeechT5Processor\n-from ..utils import is_datasets_available\n-from .tools import PipelineTool\n-\n-\n-if is_datasets_available():\n-    from datasets import load_dataset\n-\n-\n-class TextToSpeechTool(PipelineTool):\n-    default_checkpoint = \"microsoft/speecht5_tts\"\n-    description = (\n-        \"This is a tool that reads an English text out loud. It returns a waveform object containing the sound.\"\n-    )\n-    name = \"text_to_speech\"\n-    pre_processor_class = SpeechT5Processor\n-    model_class = SpeechT5ForTextToSpeech\n-    post_processor_class = SpeechT5HifiGan\n-\n-    inputs = {\"text\": {\"type\": \"string\", \"description\": \"The text to read out loud (in English)\"}}\n-    output_type = \"audio\"\n-\n-    def setup(self):\n-        if self.post_processor is None:\n-            self.post_processor = \"microsoft/speecht5_hifigan\"\n-        super().setup()\n-\n-    def encode(self, text, speaker_embeddings=None):\n-        inputs = self.pre_processor(text=text, return_tensors=\"pt\", truncation=True)\n-\n-        if speaker_embeddings is None:\n-            if not is_datasets_available():\n-                raise ImportError(\"Datasets needs to be installed if not passing speaker embeddings.\")\n-\n-            embeddings_dataset = load_dataset(\n-                \"Matthijs/cmu-arctic-xvectors\", split=\"validation\", trust_remote_code=True\n-            )\n-            speaker_embeddings = torch.tensor(embeddings_dataset[7305][\"xvector\"]).unsqueeze(0)\n-\n-        return {\"input_ids\": inputs[\"input_ids\"], \"speaker_embeddings\": speaker_embeddings}\n-\n-    def forward(self, inputs):\n-        with torch.no_grad():\n-            return self.model.generate_speech(**inputs)\n-\n-    def decode(self, outputs):\n-        with torch.no_grad():\n-            return self.post_processor(outputs).detach().cpu()"
        },
        {
            "sha": "f7ead9f2ebe72bf1b0576ff902925f7dec928729",
            "filename": "src/transformers/agents/tools.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1012,
            "changes": 1012,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Ftools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Ftools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftools.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,1012 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import ast\n-import base64\n-import importlib\n-import inspect\n-import io\n-import json\n-import os\n-import tempfile\n-from functools import lru_cache, wraps\n-from pathlib import Path\n-from typing import Any, Callable, Dict, List, Optional, Union\n-\n-from huggingface_hub import create_repo, get_collection, hf_hub_download, metadata_update, upload_folder\n-from huggingface_hub.utils import RepositoryNotFoundError, build_hf_headers, get_session\n-from huggingface_hub.utils._deprecation import _deprecate_method\n-from packaging import version\n-\n-from ..dynamic_module_utils import (\n-    custom_object_save,\n-    get_class_from_dynamic_module,\n-    get_imports,\n-)\n-from ..models.auto import AutoProcessor\n-from ..utils import (\n-    CONFIG_NAME,\n-    TypeHintParsingException,\n-    cached_file,\n-    get_json_schema,\n-    is_accelerate_available,\n-    is_torch_available,\n-    is_vision_available,\n-    logging,\n-)\n-from .agent_types import ImageType, handle_agent_inputs, handle_agent_outputs\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_accelerate_available():\n-    from accelerate import PartialState\n-    from accelerate.utils import send_to_device\n-\n-\n-TOOL_CONFIG_FILE = \"tool_config.json\"\n-\n-\n-def get_repo_type(repo_id, repo_type=None, **hub_kwargs):\n-    if repo_type is not None:\n-        return repo_type\n-    try:\n-        hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type=\"space\", **hub_kwargs)\n-        return \"space\"\n-    except RepositoryNotFoundError:\n-        try:\n-            hf_hub_download(repo_id, TOOL_CONFIG_FILE, repo_type=\"model\", **hub_kwargs)\n-            return \"model\"\n-        except RepositoryNotFoundError:\n-            raise EnvironmentError(f\"`{repo_id}` does not seem to be a valid repo identifier on the Hub.\")\n-        except Exception:\n-            return \"model\"\n-    except Exception:\n-        return \"space\"\n-\n-\n-# docstyle-ignore\n-APP_FILE_TEMPLATE = \"\"\"from transformers import launch_gradio_demo\n-from {module_name} import {class_name}\n-\n-launch_gradio_demo({class_name})\n-\"\"\"\n-\n-\n-def validate_after_init(cls, do_validate_forward: bool = True):\n-    original_init = cls.__init__\n-\n-    @wraps(original_init)\n-    def new_init(self, *args, **kwargs):\n-        original_init(self, *args, **kwargs)\n-        if not isinstance(self, PipelineTool):\n-            self.validate_arguments(do_validate_forward=do_validate_forward)\n-\n-    cls.__init__ = new_init\n-    return cls\n-\n-\n-CONVERSION_DICT = {\"str\": \"string\", \"int\": \"integer\", \"float\": \"number\"}\n-\n-\n-class Tool:\n-    \"\"\"\n-    A base class for the functions used by the agent. Subclass this and implement the `__call__` method as well as the\n-    following class attributes:\n-\n-    - **description** (`str`) -- A short description of what your tool does, the inputs it expects and the output(s) it\n-      will return. For instance 'This is a tool that downloads a file from a `url`. It takes the `url` as input, and\n-      returns the text contained in the file'.\n-    - **name** (`str`) -- A performative name that will be used for your tool in the prompt to the agent. For instance\n-      `\"text-classifier\"` or `\"image_generator\"`.\n-    - **inputs** (`Dict[str, Dict[str, Union[str, type]]]`) -- The dict of modalities expected for the inputs.\n-      It has one `type`key and a `description`key.\n-      This is used by `launch_gradio_demo` or to make a nice space from your tool, and also can be used in the generated\n-      description for your tool.\n-    - **output_type** (`type`) -- The type of the tool output. This is used by `launch_gradio_demo`\n-      or to make a nice space from your tool, and also can be used in the generated description for your tool.\n-\n-    You can also override the method [`~Tool.setup`] if your tool as an expensive operation to perform before being\n-    usable (such as loading a model). [`~Tool.setup`] will be called the first time you use your tool, but not at\n-    instantiation.\n-    \"\"\"\n-\n-    name: str\n-    description: str\n-    inputs: Dict[str, Dict[str, Union[str, type]]]\n-    output_type: type\n-\n-    @_deprecate_method(\n-        version=\"4.51.0\",\n-        message=\"Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\",\n-    )\n-    def __init__(self, *args, **kwargs):\n-        self.is_initialized = False\n-\n-    @_deprecate_method(\n-        version=\"4.51.0\",\n-        message=\"Switch to smolagents instead, with the same functionalities and similar API (https://huggingface.co/docs/smolagents/index)\",\n-    )\n-    def __init_subclass__(cls, **kwargs):\n-        super().__init_subclass__(**kwargs)\n-        validate_after_init(cls, do_validate_forward=False)\n-\n-    def validate_arguments(self, do_validate_forward: bool = True):\n-        required_attributes = {\n-            \"description\": str,\n-            \"name\": str,\n-            \"inputs\": dict,\n-            \"output_type\": str,\n-        }\n-        authorized_types = [\"string\", \"integer\", \"number\", \"image\", \"audio\", \"any\", \"boolean\"]\n-\n-        for attr, expected_type in required_attributes.items():\n-            attr_value = getattr(self, attr, None)\n-            if attr_value is None:\n-                raise TypeError(f\"You must set an attribute {attr}.\")\n-            if not isinstance(attr_value, expected_type):\n-                raise TypeError(\n-                    f\"Attribute {attr} should have type {expected_type.__name__}, got {type(attr_value)} instead.\"\n-                )\n-        for input_name, input_content in self.inputs.items():\n-            assert isinstance(input_content, dict), f\"Input '{input_name}' should be a dictionary.\"\n-            assert \"type\" in input_content and \"description\" in input_content, (\n-                f\"Input '{input_name}' should have keys 'type' and 'description', has only {list(input_content.keys())}.\"\n-            )\n-            if input_content[\"type\"] not in authorized_types:\n-                raise Exception(\n-                    f\"Input '{input_name}': type '{input_content['type']}' is not an authorized value, should be one of {authorized_types}.\"\n-                )\n-\n-        assert getattr(self, \"output_type\", None) in authorized_types\n-        if do_validate_forward:\n-            if not isinstance(self, PipelineTool):\n-                signature = inspect.signature(self.forward)\n-                if not set(signature.parameters.keys()) == set(self.inputs.keys()):\n-                    raise Exception(\n-                        \"Tool's 'forward' method should take 'self' as its first argument, then its next arguments should match the keys of tool attribute 'inputs'.\"\n-                    )\n-\n-    def forward(self, *args, **kwargs):\n-        return NotImplemented(\"Write this method in your subclass of `Tool`.\")\n-\n-    def __call__(self, *args, **kwargs):\n-        args, kwargs = handle_agent_inputs(*args, **kwargs)\n-        outputs = self.forward(*args, **kwargs)\n-        return handle_agent_outputs(outputs, self.output_type)\n-\n-    def setup(self):\n-        \"\"\"\n-        Overwrite this method here for any operation that is expensive and needs to be executed before you start using\n-        your tool. Such as loading a big model.\n-        \"\"\"\n-        self.is_initialized = True\n-\n-    def save(self, output_dir):\n-        \"\"\"\n-        Saves the relevant code files for your tool so it can be pushed to the Hub. This will copy the code of your\n-        tool in `output_dir` as well as autogenerate:\n-\n-        - a config file named `tool_config.json`\n-        - an `app.py` file so that your tool can be converted to a space\n-        - a `requirements.txt` containing the names of the module used by your tool (as detected when inspecting its\n-          code)\n-\n-        You should only use this method to save tools that are defined in a separate module (not `__main__`).\n-\n-        Args:\n-            output_dir (`str`): The folder in which you want to save your tool.\n-        \"\"\"\n-        os.makedirs(output_dir, exist_ok=True)\n-        # Save module file\n-        if self.__module__ == \"__main__\":\n-            raise ValueError(\n-                f\"We can't save the code defining {self} in {output_dir} as it's been defined in __main__. You \"\n-                \"have to put this code in a separate module so we can include it in the saved folder.\"\n-            )\n-        module_files = custom_object_save(self, output_dir)\n-\n-        module_name = self.__class__.__module__\n-        last_module = module_name.split(\".\")[-1]\n-        full_name = f\"{last_module}.{self.__class__.__name__}\"\n-\n-        # Save config file\n-        config_file = os.path.join(output_dir, \"tool_config.json\")\n-        if os.path.isfile(config_file):\n-            with open(config_file, \"r\", encoding=\"utf-8\") as f:\n-                tool_config = json.load(f)\n-        else:\n-            tool_config = {}\n-\n-        tool_config = {\n-            \"tool_class\": full_name,\n-            \"description\": self.description,\n-            \"name\": self.name,\n-            \"inputs\": self.inputs,\n-            \"output_type\": str(self.output_type),\n-        }\n-        with open(config_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(tool_config, indent=2, sort_keys=True) + \"\\n\")\n-\n-        # Save app file\n-        app_file = os.path.join(output_dir, \"app.py\")\n-        with open(app_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(APP_FILE_TEMPLATE.format(module_name=last_module, class_name=self.__class__.__name__))\n-\n-        # Save requirements file\n-        requirements_file = os.path.join(output_dir, \"requirements.txt\")\n-        imports = []\n-        for module in module_files:\n-            imports.extend(get_imports(module))\n-        imports = list(set(imports))\n-        with open(requirements_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(\"\\n\".join(imports) + \"\\n\")\n-\n-    @classmethod\n-    def from_hub(\n-        cls,\n-        repo_id: str,\n-        token: Optional[str] = None,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Loads a tool defined on the Hub.\n-\n-        <Tip warning={true}>\n-\n-        Loading a tool from the Hub means that you'll download the tool and execute it locally.\n-        ALWAYS inspect the tool you're downloading before loading it within your runtime, as you would do when\n-        installing a package using pip/npm/apt.\n-\n-        </Tip>\n-\n-        Args:\n-            repo_id (`str`):\n-                The name of the repo on the Hub where your tool is defined.\n-            token (`str`, *optional*):\n-                The token to identify you on hf.co. If unset, will use the token generated when running\n-                `huggingface-cli login` (stored in `~/.huggingface`).\n-            kwargs (additional keyword arguments, *optional*):\n-                Additional keyword arguments that will be split in two: all arguments relevant to the Hub (such as\n-                `cache_dir`, `revision`, `subfolder`) will be used when downloading the files for your tool, and the\n-                others will be passed along to its init.\n-        \"\"\"\n-        hub_kwargs_names = [\n-            \"cache_dir\",\n-            \"force_download\",\n-            \"resume_download\",\n-            \"proxies\",\n-            \"revision\",\n-            \"repo_type\",\n-            \"subfolder\",\n-            \"local_files_only\",\n-        ]\n-        hub_kwargs = {k: v for k, v in kwargs.items() if k in hub_kwargs_names}\n-\n-        # Try to get the tool config first.\n-        hub_kwargs[\"repo_type\"] = get_repo_type(repo_id, **hub_kwargs)\n-        resolved_config_file = cached_file(\n-            repo_id,\n-            TOOL_CONFIG_FILE,\n-            token=token,\n-            **hub_kwargs,\n-            _raise_exceptions_for_gated_repo=False,\n-            _raise_exceptions_for_missing_entries=False,\n-            _raise_exceptions_for_connection_errors=False,\n-        )\n-        is_tool_config = resolved_config_file is not None\n-        if resolved_config_file is None:\n-            resolved_config_file = cached_file(\n-                repo_id,\n-                CONFIG_NAME,\n-                token=token,\n-                **hub_kwargs,\n-                _raise_exceptions_for_gated_repo=False,\n-                _raise_exceptions_for_missing_entries=False,\n-                _raise_exceptions_for_connection_errors=False,\n-            )\n-        if resolved_config_file is None:\n-            raise EnvironmentError(\n-                f\"{repo_id} does not appear to provide a valid configuration in `tool_config.json` or `config.json`.\"\n-            )\n-\n-        with open(resolved_config_file, encoding=\"utf-8\") as reader:\n-            config = json.load(reader)\n-\n-        if not is_tool_config:\n-            if \"custom_tool\" not in config:\n-                raise EnvironmentError(\n-                    f\"{repo_id} does not provide a mapping to custom tools in its configuration `config.json`.\"\n-                )\n-            custom_tool = config[\"custom_tool\"]\n-        else:\n-            custom_tool = config\n-\n-        tool_class = custom_tool[\"tool_class\"]\n-        tool_class = get_class_from_dynamic_module(tool_class, repo_id, token=token, **hub_kwargs)\n-\n-        if len(tool_class.name) == 0:\n-            tool_class.name = custom_tool[\"name\"]\n-        if tool_class.name != custom_tool[\"name\"]:\n-            logger.warning(\n-                f\"{tool_class.__name__} implements a different name in its configuration and class. Using the tool \"\n-                \"configuration name.\"\n-            )\n-            tool_class.name = custom_tool[\"name\"]\n-\n-        if len(tool_class.description) == 0:\n-            tool_class.description = custom_tool[\"description\"]\n-        if tool_class.description != custom_tool[\"description\"]:\n-            logger.warning(\n-                f\"{tool_class.__name__} implements a different description in its configuration and class. Using the \"\n-                \"tool configuration description.\"\n-            )\n-            tool_class.description = custom_tool[\"description\"]\n-\n-        if tool_class.inputs != custom_tool[\"inputs\"]:\n-            tool_class.inputs = custom_tool[\"inputs\"]\n-        if tool_class.output_type != custom_tool[\"output_type\"]:\n-            tool_class.output_type = custom_tool[\"output_type\"]\n-\n-        if not isinstance(tool_class.inputs, dict):\n-            tool_class.inputs = ast.literal_eval(tool_class.inputs)\n-\n-        return tool_class(**kwargs)\n-\n-    def push_to_hub(\n-        self,\n-        repo_id: str,\n-        commit_message: str = \"Upload tool\",\n-        private: Optional[bool] = None,\n-        token: Optional[Union[bool, str]] = None,\n-        create_pr: bool = False,\n-    ) -> str:\n-        \"\"\"\n-        Upload the tool to the Hub.\n-\n-        For this method to work properly, your tool must have been defined in a separate module (not `__main__`).\n-        For instance:\n-        ```\n-        from my_tool_module import MyTool\n-        my_tool = MyTool()\n-        my_tool.push_to_hub(\"my-username/my-space\")\n-        ```\n-\n-        Parameters:\n-            repo_id (`str`):\n-                The name of the repository you want to push your tool to. It should contain your organization name when\n-                pushing to a given organization.\n-            commit_message (`str`, *optional*, defaults to `\"Upload tool\"`):\n-                Message to commit while pushing.\n-            private (`bool`, *optional*):\n-                Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n-            token (`bool` or `str`, *optional*):\n-                The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated\n-                when running `huggingface-cli login` (stored in `~/.huggingface`).\n-            create_pr (`bool`, *optional*, defaults to `False`):\n-                Whether or not to create a PR with the uploaded files or directly commit.\n-        \"\"\"\n-        repo_url = create_repo(\n-            repo_id=repo_id,\n-            token=token,\n-            private=private,\n-            exist_ok=True,\n-            repo_type=\"space\",\n-            space_sdk=\"gradio\",\n-        )\n-        repo_id = repo_url.repo_id\n-        metadata_update(repo_id, {\"tags\": [\"tool\"]}, repo_type=\"space\")\n-\n-        with tempfile.TemporaryDirectory() as work_dir:\n-            # Save all files.\n-            self.save(work_dir)\n-            logger.info(f\"Uploading the following files to {repo_id}: {','.join(os.listdir(work_dir))}\")\n-            return upload_folder(\n-                repo_id=repo_id,\n-                commit_message=commit_message,\n-                folder_path=work_dir,\n-                token=token,\n-                create_pr=create_pr,\n-                repo_type=\"space\",\n-            )\n-\n-    @staticmethod\n-    def from_space(\n-        space_id: str, name: str, description: str, api_name: Optional[str] = None, token: Optional[str] = None\n-    ):\n-        \"\"\"\n-        Creates a [`Tool`] from a Space given its id on the Hub.\n-\n-        Args:\n-            space_id (`str`):\n-                The id of the Space on the Hub.\n-            name (`str`):\n-                The name of the tool.\n-            description (`str`):\n-                The description of the tool.\n-            api_name (`str`, *optional*):\n-                The specific api_name to use, if the space has several tabs. If not precised, will default to the first available api.\n-            token (`str`, *optional*):\n-                Add your token to access private spaces or increase your GPU quotas.\n-        Returns:\n-            [`Tool`]:\n-                The Space, as a tool.\n-\n-        Examples:\n-        ```\n-        image_generator = Tool.from_space(\n-            space_id=\"black-forest-labs/FLUX.1-schnell\",\n-            name=\"image-generator\",\n-            description=\"Generate an image from a prompt\"\n-        )\n-        image = image_generator(\"Generate an image of a cool surfer in Tahiti\")\n-        ```\n-        ```\n-        face_swapper = Tool.from_space(\n-            \"tuan2308/face-swap\",\n-            \"face_swapper\",\n-            \"Tool that puts the face shown on the first image on the second image. You can give it paths to images.\",\n-        )\n-        image = face_swapper('./aymeric.jpeg', './ruth.jpg')\n-        ```\n-        \"\"\"\n-        from gradio_client import Client, handle_file\n-        from gradio_client.utils import is_http_url_like\n-\n-        class SpaceToolWrapper(Tool):\n-            def __init__(\n-                self,\n-                space_id: str,\n-                name: str,\n-                description: str,\n-                api_name: Optional[str] = None,\n-                token: Optional[str] = None,\n-            ):\n-                self.client = Client(space_id, hf_token=token)\n-                self.name = name\n-                self.description = description\n-                space_description = self.client.view_api(return_format=\"dict\", print_info=False)[\"named_endpoints\"]\n-\n-                # If api_name is not defined, take the first of the available APIs for this space\n-                if api_name is None:\n-                    api_name = list(space_description.keys())[0]\n-                    logger.warning(\n-                        f\"Since `api_name` was not defined, it was automatically set to the first available API: `{api_name}`.\"\n-                    )\n-                self.api_name = api_name\n-\n-                try:\n-                    space_description_api = space_description[api_name]\n-                except KeyError:\n-                    raise KeyError(f\"Could not find specified {api_name=} among available api names.\")\n-\n-                self.inputs = {}\n-                for parameter in space_description_api[\"parameters\"]:\n-                    if not parameter[\"parameter_has_default\"]:\n-                        parameter_type = parameter[\"type\"][\"type\"]\n-                        if parameter_type == \"object\":\n-                            parameter_type = \"any\"\n-                        self.inputs[parameter[\"parameter_name\"]] = {\n-                            \"type\": parameter_type,\n-                            \"description\": parameter[\"python_type\"][\"description\"],\n-                        }\n-                output_component = space_description_api[\"returns\"][0][\"component\"]\n-                if output_component == \"Image\":\n-                    self.output_type = \"image\"\n-                elif output_component == \"Audio\":\n-                    self.output_type = \"audio\"\n-                else:\n-                    self.output_type = \"any\"\n-\n-            def sanitize_argument_for_prediction(self, arg):\n-                if isinstance(arg, ImageType):\n-                    temp_file = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n-                    arg.save(temp_file.name)\n-                    arg = temp_file.name\n-                if (isinstance(arg, (str, Path)) and Path(arg).exists() and Path(arg).is_file()) or is_http_url_like(\n-                    arg\n-                ):\n-                    arg = handle_file(arg)\n-                return arg\n-\n-            def forward(self, *args, **kwargs):\n-                # Preprocess args and kwargs:\n-                args = list(args)\n-                for i, arg in enumerate(args):\n-                    args[i] = self.sanitize_argument_for_prediction(arg)\n-                for arg_name, arg in kwargs.items():\n-                    kwargs[arg_name] = self.sanitize_argument_for_prediction(arg)\n-\n-                output = self.client.predict(*args, api_name=self.api_name, **kwargs)\n-                if isinstance(output, tuple) or isinstance(output, list):\n-                    return output[\n-                        0\n-                    ]  # Sometime the space also returns the generation seed, in which case the result is at index 0\n-                return output\n-\n-        return SpaceToolWrapper(space_id, name, description, api_name=api_name, token=token)\n-\n-    @staticmethod\n-    def from_gradio(gradio_tool):\n-        \"\"\"\n-        Creates a [`Tool`] from a gradio tool.\n-        \"\"\"\n-        import inspect\n-\n-        class GradioToolWrapper(Tool):\n-            def __init__(self, _gradio_tool):\n-                self.name = _gradio_tool.name\n-                self.description = _gradio_tool.description\n-                self.output_type = \"string\"\n-                self._gradio_tool = _gradio_tool\n-                func_args = list(inspect.signature(_gradio_tool.run).parameters.items())\n-                self.inputs = {\n-                    key: {\"type\": CONVERSION_DICT[value.annotation], \"description\": \"\"} for key, value in func_args\n-                }\n-                self.forward = self._gradio_tool.run\n-\n-        return GradioToolWrapper(gradio_tool)\n-\n-    @staticmethod\n-    def from_langchain(langchain_tool):\n-        \"\"\"\n-        Creates a [`Tool`] from a langchain tool.\n-        \"\"\"\n-\n-        class LangChainToolWrapper(Tool):\n-            def __init__(self, _langchain_tool):\n-                self.name = _langchain_tool.name.lower()\n-                self.description = _langchain_tool.description\n-                self.inputs = _langchain_tool.args.copy()\n-                for input_content in self.inputs.values():\n-                    if \"title\" in input_content:\n-                        input_content.pop(\"title\")\n-                    input_content[\"description\"] = \"\"\n-                self.output_type = \"string\"\n-                self.langchain_tool = _langchain_tool\n-\n-            def forward(self, *args, **kwargs):\n-                tool_input = kwargs.copy()\n-                for index, argument in enumerate(args):\n-                    if index < len(self.inputs):\n-                        input_key = next(iter(self.inputs))\n-                        tool_input[input_key] = argument\n-                return self.langchain_tool.run(tool_input)\n-\n-        return LangChainToolWrapper(langchain_tool)\n-\n-\n-DEFAULT_TOOL_DESCRIPTION_TEMPLATE = \"\"\"\n-- {{ tool.name }}: {{ tool.description }}\n-    Takes inputs: {{tool.inputs}}\n-    Returns an output of type: {{tool.output_type}}\n-\"\"\"\n-\n-\n-def get_tool_description_with_args(tool: Tool, description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE) -> str:\n-    compiled_template = compile_jinja_template(description_template)\n-    rendered = compiled_template.render(\n-        tool=tool,\n-    )\n-    return rendered\n-\n-\n-@lru_cache\n-def compile_jinja_template(template):\n-    try:\n-        import jinja2\n-        from jinja2.exceptions import TemplateError\n-        from jinja2.sandbox import ImmutableSandboxedEnvironment\n-    except ImportError:\n-        raise ImportError(\"template requires jinja2 to be installed.\")\n-\n-    if version.parse(jinja2.__version__) < version.parse(\"3.1.0\"):\n-        raise ImportError(f\"template requires jinja2>=3.1.0 to be installed. Your version is {jinja2.__version__}.\")\n-\n-    def raise_exception(message):\n-        raise TemplateError(message)\n-\n-    jinja_env = ImmutableSandboxedEnvironment(trim_blocks=True, lstrip_blocks=True)\n-    jinja_env.globals[\"raise_exception\"] = raise_exception\n-    return jinja_env.from_string(template)\n-\n-\n-class PipelineTool(Tool):\n-    \"\"\"\n-    A [`Tool`] tailored towards Transformer models. On top of the class attributes of the base class [`Tool`], you will\n-    need to specify:\n-\n-    - **model_class** (`type`) -- The class to use to load the model in this tool.\n-    - **default_checkpoint** (`str`) -- The default checkpoint that should be used when the user doesn't specify one.\n-    - **pre_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the\n-      pre-processor\n-    - **post_processor_class** (`type`, *optional*, defaults to [`AutoProcessor`]) -- The class to use to load the\n-      post-processor (when different from the pre-processor).\n-\n-    Args:\n-        model (`str` or [`PreTrainedModel`], *optional*):\n-            The name of the checkpoint to use for the model, or the instantiated model. If unset, will default to the\n-            value of the class attribute `default_checkpoint`.\n-        pre_processor (`str` or `Any`, *optional*):\n-            The name of the checkpoint to use for the pre-processor, or the instantiated pre-processor (can be a\n-            tokenizer, an image processor, a feature extractor or a processor). Will default to the value of `model` if\n-            unset.\n-        post_processor (`str` or `Any`, *optional*):\n-            The name of the checkpoint to use for the post-processor, or the instantiated pre-processor (can be a\n-            tokenizer, an image processor, a feature extractor or a processor). Will default to the `pre_processor` if\n-            unset.\n-        device (`int`, `str` or `torch.device`, *optional*):\n-            The device on which to execute the model. Will default to any accelerator available (GPU, MPS etc...), the\n-            CPU otherwise.\n-        device_map (`str` or `dict`, *optional*):\n-            If passed along, will be used to instantiate the model.\n-        model_kwargs (`dict`, *optional*):\n-            Any keyword argument to send to the model instantiation.\n-        token (`str`, *optional*):\n-            The token to use as HTTP bearer authorization for remote files. If unset, will use the token generated when\n-            running `huggingface-cli login` (stored in `~/.huggingface`).\n-        hub_kwargs (additional keyword arguments, *optional*):\n-            Any additional keyword argument to send to the methods that will load the data from the Hub.\n-    \"\"\"\n-\n-    pre_processor_class = AutoProcessor\n-    model_class = None\n-    post_processor_class = AutoProcessor\n-    default_checkpoint = None\n-    description = \"This is a pipeline tool\"\n-    name = \"pipeline\"\n-    inputs = {\"prompt\": str}\n-    output_type = str\n-\n-    def __init__(\n-        self,\n-        model=None,\n-        pre_processor=None,\n-        post_processor=None,\n-        device=None,\n-        device_map=None,\n-        model_kwargs=None,\n-        token=None,\n-        **hub_kwargs,\n-    ):\n-        if not is_torch_available():\n-            raise ImportError(\"Please install torch in order to use this tool.\")\n-\n-        if not is_accelerate_available():\n-            raise ImportError(\"Please install accelerate in order to use this tool.\")\n-\n-        if model is None:\n-            if self.default_checkpoint is None:\n-                raise ValueError(\"This tool does not implement a default checkpoint, you need to pass one.\")\n-            model = self.default_checkpoint\n-        if pre_processor is None:\n-            pre_processor = model\n-\n-        self.model = model\n-        self.pre_processor = pre_processor\n-        self.post_processor = post_processor\n-        self.device = device\n-        self.device_map = device_map\n-        self.model_kwargs = {} if model_kwargs is None else model_kwargs\n-        if device_map is not None:\n-            self.model_kwargs[\"device_map\"] = device_map\n-        self.hub_kwargs = hub_kwargs\n-        self.hub_kwargs[\"token\"] = token\n-\n-        super().__init__()\n-\n-    def setup(self):\n-        \"\"\"\n-        Instantiates the `pre_processor`, `model` and `post_processor` if necessary.\n-        \"\"\"\n-        if isinstance(self.pre_processor, str):\n-            self.pre_processor = self.pre_processor_class.from_pretrained(self.pre_processor, **self.hub_kwargs)\n-\n-        if isinstance(self.model, str):\n-            self.model = self.model_class.from_pretrained(self.model, **self.model_kwargs, **self.hub_kwargs)\n-\n-        if self.post_processor is None:\n-            self.post_processor = self.pre_processor\n-        elif isinstance(self.post_processor, str):\n-            self.post_processor = self.post_processor_class.from_pretrained(self.post_processor, **self.hub_kwargs)\n-\n-        if self.device is None:\n-            if self.device_map is not None:\n-                self.device = list(self.model.hf_device_map.values())[0]\n-            else:\n-                self.device = PartialState().default_device\n-\n-        if self.device_map is None:\n-            self.model.to(self.device)\n-\n-        super().setup()\n-\n-    def encode(self, raw_inputs):\n-        \"\"\"\n-        Uses the `pre_processor` to prepare the inputs for the `model`.\n-        \"\"\"\n-        return self.pre_processor(raw_inputs)\n-\n-    def forward(self, inputs):\n-        \"\"\"\n-        Sends the inputs through the `model`.\n-        \"\"\"\n-        with torch.no_grad():\n-            return self.model(**inputs)\n-\n-    def decode(self, outputs):\n-        \"\"\"\n-        Uses the `post_processor` to decode the model output.\n-        \"\"\"\n-        return self.post_processor(outputs)\n-\n-    def __call__(self, *args, **kwargs):\n-        args, kwargs = handle_agent_inputs(*args, **kwargs)\n-\n-        if not self.is_initialized:\n-            self.setup()\n-\n-        encoded_inputs = self.encode(*args, **kwargs)\n-\n-        tensor_inputs = {k: v for k, v in encoded_inputs.items() if isinstance(v, torch.Tensor)}\n-        non_tensor_inputs = {k: v for k, v in encoded_inputs.items() if not isinstance(v, torch.Tensor)}\n-\n-        encoded_inputs = send_to_device(tensor_inputs, self.device)\n-        outputs = self.forward({**encoded_inputs, **non_tensor_inputs})\n-        outputs = send_to_device(outputs, \"cpu\")\n-        decoded_outputs = self.decode(outputs)\n-\n-        return handle_agent_outputs(decoded_outputs, self.output_type)\n-\n-\n-def launch_gradio_demo(tool_class: Tool):\n-    \"\"\"\n-    Launches a gradio demo for a tool. The corresponding tool class needs to properly implement the class attributes\n-    `inputs` and `output_type`.\n-\n-    Args:\n-        tool_class (`type`): The class of the tool for which to launch the demo.\n-    \"\"\"\n-    try:\n-        import gradio as gr\n-    except ImportError:\n-        raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n-\n-    tool = tool_class()\n-\n-    def fn(*args, **kwargs):\n-        return tool(*args, **kwargs)\n-\n-    TYPE_TO_COMPONENT_CLASS_MAPPING = {\n-        \"image\": gr.Image,\n-        \"audio\": gr.Audio,\n-        \"string\": gr.Textbox,\n-        \"integer\": gr.Textbox,\n-        \"number\": gr.Textbox,\n-    }\n-\n-    gradio_inputs = []\n-    for input_name, input_details in tool_class.inputs.items():\n-        input_gradio_component_class = TYPE_TO_COMPONENT_CLASS_MAPPING[input_details[\"type\"]]\n-        new_component = input_gradio_component_class(label=input_name)\n-        gradio_inputs.append(new_component)\n-\n-    output_gradio_componentclass = TYPE_TO_COMPONENT_CLASS_MAPPING[tool_class.output_type]\n-    gradio_output = output_gradio_componentclass(label=input_name)\n-\n-    gr.Interface(\n-        fn=fn,\n-        inputs=gradio_inputs,\n-        outputs=gradio_output,\n-        title=tool_class.__name__,\n-        article=tool.description,\n-    ).launch()\n-\n-\n-TOOL_MAPPING = {\n-    \"document_question_answering\": \"DocumentQuestionAnsweringTool\",\n-    \"image_question_answering\": \"ImageQuestionAnsweringTool\",\n-    \"speech_to_text\": \"SpeechToTextTool\",\n-    \"text_to_speech\": \"TextToSpeechTool\",\n-    \"translation\": \"TranslationTool\",\n-    \"python_interpreter\": \"PythonInterpreterTool\",\n-    \"web_search\": \"DuckDuckGoSearchTool\",\n-}\n-\n-\n-def load_tool(task_or_repo_id, model_repo_id=None, token=None, **kwargs):\n-    \"\"\"\n-    Main function to quickly load a tool, be it on the Hub or in the Transformers library.\n-\n-    <Tip warning={true}>\n-\n-    Loading a tool means that you'll download the tool and execute it locally.\n-    ALWAYS inspect the tool you're downloading before loading it within your runtime, as you would do when\n-    installing a package using pip/npm/apt.\n-\n-    </Tip>\n-\n-    Args:\n-        task_or_repo_id (`str`):\n-            The task for which to load the tool or a repo ID of a tool on the Hub. Tasks implemented in Transformers\n-            are:\n-\n-            - `\"document_question_answering\"`\n-            - `\"image_question_answering\"`\n-            - `\"speech_to_text\"`\n-            - `\"text_to_speech\"`\n-            - `\"translation\"`\n-\n-        model_repo_id (`str`, *optional*):\n-            Use this argument to use a different model than the default one for the tool you selected.\n-        token (`str`, *optional*):\n-            The token to identify you on hf.co. If unset, will use the token generated when running `huggingface-cli\n-            login` (stored in `~/.huggingface`).\n-        kwargs (additional keyword arguments, *optional*):\n-            Additional keyword arguments that will be split in two: all arguments relevant to the Hub (such as\n-            `cache_dir`, `revision`, `subfolder`) will be used when downloading the files for your tool, and the others\n-            will be passed along to its init.\n-    \"\"\"\n-    if task_or_repo_id in TOOL_MAPPING:\n-        tool_class_name = TOOL_MAPPING[task_or_repo_id]\n-        main_module = importlib.import_module(\"transformers\")\n-        tools_module = main_module.agents\n-        tool_class = getattr(tools_module, tool_class_name)\n-        return tool_class(model_repo_id, token=token, **kwargs)\n-    else:\n-        logger.warning_once(\n-            f\"You're loading a tool from the Hub from {model_repo_id}. Please make sure this is a source that you \"\n-            f\"trust as the code within that tool will be executed on your machine. Always verify the code of \"\n-            f\"the tools that you load. We recommend specifying a `revision` to ensure you're loading the \"\n-            f\"code that you have checked.\"\n-        )\n-        return Tool.from_hub(task_or_repo_id, model_repo_id=model_repo_id, token=token, **kwargs)\n-\n-\n-def add_description(description):\n-    \"\"\"\n-    A decorator that adds a description to a function.\n-    \"\"\"\n-\n-    def inner(func):\n-        func.description = description\n-        func.name = func.__name__\n-        return func\n-\n-    return inner\n-\n-\n-## Will move to the Hub\n-class EndpointClient:\n-    def __init__(self, endpoint_url: str, token: Optional[str] = None):\n-        self.headers = {\n-            **build_hf_headers(token=token),\n-            \"Content-Type\": \"application/json\",\n-        }\n-        self.endpoint_url = endpoint_url\n-\n-    @staticmethod\n-    def encode_image(image):\n-        _bytes = io.BytesIO()\n-        image.save(_bytes, format=\"PNG\")\n-        b64 = base64.b64encode(_bytes.getvalue())\n-        return b64.decode(\"utf-8\")\n-\n-    @staticmethod\n-    def decode_image(raw_image):\n-        if not is_vision_available():\n-            raise ImportError(\n-                \"This tool returned an image but Pillow is not installed. Please install it (`pip install Pillow`).\"\n-            )\n-\n-        from PIL import Image\n-\n-        b64 = base64.b64decode(raw_image)\n-        _bytes = io.BytesIO(b64)\n-        return Image.open(_bytes)\n-\n-    def __call__(\n-        self,\n-        inputs: Optional[Union[str, Dict, List[str], List[List[str]]]] = None,\n-        params: Optional[Dict] = None,\n-        data: Optional[bytes] = None,\n-        output_image: bool = False,\n-    ) -> Any:\n-        # Build payload\n-        payload = {}\n-        if inputs:\n-            payload[\"inputs\"] = inputs\n-        if params:\n-            payload[\"parameters\"] = params\n-\n-        # Make API call\n-        response = get_session().post(self.endpoint_url, headers=self.headers, json=payload, data=data)\n-\n-        # By default, parse the response for the user.\n-        if output_image:\n-            return self.decode_image(response.content)\n-        else:\n-            return response.json()\n-\n-\n-class ToolCollection:\n-    \"\"\"\n-    Tool collections enable loading all Spaces from a collection in order to be added to the agent's toolbox.\n-\n-    > [!NOTE]\n-    > Only Spaces will be fetched, so you can feel free to add models and datasets to your collection if you'd\n-    > like for this collection to showcase them.\n-\n-    Args:\n-        collection_slug (str):\n-            The collection slug referencing the collection.\n-        token (str, *optional*):\n-            The authentication token if the collection is private.\n-\n-    Example:\n-\n-    ```py\n-    >>> from transformers import ToolCollection, ReactCodeAgent\n-\n-    >>> image_tool_collection = ToolCollection(collection_slug=\"huggingface-tools/diffusion-tools-6630bb19a942c2306a2cdb6f\")\n-    >>> agent = ReactCodeAgent(tools=[*image_tool_collection.tools], add_base_tools=True)\n-\n-    >>> agent.run(\"Please draw me a picture of rivers and lakes.\")\n-    ```\n-    \"\"\"\n-\n-    def __init__(self, collection_slug: str, token: Optional[str] = None):\n-        self._collection = get_collection(collection_slug, token=token)\n-        self._hub_repo_ids = {item.item_id for item in self._collection.items if item.item_type == \"space\"}\n-        self.tools = {Tool.from_hub(repo_id) for repo_id in self._hub_repo_ids}\n-\n-\n-def tool(tool_function: Callable) -> Tool:\n-    \"\"\"\n-    Converts a function into an instance of a Tool subclass.\n-\n-    Args:\n-        tool_function: Your function. Should have type hints for each input and a type hint for the output.\n-        Should also have a docstring description including an 'Args:' part where each argument is described.\n-    \"\"\"\n-    parameters = get_json_schema(tool_function)[\"function\"]\n-    if \"return\" not in parameters:\n-        raise TypeHintParsingException(\"Tool return type not found: make sure your function has a return type hint!\")\n-    class_name = f\"{parameters['name'].capitalize()}Tool\"\n-\n-    class SpecificTool(Tool):\n-        name = parameters[\"name\"]\n-        description = parameters[\"description\"]\n-        inputs = parameters[\"parameters\"][\"properties\"]\n-        output_type = parameters[\"return\"][\"type\"]\n-\n-        @wraps(tool_function)\n-        def forward(self, *args, **kwargs):\n-            return tool_function(*args, **kwargs)\n-\n-    original_signature = inspect.signature(tool_function)\n-    new_parameters = [inspect.Parameter(\"self\", inspect.Parameter.POSITIONAL_OR_KEYWORD)] + list(\n-        original_signature.parameters.values()\n-    )\n-    new_signature = original_signature.replace(parameters=new_parameters)\n-    SpecificTool.forward.__signature__ = new_signature\n-\n-    SpecificTool.__name__ = class_name\n-    return SpecificTool()"
        },
        {
            "sha": "6875aac4bc64f8fa5609c759d788866567591a8c",
            "filename": "src/transformers/agents/translation.py",
            "status": "removed",
            "additions": 0,
            "deletions": 279,
            "changes": 279,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Ftranslation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/src%2Ftransformers%2Fagents%2Ftranslation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftranslation.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,279 +0,0 @@\n-#!/usr/bin/env python\n-# coding=utf-8\n-\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from ..models.auto import AutoModelForSeq2SeqLM, AutoTokenizer\n-from .tools import PipelineTool\n-\n-\n-LANGUAGE_CODES = {\n-    \"Acehnese Arabic\": \"ace_Arab\",\n-    \"Acehnese Latin\": \"ace_Latn\",\n-    \"Mesopotamian Arabic\": \"acm_Arab\",\n-    \"Ta'izzi-Adeni Arabic\": \"acq_Arab\",\n-    \"Tunisian Arabic\": \"aeb_Arab\",\n-    \"Afrikaans\": \"afr_Latn\",\n-    \"South Levantine Arabic\": \"ajp_Arab\",\n-    \"Akan\": \"aka_Latn\",\n-    \"Amharic\": \"amh_Ethi\",\n-    \"North Levantine Arabic\": \"apc_Arab\",\n-    \"Modern Standard Arabic\": \"arb_Arab\",\n-    \"Modern Standard Arabic Romanized\": \"arb_Latn\",\n-    \"Najdi Arabic\": \"ars_Arab\",\n-    \"Moroccan Arabic\": \"ary_Arab\",\n-    \"Egyptian Arabic\": \"arz_Arab\",\n-    \"Assamese\": \"asm_Beng\",\n-    \"Asturian\": \"ast_Latn\",\n-    \"Awadhi\": \"awa_Deva\",\n-    \"Central Aymara\": \"ayr_Latn\",\n-    \"South Azerbaijani\": \"azb_Arab\",\n-    \"North Azerbaijani\": \"azj_Latn\",\n-    \"Bashkir\": \"bak_Cyrl\",\n-    \"Bambara\": \"bam_Latn\",\n-    \"Balinese\": \"ban_Latn\",\n-    \"Belarusian\": \"bel_Cyrl\",\n-    \"Bemba\": \"bem_Latn\",\n-    \"Bengali\": \"ben_Beng\",\n-    \"Bhojpuri\": \"bho_Deva\",\n-    \"Banjar Arabic\": \"bjn_Arab\",\n-    \"Banjar Latin\": \"bjn_Latn\",\n-    \"Standard Tibetan\": \"bod_Tibt\",\n-    \"Bosnian\": \"bos_Latn\",\n-    \"Buginese\": \"bug_Latn\",\n-    \"Bulgarian\": \"bul_Cyrl\",\n-    \"Catalan\": \"cat_Latn\",\n-    \"Cebuano\": \"ceb_Latn\",\n-    \"Czech\": \"ces_Latn\",\n-    \"Chokwe\": \"cjk_Latn\",\n-    \"Central Kurdish\": \"ckb_Arab\",\n-    \"Crimean Tatar\": \"crh_Latn\",\n-    \"Welsh\": \"cym_Latn\",\n-    \"Danish\": \"dan_Latn\",\n-    \"German\": \"deu_Latn\",\n-    \"Southwestern Dinka\": \"dik_Latn\",\n-    \"Dyula\": \"dyu_Latn\",\n-    \"Dzongkha\": \"dzo_Tibt\",\n-    \"Greek\": \"ell_Grek\",\n-    \"English\": \"eng_Latn\",\n-    \"Esperanto\": \"epo_Latn\",\n-    \"Estonian\": \"est_Latn\",\n-    \"Basque\": \"eus_Latn\",\n-    \"Ewe\": \"ewe_Latn\",\n-    \"Faroese\": \"fao_Latn\",\n-    \"Fijian\": \"fij_Latn\",\n-    \"Finnish\": \"fin_Latn\",\n-    \"Fon\": \"fon_Latn\",\n-    \"French\": \"fra_Latn\",\n-    \"Friulian\": \"fur_Latn\",\n-    \"Nigerian Fulfulde\": \"fuv_Latn\",\n-    \"Scottish Gaelic\": \"gla_Latn\",\n-    \"Irish\": \"gle_Latn\",\n-    \"Galician\": \"glg_Latn\",\n-    \"Guarani\": \"grn_Latn\",\n-    \"Gujarati\": \"guj_Gujr\",\n-    \"Haitian Creole\": \"hat_Latn\",\n-    \"Hausa\": \"hau_Latn\",\n-    \"Hebrew\": \"heb_Hebr\",\n-    \"Hindi\": \"hin_Deva\",\n-    \"Chhattisgarhi\": \"hne_Deva\",\n-    \"Croatian\": \"hrv_Latn\",\n-    \"Hungarian\": \"hun_Latn\",\n-    \"Armenian\": \"hye_Armn\",\n-    \"Igbo\": \"ibo_Latn\",\n-    \"Ilocano\": \"ilo_Latn\",\n-    \"Indonesian\": \"ind_Latn\",\n-    \"Icelandic\": \"isl_Latn\",\n-    \"Italian\": \"ita_Latn\",\n-    \"Javanese\": \"jav_Latn\",\n-    \"Japanese\": \"jpn_Jpan\",\n-    \"Kabyle\": \"kab_Latn\",\n-    \"Jingpho\": \"kac_Latn\",\n-    \"Kamba\": \"kam_Latn\",\n-    \"Kannada\": \"kan_Knda\",\n-    \"Kashmiri Arabic\": \"kas_Arab\",\n-    \"Kashmiri Devanagari\": \"kas_Deva\",\n-    \"Georgian\": \"kat_Geor\",\n-    \"Central Kanuri Arabic\": \"knc_Arab\",\n-    \"Central Kanuri Latin\": \"knc_Latn\",\n-    \"Kazakh\": \"kaz_Cyrl\",\n-    \"KabiyÃ¨\": \"kbp_Latn\",\n-    \"Kabuverdianu\": \"kea_Latn\",\n-    \"Khmer\": \"khm_Khmr\",\n-    \"Kikuyu\": \"kik_Latn\",\n-    \"Kinyarwanda\": \"kin_Latn\",\n-    \"Kyrgyz\": \"kir_Cyrl\",\n-    \"Kimbundu\": \"kmb_Latn\",\n-    \"Northern Kurdish\": \"kmr_Latn\",\n-    \"Kikongo\": \"kon_Latn\",\n-    \"Korean\": \"kor_Hang\",\n-    \"Lao\": \"lao_Laoo\",\n-    \"Ligurian\": \"lij_Latn\",\n-    \"Limburgish\": \"lim_Latn\",\n-    \"Lingala\": \"lin_Latn\",\n-    \"Lithuanian\": \"lit_Latn\",\n-    \"Lombard\": \"lmo_Latn\",\n-    \"Latgalian\": \"ltg_Latn\",\n-    \"Luxembourgish\": \"ltz_Latn\",\n-    \"Luba-Kasai\": \"lua_Latn\",\n-    \"Ganda\": \"lug_Latn\",\n-    \"Luo\": \"luo_Latn\",\n-    \"Mizo\": \"lus_Latn\",\n-    \"Standard Latvian\": \"lvs_Latn\",\n-    \"Magahi\": \"mag_Deva\",\n-    \"Maithili\": \"mai_Deva\",\n-    \"Malayalam\": \"mal_Mlym\",\n-    \"Marathi\": \"mar_Deva\",\n-    \"Minangkabau Arabic \": \"min_Arab\",\n-    \"Minangkabau Latin\": \"min_Latn\",\n-    \"Macedonian\": \"mkd_Cyrl\",\n-    \"Plateau Malagasy\": \"plt_Latn\",\n-    \"Maltese\": \"mlt_Latn\",\n-    \"Meitei Bengali\": \"mni_Beng\",\n-    \"Halh Mongolian\": \"khk_Cyrl\",\n-    \"Mossi\": \"mos_Latn\",\n-    \"Maori\": \"mri_Latn\",\n-    \"Burmese\": \"mya_Mymr\",\n-    \"Dutch\": \"nld_Latn\",\n-    \"Norwegian Nynorsk\": \"nno_Latn\",\n-    \"Norwegian BokmÃ¥l\": \"nob_Latn\",\n-    \"Nepali\": \"npi_Deva\",\n-    \"Northern Sotho\": \"nso_Latn\",\n-    \"Nuer\": \"nus_Latn\",\n-    \"Nyanja\": \"nya_Latn\",\n-    \"Occitan\": \"oci_Latn\",\n-    \"West Central Oromo\": \"gaz_Latn\",\n-    \"Odia\": \"ory_Orya\",\n-    \"Pangasinan\": \"pag_Latn\",\n-    \"Eastern Panjabi\": \"pan_Guru\",\n-    \"Papiamento\": \"pap_Latn\",\n-    \"Western Persian\": \"pes_Arab\",\n-    \"Polish\": \"pol_Latn\",\n-    \"Portuguese\": \"por_Latn\",\n-    \"Dari\": \"prs_Arab\",\n-    \"Southern Pashto\": \"pbt_Arab\",\n-    \"Ayacucho Quechua\": \"quy_Latn\",\n-    \"Romanian\": \"ron_Latn\",\n-    \"Rundi\": \"run_Latn\",\n-    \"Russian\": \"rus_Cyrl\",\n-    \"Sango\": \"sag_Latn\",\n-    \"Sanskrit\": \"san_Deva\",\n-    \"Santali\": \"sat_Olck\",\n-    \"Sicilian\": \"scn_Latn\",\n-    \"Shan\": \"shn_Mymr\",\n-    \"Sinhala\": \"sin_Sinh\",\n-    \"Slovak\": \"slk_Latn\",\n-    \"Slovenian\": \"slv_Latn\",\n-    \"Samoan\": \"smo_Latn\",\n-    \"Shona\": \"sna_Latn\",\n-    \"Sindhi\": \"snd_Arab\",\n-    \"Somali\": \"som_Latn\",\n-    \"Southern Sotho\": \"sot_Latn\",\n-    \"Spanish\": \"spa_Latn\",\n-    \"Tosk Albanian\": \"als_Latn\",\n-    \"Sardinian\": \"srd_Latn\",\n-    \"Serbian\": \"srp_Cyrl\",\n-    \"Swati\": \"ssw_Latn\",\n-    \"Sundanese\": \"sun_Latn\",\n-    \"Swedish\": \"swe_Latn\",\n-    \"Swahili\": \"swh_Latn\",\n-    \"Silesian\": \"szl_Latn\",\n-    \"Tamil\": \"tam_Taml\",\n-    \"Tatar\": \"tat_Cyrl\",\n-    \"Telugu\": \"tel_Telu\",\n-    \"Tajik\": \"tgk_Cyrl\",\n-    \"Tagalog\": \"tgl_Latn\",\n-    \"Thai\": \"tha_Thai\",\n-    \"Tigrinya\": \"tir_Ethi\",\n-    \"Tamasheq Latin\": \"taq_Latn\",\n-    \"Tamasheq Tifinagh\": \"taq_Tfng\",\n-    \"Tok Pisin\": \"tpi_Latn\",\n-    \"Tswana\": \"tsn_Latn\",\n-    \"Tsonga\": \"tso_Latn\",\n-    \"Turkmen\": \"tuk_Latn\",\n-    \"Tumbuka\": \"tum_Latn\",\n-    \"Turkish\": \"tur_Latn\",\n-    \"Twi\": \"twi_Latn\",\n-    \"Central Atlas Tamazight\": \"tzm_Tfng\",\n-    \"Uyghur\": \"uig_Arab\",\n-    \"Ukrainian\": \"ukr_Cyrl\",\n-    \"Umbundu\": \"umb_Latn\",\n-    \"Urdu\": \"urd_Arab\",\n-    \"Northern Uzbek\": \"uzn_Latn\",\n-    \"Venetian\": \"vec_Latn\",\n-    \"Vietnamese\": \"vie_Latn\",\n-    \"Waray\": \"war_Latn\",\n-    \"Wolof\": \"wol_Latn\",\n-    \"Xhosa\": \"xho_Latn\",\n-    \"Eastern Yiddish\": \"ydd_Hebr\",\n-    \"Yoruba\": \"yor_Latn\",\n-    \"Yue Chinese\": \"yue_Hant\",\n-    \"Chinese Simplified\": \"zho_Hans\",\n-    \"Chinese Traditional\": \"zho_Hant\",\n-    \"Standard Malay\": \"zsm_Latn\",\n-    \"Zulu\": \"zul_Latn\",\n-}\n-\n-\n-class TranslationTool(PipelineTool):\n-    \"\"\"\n-    Example:\n-\n-    ```py\n-    from transformers.agents import TranslationTool\n-\n-    translator = TranslationTool()\n-    translator(\"This is a super nice API!\", src_lang=\"English\", tgt_lang=\"French\")\n-    ```\n-    \"\"\"\n-\n-    lang_to_code = LANGUAGE_CODES\n-    default_checkpoint = \"facebook/nllb-200-distilled-600M\"\n-    description = (\n-        \"This is a tool that translates text from a language to another.\"\n-        f\"Both `src_lang`and `tgt_lang` should belong to this list of languages: {list(lang_to_code.keys())}.\"\n-    )\n-    name = \"translator\"\n-    pre_processor_class = AutoTokenizer\n-    model_class = AutoModelForSeq2SeqLM\n-\n-    inputs = {\n-        \"text\": {\"type\": \"string\", \"description\": \"The text to translate\"},\n-        \"src_lang\": {\n-            \"type\": \"string\",\n-            \"description\": \"The language of the text to translate. Written in plain English, such as 'Romanian', or 'Albanian'\",\n-        },\n-        \"tgt_lang\": {\n-            \"type\": \"string\",\n-            \"description\": \"The language for the desired output language. Written in plain English, such as 'Romanian', or 'Albanian'\",\n-        },\n-    }\n-    output_type = \"string\"\n-\n-    def encode(self, text, src_lang, tgt_lang):\n-        if src_lang not in self.lang_to_code:\n-            raise ValueError(f\"{src_lang} is not a supported language.\")\n-        if tgt_lang not in self.lang_to_code:\n-            raise ValueError(f\"{tgt_lang} is not a supported language.\")\n-        src_lang = self.lang_to_code[src_lang]\n-        tgt_lang = self.lang_to_code[tgt_lang]\n-        return self.pre_processor._build_translation_inputs(\n-            text, return_tensors=\"pt\", src_lang=src_lang, tgt_lang=tgt_lang\n-        )\n-\n-    def forward(self, inputs):\n-        return self.model.generate(**inputs)\n-\n-    def decode(self, outputs):\n-        return self.post_processor.decode(outputs[0].tolist(), skip_special_tokens=True)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/agents/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2F__init__.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e"
        },
        {
            "sha": "48c58f016a9338688905945a5a165868e7734ed2",
            "filename": "tests/agents/test_agent_types.py",
            "status": "removed",
            "additions": 0,
            "deletions": 120,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_agent_types.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_agent_types.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_agent_types.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,120 +0,0 @@\n-# Copyright 2023 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import os\n-import tempfile\n-import unittest\n-import uuid\n-from pathlib import Path\n-\n-from transformers.agents.agent_types import AgentAudio, AgentImage, AgentText\n-from transformers.testing_utils import get_tests_dir, require_soundfile, require_torch, require_vision\n-from transformers.utils import is_soundfile_available, is_torch_available, is_vision_available\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_soundfile_available():\n-    import soundfile as sf\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-def get_new_path(suffix=\"\") -> str:\n-    directory = tempfile.mkdtemp()\n-    return os.path.join(directory, str(uuid.uuid4()) + suffix)\n-\n-\n-@require_soundfile\n-@require_torch\n-class AgentAudioTests(unittest.TestCase):\n-    def test_from_tensor(self):\n-        tensor = torch.rand(12, dtype=torch.float64) - 0.5\n-        agent_type = AgentAudio(tensor)\n-        path = str(agent_type.to_string())\n-\n-        # Ensure that the tensor and the agent_type's tensor are the same\n-        torch.testing.assert_close(tensor, agent_type.to_raw(), rtol=1e-4, atol=1e-4)\n-\n-        del agent_type\n-\n-        # Ensure the path remains even after the object deletion\n-        self.assertTrue(os.path.exists(path))\n-\n-        # Ensure that the file contains the same value as the original tensor\n-        new_tensor, _ = sf.read(path)\n-        torch.testing.assert_close(tensor, torch.tensor(new_tensor), rtol=1e-4, atol=1e-4)\n-\n-    def test_from_string(self):\n-        tensor = torch.rand(12, dtype=torch.float64) - 0.5\n-        path = get_new_path(suffix=\".wav\")\n-        sf.write(path, tensor, 16000)\n-\n-        agent_type = AgentAudio(path)\n-\n-        torch.testing.assert_close(tensor, agent_type.to_raw(), rtol=1e-4, atol=1e-4)\n-        self.assertEqual(agent_type.to_string(), path)\n-\n-\n-@require_vision\n-@require_torch\n-class AgentImageTests(unittest.TestCase):\n-    def test_from_tensor(self):\n-        tensor = torch.randint(0, 256, (64, 64, 3))\n-        agent_type = AgentImage(tensor)\n-        path = str(agent_type.to_string())\n-\n-        # Ensure that the tensor and the agent_type's tensor are the same\n-        torch.testing.assert_close(tensor, agent_type._tensor, rtol=1e-4, atol=1e-4)\n-\n-        self.assertIsInstance(agent_type.to_raw(), Image.Image)\n-\n-        # Ensure the path remains even after the object deletion\n-        del agent_type\n-        self.assertTrue(os.path.exists(path))\n-\n-    def test_from_string(self):\n-        path = Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\"\n-        image = Image.open(path)\n-        agent_type = AgentImage(path)\n-\n-        self.assertTrue(path.samefile(agent_type.to_string()))\n-        self.assertTrue(image == agent_type.to_raw())\n-\n-        # Ensure the path remains even after the object deletion\n-        del agent_type\n-        self.assertTrue(os.path.exists(path))\n-\n-    def test_from_image(self):\n-        path = Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\"\n-        image = Image.open(path)\n-        agent_type = AgentImage(image)\n-\n-        self.assertFalse(path.samefile(agent_type.to_string()))\n-        self.assertTrue(image == agent_type.to_raw())\n-\n-        # Ensure the path remains even after the object deletion\n-        del agent_type\n-        self.assertTrue(os.path.exists(path))\n-\n-\n-class AgentTextTests(unittest.TestCase):\n-    def test_from_string(self):\n-        string = \"Hey!\"\n-        agent_type = AgentText(string)\n-\n-        self.assertEqual(string, agent_type.to_string())\n-        self.assertEqual(string, agent_type.to_raw())\n-        self.assertEqual(string, agent_type)"
        },
        {
            "sha": "9a3c8e7a9f3b8ead9f1e0fafd608701ef157a5f8",
            "filename": "tests/agents/test_agents.py",
            "status": "removed",
            "additions": 0,
            "deletions": 257,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_agents.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_agents.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_agents.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,257 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import os\n-import tempfile\n-import unittest\n-import uuid\n-\n-import pytest\n-\n-from transformers.agents.agent_types import AgentText\n-from transformers.agents.agents import (\n-    AgentMaxIterationsError,\n-    CodeAgent,\n-    ManagedAgent,\n-    ReactCodeAgent,\n-    ReactJsonAgent,\n-    Toolbox,\n-)\n-from transformers.agents.default_tools import PythonInterpreterTool\n-from transformers.testing_utils import require_torch\n-\n-\n-def get_new_path(suffix=\"\") -> str:\n-    directory = tempfile.mkdtemp()\n-    return os.path.join(directory, str(uuid.uuid4()) + suffix)\n-\n-\n-def fake_react_json_llm(messages, stop_sequences=None, grammar=None) -> str:\n-    prompt = str(messages)\n-\n-    if \"special_marker\" not in prompt:\n-        return \"\"\"\n-Thought: I should multiply 2 by 3.6452. special_marker\n-Action:\n-{\n-    \"action\": \"python_interpreter\",\n-    \"action_input\": {\"code\": \"2*3.6452\"}\n-}\n-\"\"\"\n-    else:  # We're at step 2\n-        return \"\"\"\n-Thought: I can now answer the initial question\n-Action:\n-{\n-    \"action\": \"final_answer\",\n-    \"action_input\": {\"answer\": \"7.2904\"}\n-}\n-\"\"\"\n-\n-\n-def fake_react_code_llm(messages, stop_sequences=None, grammar=None) -> str:\n-    prompt = str(messages)\n-    if \"special_marker\" not in prompt:\n-        return \"\"\"\n-Thought: I should multiply 2 by 3.6452. special_marker\n-Code:\n-```py\n-result = 2**3.6452\n-```<end_code>\n-\"\"\"\n-    else:  # We're at step 2\n-        return \"\"\"\n-Thought: I can now answer the initial question\n-Code:\n-```py\n-final_answer(7.2904)\n-```<end_code>\n-\"\"\"\n-\n-\n-def fake_react_code_llm_error(messages, stop_sequences=None) -> str:\n-    prompt = str(messages)\n-    if \"special_marker\" not in prompt:\n-        return \"\"\"\n-Thought: I should multiply 2 by 3.6452. special_marker\n-Code:\n-```py\n-print = 2\n-```<end_code>\n-\"\"\"\n-    else:  # We're at step 2\n-        return \"\"\"\n-Thought: I can now answer the initial question\n-Code:\n-```py\n-final_answer(\"got an error\")\n-```<end_code>\n-\"\"\"\n-\n-\n-def fake_react_code_functiondef(messages, stop_sequences=None) -> str:\n-    prompt = str(messages)\n-    if \"special_marker\" not in prompt:\n-        return \"\"\"\n-Thought: Let's define the function. special_marker\n-Code:\n-```py\n-import numpy as np\n-\n-def moving_average(x, w):\n-    return np.convolve(x, np.ones(w), 'valid') / w\n-```<end_code>\n-\"\"\"\n-    else:  # We're at step 2\n-        return \"\"\"\n-Thought: I can now answer the initial question\n-Code:\n-```py\n-x, w = [0, 1, 2, 3, 4, 5], 2\n-res = moving_average(x, w)\n-final_answer(res)\n-```<end_code>\n-\"\"\"\n-\n-\n-def fake_code_llm_oneshot(messages, stop_sequences=None, grammar=None) -> str:\n-    return \"\"\"\n-Thought: I should multiply 2 by 3.6452. special_marker\n-Code:\n-```py\n-result = python_interpreter(code=\"2*3.6452\")\n-final_answer(result)\n-```\n-\"\"\"\n-\n-\n-def fake_code_llm_no_return(messages, stop_sequences=None, grammar=None) -> str:\n-    return \"\"\"\n-Thought: I should multiply 2 by 3.6452. special_marker\n-Code:\n-```py\n-result = python_interpreter(code=\"2*3.6452\")\n-print(result)\n-```\n-\"\"\"\n-\n-\n-class AgentTests(unittest.TestCase):\n-    def test_fake_code_agent(self):\n-        agent = CodeAgent(tools=[PythonInterpreterTool()], llm_engine=fake_code_llm_oneshot)\n-        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n-        assert isinstance(output, str)\n-        assert output == \"7.2904\"\n-\n-    def test_fake_react_json_agent(self):\n-        agent = ReactJsonAgent(tools=[PythonInterpreterTool()], llm_engine=fake_react_json_llm)\n-        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n-        assert isinstance(output, str)\n-        assert output == \"7.2904\"\n-        assert agent.logs[0][\"task\"] == \"What is 2 multiplied by 3.6452?\"\n-        assert agent.logs[1][\"observation\"] == \"7.2904\"\n-        assert agent.logs[1][\"rationale\"].strip() == \"Thought: I should multiply 2 by 3.6452. special_marker\"\n-        assert (\n-            agent.logs[2][\"llm_output\"]\n-            == \"\"\"\n-Thought: I can now answer the initial question\n-Action:\n-{\n-    \"action\": \"final_answer\",\n-    \"action_input\": {\"answer\": \"7.2904\"}\n-}\n-\"\"\"\n-        )\n-\n-    def test_fake_react_code_agent(self):\n-        agent = ReactCodeAgent(tools=[PythonInterpreterTool()], llm_engine=fake_react_code_llm)\n-        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n-        assert isinstance(output, float)\n-        assert output == 7.2904\n-        assert agent.logs[0][\"task\"] == \"What is 2 multiplied by 3.6452?\"\n-        assert agent.logs[2][\"tool_call\"] == {\n-            \"tool_arguments\": \"final_answer(7.2904)\",\n-            \"tool_name\": \"code interpreter\",\n-        }\n-\n-    def test_react_code_agent_code_errors_show_offending_lines(self):\n-        agent = ReactCodeAgent(tools=[PythonInterpreterTool()], llm_engine=fake_react_code_llm_error)\n-        output = agent.run(\"What is 2 multiplied by 3.6452?\")\n-        assert isinstance(output, AgentText)\n-        assert output == \"got an error\"\n-        assert \"Evaluation stopped at line 'print = 2' because of\" in str(agent.logs)\n-\n-    def test_setup_agent_with_empty_toolbox(self):\n-        ReactJsonAgent(llm_engine=fake_react_json_llm, tools=[])\n-\n-    def test_react_fails_max_iterations(self):\n-        agent = ReactCodeAgent(\n-            tools=[PythonInterpreterTool()],\n-            llm_engine=fake_code_llm_no_return,  # use this callable because it never ends\n-            max_iterations=5,\n-        )\n-        agent.run(\"What is 2 multiplied by 3.6452?\")\n-        assert len(agent.logs) == 7\n-        assert type(agent.logs[-1][\"error\"]) is AgentMaxIterationsError\n-\n-    @require_torch\n-    def test_init_agent_with_different_toolsets(self):\n-        toolset_1 = []\n-        agent = ReactCodeAgent(tools=toolset_1, llm_engine=fake_react_code_llm)\n-        assert (\n-            len(agent.toolbox.tools) == 1\n-        )  # when no tools are provided, only the final_answer tool is added by default\n-\n-        toolset_2 = [PythonInterpreterTool(), PythonInterpreterTool()]\n-        agent = ReactCodeAgent(tools=toolset_2, llm_engine=fake_react_code_llm)\n-        assert (\n-            len(agent.toolbox.tools) == 2\n-        )  # deduplication of tools, so only one python_interpreter tool is added in addition to final_answer\n-\n-        toolset_3 = Toolbox(toolset_2)\n-        agent = ReactCodeAgent(tools=toolset_3, llm_engine=fake_react_code_llm)\n-        assert (\n-            len(agent.toolbox.tools) == 2\n-        )  # same as previous one, where toolset_3 is an instantiation of previous one\n-\n-        # check that add_base_tools will not interfere with existing tools\n-        with pytest.raises(KeyError) as e:\n-            agent = ReactJsonAgent(tools=toolset_3, llm_engine=fake_react_json_llm, add_base_tools=True)\n-        assert \"already exists in the toolbox\" in str(e)\n-\n-        # check that python_interpreter base tool does not get added to code agents\n-        agent = ReactCodeAgent(tools=[], llm_engine=fake_react_code_llm, add_base_tools=True)\n-        assert len(agent.toolbox.tools) == 7  # added final_answer tool + 6 base tools (excluding interpreter)\n-\n-    def test_function_persistence_across_steps(self):\n-        agent = ReactCodeAgent(\n-            tools=[], llm_engine=fake_react_code_functiondef, max_iterations=2, additional_authorized_imports=[\"numpy\"]\n-        )\n-        res = agent.run(\"ok\")\n-        assert res[0] == 0.5\n-\n-    def test_init_managed_agent(self):\n-        agent = ReactCodeAgent(tools=[], llm_engine=fake_react_code_functiondef)\n-        managed_agent = ManagedAgent(agent, name=\"managed_agent\", description=\"Empty\")\n-        assert managed_agent.name == \"managed_agent\"\n-        assert managed_agent.description == \"Empty\"\n-\n-    def test_agent_description_gets_correctly_inserted_in_system_prompt(self):\n-        agent = ReactCodeAgent(tools=[], llm_engine=fake_react_code_functiondef)\n-        managed_agent = ManagedAgent(agent, name=\"managed_agent\", description=\"Empty\")\n-        manager_agent = ReactCodeAgent(\n-            tools=[], llm_engine=fake_react_code_functiondef, managed_agents=[managed_agent]\n-        )\n-        assert \"You can also give requests to team members.\" not in agent.system_prompt\n-        assert \"<<managed_agents_descriptions>>\" not in agent.system_prompt\n-        assert \"You can also give requests to team members.\" in manager_agent.system_prompt"
        },
        {
            "sha": "ff5d13350cac4c580600ecded0b3e356e58734fc",
            "filename": "tests/agents/test_document_question_answering.py",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_document_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_document_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_document_question_answering.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,40 +0,0 @@\n-# Copyright 2023 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from datasets import load_dataset\n-\n-from transformers import load_tool\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-class DocumentQuestionAnsweringToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"document_question_answering\")\n-        self.tool.setup()\n-\n-    def test_exact_match_arg(self):\n-        dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n-        document = dataset[0][\"image\"]\n-\n-        result = self.tool(document, \"When is the coffee break?\")\n-        self.assertEqual(result, \"11-14 to 11:39 a.m.\")\n-\n-    def test_exact_match_kwarg(self):\n-        dataset = load_dataset(\"hf-internal-testing/example-documents\", split=\"test\")\n-        document = dataset[0][\"image\"]\n-\n-        self.tool(document=document, question=\"When is the coffee break?\")"
        },
        {
            "sha": "e7f5ba6a89132369673f061cbe93a2c2ae4a35cf",
            "filename": "tests/agents/test_final_answer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_final_answer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_final_answer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_final_answer.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,70 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-from pathlib import Path\n-\n-import numpy as np\n-from PIL import Image\n-\n-from transformers import is_torch_available\n-from transformers.agents.agent_types import AGENT_TYPE_MAPPING\n-from transformers.agents.default_tools import FinalAnswerTool\n-from transformers.testing_utils import get_tests_dir, require_torch\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-class FinalAnswerToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.inputs = {\"answer\": \"Final answer\"}\n-        self.tool = FinalAnswerTool()\n-\n-    def test_exact_match_arg(self):\n-        result = self.tool(\"Final answer\")\n-        self.assertEqual(result, \"Final answer\")\n-\n-    def test_exact_match_kwarg(self):\n-        result = self.tool(answer=self.inputs[\"answer\"])\n-        self.assertEqual(result, \"Final answer\")\n-\n-    def create_inputs(self):\n-        inputs_text = {\"answer\": \"Text input\"}\n-        inputs_image = {\n-            \"answer\": Image.open(Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\").resize(\n-                (512, 512)\n-            )\n-        }\n-        inputs_audio = {\"answer\": torch.Tensor(np.ones(3000))}\n-        return {\"string\": inputs_text, \"image\": inputs_image, \"audio\": inputs_audio}\n-\n-    @require_torch\n-    def test_agent_type_output(self):\n-        inputs = self.create_inputs()\n-        for input_type, input in inputs.items():\n-            output = self.tool(**input)\n-            agent_type = AGENT_TYPE_MAPPING[input_type]\n-            self.assertTrue(isinstance(output, agent_type))\n-\n-    @require_torch\n-    def test_agent_types_inputs(self):\n-        inputs = self.create_inputs()\n-        for input_type, input in inputs.items():\n-            output = self.tool(**input)\n-            agent_type = AGENT_TYPE_MAPPING[input_type]\n-            self.assertTrue(isinstance(output, agent_type))"
        },
        {
            "sha": "60e3a1895e0272a2deebe4699cce9d42f692a82b",
            "filename": "tests/agents/test_image_question_answering.py",
            "status": "removed",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_image_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_image_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_image_question_answering.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,41 +0,0 @@\n-# Copyright 2023 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-from pathlib import Path\n-\n-from transformers import is_vision_available, load_tool\n-from transformers.testing_utils import get_tests_dir\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-class ImageQuestionAnsweringToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"image_question_answering\")\n-        self.tool.setup()\n-\n-    def test_exact_match_arg(self):\n-        image = Image.open(Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\")\n-        result = self.tool(image, \"How many cats are sleeping on the couch?\")\n-        self.assertEqual(result, \"2\")\n-\n-    def test_exact_match_kwarg(self):\n-        image = Image.open(Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\")\n-        result = self.tool(image=image, question=\"How many cats are sleeping on the couch?\")\n-        self.assertEqual(result, \"2\")"
        },
        {
            "sha": "b7b6b346f7c9ecbbe356c47076c096b35d299601",
            "filename": "tests/agents/test_monitoring.py",
            "status": "removed",
            "additions": 0,
            "deletions": 165,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_monitoring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_monitoring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_monitoring.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,165 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers.agents.agent_types import AgentImage\n-from transformers.agents.agents import AgentError, ReactCodeAgent, ReactJsonAgent\n-from transformers.agents.monitoring import stream_to_gradio\n-\n-\n-class MonitoringTester(unittest.TestCase):\n-    def test_code_agent_metrics(self):\n-        class FakeLLMEngine:\n-            def __init__(self):\n-                self.last_input_token_count = 10\n-                self.last_output_token_count = 20\n-\n-            def __call__(self, prompt, **kwargs):\n-                return \"\"\"\n-Code:\n-```py\n-final_answer('This is the final answer.')\n-```\"\"\"\n-\n-        agent = ReactCodeAgent(\n-            tools=[],\n-            llm_engine=FakeLLMEngine(),\n-            max_iterations=1,\n-        )\n-\n-        agent.run(\"Fake task\")\n-\n-        self.assertEqual(agent.monitor.total_input_token_count, 10)\n-        self.assertEqual(agent.monitor.total_output_token_count, 20)\n-\n-    def test_json_agent_metrics(self):\n-        class FakeLLMEngine:\n-            def __init__(self):\n-                self.last_input_token_count = 10\n-                self.last_output_token_count = 20\n-\n-            def __call__(self, prompt, **kwargs):\n-                return 'Action:{\"action\": \"final_answer\", \"action_input\": {\"answer\": \"image\"}}'\n-\n-        agent = ReactJsonAgent(\n-            tools=[],\n-            llm_engine=FakeLLMEngine(),\n-            max_iterations=1,\n-        )\n-\n-        agent.run(\"Fake task\")\n-\n-        self.assertEqual(agent.monitor.total_input_token_count, 10)\n-        self.assertEqual(agent.monitor.total_output_token_count, 20)\n-\n-    def test_code_agent_metrics_max_iterations(self):\n-        class FakeLLMEngine:\n-            def __init__(self):\n-                self.last_input_token_count = 10\n-                self.last_output_token_count = 20\n-\n-            def __call__(self, prompt, **kwargs):\n-                return \"Malformed answer\"\n-\n-        agent = ReactCodeAgent(\n-            tools=[],\n-            llm_engine=FakeLLMEngine(),\n-            max_iterations=1,\n-        )\n-\n-        agent.run(\"Fake task\")\n-\n-        self.assertEqual(agent.monitor.total_input_token_count, 20)\n-        self.assertEqual(agent.monitor.total_output_token_count, 40)\n-\n-    def test_code_agent_metrics_generation_error(self):\n-        class FakeLLMEngine:\n-            def __init__(self):\n-                self.last_input_token_count = 10\n-                self.last_output_token_count = 20\n-\n-            def __call__(self, prompt, **kwargs):\n-                raise AgentError\n-\n-        agent = ReactCodeAgent(\n-            tools=[],\n-            llm_engine=FakeLLMEngine(),\n-            max_iterations=1,\n-        )\n-\n-        agent.run(\"Fake task\")\n-\n-        self.assertEqual(agent.monitor.total_input_token_count, 20)\n-        self.assertEqual(agent.monitor.total_output_token_count, 40)\n-\n-    def test_streaming_agent_text_output(self):\n-        def dummy_llm_engine(prompt, **kwargs):\n-            return \"\"\"\n-Code:\n-```py\n-final_answer('This is the final answer.')\n-```\"\"\"\n-\n-        agent = ReactCodeAgent(\n-            tools=[],\n-            llm_engine=dummy_llm_engine,\n-            max_iterations=1,\n-        )\n-\n-        # Use stream_to_gradio to capture the output\n-        outputs = list(stream_to_gradio(agent, task=\"Test task\", test_mode=True))\n-\n-        self.assertEqual(len(outputs), 3)\n-        final_message = outputs[-1]\n-        self.assertEqual(final_message.role, \"assistant\")\n-        self.assertIn(\"This is the final answer.\", final_message.content)\n-\n-    def test_streaming_agent_image_output(self):\n-        def dummy_llm_engine(prompt, **kwargs):\n-            return 'Action:{\"action\": \"final_answer\", \"action_input\": {\"answer\": \"image\"}}'\n-\n-        agent = ReactJsonAgent(\n-            tools=[],\n-            llm_engine=dummy_llm_engine,\n-            max_iterations=1,\n-        )\n-\n-        # Use stream_to_gradio to capture the output\n-        outputs = list(stream_to_gradio(agent, task=\"Test task\", image=AgentImage(value=\"path.png\"), test_mode=True))\n-\n-        self.assertEqual(len(outputs), 2)\n-        final_message = outputs[-1]\n-        self.assertEqual(final_message.role, \"assistant\")\n-        self.assertIsInstance(final_message.content, dict)\n-        self.assertEqual(final_message.content[\"path\"], \"path.png\")\n-        self.assertEqual(final_message.content[\"mime_type\"], \"image/png\")\n-\n-    def test_streaming_with_agent_error(self):\n-        def dummy_llm_engine(prompt, **kwargs):\n-            raise AgentError(\"Simulated agent error\")\n-\n-        agent = ReactCodeAgent(\n-            tools=[],\n-            llm_engine=dummy_llm_engine,\n-            max_iterations=1,\n-        )\n-\n-        # Use stream_to_gradio to capture the output\n-        outputs = list(stream_to_gradio(agent, task=\"Test task\", test_mode=True))\n-\n-        self.assertEqual(len(outputs), 3)\n-        final_message = outputs[-1]\n-        self.assertEqual(final_message.role, \"assistant\")\n-        self.assertIn(\"Simulated agent error\", final_message.content)"
        },
        {
            "sha": "8f9e2658794fc549e38f459eb4388eb2078ac299",
            "filename": "tests/agents/test_python_interpreter.py",
            "status": "removed",
            "additions": 0,
            "deletions": 836,
            "changes": 836,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_python_interpreter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_python_interpreter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_python_interpreter.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,836 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-import pytest\n-\n-from transformers import load_tool\n-from transformers.agents.agent_types import AGENT_TYPE_MAPPING\n-from transformers.agents.default_tools import BASE_PYTHON_TOOLS\n-from transformers.agents.python_interpreter import InterpreterError, evaluate_python_code\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-# Fake function we will use as tool\n-def add_two(x):\n-    return x + 2\n-\n-\n-class PythonInterpreterToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"python_interpreter\", authorized_imports=[\"sqlite3\"])\n-        self.tool.setup()\n-\n-    def test_exact_match_arg(self):\n-        result = self.tool(\"(2 / 2) * 4\")\n-        self.assertEqual(result, \"4.0\")\n-\n-    def test_exact_match_kwarg(self):\n-        result = self.tool(code=\"(2 / 2) * 4\")\n-        self.assertEqual(result, \"4.0\")\n-\n-    def test_agent_type_output(self):\n-        inputs = [\"2 * 2\"]\n-        output = self.tool(*inputs)\n-        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n-        self.assertTrue(isinstance(output, output_type))\n-\n-    def test_agent_types_inputs(self):\n-        inputs = [\"2 * 2\"]\n-        _inputs = []\n-\n-        for _input, expected_input in zip(inputs, self.tool.inputs.values()):\n-            input_type = expected_input[\"type\"]\n-            if isinstance(input_type, list):\n-                _inputs.append([AGENT_TYPE_MAPPING[_input_type](_input) for _input_type in input_type])\n-            else:\n-                _inputs.append(AGENT_TYPE_MAPPING[input_type](_input))\n-\n-        # Should not raise an error\n-        output = self.tool(*inputs)\n-        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n-        self.assertTrue(isinstance(output, output_type))\n-\n-\n-class PythonInterpreterTester(unittest.TestCase):\n-    def test_evaluate_assign(self):\n-        code = \"x = 3\"\n-        state = {}\n-        result = evaluate_python_code(code, {}, state=state)\n-        assert result == 3\n-        self.assertDictEqual(state, {\"x\": 3, \"print_outputs\": \"\"})\n-\n-        code = \"x = y\"\n-        state = {\"y\": 5}\n-        result = evaluate_python_code(code, {}, state=state)\n-        # evaluate returns the value of the last assignment.\n-        assert result == 5\n-        self.assertDictEqual(state, {\"x\": 5, \"y\": 5, \"print_outputs\": \"\"})\n-\n-        code = \"a=1;b=None\"\n-        result = evaluate_python_code(code, {}, state={})\n-        # evaluate returns the value of the last assignment.\n-        assert result is None\n-\n-    def test_assignment_cannot_overwrite_tool(self):\n-        code = \"print = '3'\"\n-        with pytest.raises(InterpreterError) as e:\n-            evaluate_python_code(code, {\"print\": print}, state={})\n-        assert \"Cannot assign to name 'print': doing this would erase the existing tool!\" in str(e)\n-\n-    def test_evaluate_call(self):\n-        code = \"y = add_two(x)\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n-        assert result == 5\n-        self.assertDictEqual(state, {\"x\": 3, \"y\": 5, \"print_outputs\": \"\"})\n-\n-        # Should not work without the tool\n-        with pytest.raises(InterpreterError) as e:\n-            evaluate_python_code(code, {}, state=state)\n-        assert \"tried to execute add_two\" in str(e.value)\n-\n-    def test_evaluate_constant(self):\n-        code = \"x = 3\"\n-        state = {}\n-        result = evaluate_python_code(code, {}, state=state)\n-        assert result == 3\n-        self.assertDictEqual(state, {\"x\": 3, \"print_outputs\": \"\"})\n-\n-    def test_evaluate_dict(self):\n-        code = \"test_dict = {'x': x, 'y': add_two(x)}\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n-        self.assertDictEqual(result, {\"x\": 3, \"y\": 5})\n-        self.assertDictEqual(state, {\"x\": 3, \"test_dict\": {\"x\": 3, \"y\": 5}, \"print_outputs\": \"\"})\n-\n-    def test_evaluate_expression(self):\n-        code = \"x = 3\\ny = 5\"\n-        state = {}\n-        result = evaluate_python_code(code, {}, state=state)\n-        # evaluate returns the value of the last assignment.\n-        assert result == 5\n-        self.assertDictEqual(state, {\"x\": 3, \"y\": 5, \"print_outputs\": \"\"})\n-\n-    def test_evaluate_f_string(self):\n-        code = \"text = f'This is x: {x}.'\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {}, state=state)\n-        # evaluate returns the value of the last assignment.\n-        assert result == \"This is x: 3.\"\n-        self.assertDictEqual(state, {\"x\": 3, \"text\": \"This is x: 3.\", \"print_outputs\": \"\"})\n-\n-    def test_evaluate_if(self):\n-        code = \"if x <= 3:\\n    y = 2\\nelse:\\n    y = 5\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {}, state=state)\n-        # evaluate returns the value of the last assignment.\n-        assert result == 2\n-        self.assertDictEqual(state, {\"x\": 3, \"y\": 2, \"print_outputs\": \"\"})\n-\n-        state = {\"x\": 8}\n-        result = evaluate_python_code(code, {}, state=state)\n-        # evaluate returns the value of the last assignment.\n-        assert result == 5\n-        self.assertDictEqual(state, {\"x\": 8, \"y\": 5, \"print_outputs\": \"\"})\n-\n-    def test_evaluate_list(self):\n-        code = \"test_list = [x, add_two(x)]\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n-        self.assertListEqual(result, [3, 5])\n-        self.assertDictEqual(state, {\"x\": 3, \"test_list\": [3, 5], \"print_outputs\": \"\"})\n-\n-    def test_evaluate_name(self):\n-        code = \"y = x\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {}, state=state)\n-        assert result == 3\n-        self.assertDictEqual(state, {\"x\": 3, \"y\": 3, \"print_outputs\": \"\"})\n-\n-    def test_evaluate_subscript(self):\n-        code = \"test_list = [x, add_two(x)]\\ntest_list[1]\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n-        assert result == 5\n-        self.assertDictEqual(state, {\"x\": 3, \"test_list\": [3, 5], \"print_outputs\": \"\"})\n-\n-        code = \"test_dict = {'x': x, 'y': add_two(x)}\\ntest_dict['y']\"\n-        state = {\"x\": 3}\n-        result = evaluate_python_code(code, {\"add_two\": add_two}, state=state)\n-        assert result == 5\n-        self.assertDictEqual(state, {\"x\": 3, \"test_dict\": {\"x\": 3, \"y\": 5}, \"print_outputs\": \"\"})\n-\n-        code = \"vendor = {'revenue': 31000, 'rent': 50312}; vendor['ratio'] = round(vendor['revenue'] / vendor['rent'], 2)\"\n-        state = {}\n-        evaluate_python_code(code, {\"min\": min, \"print\": print, \"round\": round}, state=state)\n-        assert state[\"vendor\"] == {\"revenue\": 31000, \"rent\": 50312, \"ratio\": 0.62}\n-\n-    def test_subscript_string_with_string_index_raises_appropriate_error(self):\n-        code = \"\"\"\n-search_results = \"[{'title': 'Paris, Ville de Paris, France Weather Forecast | AccuWeather', 'href': 'https://www.accuweather.com/en/fr/paris/623/weather-forecast/623', 'body': 'Get the latest weather forecast for Paris, Ville de Paris, France , including hourly, daily, and 10-day outlooks. AccuWeather provides you with reliable and accurate information on temperature ...'}]\"\n-for result in search_results:\n-    if 'current' in result['title'].lower() or 'temperature' in result['title'].lower():\n-        current_weather_url = result['href']\n-        print(current_weather_url)\n-        break\"\"\"\n-        with pytest.raises(InterpreterError) as e:\n-            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-            assert \"You're trying to subscript a string with a string index\" in e\n-\n-    def test_evaluate_for(self):\n-        code = \"x = 0\\nfor i in range(3):\\n    x = i\"\n-        state = {}\n-        result = evaluate_python_code(code, {\"range\": range}, state=state)\n-        assert result == 2\n-        self.assertDictEqual(state, {\"x\": 2, \"i\": 2, \"print_outputs\": \"\"})\n-\n-    def test_evaluate_binop(self):\n-        code = \"y + x\"\n-        state = {\"x\": 3, \"y\": 6}\n-        result = evaluate_python_code(code, {}, state=state)\n-        assert result == 9\n-        self.assertDictEqual(state, {\"x\": 3, \"y\": 6, \"print_outputs\": \"\"})\n-\n-    def test_recursive_function(self):\n-        code = \"\"\"\n-def recur_fibo(n):\n-    if n <= 1:\n-        return n\n-    else:\n-        return(recur_fibo(n-1) + recur_fibo(n-2))\n-recur_fibo(6)\"\"\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == 8\n-\n-    def test_evaluate_string_methods(self):\n-        code = \"'hello'.replace('h', 'o').split('e')\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == [\"o\", \"llo\"]\n-\n-    def test_evaluate_slicing(self):\n-        code = \"'hello'[1:3][::-1]\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == \"le\"\n-\n-    def test_access_attributes(self):\n-        code = \"integer = 1\\nobj_class = integer.__class__\\nobj_class\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result is int\n-\n-    def test_list_comprehension(self):\n-        code = \"sentence = 'THESEAGULL43'\\nmeaningful_sentence = '-'.join([char.lower() for char in sentence if char.isalpha()])\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == \"t-h-e-s-e-a-g-u-l-l\"\n-\n-    def test_string_indexing(self):\n-        code = \"\"\"text_block = [\n-    \"THESE\",\n-    \"AGULL\"\n-]\n-sentence = \"\"\n-for block in text_block:\n-    for col in range(len(text_block[0])):\n-        sentence += block[col]\n-        \"\"\"\n-        result = evaluate_python_code(code, {\"len\": len, \"range\": range}, state={})\n-        assert result == \"THESEAGULL\"\n-\n-    def test_tuples(self):\n-        code = \"x = (1, 2, 3)\\nx[1]\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == 2\n-\n-        code = \"\"\"\n-digits, i = [1, 2, 3], 1\n-digits[i], digits[i + 1] = digits[i + 1], digits[i]\"\"\"\n-        evaluate_python_code(code, {\"range\": range, \"print\": print, \"int\": int}, {})\n-\n-        code = \"\"\"\n-def calculate_isbn_10_check_digit(number):\n-    total = sum((10 - i) * int(digit) for i, digit in enumerate(number))\n-    remainder = total % 11\n-    check_digit = 11 - remainder\n-    if check_digit == 10:\n-        return 'X'\n-    elif check_digit == 11:\n-        return '0'\n-    else:\n-        return str(check_digit)\n-\n-# Given 9-digit numbers\n-numbers = [\n-    \"478225952\",\n-    \"643485613\",\n-    \"739394228\",\n-    \"291726859\",\n-    \"875262394\",\n-    \"542617795\",\n-    \"031810713\",\n-    \"957007669\",\n-    \"871467426\"\n-]\n-\n-# Calculate check digits for each number\n-check_digits = [calculate_isbn_10_check_digit(number) for number in numbers]\n-print(check_digits)\n-\"\"\"\n-        state = {}\n-        evaluate_python_code(\n-            code, {\"range\": range, \"print\": print, \"sum\": sum, \"enumerate\": enumerate, \"int\": int, \"str\": str}, state\n-        )\n-\n-    def test_listcomp(self):\n-        code = \"x = [i for i in range(3)]\"\n-        result = evaluate_python_code(code, {\"range\": range}, state={})\n-        assert result == [0, 1, 2]\n-\n-    def test_break_continue(self):\n-        code = \"for i in range(10):\\n    if i == 5:\\n        break\\ni\"\n-        result = evaluate_python_code(code, {\"range\": range}, state={})\n-        assert result == 5\n-\n-        code = \"for i in range(10):\\n    if i == 5:\\n        continue\\ni\"\n-        result = evaluate_python_code(code, {\"range\": range}, state={})\n-        assert result == 9\n-\n-    def test_call_int(self):\n-        code = \"import math\\nstr(math.ceil(149))\"\n-        result = evaluate_python_code(code, {\"str\": lambda x: str(x)}, state={})\n-        assert result == \"149\"\n-\n-    def test_lambda(self):\n-        code = \"f = lambda x: x + 2\\nf(3)\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == 5\n-\n-    def test_dictcomp(self):\n-        code = \"x = {i: i**2 for i in range(3)}\"\n-        result = evaluate_python_code(code, {\"range\": range}, state={})\n-        assert result == {0: 0, 1: 1, 2: 4}\n-\n-        code = \"{num: name for num, name in {101: 'a', 102: 'b'}.items() if name not in ['a']}\"\n-        result = evaluate_python_code(code, {\"print\": print}, state={}, authorized_imports=[\"pandas\"])\n-        assert result == {102: \"b\"}\n-\n-        code = \"\"\"\n-shifts = {'A': ('6:45', '8:00'), 'B': ('10:00', '11:45')}\n-shift_minutes = {worker: ('a', 'b') for worker, (start, end) in shifts.items()}\n-\"\"\"\n-        result = evaluate_python_code(code, {}, state={})\n-        assert result == {\"A\": (\"a\", \"b\"), \"B\": (\"a\", \"b\")}\n-\n-    def test_tuple_assignment(self):\n-        code = \"a, b = 0, 1\\nb\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == 1\n-\n-    def test_while(self):\n-        code = \"i = 0\\nwhile i < 3:\\n    i += 1\\ni\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == 3\n-\n-        # test infinite loop\n-        code = \"i = 0\\nwhile i < 3:\\n    i -= 1\\ni\"\n-        with pytest.raises(InterpreterError) as e:\n-            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert \"iterations in While loop exceeded\" in str(e)\n-\n-        # test lazy evaluation\n-        code = \"\"\"\n-house_positions = [0, 7, 10, 15, 18, 22, 22]\n-i, n, loc = 0, 7, 30\n-while i < n and house_positions[i] <= loc:\n-    i += 1\n-\"\"\"\n-        state = {}\n-        evaluate_python_code(code, BASE_PYTHON_TOOLS, state=state)\n-\n-    def test_generator(self):\n-        code = \"a = [1, 2, 3, 4, 5]; b = (i**2 for i in a); list(b)\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == [1, 4, 9, 16, 25]\n-\n-    def test_boolops(self):\n-        code = \"\"\"if (not (a > b and a > c)) or d > e:\n-    best_city = \"Brooklyn\"\n-else:\n-    best_city = \"Manhattan\"\n-    best_city\n-    \"\"\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5})\n-        assert result == \"Brooklyn\"\n-\n-        code = \"\"\"if d > e and a < b:\n-    best_city = \"Brooklyn\"\n-elif d < e and a < b:\n-    best_city = \"Sacramento\"\n-else:\n-    best_city = \"Manhattan\"\n-    best_city\n-    \"\"\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, \"e\": 5})\n-        assert result == \"Sacramento\"\n-\n-    def test_if_conditions(self):\n-        code = \"\"\"char='a'\n-if char.isalpha():\n-    print('2')\"\"\"\n-        state = {}\n-        evaluate_python_code(code, BASE_PYTHON_TOOLS, state=state)\n-        assert state[\"print_outputs\"] == \"2\\n\"\n-\n-    def test_imports(self):\n-        code = \"import math\\nmath.sqrt(4)\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == 2.0\n-\n-        code = \"from random import choice, seed\\nseed(12)\\nchoice(['win', 'lose', 'draw'])\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == \"lose\"\n-\n-        code = \"import time, re\\ntime.sleep(0.1)\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result is None\n-\n-        code = \"from queue import Queue\\nq = Queue()\\nq.put(1)\\nq.get()\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == 1\n-\n-        code = \"import itertools\\nlist(itertools.islice(range(10), 3))\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == [0, 1, 2]\n-\n-        code = \"import re\\nre.search('a', 'abc').group()\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == \"a\"\n-\n-        code = \"import stat\\nstat.S_ISREG(0o100644)\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result\n-\n-        code = \"import statistics\\nstatistics.mean([1, 2, 3, 4, 4])\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == 2.8\n-\n-        code = \"import unicodedata\\nunicodedata.name('A')\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == \"LATIN CAPITAL LETTER A\"\n-\n-        # Test submodules are handled properly, thus not raising error\n-        code = \"import numpy.random as rd\\nrng = rd.default_rng(12345)\\nrng.random()\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={}, authorized_imports=[\"numpy\"])\n-\n-        code = \"from numpy.random import default_rng as d_rng\\nrng = d_rng(12345)\\nrng.random()\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={}, authorized_imports=[\"numpy\"])\n-\n-    def test_additional_imports(self):\n-        code = \"import numpy as np\"\n-        evaluate_python_code(code, authorized_imports=[\"numpy\"], state={})\n-\n-        code = \"import numpy.random as rd\"\n-        evaluate_python_code(code, authorized_imports=[\"numpy.random\"], state={})\n-        evaluate_python_code(code, authorized_imports=[\"numpy\"], state={})\n-        with pytest.raises(InterpreterError):\n-            evaluate_python_code(code, authorized_imports=[\"random\"], state={})\n-\n-    def test_multiple_comparators(self):\n-        code = \"0 <= -1 < 4 and 0 <= -5 < 4\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert not result\n-\n-        code = \"0 <= 1 < 4 and 0 <= -5 < 4\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert not result\n-\n-        code = \"0 <= 4 < 4 and 0 <= 3 < 4\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert not result\n-\n-        code = \"0 <= 3 < 4 and 0 <= 3 < 4\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result\n-\n-    def test_print_output(self):\n-        code = \"print('Hello world!')\\nprint('Ok no one cares')\"\n-        state = {}\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state=state)\n-        assert result is None\n-        assert state[\"print_outputs\"] == \"Hello world!\\nOk no one cares\\n\"\n-\n-        # test print in function\n-        code = \"\"\"\n-print(\"1\")\n-def function():\n-    print(\"2\")\n-function()\"\"\"\n-        state = {}\n-        evaluate_python_code(code, {\"print\": print}, state=state)\n-        assert state[\"print_outputs\"] == \"1\\n2\\n\"\n-\n-    def test_tuple_target_in_iterator(self):\n-        code = \"for a, b in [('Ralf Weikert', 'Austria'), ('Samuel Seungwon Lee', 'South Korea')]:res = a.split()[0]\"\n-        result = evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert result == \"Samuel\"\n-\n-    def test_classes(self):\n-        code = \"\"\"\n-class Animal:\n-    species = \"Generic Animal\"\n-\n-    def __init__(self, name, age):\n-        self.name = name\n-        self.age = age\n-\n-    def sound(self):\n-        return \"The animal makes a sound.\"\n-\n-    def __str__(self):\n-        return f\"{self.name}, {self.age} years old\"\n-\n-class Dog(Animal):\n-    species = \"Canine\"\n-\n-    def __init__(self, name, age, breed):\n-        super().__init__(name, age)\n-        self.breed = breed\n-\n-    def sound(self):\n-        return \"The dog barks.\"\n-\n-    def __str__(self):\n-        return f\"{self.name}, {self.age} years old, {self.breed}\"\n-\n-class Cat(Animal):\n-    def sound(self):\n-        return \"The cat meows.\"\n-\n-    def __str__(self):\n-        return f\"{self.name}, {self.age} years old, {self.species}\"\n-\n-\n-# Testing multiple instances\n-dog1 = Dog(\"Fido\", 3, \"Labrador\")\n-dog2 = Dog(\"Buddy\", 5, \"Golden Retriever\")\n-\n-# Testing method with built-in function\n-animals = [dog1, dog2, Cat(\"Whiskers\", 2)]\n-num_animals = len(animals)\n-\n-# Testing exceptions in methods\n-class ExceptionTest:\n-    def method_that_raises(self):\n-        raise ValueError(\"An error occurred\")\n-\n-try:\n-    exc_test = ExceptionTest()\n-    exc_test.method_that_raises()\n-except ValueError as e:\n-    exception_message = str(e)\n-\n-\n-# Collecting results\n-dog1_sound = dog1.sound()\n-dog1_str = str(dog1)\n-dog2_sound = dog2.sound()\n-dog2_str = str(dog2)\n-cat = Cat(\"Whiskers\", 2)\n-cat_sound = cat.sound()\n-cat_str = str(cat)\n-    \"\"\"\n-        state = {}\n-        evaluate_python_code(code, {\"print\": print, \"len\": len, \"super\": super, \"str\": str, \"sum\": sum}, state=state)\n-\n-        # Assert results\n-        assert state[\"dog1_sound\"] == \"The dog barks.\"\n-        assert state[\"dog1_str\"] == \"Fido, 3 years old, Labrador\"\n-        assert state[\"dog2_sound\"] == \"The dog barks.\"\n-        assert state[\"dog2_str\"] == \"Buddy, 5 years old, Golden Retriever\"\n-        assert state[\"cat_sound\"] == \"The cat meows.\"\n-        assert state[\"cat_str\"] == \"Whiskers, 2 years old, Generic Animal\"\n-        assert state[\"num_animals\"] == 3\n-        assert state[\"exception_message\"] == \"An error occurred\"\n-\n-    def test_variable_args(self):\n-        code = \"\"\"\n-def var_args_method(self, *args, **kwargs):\n-    return sum(args) + sum(kwargs.values())\n-\n-var_args_method(1, 2, 3, x=4, y=5)\n-\"\"\"\n-        state = {}\n-        result = evaluate_python_code(code, {\"sum\": sum}, state=state)\n-        assert result == 15\n-\n-    def test_exceptions(self):\n-        code = \"\"\"\n-def method_that_raises(self):\n-    raise ValueError(\"An error occurred\")\n-\n-try:\n-    method_that_raises()\n-except ValueError as e:\n-    exception_message = str(e)\n-    \"\"\"\n-        state = {}\n-        evaluate_python_code(code, {\"print\": print, \"len\": len, \"super\": super, \"str\": str, \"sum\": sum}, state=state)\n-        assert state[\"exception_message\"] == \"An error occurred\"\n-\n-    def test_print(self):\n-        code = \"print(min([1, 2, 3]))\"\n-        state = {}\n-        evaluate_python_code(code, {\"min\": min, \"print\": print}, state=state)\n-        assert state[\"print_outputs\"] == \"1\\n\"\n-\n-    def test_types_as_objects(self):\n-        code = \"type_a = float(2); type_b = str; type_c = int\"\n-        state = {}\n-        result = evaluate_python_code(code, {\"float\": float, \"str\": str, \"int\": int}, state=state)\n-        assert result is int\n-\n-    def test_tuple_id(self):\n-        code = \"\"\"\n-food_items = {\"apple\": 2, \"banana\": 3, \"orange\": 1, \"pear\": 1}\n-unique_food_items = [item for item, count in food_item_counts.items() if count == 1]\n-\"\"\"\n-        state = {}\n-        result = evaluate_python_code(code, {}, state=state)\n-        assert result == [\"orange\", \"pear\"]\n-\n-    def test_nonsimple_augassign(self):\n-        code = \"\"\"\n-counts_dict = {'a': 0}\n-counts_dict['a'] += 1\n-counts_list = [1, 2, 3]\n-counts_list += [4, 5, 6]\n-\n-class Counter:\n-    self.count = 0\n-\n-a = Counter()\n-a.count += 1\n-\"\"\"\n-        state = {}\n-        evaluate_python_code(code, {}, state=state)\n-        assert state[\"counts_dict\"] == {\"a\": 1}\n-        assert state[\"counts_list\"] == [1, 2, 3, 4, 5, 6]\n-        assert state[\"a\"].count == 1\n-\n-    def test_adding_int_to_list_raises_error(self):\n-        code = \"\"\"\n-counts = [1, 2, 3]\n-counts += 1\"\"\"\n-        with pytest.raises(InterpreterError) as e:\n-            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert \"Cannot add non-list value 1 to a list.\" in str(e)\n-\n-    def test_error_highlights_correct_line_of_code(self):\n-        code = \"\"\"# Ok this is a very long code\n-# It has many commented lines\n-a = 1\n-b = 2\n-\n-# Here is another piece\n-counts = [1, 2, 3]\n-counts += 1\n-b += 1\"\"\"\n-        with pytest.raises(InterpreterError) as e:\n-            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert \"Evaluation stopped at line 'counts += 1\" in str(e)\n-\n-    def test_assert(self):\n-        code = \"\"\"\n-assert 1 == 1\n-assert 1 == 2\n-\"\"\"\n-        with pytest.raises(AssertionError) as e:\n-            evaluate_python_code(code, BASE_PYTHON_TOOLS, state={})\n-        assert \"1 == 2\" in str(e) and \"1 == 1\" not in str(e)\n-\n-    def test_with_context_manager(self):\n-        code = \"\"\"\n-class SimpleLock:\n-    def __init__(self):\n-        self.locked = False\n-\n-    def __enter__(self):\n-        self.locked = True\n-        return self\n-\n-    def __exit__(self, exc_type, exc_value, traceback):\n-        self.locked = False\n-\n-lock = SimpleLock()\n-\n-with lock as l:\n-    assert l.locked == True\n-\n-assert lock.locked == False\n-    \"\"\"\n-        state = {}\n-        tools = {}\n-        evaluate_python_code(code, tools, state=state)\n-\n-    def test_default_arg_in_function(self):\n-        code = \"\"\"\n-def f(a, b=333, n=1000):\n-    return b + n\n-n = f(1, n=667)\n-\"\"\"\n-        res = evaluate_python_code(code, {}, {})\n-        assert res == 1000\n-\n-    def test_set(self):\n-        code = \"\"\"\n-S1 = {'a', 'b', 'c'}\n-S2 = {'b', 'c', 'd'}\n-S3 = S1.difference(S2)\n-S4 = S1.intersection(S2)\n-\"\"\"\n-        state = {}\n-        evaluate_python_code(code, {}, state=state)\n-        assert state[\"S3\"] == {\"a\"}\n-        assert state[\"S4\"] == {\"b\", \"c\"}\n-\n-    def test_break(self):\n-        code = \"\"\"\n-i = 0\n-\n-while True:\n-    i+= 1\n-    if i==3:\n-        break\n-\n-i\"\"\"\n-        result = evaluate_python_code(code, {\"print\": print, \"round\": round}, state={})\n-        assert result == 3\n-\n-    def test_return(self):\n-        # test early returns\n-        code = \"\"\"\n-def add_one(n, shift):\n-    if True:\n-        return n + shift\n-    return n\n-\n-add_one(1, 1)\n-\"\"\"\n-        state = {}\n-        result = evaluate_python_code(code, {\"print\": print, \"range\": range, \"ord\": ord, \"chr\": chr}, state=state)\n-        assert result == 2\n-\n-        # test returning None\n-        code = \"\"\"\n-def returns_none(a):\n-    return\n-\n-returns_none(1)\n-\"\"\"\n-        state = {}\n-        result = evaluate_python_code(code, {\"print\": print, \"range\": range, \"ord\": ord, \"chr\": chr}, state=state)\n-        assert result is None\n-\n-    def test_nested_for_loop(self):\n-        code = \"\"\"\n-all_res = []\n-for i in range(10):\n-    subres = []\n-    for j in range(i):\n-        subres.append(j)\n-    all_res.append(subres)\n-\n-out = [i for sublist in all_res for i in sublist]\n-out[:10]\n-\"\"\"\n-        state = {}\n-        result = evaluate_python_code(code, {\"print\": print, \"range\": range}, state=state)\n-        assert result == [0, 0, 1, 0, 1, 2, 0, 1, 2, 3]\n-\n-    def test_pandas(self):\n-        code = \"\"\"\n-import pandas as pd\n-\n-df = pd.DataFrame.from_dict({'SetCount': ['5', '4', '5'], 'Quantity': [1, 0, -1]})\n-\n-df['SetCount'] = pd.to_numeric(df['SetCount'], errors='coerce')\n-\n-parts_with_5_set_count = df[df['SetCount'] == 5.0]\n-parts_with_5_set_count[['Quantity', 'SetCount']].values[1]\n-\"\"\"\n-        state = {}\n-        result = evaluate_python_code(code, {}, state=state, authorized_imports=[\"pandas\"])\n-        assert np.array_equal(result, [-1, 5])\n-\n-        code = \"\"\"\n-import pandas as pd\n-\n-df = pd.DataFrame.from_dict({\"AtomicNumber\": [111, 104, 105], \"ok\": [0, 1, 2]})\n-print(\"HH0\")\n-\n-# Filter the DataFrame to get only the rows with outdated atomic numbers\n-filtered_df = df.loc[df['AtomicNumber'].isin([104])]\n-\"\"\"\n-        result = evaluate_python_code(code, {\"print\": print}, state={}, authorized_imports=[\"pandas\"])\n-        assert np.array_equal(result.values[0], [104, 1])\n-\n-        code = \"\"\"import pandas as pd\n-data = pd.DataFrame.from_dict([\n-    {\"Pclass\": 1, \"Survived\": 1},\n-    {\"Pclass\": 2, \"Survived\": 0},\n-    {\"Pclass\": 2, \"Survived\": 1}\n-])\n-survival_rate_by_class = data.groupby('Pclass')['Survived'].mean()\n-\"\"\"\n-        result = evaluate_python_code(code, {}, state={}, authorized_imports=[\"pandas\"])\n-        assert result.values[1] == 0.5\n-\n-    def test_starred(self):\n-        code = \"\"\"\n-from math import radians, sin, cos, sqrt, atan2\n-\n-def haversine(lat1, lon1, lat2, lon2):\n-    R = 6371000  # Radius of the Earth in meters\n-    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n-    dlat = lat2 - lat1\n-    dlon = lon2 - lon1\n-    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n-    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n-    distance = R * c\n-    return distance\n-\n-coords_geneva = (46.1978, 6.1342)\n-coords_barcelona = (41.3869, 2.1660)\n-\n-distance_geneva_barcelona = haversine(*coords_geneva, *coords_barcelona)\n-\"\"\"\n-        result = evaluate_python_code(code, {\"print\": print, \"map\": map}, state={}, authorized_imports=[\"math\"])\n-        assert round(result, 1) == 622395.4\n-\n-    def test_for(self):\n-        code = \"\"\"\n-shifts = {\n-    \"Worker A\": (\"6:45 pm\", \"8:00 pm\"),\n-    \"Worker B\": (\"10:00 am\", \"11:45 am\")\n-}\n-\n-shift_intervals = {}\n-for worker, (start, end) in shifts.items():\n-    shift_intervals[worker] = end\n-shift_intervals\n-\"\"\"\n-        result = evaluate_python_code(code, {\"print\": print, \"map\": map}, state={})\n-        assert result == {\"Worker A\": \"8:00 pm\", \"Worker B\": \"11:45 am\"}"
        },
        {
            "sha": "99bb3f25cbc875a1ccc1299fd52ecea98653773b",
            "filename": "tests/agents/test_search.py",
            "status": "removed",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_search.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,29 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers import load_tool\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-class DuckDuckGoSearchToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"web_search\")\n-        self.tool.setup()\n-\n-    def test_exact_match_arg(self):\n-        result = self.tool(\"Agents\")\n-        assert isinstance(result, list) and isinstance(result[0], dict)"
        },
        {
            "sha": "1c4c0a401ccfb28e4938346ada87408409b03bf0",
            "filename": "tests/agents/test_speech_to_text.py",
            "status": "removed",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_speech_to_text.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,35 +0,0 @@\n-# Copyright 2023 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import load_tool\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-class SpeechToTextToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"speech_to_text\")\n-        self.tool.setup()\n-\n-    def test_exact_match_arg(self):\n-        result = self.tool(np.ones(3000))\n-        self.assertEqual(result, \" Thank you.\")\n-\n-    def test_exact_match_kwarg(self):\n-        result = self.tool(audio=np.ones(3000))\n-        self.assertEqual(result, \" Thank you.\")"
        },
        {
            "sha": "60458066a971435dab7740aa2448d0413dcf9d83",
            "filename": "tests/agents/test_text_to_speech.py",
            "status": "removed",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_text_to_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_text_to_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_text_to_speech.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,49 +0,0 @@\n-# Copyright 2023 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers import load_tool\n-from transformers.utils import is_torch_available\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-from transformers.testing_utils import require_torch\n-\n-from .test_tools_common import ToolTesterMixin\n-\n-\n-@require_torch\n-class TextToSpeechToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"text_to_speech\")\n-        self.tool.setup()\n-\n-    def test_exact_match_arg(self):\n-        # SpeechT5 isn't deterministic\n-        torch.manual_seed(0)\n-        result = self.tool(\"hey\")\n-        resulting_tensor = result.to_raw()\n-        self.assertTrue(len(resulting_tensor.detach().shape) == 1)\n-        self.assertTrue(resulting_tensor.detach().shape[0] > 1000)\n-\n-    def test_exact_match_kwarg(self):\n-        # SpeechT5 isn't deterministic\n-        torch.manual_seed(0)\n-        result = self.tool(\"hey\")\n-        resulting_tensor = result.to_raw()\n-        self.assertTrue(len(resulting_tensor.detach().shape) == 1)\n-        self.assertTrue(resulting_tensor.detach().shape[0] > 1000)"
        },
        {
            "sha": "96fbf23e738f811d77c3bb31a5d9b6717c8ee586",
            "filename": "tests/agents/test_tools_common.py",
            "status": "removed",
            "additions": 0,
            "deletions": 170,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_tools_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_tools_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_tools_common.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,170 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import unittest\n-from pathlib import Path\n-from typing import Union\n-\n-import numpy as np\n-import pytest\n-\n-from transformers import is_torch_available, is_vision_available\n-from transformers.agents.agent_types import AGENT_TYPE_MAPPING, AgentAudio, AgentImage, AgentText\n-from transformers.agents.tools import Tool, tool\n-from transformers.testing_utils import get_tests_dir, is_agent_test\n-\n-\n-if is_torch_available():\n-    import torch\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-AUTHORIZED_TYPES = [\"string\", \"boolean\", \"integer\", \"number\", \"audio\", \"image\", \"any\"]\n-\n-\n-def create_inputs(tool_inputs: dict[str, dict[Union[str, type], str]]):\n-    inputs = {}\n-\n-    for input_name, input_desc in tool_inputs.items():\n-        input_type = input_desc[\"type\"]\n-\n-        if input_type == \"string\":\n-            inputs[input_name] = \"Text input\"\n-        elif input_type == \"image\":\n-            inputs[input_name] = Image.open(\n-                Path(get_tests_dir(\"fixtures/tests_samples/COCO\")) / \"000000039769.png\"\n-            ).resize((512, 512))\n-        elif input_type == \"audio\":\n-            inputs[input_name] = np.ones(3000)\n-        else:\n-            raise ValueError(f\"Invalid type requested: {input_type}\")\n-\n-    return inputs\n-\n-\n-def output_type(output):\n-    if isinstance(output, (str, AgentText)):\n-        return \"string\"\n-    elif isinstance(output, (Image.Image, AgentImage)):\n-        return \"image\"\n-    elif isinstance(output, (torch.Tensor, AgentAudio)):\n-        return \"audio\"\n-    else:\n-        raise TypeError(f\"Invalid output: {output}\")\n-\n-\n-@is_agent_test\n-class ToolTesterMixin:\n-    def test_inputs_output(self):\n-        self.assertTrue(hasattr(self.tool, \"inputs\"))\n-        self.assertTrue(hasattr(self.tool, \"output_type\"))\n-\n-        inputs = self.tool.inputs\n-        self.assertTrue(isinstance(inputs, dict))\n-\n-        for _, input_spec in inputs.items():\n-            self.assertTrue(\"type\" in input_spec)\n-            self.assertTrue(\"description\" in input_spec)\n-            self.assertTrue(input_spec[\"type\"] in AUTHORIZED_TYPES)\n-            self.assertTrue(isinstance(input_spec[\"description\"], str))\n-\n-        output_type = self.tool.output_type\n-        self.assertTrue(output_type in AUTHORIZED_TYPES)\n-\n-    def test_common_attributes(self):\n-        self.assertTrue(hasattr(self.tool, \"description\"))\n-        self.assertTrue(hasattr(self.tool, \"name\"))\n-        self.assertTrue(hasattr(self.tool, \"inputs\"))\n-        self.assertTrue(hasattr(self.tool, \"output_type\"))\n-\n-    def test_agent_type_output(self):\n-        inputs = create_inputs(self.tool.inputs)\n-        output = self.tool(**inputs)\n-        if self.tool.output_type != \"any\":\n-            agent_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n-            self.assertTrue(isinstance(output, agent_type))\n-\n-    def test_agent_types_inputs(self):\n-        inputs = create_inputs(self.tool.inputs)\n-        _inputs = []\n-        for _input, expected_input in zip(inputs, self.tool.inputs.values()):\n-            input_type = expected_input[\"type\"]\n-            _inputs.append(AGENT_TYPE_MAPPING[input_type](_input))\n-\n-\n-class ToolTests(unittest.TestCase):\n-    def test_tool_init_with_decorator(self):\n-        @tool\n-        def coolfunc(a: str, b: int) -> float:\n-            \"\"\"Cool function\n-\n-            Args:\n-                a: The first argument\n-                b: The second one\n-            \"\"\"\n-            return b + 2, a\n-\n-        assert coolfunc.output_type == \"number\"\n-\n-    def test_tool_init_vanilla(self):\n-        class HFModelDownloadsTool(Tool):\n-            name = \"model_download_counter\"\n-            description = \"\"\"\n-            This is a tool that returns the most downloaded model of a given task on the Hugging Face Hub.\n-            It returns the name of the checkpoint.\"\"\"\n-\n-            inputs = {\n-                \"task\": {\n-                    \"type\": \"string\",\n-                    \"description\": \"the task category (such as text-classification, depth-estimation, etc)\",\n-                }\n-            }\n-            output_type = \"integer\"\n-\n-            def forward(self, task):\n-                return \"best model\"\n-\n-        tool = HFModelDownloadsTool()\n-        assert list(tool.inputs.keys())[0] == \"task\"\n-\n-    def test_tool_init_decorator_raises_issues(self):\n-        with pytest.raises(Exception) as e:\n-\n-            @tool\n-            def coolfunc(a: str, b: int):\n-                \"\"\"Cool function\n-\n-                Args:\n-                    a: The first argument\n-                    b: The second one\n-                \"\"\"\n-                return a + b\n-\n-            assert coolfunc.output_type == \"number\"\n-        assert \"Tool return type not found\" in str(e)\n-\n-        with pytest.raises(Exception) as e:\n-\n-            @tool\n-            def coolfunc(a: str, b: int) -> int:\n-                \"\"\"Cool function\n-\n-                Args:\n-                    a: The first argument\n-                \"\"\"\n-                return b + a\n-\n-            assert coolfunc.output_type == \"number\"\n-        assert \"docstring has no description for the argument\" in str(e)"
        },
        {
            "sha": "bd045b4335db9ee103ca6c3ccf1a043312b61558",
            "filename": "tests/agents/test_translation.py",
            "status": "removed",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e6ddf27fdfd7835ab8857742915d4b8558841e/tests%2Fagents%2Ftest_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_translation.py?ref=69e6ddf27fdfd7835ab8857742915d4b8558841e",
            "patch": "@@ -1,66 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers import load_tool\n-from transformers.agents.agent_types import AGENT_TYPE_MAPPING\n-\n-from .test_tools_common import ToolTesterMixin, output_type\n-\n-\n-class TranslationToolTester(unittest.TestCase, ToolTesterMixin):\n-    def setUp(self):\n-        self.tool = load_tool(\"translation\")\n-        self.tool.setup()\n-        self.remote_tool = load_tool(\"translation\", remote=True)\n-\n-    def test_exact_match_arg(self):\n-        result = self.tool(\"Hey, what's up?\", src_lang=\"English\", tgt_lang=\"French\")\n-        self.assertEqual(result, \"- HÃ©, comment Ã§a va?\")\n-\n-    def test_exact_match_kwarg(self):\n-        result = self.tool(text=\"Hey, what's up?\", src_lang=\"English\", tgt_lang=\"French\")\n-        self.assertEqual(result, \"- HÃ©, comment Ã§a va?\")\n-\n-    def test_call(self):\n-        inputs = [\"Hey, what's up?\", \"English\", \"Spanish\"]\n-        output = self.tool(*inputs)\n-\n-        self.assertEqual(output_type(output), self.tool.output_type)\n-\n-    def test_agent_type_output(self):\n-        inputs = [\"Hey, what's up?\", \"English\", \"Spanish\"]\n-        output = self.tool(*inputs)\n-        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n-        self.assertTrue(isinstance(output, output_type))\n-\n-    def test_agent_types_inputs(self):\n-        example_inputs = {\n-            \"text\": \"Hey, what's up?\",\n-            \"src_lang\": \"English\",\n-            \"tgt_lang\": \"Spanish\",\n-        }\n-\n-        _inputs = []\n-        for input_name in example_inputs.keys():\n-            example_input = example_inputs[input_name]\n-            input_description = self.tool.inputs[input_name]\n-            input_type = input_description[\"type\"]\n-            _inputs.append(AGENT_TYPE_MAPPING[input_type](example_input))\n-\n-        # Should not raise an error\n-        output = self.tool(**example_inputs)\n-        output_type = AGENT_TYPE_MAPPING[self.tool.output_type]\n-        self.assertTrue(isinstance(output, output_type))"
        },
        {
            "sha": "e246a1bc4d66d88bb61c9c4ae9cbe72e33f6bda3",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=aaf129cdaed0f339e1e3ef12bfe89f1e3ba4139b",
            "patch": "@@ -2,8 +2,6 @@ docs/source/en/_config.py\n docs/source/en/accelerate.md\n docs/source/en/add_new_model.md\n docs/source/en/add_new_pipeline.md\n-docs/source/en/agents.md\n-docs/source/en/agents.md\n docs/source/en/attention.md\n docs/source/en/community.md\n docs/source/en/contributing.md\n@@ -23,7 +21,6 @@ docs/source/en/internal/time_series_utils.md\n docs/source/en/internal/tokenization_utils.md\n docs/source/en/internal/trainer_utils.md\n docs/source/en/llm_tutorial.md\n-docs/source/en/main_classes/agent.md\n docs/source/en/main_classes/callback.md\n docs/source/en/main_classes/configuration.md\n docs/source/en/main_classes/data_collator.md\n@@ -320,17 +317,6 @@ docs/source/en/training.md\n docs/source/en/troubleshooting.md\n src/transformers/activations.py\n src/transformers/activations_tf.py\n-src/transformers/agents/agent_types.py\n-src/transformers/agents/agents.py\n-src/transformers/agents/document_question_answering.py\n-src/transformers/agents/evaluate_agent.py\n-src/transformers/agents/image_question_answering.py\n-src/transformers/agents/prompts.py\n-src/transformers/agents/python_interpreter.py\n-src/transformers/agents/speech_to_text.py\n-src/transformers/agents/text_to_speech.py\n-src/transformers/agents/tools.py\n-src/transformers/agents/translation.py\n src/transformers/audio_utils.py\n src/transformers/commands/add_new_model_like.py\n src/transformers/commands/convert.py"
        }
    ],
    "stats": {
        "total": 10920,
        "additions": 4,
        "deletions": 10916
    }
}