{
    "author": "CezaPasc",
    "message": "fix(wandb): pass fake dataset to avoid exception in trainer (see #34455) (#34720)",
    "sha": "7b3d615bc23c40daa9f38dbae0abd979936f6414",
    "files": [
        {
            "sha": "4b236b9155f15892ca281f7da8797c99939ed2d2",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7b3d615bc23c40daa9f38dbae0abd979936f6414/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7b3d615bc23c40daa9f38dbae0abd979936f6414/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=7b3d615bc23c40daa9f38dbae0abd979936f6414",
            "patch": "@@ -918,7 +918,7 @@ def on_train_end(self, args, state, control, model=None, tokenizer=None, **kwarg\n         if self._log_model.is_enabled and self._initialized and state.is_world_process_zero:\n             from ..trainer import Trainer\n \n-            fake_trainer = Trainer(args=args, model=model, processing_class=tokenizer)\n+            fake_trainer = Trainer(args=args, model=model, processing_class=tokenizer, eval_dataset=[\"fake\"])\n             with tempfile.TemporaryDirectory() as temp_dir:\n                 fake_trainer.save_model(temp_dir)\n                 metadata = ("
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}