{
    "author": "Abdennacer-Badaoui",
    "message": "Fix gpt2 modeling tests (#42321)\n\n* fix gpt2 modeling tests\n\n* use Expectations\n\n* ruff format",
    "sha": "923cebd2845f339ffb04b614275e807dd6b0b0a8",
    "files": [
        {
            "sha": "b2309a108853daf24fc1bb80f279c1c41c02bf60",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/923cebd2845f339ffb04b614275e807dd6b0b0a8/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/923cebd2845f339ffb04b614275e807dd6b0b0a8/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=923cebd2845f339ffb04b614275e807dd6b0b0a8",
            "patch": "@@ -171,6 +171,7 @@ class GPT2ModelTest(CausalLMModelTest, unittest.TestCase):\n     )\n     test_missing_keys = False\n     model_tester_class = GPT2ModelTester\n+    model_split_percents = [0.5, 0.6, 0.7]\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # Overwritten: special case for DoubleHeads model\n@@ -437,10 +438,18 @@ def test_flash_attn_2_generate_padding_left(self):\n         output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n         output_fa_2 = tokenizer.batch_decode(output_fa_2)\n \n-        expected_output = [\n-            \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>hi, who was born in the city of Kolkata, was a member of the Kolkata\",\n-            \"Hello this is a very long sentence. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry\",\n-        ]\n+        expected_output = Expectations(\n+            {\n+                (\"cuda\", (8, 6)): [\n+                    \"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>hi, who was born in the city of Kolkata, was a member of the Kolkata\",\n+                    \"Hello this is a very long sentence. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry\",\n+                ],\n+                (\"rocm\", (9, 4)): [\n+                    '<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>hi, who was also a member of the group, said: \"We are very happy to have been',\n+                    \"Hello this is a very long sentence. I'm sorry. I'm sorry. I'm sorry. I'm sorry. I'm sorry\",\n+                ],\n+            }\n+        ).get_expectation()\n \n         self.assertListEqual(output_native, output_fa_2)\n         self.assertListEqual(output_native, expected_output)\n@@ -491,7 +500,9 @@ def test_batch_generation(self):\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+        output_padded = model.generate(\n+            input_ids=inputs_padded, max_length=model.generation_config.max_length - num_paddings\n+        )\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)\n@@ -553,7 +564,9 @@ def test_batch_generation_2heads(self):\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+        output_padded = model.generate(\n+            input_ids=inputs_padded, max_length=model.generation_config.max_length - num_paddings\n+        )\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 19,
        "deletions": 6
    }
}