{
    "author": "MekkCyber",
    "message": "Fixing _pre_quantization_dtype when torch_dtype is None (#36930)\n\nfix",
    "sha": "be2c0e7bff1735c5771d007ed5bcc04e9ea86a96",
    "files": [
        {
            "sha": "ec3b37404d571cecab127d7a601f3188868f80fe",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/be2c0e7bff1735c5771d007ed5bcc04e9ea86a96/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be2c0e7bff1735c5771d007ed5bcc04e9ea86a96/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=be2c0e7bff1735c5771d007ed5bcc04e9ea86a96",
            "patch": "@@ -4454,7 +4454,7 @@ def from_pretrained(\n             # once the weights have been quantized\n             # Note that once you have loaded a quantized model, you can't change its dtype so this will\n             # remain a single source of truth\n-            config._pre_quantization_dtype = torch_dtype\n+            config._pre_quantization_dtype = torch_dtype if torch_dtype is not None else torch.get_default_dtype()\n \n         # Prepare the full device map\n         if device_map is not None:"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}