{
    "author": "zucchini-nlp",
    "message": "Chat template: return vectorized output in processors (#34275)\n\n* update chat template\r\n\r\n* style\r\n\r\n* fix tests\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* typehints + docs\r\n\r\n* fix tests\r\n\r\n* remove unnecessary warnings\r\n\r\n* forgot code style :(\r\n\r\n* allow users to pass backend and num frames\r\n\r\n* Update docs/source/en/chat_templating.md\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/image_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/processing_utils.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* typo fix\r\n\r\n* style\r\n\r\n* address comments\r\n\r\n* align with \"pipeline\" template\r\n\r\n* update docs\r\n\r\n* update docs\r\n\r\n* unpack for all kwargs?\r\n\r\n* wrong conflict resolution while rebasing\r\n\r\n* tmp\r\n\r\n* update docs\r\n\r\n* Update docs/source/en/chat_templating.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_templating.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_templating.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update docs/source/en/chat_templating.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "e0646f3dce229a2552c56746eac618baf8210efa",
    "files": [
        {
            "sha": "2c6d1967137185a38d47c6f424beb91a93d7cdb7",
            "filename": "benchmark.py",
            "status": "added",
            "additions": 132,
            "deletions": 0,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/benchmark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/benchmark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -0,0 +1,132 @@\n+import os\n+import time\n+\n+import cv2\n+import av\n+import numpy as np \n+from numba import jit, cuda\n+from decord import VideoReader, cpu, gpu\n+\n+import torch\n+from torchvision import io\n+\n+\n+video_dir = \"/raid/raushan/temp_dir/\"\n+NUM_FRAMES = 32\n+\n+\n+# @jit(nopython=True, target_backend='cuda') # <-- If you have a cuda GPU\n+def process_video_cv2(video: cv2.VideoCapture, indices: np.array, length: int):\n+    index = 0\n+    frames = []\n+    while video.isOpened():\n+        success, frame = video.read()\n+        if index in indices:\n+            # Channel 0:B 1:G 2:R\n+            height, width, channel = frame.shape\n+            frames.append(frame[0:height, 0:width, 0:channel])\n+        if success:\n+            index += 1\n+        if index >= length:\n+            break\n+\n+    video.release()\n+    return frames\n+\n+\n+def read_video_opencv(video_path, num_frames=NUM_FRAMES):\n+    '''\n+    Decode the video with open-cv decoder.\n+\n+    Args:\n+        video_path (str): Path to the video file.\n+        num_frames (int): Number of frames to sample uniformly. Defaults to NUM_FRAMES\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    '''\n+    video = cv2.VideoCapture(video_path)\n+    fps = int(video.get(cv2.CAP_PROP_FPS))\n+    total_num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n+    indices = np.arange(0, total_num_frames, total_num_frames / num_frames).astype(int)\n+    frames = process_video_cv2(video, indices, total_num_frames)\n+    return np.stack(frames)\n+\n+\n+\n+def read_video_decord(video_path, num_frames=NUM_FRAMES):\n+    '''\n+    Decode the video with Decord decoder.\n+\n+    Args:\n+        video_path (str): Path to the video file.\n+        num_frames (int): Number of frames to sample uniformly. Defaults to NUM_FRAMES\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    '''\n+    vr = VideoReader(uri=video_path, ctx=cpu(0)) # you need to install from source to use gpu ctx\n+    indices = np.arange(0, len(vr), len(vr) / num_frames).astype(int)\n+    frames = vr.get_batch(indices).asnumpy()\n+    return frames\n+\n+\n+def read_video_pyav(video_path, num_frames=NUM_FRAMES):\n+    '''\n+    Decode the video with PyAV decoder.\n+\n+    Args:\n+        video_path (str): Path to the video file.\n+        num_frames (int): Number of frames to sample uniformly. Defaults to NUM_FRAMES\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    '''\n+    container = av.open(video_path)\n+\n+    # sample uniformly \"num_frames\" frames from the video\n+    total_frames = container.streams.video[0].frames\n+    indices = np.arange(0, total_frames, total_frames / num_frames).astype(int)\n+\n+    frames = []\n+    container.seek(0)\n+    start_index = indices[0]\n+    end_index = indices[-1]\n+    for i, frame in enumerate(container.decode(video=0)):\n+        if i > end_index:\n+            break\n+        if i >= start_index and i in indices:\n+            frames.append(frame)\n+    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+\n+\n+\n+def read_video_torchvision(video_path, num_frames=NUM_FRAMES):\n+    video, _, info = io.read_video(\n+        video_path,\n+        start_pts=0.0,\n+        end_pts=None,\n+        pts_unit=\"sec\",\n+        output_format=\"TCHW\",\n+    )\n+\n+    idx = torch.linspace(0, video.size(0) - 1, num_frames, dtype=torch.int64)\n+    return video[idx]\n+\n+\n+decoders = {\"decord\": read_video_decord, \"opencv\": read_video_opencv, \"av\": read_video_pyav, \"torchvision\": read_video_torchvision}\n+for name, fn in decoders.items():\n+    start = time.perf_counter()\n+    for video_file in os.listdir(video_dir):\n+        path = f\"{video_dir}/{video_file}\"\n+        output = fn(path)\n+\n+    end = time.perf_counter()\n+    print(f\"Time taken for {name}: {(end-start):.04f} sec\")\n+\n+\n+# Time taken for decord: 475.2979 sec\n+# Time taken for opencv: 614.6062 sec\n+# Time taken for av: 1067.0860 sec\n+# Time taken for torchvision: 1924.0433 sec\n+"
        },
        {
            "sha": "c44420543cf89f0f6c83399c7894606579641451",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 42,
            "deletions": 2,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -23,7 +23,7 @@ of text (as is the case with a standard language model), the model instead conti\n of one or more **messages**, each of which includes a **role**, like \"user\" or \"assistant\", as well as message text.\n \n Much like tokenization, different models expect very different input formats for chat. This is the reason we added\n-**chat templates** as a feature. Chat templates are part of the tokenizer. They specify how to convert conversations, \n+**chat templates** as a feature. Chat templates are part of the tokenizer for text-only LLMs or processor for multimodal LLMs. They specify how to convert conversations, \n represented as lists of messages, into a single tokenizable string in the format that the model expects. \n \n Let's make this concrete with a quick example using the `mistralai/Mistral-7B-Instruct-v0.1` model:\n@@ -66,10 +66,12 @@ for you, allowing you to write universal code that works for any model.\n ## How do I use chat templates?\n \n As you can see in the example above, chat templates are easy to use. Simply build a list of messages, with `role`\n-and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_template`] method. Once you do that,\n+and `content` keys, and then pass it to the [`~PreTrainedTokenizer.apply_chat_template`] or [`~ProcessorMixin.apply_chat_template`] method\n+depending on what type of model you are using. Once you do that,\n you'll get output that's ready to go! When using chat templates as input for model generation, it's also a good idea\n to use `add_generation_prompt=True` to add a [generation prompt](#what-are-generation-prompts). \n \n+## Usage with text-only LLMs\n Here's an example of preparing input for `model.generate()`, using `Zephyr` again:\n \n ```python\n@@ -116,6 +118,44 @@ How many helicopters can a human eat in one sitting?</s>\n Matey, I'm afraid I must inform ye that humans cannot eat helicopters. Helicopters are not food, they are flying machines. Food is meant to be eaten, like a hearty plate o' grog, a savory bowl o' stew, or a delicious loaf o' bread. But helicopters, they be for transportin' and movin' around, not for eatin'. So, I'd say none, me hearties. None at all.\n ```\n \n+## Usage with multimodal LLMs\n+\n+For multimodal LLMs such as [LLaVA](https://huggingface.co/llava-hf) the prompts can be formatted in a similar way. The only difference is you need to pass input images/videos as well along with the text. Each `\"content\"`\n+has to be a list containing either a text or an image/video.\n+\n+Here's an example of preparing input for using `LLaVA` model:\n+\n+```python\n+from transformers import AutoProcessor, LlavaOnevisionForConditionalGeneration\n+\n+model_id = \"llava-hf/llava-onevision-qwen2-0.5b-ov-hf\"\n+model = LlavaOnevisionForConditionalGeneration.from_pretrained(model_id)  # You may want to use bfloat16 and/or move to GPU here\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+      \"role\": \"user\",\n+      \"content\": [\n+          {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+          {\"type\": \"text\", \"text\": \"What are these?\"},\n+        ],\n+    },\n+]\n+\n+processed_chat = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\")\n+print(processor.batch_decode(processed_chat[\"input_ids\"][:, :30]))\n+```\n+This yields a string in LLaVAs expected input format with many `<image>` tokens at the end.\n+The `<image>` tokens are placeholders and each one will be replaced by image embeddings when the mode is run in the forward call. The `processed_chat` can be further passed into [`~GenerationMixin.generate`] to generate text.\n+```text\n+'<|im_start|>system \n+You are a friendly chatbot who always responds in the style of a pirate<|im_end|><|im_start|>user <image><image><image><image><image><image><image><image>'\n+```\n+\n Arr, 'twas easy after all!\n \n ## Is there an automated pipeline for chat?"
        },
        {
            "sha": "25e201a6e48d83f8444e55e00c410524a6e003a7",
            "filename": "read_video.py",
            "status": "added",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/read_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/read_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/read_video.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -0,0 +1,77 @@\n+import numpy as np\n+import cv2\n+import requests\n+from yt_dlp import YoutubeDL\n+from contextlib import redirect_stdout\n+from pathlib import Path\n+import io\n+import imageio.v3 as iio\n+\n+\n+url = \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"\n+vid = cv2.VideoCapture(url)\n+# ret, frame = vid.read()\n+\n+while(True):\n+    # Capture frame-by-frame\n+    ret, frame = vid.read()\n+    #print cap.isOpened(), ret\n+    if frame is not None:\n+        pass\n+        # print(frame.shape)\n+    else:\n+        break\n+\n+print(vid.isOpened(), frame is not None)\n+\n+buffer = io.BytesIO(requests.get(url).content)\n+video = buffer.getvalue()\n+frames = iio.imread(video, index=None)\n+print(frames.shape)\n+\n+\n+\n+\n+\n+youtube_id = \"https://www.youtube.com/watch?v=BaW_jenozKc\"\n+\n+ctx = {\n+    \"outtmpl\": \"-\",\n+    'logtostderr': True\n+}\n+\n+buffer = io.BytesIO()\n+with redirect_stdout(buffer), YoutubeDL(ctx) as foo:\n+    foo.download([youtube_id])\n+# Path(f\"vi.mp4\").write_bytes(buffer.getvalue())\n+\n+video = buffer.getvalue()\n+print(type(video))\n+frames = iio.imread(video, index=None)\n+print(frames.shape)\n+\n+\n+import decord\n+file_obj = io.BytesIO(video)\n+container = decord.VideoReader(file_obj)\n+print(container[2].shape)\n+\n+# print(np.frombuffer(video, dtype=np.uint8).shape)\n+# img_array = np.asarray(bytearray(video), dtype=np.uint8)\n+# im = cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\n+\n+\n+\n+import av\n+\n+file_obj = io.BytesIO(video)\n+container = av.open(file_obj)\n+container.seek(0)\n+frames = []\n+for i, frame in enumerate(container.decode(video=0)):\n+    if i > 10:\n+        break\n+    if i >= 0:\n+        frames.append(frame)\n+out = np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+print(out.shape)"
        },
        {
            "sha": "b79ba1ecf3fb9f5c7f5e37b9f2ff5d5821da9057",
            "filename": "run.py",
            "status": "added",
            "additions": 107,
            "deletions": 0,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/run.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/run.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/run.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -0,0 +1,107 @@\n+import av\n+import torch\n+import decord\n+from decord import VideoReader, cpu\n+\n+import numpy as np\n+from PIL import Image\n+from huggingface_hub import hf_hub_download\n+from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration, SiglipImageProcessor\n+\n+model_id = \"/raid/raushan/llava-next-video-qwen-7b\"\n+\n+model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n+    model_id, \n+    torch_dtype=torch.bfloat16, \n+    low_cpu_mem_usage=True, \n+).to(0)\n+\n+processor = LlavaNextVideoProcessor.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n+img_proc = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n+\n+image = Image.open(\"/raid/raushan/image.png\")\n+\n+\n+def load_video(video_path, max_frames_num,fps=1,force_sample=False):\n+\n+    vr = VideoReader(video_path)\n+    total_frame_num = len(vr)\n+    video_time = total_frame_num / vr.get_avg_fps()\n+    fps = round(vr.get_avg_fps()/fps)\n+    frame_idx = [i for i in range(0, len(vr), fps)]\n+    frame_time = [i/fps for i in frame_idx]\n+    if len(frame_idx) > max_frames_num or force_sample:\n+        sample_fps = max_frames_num\n+        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)\n+        frame_idx = uniform_sampled_frames.tolist()\n+        frame_time = [i/vr.get_avg_fps() for i in frame_idx]\n+    frame_time = \",\".join([f\"{i:.2f}s\" for i in frame_time])\n+    spare_frames = vr.get_batch(frame_idx).asnumpy()\n+    print(spare_frames.shape)\n+    return spare_frames,frame_time,video_time\n+\n+\n+def read_video_pyav(container, indices):\n+    '''\n+    Decode the video with PyAV decoder.\n+    Args:\n+        container (`av.container.input.InputContainer`): PyAV container.\n+        indices (`List[int]`): List of frame indices to decode.\n+    Returns:\n+        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n+    '''\n+    frames = []\n+    container.seek(0)\n+    start_index = indices[0]\n+    end_index = indices[-1]\n+    for i, frame in enumerate(container.decode(video=0)):\n+        if i > end_index:\n+            break\n+        if i >= start_index and i in indices:\n+            frames.append(frame)\n+    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+\n+\n+# define a chat history and use `apply_chat_template` to get correctly formatted prompt\n+# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\") \n+# <|im_start|>system\n+# You are a helpful assistant.<|im_end|>\n+# <|im_start|>user\n+# <image>Time farmes are this moments and we ahev 64 frames\n+# Please describe this video in detail.<|im_end|>\n+# <|im_start|>assistant\n+\n+conversation = [\n+    {\n+\n+        \"role\": \"system\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n+            ],\n+    },\n+    {\n+\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"The video lasts for 19.97 seconds, and 64 frames are uniformly sampled from it. These frames are located at 0.00s,0.30s,0.60s,0.93s,1.23s,1.57s,1.87s,2.20s,2.50s,2.83s,3.13s,3.47s,3.77s,4.10s,4.40s,4.73s,5.03s,5.37s,5.67s,6.00s,6.30s,6.63s,6.93s,7.27s,7.57s,7.90s,8.20s,8.53s,8.83s,9.17s,9.47s,9.80s,10.10s,10.43s,10.73s,11.07s,11.37s,11.70s,12.00s,12.33s,12.63s,12.97s,13.27s,13.60s,13.90s,14.23s,14.53s,14.87s,15.17s,15.50s,15.80s,16.13s,16.43s,16.77s,17.07s,17.40s,17.70s,18.03s,18.33s,18.67s,18.97s,19.30s,19.60s,19.93s.Please answer the following questions related to this video.\\nPlease describe this video in detail.\"},\n+            {\"type\": \"video\"},\n+            ],\n+    },\n+]\n+\n+prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<video>The video lasts for 19.97 seconds, and 64 frames are uniformly sampled from it. These frames are located at 0.00s,0.30s,0.60s,0.93s,1.23s,1.57s,1.87s,2.20s,2.50s,2.83s,3.13s,3.47s,3.77s,4.10s,4.40s,4.73s,5.03s,5.37s,5.67s,6.00s,6.30s,6.63s,6.93s,7.27s,7.57s,7.90s,8.20s,8.53s,8.83s,9.17s,9.47s,9.80s,10.10s,10.43s,10.73s,11.07s,11.37s,11.70s,12.00s,12.33s,12.63s,12.97s,13.27s,13.60s,13.90s,14.23s,14.53s,14.87s,15.17s,15.50s,15.80s,16.13s,16.43s,16.77s,17.07s,17.40s,17.70s,18.03s,18.33s,18.67s,18.97s,19.30s,19.60s,19.93s.Please answer the following questions related to this video.\\nPlease describe this video in detail.<|im_end|>\\n<|im_start|>assistant\"\n+\n+video_path = \"/raid/raushan/karate.mp4\" # hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n+container = av.open(video_path)\n+\n+# sample uniformly 8 frames from the video, can sample more for longer videos\n+total_frames = container.streams.video[0].frames\n+indices = np.arange(0, total_frames, total_frames / 64).astype(int)\n+clip = read_video_pyav(container, indices)\n+\n+clip, frame_time,video_time = load_video(video_path, max_frames_num=64, force_sample=True)\n+inputs_video = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(device=model.device, dtype=torch.bfloat16)\n+\n+output = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\n+print(processor.decode(output[0][2:], skip_special_tokens=True))"
        },
        {
            "sha": "90b5f44c563938c7df1191490ae17ec8f6fa380b",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 215,
            "deletions": 0,
            "changes": 215,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -15,6 +15,7 @@\n \n import base64\n import os\n+from contextlib import redirect_stdout\n from io import BytesIO\n from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple, Union\n \n@@ -25,13 +26,17 @@\n from .utils import (\n     ExplicitEnum,\n     TensorType,\n+    is_av_available,\n+    is_cv2_available,\n+    is_decord_available,\n     is_jax_tensor,\n     is_numpy_array,\n     is_tf_tensor,\n     is_torch_available,\n     is_torch_tensor,\n     is_torchvision_available,\n     is_vision_available,\n+    is_yt_dlp_available,\n     logging,\n     requires_backends,\n     to_numpy,\n@@ -56,6 +61,7 @@\n         PILImageResampling = PIL.Image\n \n     if is_torchvision_available():\n+        from torchvision import io as torchvision_io\n         from torchvision.transforms import InterpolationMode\n \n         pil_torch_interpolation_mapping = {\n@@ -67,6 +73,17 @@\n             PILImageResampling.LANCZOS: InterpolationMode.LANCZOS,\n         }\n \n+if is_decord_available():\n+    from decord import VideoReader, cpu\n+\n+if is_av_available():\n+    import av\n+\n+if is_cv2_available():\n+    import cv2\n+\n+if is_yt_dlp_available():\n+    from yt_dlp import YoutubeDL\n \n if TYPE_CHECKING:\n     if is_torch_available():\n@@ -386,6 +403,204 @@ def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] =\n     return image\n \n \n+def get_uniform_frame_indices(total_num_frames: int, num_frames: Optional[int] = None):\n+    \"\"\"\n+    Creates a numpy array for uniform sampling of `num_frame` frames from `total_num_frames`\n+    when loading a video.\n+\n+    Args:\n+        total_num_frames (`int`):\n+            Total number of frames that a video has.\n+        num_frames (`int`, *optional*):\n+            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+\n+    Returns:\n+        np.ndarray: np array of frame indices that will be sampled.\n+    \"\"\"\n+    if num_frames is not None:\n+        indices = np.arange(0, total_num_frames, total_num_frames / num_frames).astype(int)\n+    else:\n+        indices = np.arange(0, total_num_frames).astype(int)\n+    return indices\n+\n+\n+def read_video_opencv(video_path: str, num_frames: Optional[int] = None):\n+    \"\"\"\n+    Decode the video with open-cv decoder.\n+\n+    Args:\n+        video_path (`str`):\n+            Path to the video file.\n+        num_frames (`int`, *optional*):\n+            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    \"\"\"\n+    video = cv2.VideoCapture(video_path)\n+    total_num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n+    indices = get_uniform_frame_indices(total_num_frames, num_frames=num_frames)\n+\n+    index = 0\n+    frames = []\n+    while video.isOpened():\n+        success, frame = video.read()\n+        if index in indices:\n+            height, width, channel = frame.shape\n+            frames.append(frame[0:height, 0:width, 0:channel])\n+        if success:\n+            index += 1\n+        if index >= total_num_frames:\n+            break\n+\n+    video.release()\n+    return np.stack(frames)\n+\n+\n+def read_video_decord(video_path: str, num_frames: Optional[int] = None):\n+    \"\"\"\n+    Decode the video with Decord decoder.\n+\n+    Args:\n+        video_path (`str`):\n+            Path to the video file.\n+        num_frames (`int`, *optional*):\n+            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    \"\"\"\n+    vr = VideoReader(uri=video_path, ctx=cpu(0))  # decord has problems with gpu\n+    indices = get_uniform_frame_indices(total_num_frames=len(vr), num_frames=num_frames)\n+    frames = vr.get_batch(indices).asnumpy()\n+    return frames\n+\n+\n+def read_video_pyav(video_path: str, num_frames: Optional[int] = None):\n+    \"\"\"\n+    Decode the video with PyAV decoder.\n+\n+    Args:\n+        video_path (`str`):\n+            Path to the video file.\n+        num_frames (`int`, *optional*):\n+            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    \"\"\"\n+    container = av.open(video_path)\n+\n+    # sample uniformly \"num_frames\" frames from the video\n+    total_num_frames = container.streams.video[0].frames\n+    indices = get_uniform_frame_indices(total_num_frames, num_frames=num_frames)\n+\n+    frames = []\n+    container.seek(0)\n+    end_index = indices[-1]\n+    for i, frame in enumerate(container.decode(video=0)):\n+        if i > end_index:\n+            break\n+        if i >= 0 and i in indices:\n+            frames.append(frame)\n+    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+\n+\n+def read_video_torchvision(video_path: str, num_frames: Optional[int] = None):\n+    \"\"\"\n+    Decode the video with torchvision decoder.\n+\n+    Args:\n+        video_path (`str`):\n+            Path to the video file.\n+        num_frames (`int`, *optional*):\n+            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+\n+    Returns:\n+        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+    \"\"\"\n+    video, _, info = torchvision_io.read_video(\n+        video_path,\n+        start_pts=0.0,\n+        end_pts=None,\n+        pts_unit=\"sec\",\n+        output_format=\"TCHW\",\n+    )\n+\n+    if num_frames is not None:\n+        idx = torch.linspace(0, video.size(0) - 1, num_frames, dtype=torch.int64)\n+        return video[idx]\n+\n+    return video\n+\n+\n+VIDEO_DECODERS = {\n+    \"decord\": read_video_decord,\n+    \"opencv\": read_video_opencv,\n+    \"pyav\": read_video_pyav,\n+    \"torchvision\": read_video_torchvision,\n+}\n+\n+\n+def load_video(video: Union[str, \"VideoInput\"], num_frames: Optional[int] = None, backend: str = \"opencv\") -> np.array:\n+    \"\"\"\n+    Loads `video` to a numpy array.\n+\n+    Args:\n+        video (`str` or `VideoInput`):\n+            The video to convert to the numpy array format. Can be a link to video or local path.\n+        num_frames (`int`, *optional*):\n+            Number of frames to sample uniformly. If not passed, the whole video is loaded.\n+        backend (`str`, *optional*, defaults to `\"opencv\"`):\n+            The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n+\n+    Returns:\n+        `np.array`: A numpy array of shape (num_frames, channels, height, width).\n+    \"\"\"\n+    if video.startswith(\"https://www.youtube.com\") or video.startswith(\"http://www.youtube.com\"):\n+        if not is_yt_dlp_available():\n+            raise ImportError(\"To load a video from YouTube url you have  to install `yt_dlp` first.\")\n+        buffer = BytesIO()\n+        with redirect_stdout(buffer), YoutubeDL() as f:\n+            f.download([video])\n+        bytes_obj = buffer.getvalue()\n+        file_obj = BytesIO(bytes_obj)\n+    elif video.startswith(\"http://\") or video.startswith(\"https://\"):\n+        file_obj = BytesIO(requests.get(video).content)\n+    elif os.path.isfile(video):\n+        file_obj = video\n+    elif is_valid_image(video) or (isinstance(video, (list, tuple) and is_valid_image(video[0]))):\n+        file_obj = None\n+    else:\n+        raise TypeError(\"Incorrect format used for video. Should be an url linking to an video or a local path.\")\n+\n+    # can also load with decord, but not cv2/torchvision\n+    # both will fail in case of url links\n+    video_is_url = video.startswith(\"http://\") or video.startswith(\"https://\")\n+    if video_is_url and backend in [\"opencv\", \"torchvision\"]:\n+        raise ValueError(\n+            \"If you are trying to load a video from URL, you can decode the video only with `pyav` or `decord` as backend\"\n+        )\n+\n+    if file_obj is None:\n+        return video\n+\n+    if (\n+        (not is_decord_available() and backend == \"decord\")\n+        or (not is_av_available() and backend == \"pyav\")\n+        or (not is_cv2_available() and backend == \"opencv\")\n+        or (not is_torchvision_available() and backend == \"torchvision\")\n+    ):\n+        raise ImportError(\n+            f\"You chose backend={backend} for loading the video but the required library is not found in your environment \"\n+            f\"Make sure to install {backend} before loading the video.\"\n+        )\n+\n+    video_decoder = VIDEO_DECODERS[backend]\n+    video = video_decoder(file_obj)\n+    return video\n+\n+\n def load_images(\n     images: Union[List, Tuple, str, \"PIL.Image.Image\"], timeout: Optional[float] = None\n ) -> Union[\"PIL.Image.Image\", List[\"PIL.Image.Image\"], List[List[\"PIL.Image.Image\"]]]:"
        },
        {
            "sha": "81760fb4087e1ccf0387f08f3bcb429dcb016a0c",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -164,7 +164,7 @@ def __call__(\n         if videos is not None:\n             video_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n \n-            one_video = to_numpy_array(video_inputs[\"pixel_values_videos\"][0])\n+            one_video = to_numpy_array(video_inputs.get(\"pixel_values_videos\")[0])\n             height, width = get_image_size(one_video[0], channel_dim=output_kwargs[\"images_kwargs\"].get(\"data_format\"))\n             num_frames = one_video.shape[0]  # frame dim is always after batch dim\n             patches_height_width = int(math.sqrt(self.num_image_tokens))"
        },
        {
            "sha": "611e2aa3f20cd07f9f60310a3f830c7fd04965c5",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 128,
            "deletions": 9,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -30,7 +30,7 @@\n import typing_extensions\n \n from .dynamic_module_utils import custom_object_save\n-from .image_utils import ChannelDimension, is_valid_image, is_vision_available\n+from .image_utils import ChannelDimension, is_valid_image, is_vision_available, load_image, load_video\n \n \n if is_vision_available():\n@@ -336,6 +336,64 @@ class CustomProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+class ChatTemplateKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for processor chat templates.\n+\n+    tokenize (`bool`, *optional*, defaults to `False`):\n+        Whether to tokenize the output or not.\n+    return_dict (`bool`, defaults to `False`):\n+        Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n+    tools (`List[Dict]`, *optional*):\n+        A list of tools (callable functions) that will be accessible to the model. If the template does not\n+        support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n+        giving the name, description and argument types for the tool. See our\n+        [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#automated-function-conversion-for-tool-use)\n+        for more information.\n+    documents (`List[Dict[str, str]]`, *optional*):\n+        A list of dicts representing documents that will be accessible to the model if it is performing RAG\n+        (retrieval-augmented generation). If the template does not support RAG, this argument will have no\n+        effect. We recommend that each document should be a dict containing \"title\" and \"text\" keys. Please\n+        see the RAG section of the [chat templating guide](https://huggingface.co/docs/transformers/main/en/chat_templating#arguments-for-RAG)\n+        for examples of passing documents with chat templates.\n+    add_generation_prompt (bool, *optional*):\n+        If this is set, a prompt with the token(s) that indicate\n+        the start of an assistant message will be appended to the formatted output. This is useful when you want to generate a response from the model.\n+        Note that this argument will be passed to the chat template, and so it must be supported in the\n+        template for this argument to have any effect.\n+    continue_final_message (bool, *optional*):\n+        If this is set, the chat will be formatted so that the final\n+        message in the chat is open-ended, without any EOS tokens. The model will continue this message\n+        rather than starting a new one. This allows you to \"prefill\" part of\n+        the model's response for it. Cannot be used at the same time as `add_generation_prompt`.\n+    return_assistant_tokens_mask (`bool`, defaults to `False`):\n+        Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n+        the mask will contain 1. For user and system tokens, the mask will contain 0.\n+        This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n+    num_frames (`int`, *optional*):\n+        Number of frames to sample uniformly. If not passed, the whole video is loaded.\n+    video_load_backend (`str`, *optional*, defaults to `\"pyav\"`):\n+        The backend to use when loading the video which will be used only when there are videos in the conversation.\n+        Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"pyav\" because it is the only backend\n+        that supports all types of sources to load from.\n+    \"\"\"\n+\n+    tokenize: Optional[bool] = False\n+    return_dict: Optional[bool] = False\n+    tools: Optional[List[Dict]] = None\n+    documents: Optional[List[Dict[str, str]]] = None\n+    add_generation_prompt: Optional[bool] = False\n+    continue_final_message: Optional[bool] = False\n+    return_assistant_tokens_mask: Optional[bool] = False\n+    num_frames: Optional[int] = None\n+    video_load_backend: Optional[str] = \"pyav\"\n+\n+\n+class AllKwargsForChatTemplate(\n+    TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs, ChatTemplateKwargs\n+): ...\n+\n+\n class ProcessorMixin(PushToHubMixin):\n     \"\"\"\n     This is a mixin used to provide saving/loading functionality for all processor classes.\n@@ -1100,23 +1158,32 @@ def apply_chat_template(\n         self,\n         conversation: Union[List[Dict[str, str]]],\n         chat_template: Optional[str] = None,\n-        tokenize: bool = False,\n-        **kwargs,\n+        **kwargs: Unpack[AllKwargsForChatTemplate],\n     ) -> str:\n         \"\"\"\n         Similar to the `apply_chat_template` method on tokenizers, this method applies a Jinja template to input\n         conversations to turn them into a single tokenizable string.\n \n+        The input is expected to be in the following format, where each message content is a list consisting of text and\n+        optionally image or video inputs. One can also provide an image, video, URL or local path which will be used to form\n+        `pixel_values` when `return_dict=True`. If not provided, one will get only the formatted text, optionally tokenized text.\n+\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"image\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"Please describe this image in detail.\"},\n+                ],\n+            },\n+        ]\n+\n         Args:\n             conversation (`List[Dict, str, str]`):\n                 The conversation to format.\n             chat_template (`Optional[str]`, *optional*):\n                 The Jinja template to use for formatting the conversation. If not provided, the tokenizer's\n                 chat template is used.\n-            tokenize (`bool`, *optional*, defaults to `False`):\n-                Whether to tokenize the output or not.\n-            **kwargs:\n-                Additional keyword arguments\n         \"\"\"\n \n         if chat_template is None:\n@@ -1128,10 +1195,62 @@ def apply_chat_template(\n                     \"or provide a chat template as an argument. See \"\n                     \"https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\"\n                 )\n-        return self.tokenizer.apply_chat_template(\n-            conversation, chat_template=chat_template, tokenize=tokenize, **kwargs\n+\n+        text_kwargs = {}\n+        for key in TextKwargs.__annotations__.keys():\n+            value = kwargs.pop(key, None)\n+            if value is not None:\n+                text_kwargs[key] = value\n+\n+        chat_template_kwargs = {}\n+        for key in ChatTemplateKwargs.__annotations__.keys():\n+            value = kwargs.pop(key, getattr(ChatTemplateKwargs, key))\n+            chat_template_kwargs[key] = value\n+\n+        # Pop kwargs that should not be used by tokenizer's `apply_chat_template`\n+        tokenize = chat_template_kwargs.pop(\"tokenize\")\n+        return_dict = chat_template_kwargs.pop(\"return_dict\")\n+        num_frames = chat_template_kwargs.pop(\"num_frames\")\n+        video_load_backend = chat_template_kwargs.pop(\"video_load_backend\")\n+\n+        prompt = self.tokenizer.apply_chat_template(\n+            conversation,\n+            chat_template=chat_template,\n+            tokenize=False,\n+            return_dict=False,\n+            **text_kwargs,\n+            **chat_template_kwargs,\n         )\n \n+        # we will have to return all processed inputs in a dict\n+        if tokenize:\n+            images, videos = [], []\n+            for message in conversation:\n+                visuals = [content for content in message[\"content\"] if content[\"type\"] in [\"image\", \"video\"]]\n+                for vision_info in visuals:\n+                    if vision_info[\"type\"] == \"image\":\n+                        for key in [\"image\", \"url\", \"path\", \"base64\"]:\n+                            if key in vision_info:\n+                                images.append(load_image(vision_info[key]))\n+                    elif vision_info[\"type\"] == \"video\":\n+                        for key in [\"video\", \"url\", \"path\"]:\n+                            if key in vision_info:\n+                                videos.append(\n+                                    load_video(vision_info[key], num_frames=num_frames, backend=video_load_backend)\n+                                )\n+\n+            out = self(\n+                text=prompt,\n+                images=images if images else None,\n+                videos=videos if videos else None,\n+                **kwargs,\n+            )\n+            if return_dict:\n+                return out\n+            else:\n+                return out[\"input_ids\"]\n+        return prompt\n+\n     def post_process_image_text_to_text(self, generated_outputs):\n         \"\"\"\n         Post-process the output of a vlm to decode the text."
        },
        {
            "sha": "8dce39fcf617a51a7e2dff2241c2b6f9116dde94",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -131,6 +131,7 @@\n     is_cv2_available,\n     is_cython_available,\n     is_datasets_available,\n+    is_decord_available,\n     is_detectron2_available,\n     is_eetq_available,\n     is_essentia_available,\n@@ -236,6 +237,7 @@\n     is_uroman_available,\n     is_vision_available,\n     is_vptq_available,\n+    is_yt_dlp_available,\n     requires_backends,\n     torch_only_method,\n )"
        },
        {
            "sha": "707903c702c78a7957943d02ba200098a4355149",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -101,6 +101,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _aqlm_available = _is_package_available(\"aqlm\")\n _vptq_available, _vptq_version = _is_package_available(\"vptq\", return_version=True)\n _av_available = importlib.util.find_spec(\"av\") is not None\n+_decord_available = importlib.util.find_spec(\"decord\") is not None\n _bitsandbytes_available = _is_package_available(\"bitsandbytes\")\n _eetq_available = _is_package_available(\"eetq\")\n _fbgemm_gpu_available = _is_package_available(\"fbgemm_gpu\")\n@@ -113,6 +114,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _coloredlogs_available = _is_package_available(\"coloredlogs\")\n # `importlib.metadata.util` doesn't work with `opencv-python-headless`.\n _cv2_available = importlib.util.find_spec(\"cv2\") is not None\n+_yt_dlp_available = importlib.util.find_spec(\"yt_dlp\") is not None\n _datasets_available = _is_package_available(\"datasets\")\n _detectron2_available = _is_package_available(\"detectron2\")\n # We need to check both `faiss` and `faiss-cpu`.\n@@ -313,6 +315,10 @@ def is_cv2_available():\n     return _cv2_available\n \n \n+def is_yt_dlp_available():\n+    return _yt_dlp_available\n+\n+\n def is_torch_available():\n     return _torch_available\n \n@@ -841,6 +847,10 @@ def is_av_available():\n     return _av_available\n \n \n+def is_decord_available():\n+    return _decord_available\n+\n+\n def is_ninja_available():\n     r\"\"\"\n     Code comes from *torch.utils.cpp_extension.is_ninja_available()*. Returns `True` if the\n@@ -1276,6 +1286,22 @@ def is_triton_available():\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n+# docstyle-ignore\n+YT_DLP_IMPORT_ERROR = \"\"\"\n+{0} requires the YT-DLP library but it was not found in your environment. You can install it with:\n+```\n+pip install yt-dlp\n+```\n+Please note that you may need to restart your runtime after installation.\n+\"\"\"\n+\n+DECORD_IMPORT_ERROR = \"\"\"\n+{0} requires the PyAv library but it was not found in your environment. You can install it with:\n+```\n+pip install decord\n+```\n+Please note that you may need to restart your runtime after installation.\n+\"\"\"\n \n # docstyle-ignore\n CV2_IMPORT_ERROR = \"\"\"\n@@ -1616,6 +1642,7 @@ def is_triton_available():\n         (\"bs4\", (is_bs4_available, BS4_IMPORT_ERROR)),\n         (\"cv2\", (is_cv2_available, CV2_IMPORT_ERROR)),\n         (\"datasets\", (is_datasets_available, DATASETS_IMPORT_ERROR)),\n+        (\"decord\", (is_decord_available, DECORD_IMPORT_ERROR)),\n         (\"detectron2\", (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n         (\"essentia\", (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n         (\"faiss\", (is_faiss_available, FAISS_IMPORT_ERROR)),\n@@ -1654,6 +1681,7 @@ def is_triton_available():\n         (\"jieba\", (is_jieba_available, JIEBA_IMPORT_ERROR)),\n         (\"peft\", (is_peft_available, PEFT_IMPORT_ERROR)),\n         (\"jinja\", (is_jinja_available, JINJA_IMPORT_ERROR)),\n+        (\"yt_dlp\", (is_yt_dlp_available, YT_DLP_IMPORT_ERROR)),\n     ]\n )\n "
        },
        {
            "sha": "fd0ba8cacc186f0909b5169ee788023777307404",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 54,
            "deletions": 2,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -17,15 +17,18 @@\n import unittest\n \n from transformers import AutoProcessor, AutoTokenizer, LlamaTokenizerFast, LlavaProcessor\n-from transformers.testing_utils import require_vision\n-from transformers.utils import is_vision_available\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n     from transformers import CLIPImageProcessor\n \n+if is_torch_available:\n+    import torch\n+\n \n @require_vision\n class LlavaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n@@ -94,6 +97,55 @@ def test_chat_template(self):\n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n \n+    def test_chat_template_dict(self):\n+        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = [[1, 3148, 1001, 29901, 29871, 32000, 29871, 13, 5618, 338, 4318, 297, 445, 1967, 29973, 319, 1799, 9047, 13566, 29901]]  # fmt: skip\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # add image URL for return dict\n+        messages[0][\"content\"][0] = {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        out_dict_with_image = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True\n+        )\n+        self.assertListEqual(list(out_dict_with_image.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n+\n+    @require_torch\n+    def test_chat_template_dict_torch(self):\n+        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+\n+        out_dict_tensors = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+        self.assertListEqual(list(out_dict_tensors.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertTrue(isinstance(out_dict_tensors[\"input_ids\"], torch.Tensor))\n+\n     def test_chat_template_with_continue_final_message(self):\n         processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n         expected_prompt = \"USER: <image>\\nDescribe this image. ASSISTANT: There is a dog and\""
        },
        {
            "sha": "04aafa11a8b04ec57550ee7f9002001586f67e0a",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 62,
            "deletions": 2,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -16,8 +16,8 @@\n import tempfile\n import unittest\n \n-from transformers.testing_utils import require_vision\n-from transformers.utils import is_vision_available\n+from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -31,6 +31,9 @@\n         Qwen2TokenizerFast,\n     )\n \n+if is_torch_available:\n+    import torch\n+\n \n @require_vision\n class LlavaOnevisionProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n@@ -100,3 +103,60 @@ def test_chat_template(self):\n \n         formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    @require_av\n+    def test_chat_template_dict(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"video\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        expected_output = [[151644, 872, 220, 151647, 198, 3838, 374, 6839, 304, 419, 2766, 30, 151645, 151644, 77091, 198]]  # fmt: skip\n+        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n+\n+        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n+        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+\n+        # add image URL for return dict\n+        messages[0][\"content\"][0] = {\n+            \"type\": \"video\",\n+            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        }\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages, add_generation_prompt=True, tokenize=True, return_dict=True\n+        )\n+        self.assertListEqual(list(out_dict_with_video.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n+\n+    @require_torch\n+    @require_av\n+    def test_chat_template_dict_torch(self):\n+        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                ],\n+            },\n+        ]\n+\n+        out_dict_tensors = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        )\n+        self.assertListEqual(list(out_dict_tensors.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n+        self.assertTrue(isinstance(out_dict_tensors[\"input_ids\"], torch.Tensor))"
        },
        {
            "sha": "795a361e22648f6afe91b2789cadf5e48bad82db",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 32,
            "deletions": 30,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/e0646f3dce229a2552c56746eac618baf8210efa/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e0646f3dce229a2552c56746eac618baf8210efa/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=e0646f3dce229a2552c56746eac618baf8210efa",
            "patch": "@@ -110,32 +110,34 @@ def test_apply_chat_template(self):\n         ]\n         input_ids = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n         expected_ids = [\n-            128000,  # <|begin_of_text|>\n-            128006,  # <|start_header_id|>\n-            9125,  # \"system\"\n-            128007,  # <|end_of_header|>\n-            271,  # \"\\n\\n\"\n-            2028,\n-            374,\n-            264,\n-            1296,\n-            11914,\n-            13,  # \"This is a test sentence.\"\n-            128009,  # <|eot_id|>\n-            128006,  # <|start_header_id|>\n-            882,  # \"user\"\n-            128007,  # <|end_of_header|>\n-            271,  # \"\\n\\n\"\n-            2028,\n-            374,\n-            264,\n-            2077,\n-            13,  # \"This is a response.\",\n-            128009,  # <|eot_id|>\n-            128006,  # <|start_header_id|>\n-            78191,  # \"assistant\"\n-            128007,  # <|end_of_header|>\n-            271,  # \"\\n\\n\"\n+            [\n+                128000,  # <|begin_of_text|>\n+                128006,  # <|start_header_id|>\n+                9125,  # \"system\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+                2028,\n+                374,\n+                264,\n+                1296,\n+                11914,\n+                13,  # \"This is a test sentence.\"\n+                128009,  # <|eot_id|>\n+                128006,  # <|start_header_id|>\n+                882,  # \"user\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+                2028,\n+                374,\n+                264,\n+                2077,\n+                13,  # \"This is a response.\",\n+                128009,  # <|eot_id|>\n+                128006,  # <|start_header_id|>\n+                78191,  # \"assistant\"\n+                128007,  # <|end_of_header|>\n+                271,  # \"\\n\\n\"\n+            ]\n         ]\n \n         self.assertEqual(input_ids, expected_ids)\n@@ -146,9 +148,9 @@ def test_apply_chat_template(self):\n                 \"role\": \"user\",\n                 \"content\": [\n                     {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n-                    {\"type\": \"image\"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n                     {\"type\": \"text\", \"text\": \" Test sentence   \"},\n-                    {\"type\": \"image\"},\n+                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n                     {\"type\": \"text\", \"text\": \"ok\\n\"},\n                 ],\n             }\n@@ -164,10 +166,10 @@ def test_apply_chat_template(self):\n \n         input_ids = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n         # fmt: off\n-        expected_ids = [\n+        expected_ids = [[\n             128000, 128006, 882, 128007, 271, 75885, 420, 2217, 304, 1403, 23719, 128256,\n             3475, 11914, 262, 128256, 564, 198, 128009, 128006, 78191, 128007, 271,\n-        ]\n+        ]]\n         # fmt: on\n         self.assertEqual(input_ids, expected_ids)\n "
        }
    ],
    "stats": {
        "total": 926,
        "additions": 880,
        "deletions": 46
    }
}