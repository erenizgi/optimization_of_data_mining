{
    "author": "dongluw",
    "message": "Command-a-vision fix (#42642)\n\n* Add test case and update image processing\n\n* Apply suggestions from code review\n\n* improve naming",
    "sha": "1b8ccf1ca027d25da70457d962ab9360bc9dbe8d",
    "files": [
        {
            "sha": "8f5da0f5cd13f9b28356dc4943359e7fba034866",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b8ccf1ca027d25da70457d962ab9360bc9dbe8d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b8ccf1ca027d25da70457d962ab9360bc9dbe8d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=1b8ccf1ca027d25da70457d962ab9360bc9dbe8d",
            "patch": "@@ -93,8 +93,9 @@ def get_optimal_tiled_canvas(\n     patch_size_height, patch_size_width = target_tile_size  # (height == width)\n \n     candidate_resolutions = np.array(possible_resolutions) * patch_size_height\n-    original_size = np.stack([image_height, image_width])\n-    required_scales = candidate_resolutions / original_size\n+    # tiles following (width, height) order to align with aspect ratio convention\n+    tile_size = np.stack([image_width, image_height])\n+    required_scales = candidate_resolutions / tile_size\n     required_scale = np.min(required_scales, axis=-1, keepdims=True)  # [n_resolutions, 1]\n     if np.all(required_scale < 1):\n         # We are forced to downscale, so try to minimize the amount of downscaling\n@@ -103,7 +104,7 @@ def get_optimal_tiled_canvas(\n         # Pick the resolution that required the least upscaling so that it most closely fits the image\n         required_scale = np.where(required_scale < 1.0, 10e9, required_scale)\n         best_grid = possible_resolutions[np.argmin(required_scale)]\n-    return best_grid\n+    return best_grid  # (width, height)\n \n \n @auto_docstring"
        },
        {
            "sha": "b85d11b99415e55d6b2d2cb67bf42c4dcdfbb930",
            "filename": "src/transformers/models/cohere2_vision/modular_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b8ccf1ca027d25da70457d962ab9360bc9dbe8d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b8ccf1ca027d25da70457d962ab9360bc9dbe8d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodular_cohere2_vision.py?ref=1b8ccf1ca027d25da70457d962ab9360bc9dbe8d",
            "patch": "@@ -295,8 +295,9 @@ def get_optimal_tiled_canvas(\n     patch_size_height, patch_size_width = target_tile_size  # (height == width)\n \n     candidate_resolutions = np.array(possible_resolutions) * patch_size_height\n-    original_size = np.stack([image_height, image_width])\n-    required_scales = candidate_resolutions / original_size\n+    # tiles following (width, height) order to align with aspect ratio convention\n+    tile_size = np.stack([image_width, image_height])\n+    required_scales = candidate_resolutions / tile_size\n     required_scale = np.min(required_scales, axis=-1, keepdims=True)  # [n_resolutions, 1]\n     if np.all(required_scale < 1):\n         # We are forced to downscale, so try to minimize the amount of downscaling\n@@ -305,7 +306,7 @@ def get_optimal_tiled_canvas(\n         # Pick the resolution that required the least upscaling so that it most closely fits the image\n         required_scale = np.where(required_scale < 1.0, 10e9, required_scale)\n         best_grid = possible_resolutions[np.argmin(required_scale)]\n-    return best_grid\n+    return best_grid  # (width, height)\n \n \n class Cohere2VisionFastImageProcessorKwargs(ImagesKwargs, total=False):"
        },
        {
            "sha": "c2b0c89a9a44979d38098e9847666dc8f3a7e698",
            "filename": "tests/models/cohere2_vision/test_image_processing_cohere2_vision.py",
            "status": "modified",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b8ccf1ca027d25da70457d962ab9360bc9dbe8d/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b8ccf1ca027d25da70457d962ab9360bc9dbe8d/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_image_processing_cohere2_vision.py?ref=1b8ccf1ca027d25da70457d962ab9360bc9dbe8d",
            "patch": "@@ -190,3 +190,128 @@ def test_call_numpy_4_channels(self):\n                 image_std=(1.0, 1.0, 1.0, 1.0),\n             ).pixel_values\n             self.assertEqual(tuple(encoded_images.shape), (70, 4, 30, 30))\n+\n+    def test_crop_to_patches_aspect_ratio(self):\n+        \"\"\"Test that row/column ordering is correct when cropping non-square images to patches.\n+\n+        This test verifies that patches can be stitched back to reconstruct the original image,\n+        which validates that the row/column ordering in get_optimal_tiled_canvas is correct.\n+        If row/column are swapped, the image would be resized to wrong dimensions and patches\n+        would not match the original content.\n+        \"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            patch_size = 64\n+            image_processor = image_processing_class(\n+                do_resize=True,\n+                size={\"height\": patch_size, \"width\": patch_size},\n+                do_normalize=False,  # Disable normalization to preserve pixel values\n+                do_rescale=False,  # Disable rescaling to preserve pixel values\n+                crop_to_patches=True,\n+                min_patches=1,\n+                max_patches=6,  # Allow up to 6 patches to test asymmetric grids like 2x3\n+            )\n+\n+            # Create a 2:3 aspect ratio image (2 rows x 3 columns of patches)\n+            # This asymmetric grid will fail if rows/columns are swapped\n+            num_rows, num_cols = 2, 3\n+            image_height = patch_size * num_rows  # 128\n+            image_width = patch_size * num_cols  # 192\n+\n+            # Create image with unique color for each patch position\n+            test_image = Image.new(\"RGB\", (image_width, image_height))\n+            for row in range(num_rows):\n+                for col in range(num_cols):\n+                    patch_idx = row * num_cols + col  # 0-5\n+                    color = (patch_idx * 40 + 20, 0, 0)  # Unique red values: 20, 60, 100, 140, 180, 220\n+                    for y in range(patch_size):\n+                        for x in range(patch_size):\n+                            test_image.putpixel(\n+                                (col * patch_size + x, row * patch_size + y),\n+                                color,\n+                            )\n+\n+            # Process image\n+            result = image_processor(test_image, return_tensors=\"pt\")\n+            patches = result.pixel_values\n+            num_patches_result = result.num_patches\n+\n+            # Should produce 7 patches (6 grid patches + 1 thumbnail)\n+            self.assertEqual(num_patches_result.tolist(), [7])\n+            self.assertEqual(tuple(patches.shape), (7, 3, patch_size, patch_size))\n+\n+            # Verify each patch has the correct color (excluding thumbnail which is last)\n+            # Patches should be ordered row by row: (0,0), (0,1), (0,2), (1,0), (1,1), (1,2)\n+            for patch_idx in range(6):\n+                expected_red = patch_idx * 40 + 20\n+                actual_red = patches[patch_idx, 0, 0, 0].item()  # Red channel, top-left pixel\n+                self.assertEqual(\n+                    actual_red,\n+                    expected_red,\n+                    f\"Patch {patch_idx} has wrong color. Expected red={expected_red}, got {actual_red}. \"\n+                    f\"This indicates row/column ordering is incorrect.\",\n+                )\n+\n+            # Stitch patches back and verify against original\n+            stitched = torch.zeros(3, image_height, image_width)\n+            for patch_idx in range(6):\n+                row = patch_idx // num_cols\n+                col = patch_idx % num_cols\n+                stitched[\n+                    :,\n+                    row * patch_size : (row + 1) * patch_size,\n+                    col * patch_size : (col + 1) * patch_size,\n+                ] = patches[patch_idx]\n+\n+            original_tensor = torch.tensor(np.array(test_image)).permute(2, 0, 1).float()\n+            self.assertTrue(\n+                torch.allclose(stitched, original_tensor),\n+                \"Patches do not stitch back to original image - row/column ordering may be wrong\",\n+            )\n+\n+    def test_get_number_of_image_patches_aspect_ratio(self):\n+        \"\"\"Test that get_number_of_image_patches returns correct count for non-square images.\n+\n+        This directly tests the row/column unpacking fix by verifying patch counts match\n+        the expected grid layout. If rows/columns are swapped, the wrong grid would be\n+        chosen for asymmetric images.\n+        \"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            patch_size = 64\n+            image_processor = image_processing_class(\n+                size={\"height\": patch_size, \"width\": patch_size},\n+                crop_to_patches=True,\n+                min_patches=1,\n+                max_patches=12,\n+            )\n+\n+            # Test 1: Tall image (4 rows x 1 column) should give 5 patches (4 + thumbnail)\n+            tall_patches = image_processor.get_number_of_image_patches(\n+                height=patch_size * 4,  # 256\n+                width=patch_size,  # 64\n+                images_kwargs={},\n+            )\n+            self.assertEqual(tall_patches, 5, \"Tall image (4:1) should produce 5 patches\")\n+\n+            # Test 2: Wide image (1 row x 4 columns) should give 5 patches (4 + thumbnail)\n+            wide_patches = image_processor.get_number_of_image_patches(\n+                height=patch_size,  # 64\n+                width=patch_size * 4,  # 256\n+                images_kwargs={},\n+            )\n+            self.assertEqual(wide_patches, 5, \"Wide image (1:4) should produce 5 patches\")\n+\n+            # Test 3: Asymmetric image (2 rows x 3 columns) should give 7 patches\n+            asym_patches = image_processor.get_number_of_image_patches(\n+                height=patch_size * 2,  # 128\n+                width=patch_size * 3,  # 192\n+                images_kwargs={\"max_patches\": 6},\n+            )\n+            self.assertEqual(asym_patches, 7, \"Asymmetric image (2:3) should produce 7 patches\")\n+\n+            # Test 4: Opposite asymmetric (3 rows x 2 columns) should also give 7 patches\n+            asym_patches2 = image_processor.get_number_of_image_patches(\n+                height=patch_size * 3,  # 192\n+                width=patch_size * 2,  # 128\n+                images_kwargs={\"max_patches\": 6},\n+            )\n+            self.assertEqual(asym_patches2, 7, \"Asymmetric image (3:2) should produce 7 patches\")"
        }
    ],
    "stats": {
        "total": 139,
        "additions": 133,
        "deletions": 6
    }
}