{
    "author": "gante",
    "message": "[Whisper] handle deprecation of `forced_decoder_ids` (#38232)\n\n* fix\n\n* working saved forced_decoder_ids\n\n* docstring\n\n* add deprecation message\n\n* exception message ordering\n\n* circular import comment",
    "sha": "f8630c778c9220defecf1e3026d3438108b0baba",
    "files": [
        {
            "sha": "4bb51d0ce804cd014b2311c5aac0969f6bfe7a90",
            "filename": "docs/source/en/model_doc/whisper.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8630c778c9220defecf1e3026d3438108b0baba/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8630c778c9220defecf1e3026d3438108b0baba/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fwhisper.md?ref=f8630c778c9220defecf1e3026d3438108b0baba",
            "patch": "@@ -95,7 +95,7 @@ transcription[0]\n \n ## Notes\n \n-- Whisper relies on [`~GenerationMixin.generate`] for inference.\n+- Whisper relies a custom [`generate`] for inference, make sure to check the docs below.\n - The [`WhisperProcessor`] can be used for preparing audio and decoding predicted ids back into text.\n \n ## WhisperConfig"
        },
        {
            "sha": "9bfa5a64d772c4f96dc91d7ef53e4e86c5bcd2a0",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=f8630c778c9220defecf1e3026d3438108b0baba",
            "patch": "@@ -280,10 +280,6 @@ class GenerationConfig(PushToHubMixin):\n         begin_suppress_tokens  (`List[int]`, *optional*):\n             A list of tokens that will be suppressed at the beginning of the generation. The `SupressBeginTokens` logit\n             processor will set their log probs to `-inf` so that they are not sampled.\n-        forced_decoder_ids (`List[List[int]]`, *optional*):\n-            A list of pairs of integers which indicates a mapping from generation indices to token indices that will be\n-            forced before sampling. For example, `[[1, 123]]` means the second generated token will always be a token\n-            of index 123.\n         sequence_bias (`Dict[Tuple[int], float]`, *optional*)):\n             Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n             sequence being selected, while negative biases do the opposite. Check\n@@ -388,12 +384,6 @@ class GenerationConfig(PushToHubMixin):\n             Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when\n             specific criteria are met, including using a compilable cache. Please open an issue if you find the\n             need to use this flag.\n-\n-        > Wild card\n-\n-        generation_kwargs:\n-            Additional generation kwargs will be forwarded to the `generate` function of the model. Kwargs that are not\n-            present in `generate`'s signature will be used in the model forward pass.\n     \"\"\"\n \n     extra_output_flags = (\"output_attentions\", \"output_hidden_states\", \"output_scores\", \"output_logits\")\n@@ -449,7 +439,6 @@ def __init__(self, **kwargs):\n         self.exponential_decay_length_penalty = kwargs.pop(\"exponential_decay_length_penalty\", None)\n         self.suppress_tokens = kwargs.pop(\"suppress_tokens\", None)\n         self.begin_suppress_tokens = kwargs.pop(\"begin_suppress_tokens\", None)\n-        self.forced_decoder_ids = kwargs.pop(\"forced_decoder_ids\", None)\n         self.sequence_bias = kwargs.pop(\"sequence_bias\", None)\n         self.token_healing = kwargs.pop(\"token_healing\", False)\n         self.guidance_scale = kwargs.pop(\"guidance_scale\", None)\n@@ -494,8 +483,6 @@ def __init__(self, **kwargs):\n         # Performance\n         self.compile_config = kwargs.pop(\"compile_config\", None)\n         self.disable_compile = kwargs.pop(\"disable_compile\", False)\n-        # Wild card\n-        self.generation_kwargs = kwargs.pop(\"generation_kwargs\", {})\n \n         # The remaining attributes do not parametrize `.generate()`, but are informative and/or used by the hub\n         # interface."
        },
        {
            "sha": "6e0f0154abd7163df1fade73f0c0ac841dbbebec",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=f8630c778c9220defecf1e3026d3438108b0baba",
            "patch": "@@ -15,7 +15,7 @@\n \n import inspect\n import math\n-from typing import Callable, Iterable, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Callable, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -25,6 +25,10 @@\n from ..utils.logging import get_logger\n \n \n+# TODO (joao): We shouldn't need this, but there would be a circular import\n+if TYPE_CHECKING:\n+    from ..generation.configuration_utils import GenerationConfig\n+\n logger = get_logger(__name__)\n \n \n@@ -1906,8 +1910,10 @@ class WhisperTimeStampLogitsProcessor(LogitsProcessor):\n                 max_initial_timestamp_index (`int`, *optional*, defaults to 1):\n                     Used to set the maximum value of the initial timestamp. This is used to prevent the model from\n                     predicting timestamps that are too far in the future.\n-        begin_index (`Optional`, *optional*): Token index of the first token that is generated by the model.\n-        _detect_timestamp_from_logprob (`bool`, *optional*): Whether timestamps can be predicted from logprobs over all timestamps.\n+        begin_index (`int`):\n+            Token index of the first token that is generated by the model.\n+        _detect_timestamp_from_logprob (`bool`, *optional*):\n+            Whether timestamps can be predicted from logprobs over all timestamps.\n \n     Examples:\n     ``` python\n@@ -1940,8 +1946,8 @@ class WhisperTimeStampLogitsProcessor(LogitsProcessor):\n \n     def __init__(\n         self,\n-        generate_config,\n-        begin_index: Optional[int] = None,\n+        generate_config: \"GenerationConfig\",\n+        begin_index: int,\n         _detect_timestamp_from_logprob: Optional[bool] = None,\n     ):  # support for the kwargs\n         self.no_timestamps_token_id = generate_config.no_timestamps_token_id\n@@ -1954,11 +1960,13 @@ def __init__(\n             if _detect_timestamp_from_logprob is not None\n             else getattr(generate_config, \"_detect_timestamp_from_logprob\", True)\n         )\n-\n-        num_forced_ids = (\n-            len(generate_config.forced_decoder_ids) if generate_config.forced_decoder_ids is not None else 0\n-        )\n-        self.begin_index = begin_index or (num_forced_ids + 1)\n+        self.begin_index = begin_index\n+        if begin_index is None:\n+            raise ValueError(\n+                \"`forced_decoder_ids` is deprecated in favor of `task` and `language` and, as such, `begin_index` \"\n+                \"must be provided to `WhisperTimeStampLogitsProcessor`. The previous default value of `begin_index` \"\n+                \"was `len(generate_config.forced_decoder_ids)`\"\n+            )\n \n         self.max_initial_timestamp_index = getattr(generate_config, \"max_initial_timestamp_index\", None)\n         # TODO(Patrick): Make sure that official models have max_initial_timestamp_index set to 50"
        },
        {
            "sha": "60859662f6183afa072a4f9029abc16bf5e88bec",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f8630c778c9220defecf1e3026d3438108b0baba",
            "patch": "@@ -1246,12 +1246,6 @@ def _get_logits_processor(\n                     device=device,\n                 )\n             )\n-        if generation_config.forced_decoder_ids is not None:\n-            # TODO (sanchit): move this exception to GenerationConfig.validate() when TF & FLAX are aligned with PT\n-            raise ValueError(\n-                \"You have explicitly specified `forced_decoder_ids`. Please remove the `forced_decoder_ids` argument \"\n-                \"in favour of `input_ids` or `decoder_input_ids` respectively.\",\n-            )\n \n         # TODO (joao): find a strategy to specify the order of the processors\n         processors = self._merge_criteria_processor_list(processors, logits_processor)"
        },
        {
            "sha": "4c29d456bf9b0e2470cbd3deb21d364a28e6649e",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 53,
            "deletions": 47,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8630c778c9220defecf1e3026d3438108b0baba/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=f8630c778c9220defecf1e3026d3438108b0baba",
            "patch": "@@ -410,8 +410,7 @@ def generate(\n             return_timestamps (`bool`, *optional*):\n                 Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n             task (`str`, *optional*):\n-                Task to use for generation, either \"translate\" or \"transcribe\". The `model.config.forced_decoder_ids`\n-                will be updated accordingly.\n+                Task to use for generation, either \"translate\" or \"transcribe\".\n             language (`str` or list of `str`, *optional*):\n                 Language token to use for generation, can be either in the form of `<|en|>`, `en` or `english`. For\n                 batched generation, a list of language tokens can be passed. You can find all the possible language\n@@ -1305,8 +1304,9 @@ def _set_return_timestamps(self, return_timestamps, is_shortform, generation_con\n         if not is_shortform:\n             if return_timestamps is False:\n                 raise ValueError(\n-                    \"You have passed more than 3000 mel input features (> 30 seconds) which automatically enables long-form generation which \"\n-                    \"requires the model to predict timestamp tokens. Please either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features.\"\n+                    \"You have passed more than 3000 mel input features (> 30 seconds) which automatically \"\n+                    \"enables long-form generation which requires the model to predict timestamp tokens. Please \"\n+                    \"either pass `return_timestamps=True` or make sure to pass no more than 3000 mel input features.\"\n                 )\n \n             logger.info(\"Setting `return_timestamps=True` for long-form generation.\")\n@@ -1315,17 +1315,19 @@ def _set_return_timestamps(self, return_timestamps, is_shortform, generation_con\n         if return_timestamps and not hasattr(generation_config, \"no_timestamps_token_id\"):\n             raise ValueError(\n                 \"You are trying to return timestamps, but the generation config is not properly set. \"\n-                \"Make sure to initialize the generation config with the correct attributes that are needed such as `no_timestamps_token_id`. \"\n-                \"For more details on how to generate the approtiate config, refer to https://github.com/huggingface/transformers/issues/21878#issuecomment-1451902363\"\n+                \"Make sure to initialize the generation config with the correct attributes that are needed such as \"\n+                \"`no_timestamps_token_id`. For more details on how to generate the approtiate config, refer to \"\n+                \"https://github.com/huggingface/transformers/issues/21878#issuecomment-1451902363\"\n             )\n \n         generation_config.return_timestamps = return_timestamps\n \n         if hasattr(generation_config, \"no_timestamps_token_id\"):\n             timestamp_begin = generation_config.no_timestamps_token_id + 1\n         else:\n-            # BC for models missing the `no_timestamps_token_id` in the generation config when generating short-form with no timestamps\n-            # We set the timestamp begin token larger than the vocab size, such that the timestamp condition is never met in the decoding loop\n+            # BC for models missing the `no_timestamps_token_id` in the generation config when generating short-form\n+            # with no timestamps. We set the timestamp begin token larger than the vocab size, such that the\n+            # timestamp condition is never met in the decoding loop\n             timestamp_begin = self.config.vocab_size + 1\n \n         return timestamp_begin\n@@ -1352,17 +1354,17 @@ def _set_language_and_task(language, task, is_multilingual, generation_config):\n             if not hasattr(generation_config, \"lang_to_id\"):\n                 raise ValueError(\n                     \"The generation config is outdated and is thus not compatible with the `language` argument \"\n-                    \"to `generate`. Either set the language using the `forced_decoder_ids` in the model config, \"\n-                    \"or update the generation config as per the instructions https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224\"\n+                    \"to `generate`. Please update the generation config as per the instructions \"\n+                    \"https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224\"\n                 )\n             generation_config.language = language\n \n         if task is not None:\n             if not hasattr(generation_config, \"task_to_id\"):\n                 raise ValueError(\n                     \"The generation config is outdated and is thus not compatible with the `task` argument \"\n-                    \"to `generate`. Either set the task using the `forced_decoder_ids` in the model config, \"\n-                    \"or update the generation config as per the instructions https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224\"\n+                    \"to `generate`. Please update the generation config as per the instructions \"\n+                    \"https://github.com/huggingface/transformers/issues/25084#issuecomment-1664398224\"\n                 )\n             generation_config.task = task\n \n@@ -1392,59 +1394,63 @@ def language_to_id(language: str) -> int:\n                 )\n             if language_token not in generation_config.lang_to_id:\n                 raise ValueError(\n-                    f\"{language_token} is not supported by this specific model as it is not in the `generation_config.lang_to_id`.\"\n-                    \"(You should just add it to the generation config)\"\n+                    f\"{language_token} is not supported by this specific model as it is not in the \"\n+                    \"`generation_config.lang_to_id`. (You should just add it to the generation config)\"\n                 )\n \n             return generation_config.lang_to_id[language_token]\n \n         task = getattr(generation_config, \"task\", None)\n         language = getattr(generation_config, \"language\", None)\n+        init_tokens = [generation_config.decoder_start_token_id]\n \n-        forced_decoder_ids = generation_config.forced_decoder_ids\n-        if forced_decoder_ids is not None:\n-            if language is None and task is None and forced_decoder_ids[0][1] is None:\n+        # TL;DR we silently ignore `forced_decoder_ids` (old flag) when `task` or `language` (new flags) are set.\n+        # `forced_decoder_ids` is an old generation config attribute that is now deprecated in favor of `task` and\n+        # `language` (see https://github.com/huggingface/transformers/pull/28687). Nevertheless, keep in mind that\n+        # the original checkpoints all contain this attribute, and thus we should maintain backwards compatibility.\n+        if task is None and language is None:\n+            forced_decoder_ids = getattr(generation_config, \"forced_decoder_ids\", None)\n+            # fallback: check the model config for forced_decoder_ids\n+            if forced_decoder_ids is None and getattr(config, \"forced_decoder_ids\", None) is not None:\n+                forced_decoder_ids = config.forced_decoder_ids\n+\n+            if forced_decoder_ids is not None:\n                 logger.warning_once(\n-                    \"Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.\"\n-                    \"This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\"\n+                    \"Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of \"\n+                    \"the `task` and `language` flags/config options.\"\n                 )\n-        elif hasattr(config, \"forced_decoder_ids\") and config.forced_decoder_ids is not None:\n-            forced_decoder_ids = config.forced_decoder_ids\n \n-        if forced_decoder_ids is not None and task is not None:\n-            logger.warning_once(\n-                f\"You have passed task={task}, but also have set `forced_decoder_ids` to {forced_decoder_ids} which creates a conflict. `forced_decoder_ids` will be ignored in favor of task={task}.\"\n-            )\n-            forced_decoder_ids = None\n-        elif forced_decoder_ids is not None and language is not None:\n-            logger.warning_once(\n-                f\"You have passed language={language}, but also have set `forced_decoder_ids` to {forced_decoder_ids} which creates a conflict. `forced_decoder_ids` will be ignored in favor of language={language}.\"\n-            )\n-            forced_decoder_ids = None\n-\n-        init_tokens = [generation_config.decoder_start_token_id]\n-        if forced_decoder_ids is not None and forced_decoder_ids[0][0] == 1:\n-            i = 1\n-            while len(forced_decoder_ids) > 0 and forced_decoder_ids[0][0] == i:\n-                init_tokens += [forced_decoder_ids[0][1]]\n-                forced_decoder_ids = forced_decoder_ids[1:]\n-                i += 1\n-\n-            if len(forced_decoder_ids) > 0:\n-                raise ValueError(\n-                    f\"You are using token ids in `forced_decoder_ids` that do not seem to correctly follow the prompt pattern of Whisper. Make sure that {forced_decoder_ids} has an entry for all indices >= 1 and < {forced_decoder_ids[0][0]}.\",\n-                )\n+                if forced_decoder_ids is not None and forced_decoder_ids[0][1] is None:\n+                    logger.warning_once(\n+                        \"Transcription using a multilingual Whisper will default to language detection followed by \"\n+                        \"transcription instead of translation to English. This might be a breaking change for your \"\n+                        \"use case. If you want to instead always translate your audio to English, make sure to pass \"\n+                        \"`language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\"\n+                    )\n \n-        # from v4.39 the forced decoder ids are always None in favour of decoder input ids\n-        generation_config.forced_decoder_ids = None\n+                if forced_decoder_ids is not None and forced_decoder_ids[0][0] == 1:\n+                    i = 1\n+                    while len(forced_decoder_ids) > 0 and forced_decoder_ids[0][0] == i:\n+                        init_tokens += [forced_decoder_ids[0][1]]\n+                        forced_decoder_ids = forced_decoder_ids[1:]\n+                        i += 1\n+\n+                    if len(forced_decoder_ids) > 0:\n+                        raise ValueError(\n+                            f\"You are using token ids in `forced_decoder_ids` that do not seem to correctly follow \"\n+                            f\"the prompt pattern of Whisper. Make sure that {forced_decoder_ids} has an entry for all \"\n+                            f\"indices >= 1 and < {forced_decoder_ids[0][0]}.\",\n+                        )\n \n         is_lang_id_undefined = len(init_tokens) <= 1 or (len(init_tokens) > 1 and init_tokens[1] is None)\n \n         # Make sure language is a list of strings of the correct length\n         if isinstance(language, (list, tuple)):\n             if any(l is None for l in language):\n                 raise TypeError(\n-                    \"Expected `language` to be `None`, a single string (e.g. `'en'`), or a list of strings with length equal to the batch size (e.g. `('en', 'fr')` for a batch size of 2). Got a list containing `None`.\"\n+                    \"Expected `language` to be `None`, a single string (e.g. `'en'`), or a list of strings with \"\n+                    \"length equal to the batch size (e.g. `('en', 'fr')` for a batch size of 2). Got a list \"\n+                    \"containing `None`.\"\n                 )\n             if len(language) != batch_size:\n                 raise ValueError("
        }
    ],
    "stats": {
        "total": 149,
        "additions": 72,
        "deletions": 77
    }
}