{
    "author": "IlyasMoutawwakil",
    "message": "Gaudi3 CI (#38790)",
    "sha": "984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
    "files": [
        {
            "sha": "73ff2ba269441e6fddc1b336bee30e8083c06ae4",
            "filename": ".github/workflows/model_jobs_intel_gaudi.yml",
            "status": "added",
            "additions": 121,
            "deletions": 0,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/.github%2Fworkflows%2Fmodel_jobs_intel_gaudi.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/.github%2Fworkflows%2Fmodel_jobs_intel_gaudi.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs_intel_gaudi.yml?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -0,0 +1,121 @@\n+name: model jobs\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      folder_slices:\n+        required: true\n+        type: string\n+      slice_id:\n+        required: true\n+        type: number\n+      runner:\n+        required: true\n+        type: string\n+      machine_type:\n+        required: true\n+        type: string\n+      report_name_prefix:\n+        required: false\n+        default: run_models_gpu\n+        type: string\n+\n+env:\n+  RUN_SLOW: yes\n+  PT_HPU_LAZY_MODE: 0\n+  TRANSFORMERS_IS_CI: yes\n+  PT_ENABLE_INT64_SUPPORT: 1\n+  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n+  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n+  HF_HOME: /mnt/cache/.cache/huggingface\n+\n+jobs:\n+  run_models_gpu:\n+    name: \" \"\n+    strategy:\n+      max-parallel: 8\n+      fail-fast: false\n+      matrix:\n+        folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}\n+    runs-on:\n+      group: ${{ inputs.runner }}\n+    container:\n+      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n+      options: --runtime=habana\n+        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface\n+        --env OMPI_MCA_btl_vader_single_copy_mechanism=none\n+        --env HABANA_VISIBLE_DEVICES\n+        --env HABANA_VISIBLE_MODULES\n+        --cap-add=sys_nice\n+        --shm-size=64G\n+    steps:\n+      - name: Echo input and matrix info\n+        shell: bash\n+        run: |\n+          echo \"${{ inputs.folder_slices }}\"\n+          echo \"${{ matrix.folders }}\"\n+          echo \"${{ toJson(fromJson(inputs.folder_slices)[inputs.slice_id]) }}\"\n+\n+      - name: Echo folder ${{ matrix.folders }}\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.folders }}\"\n+          matrix_folders=${{ matrix.folders }}\n+          matrix_folders=${matrix_folders/'models/'/'models_'}\n+          echo \"$matrix_folders\"\n+          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n+\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - name: Install dependencies\n+        run: |\n+          pip install -e .[testing,torch] \"numpy<2.0.0\" scipy scikit-learn\n+\n+      - name: HL-SMI\n+        run: |\n+          hl-smi\n+          echo \"HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}\"\n+          echo \"HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}\"\n+\n+      - name: Environment\n+        run: python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        run: pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        shell: bash\n+        run: |\n+          if [ \"${{ inputs.machine_type }}\" = \"1gaudi\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ inputs.machine_type }}\" = \"2gaudi\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ inputs.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n+      - name: Run all tests on Gaudi\n+        run: python3 -m pytest -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: cat reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/failures_short.txt\n+\n+      - name: Run test\n+        shell: bash\n+        run: |\n+          mkdir -p reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports\n+          echo \"hello\" > reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports/hello.txt\n+          echo \"${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports\"\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports\n+          path: reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ matrix.folders }}_test_reports"
        },
        {
            "sha": "2db5ece064bf7e7f9680fbcdb3b7c9bddf147995",
            "filename": ".github/workflows/self-scheduled-intel-gaudi.yml",
            "status": "added",
            "additions": 345,
            "deletions": 0,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/.github%2Fworkflows%2Fself-scheduled-intel-gaudi.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/.github%2Fworkflows%2Fself-scheduled-intel-gaudi.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-intel-gaudi.yml?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -0,0 +1,345 @@\n+name: Self-hosted runner (scheduled-intel-gaudi)\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      job:\n+        required: true\n+        type: string\n+      slack_report_channel:\n+        required: true\n+        type: string\n+      runner_scale_set:\n+        required: true\n+        type: string\n+      ci_event:\n+        required: true\n+        type: string\n+      report_repo_id:\n+        required: true\n+        type: string\n+\n+env:\n+  NUM_SLICES: 2\n+  RUN_SLOW: yes\n+  PT_HPU_LAZY_MODE: 0\n+  TRANSFORMERS_IS_CI: yes\n+  PT_ENABLE_INT64_SUPPORT: 1\n+  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n+  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n+  HF_HOME: /mnt/cache/.cache/huggingface\n+\n+jobs:\n+  setup:\n+    if: contains(fromJSON('[\"run_models_gpu\", \"run_trainer_and_fsdp_gpu\"]'), inputs.job)\n+    name: Setup\n+    runs-on: ubuntu-latest\n+    outputs:\n+      slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}\n+      folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}\n+      quantization_matrix: ${{ steps.set-matrix.outputs.quantization_matrix }}\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - name: Set up Python\n+        uses: actions/setup-python@v5\n+        with:\n+          python-version: \"3.10\"\n+\n+      - id: set-matrix\n+        if: contains(fromJSON('[\"run_models_gpu\", \"run_trainer_and_fsdp_gpu\"]'), inputs.job)\n+        name: Identify models to test\n+        working-directory: tests\n+        run: |\n+          if [ \"${{ inputs.job }}\" = \"run_models_gpu\" ]; then\n+            echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n+            echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n+          elif [ \"${{ inputs.job }}\" = \"run_trainer_and_fsdp_gpu\" ]; then\n+            echo \"folder_slices=[['trainer'], ['fsdp']]\" >> $GITHUB_OUTPUT\n+            echo \"slice_ids=[0, 1]\" >> $GITHUB_OUTPUT\n+          fi\n+\n+      - id: set-matrix-quantization\n+        if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n+        name: Identify quantization method to test\n+        working-directory: tests\n+        run: |\n+          echo \"quantization_matrix=$(python3 -c 'import os; tests = os.getcwd(); quantization_tests = os.listdir(os.path.join(tests, \"quantization\")); d = sorted(list(filter(os.path.isdir, [f\"quantization/{x}\" for x in quantization_tests]))) ;  print(d)')\" >> $GITHUB_OUTPUT\n+\n+  run_models_gpu:\n+    if: ${{ inputs.job == 'run_models_gpu' }}\n+    name: \" \"\n+    needs: setup\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [1gaudi, 2gaudi]\n+        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n+    uses: ./.github/workflows/model_jobs_intel_gaudi.yml\n+    with:\n+      slice_id: ${{ matrix.slice_id }}\n+      machine_type: ${{ matrix.machine_type }}\n+      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n+      runner: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}\n+      report_name_prefix: run_models_gpu\n+\n+    secrets: inherit\n+\n+  run_trainer_and_fsdp_gpu:\n+    if: ${{ inputs.job == 'run_trainer_and_fsdp_gpu' }}\n+    name: \" \"\n+    needs: setup\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [1gaudi, 2gaudi]\n+        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n+    uses: ./.github/workflows/model_jobs_intel_gaudi.yml\n+    with:\n+      slice_id: ${{ matrix.slice_id }}\n+      machine_type: ${{ matrix.machine_type }}\n+      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n+      runner: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}\n+      report_name_prefix: run_trainer_and_fsdp_gpu\n+\n+    secrets: inherit\n+\n+  run_pipelines_gpu:\n+    if: ${{ inputs.job == 'run_pipelines_gpu' }}\n+    name: Pipelines\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [1gaudi, 2gaudi]\n+    runs-on:\n+      group: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}\n+    container:\n+      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n+      options: --runtime=habana\n+        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface\n+        --env OMPI_MCA_btl_vader_single_copy_mechanism=none\n+        --env HABANA_VISIBLE_DEVICES\n+        --env HABANA_VISIBLE_MODULES\n+        --cap-add=sys_nice\n+        --shm-size=64G\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - name: Install dependencies\n+        run: |\n+          pip install -e .[testing,torch] \"numpy<2.0.0\" scipy scikit-learn librosa soundfile\n+\n+      - name: HL-SMI\n+        run: |\n+          hl-smi\n+          echo \"HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}\"\n+          echo \"HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}\"\n+\n+      - name: Environment\n+        run: python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        run: pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"1gaudi\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"2gaudi\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n+      - name: Run all pipeline tests on Intel Gaudi\n+        run: |\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_pipelines_gpu_test_reports tests/pipelines -m \"not not_device_test\"\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: |\n+          cat reports/${{ env.machine_type }}_run_pipelines_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_pipelines_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_pipelines_gpu_test_reports\n+          path: reports/${{ env.machine_type }}_run_pipelines_gpu_test_reports\n+\n+  run_examples_gpu:\n+    if: ${{ inputs.job == 'run_examples_gpu' }}\n+    name: Examples directory\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [1gaudi]\n+    runs-on:\n+      group: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}\n+    container:\n+      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n+      options: --runtime=habana\n+        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface\n+        --env OMPI_MCA_btl_vader_single_copy_mechanism=none\n+        --env HABANA_VISIBLE_DEVICES\n+        --env HABANA_VISIBLE_MODULES\n+        --cap-add=sys_nice\n+        --shm-size=64G\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - name: Install dependencies\n+        run: |\n+          pip install -e .[testing,torch] \"numpy<2.0.0\" scipy scikit-learn librosa soundfile\n+\n+      - name: HL-SMI\n+        run: |\n+          hl-smi\n+          echo \"HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}\"\n+          echo \"HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}\"\n+\n+      - name: Environment\n+        run: |\n+          python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        run: |\n+          pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"1gaudi\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"2gaudi\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n+      - name: Run examples tests on Intel Gaudi\n+        run: |\n+          pip install -r examples/pytorch/_tests_requirements.txt\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_examples_gpu_test_reports examples/pytorch -m \"not not_device_test\"\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: |\n+          cat reports/${{ env.machine_type }}_run_examples_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_examples_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_examples_gpu_test_reports\n+          path: reports/${{ env.machine_type }}_run_examples_gpu_test_reports\n+\n+  run_deepspeed_gpu:\n+    if: ${{ inputs.job == 'run_deepspeed_gpu' }}\n+    name: Intel Gaudi deepspeed tests\n+    strategy:\n+      fail-fast: false\n+      matrix:\n+        machine_type: [1gaudi, 2gaudi]\n+    runs-on:\n+      group: ${{ inputs.runner_scale_set }}-${{ matrix.machine_type }}\n+    container:\n+      image: vault.habana.ai/gaudi-docker/1.21.1/ubuntu22.04/habanalabs/pytorch-installer-2.6.0:latest\n+      options: --runtime=habana\n+        -v /mnt/cache/.cache/huggingface:/mnt/cache/.cache/huggingface\n+        --env OMPI_MCA_btl_vader_single_copy_mechanism=none\n+        --env HABANA_VISIBLE_DEVICES\n+        --env HABANA_VISIBLE_MODULES\n+        --cap-add=sys_nice\n+        --shm-size=64G\n+    steps:\n+      - name: Checkout\n+        uses: actions/checkout@v4\n+        with:\n+          fetch-depth: 0\n+\n+      - name: Install dependencies\n+        run: |\n+          pip install -e .[testing,torch] \"numpy<2.0.0\" scipy scikit-learn librosa soundfile\n+          pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.20.0\n+\n+      - name: HL-SMI\n+        run: |\n+          hl-smi\n+          echo \"HABANA_VISIBLE_DEVICES=${HABANA_VISIBLE_DEVICES}\"\n+          echo \"HABANA_VISIBLE_MODULES=${HABANA_VISIBLE_MODULES}\"\n+\n+      - name: Environment\n+        run: |\n+          python3 utils/print_env.py\n+\n+      - name: Show installed libraries and their versions\n+        run: |\n+          pip freeze\n+\n+      - name: Set `machine_type` for report and artifact names\n+        shell: bash\n+        run: |\n+          if [ \"${{ matrix.machine_type }}\" = \"1gaudi\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"2gaudi\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n+      - name: Run all deepspeed tests on intel Gaudi\n+        run: |\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_deepspeed_gpu_test_reports tests/deepspeed -m \"not not_device_test\"\n+\n+      - name: Failure short reports\n+        if: ${{ failure() }}\n+        continue-on-error: true\n+        run: |\n+          cat reports/${{ env.machine_type }}_run_deepspeed_gpu_test_reports/failures_short.txt\n+\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_deepspeed_gpu_test_reports\"\n+        if: ${{ always() }}\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: ${{ env.machine_type }}_run_deepspeed_gpu_test_reports\n+          path: reports/${{ env.machine_type }}_run_deepspeed_gpu_test_reports\n+\n+  send_results:\n+    name: Slack Report\n+    needs:\n+      [\n+        setup,\n+        run_models_gpu,\n+        run_examples_gpu,\n+        run_pipelines_gpu,\n+        run_deepspeed_gpu,\n+        run_trainer_and_fsdp_gpu,\n+      ]\n+    if: ${{ always() }}\n+    uses: ./.github/workflows/slack-report.yml\n+    with:\n+      job: ${{ inputs.job }}\n+      setup_status: ${{ needs.setup.result }}\n+      slack_report_channel: ${{ inputs.slack_report_channel }}\n+      quantization_matrix: ${{ needs.setup.outputs.quantization_matrix }}\n+      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n+      report_repo_id: ${{ inputs.report_repo_id }}\n+      ci_event: ${{ inputs.ci_event }}\n+\n+    secrets: inherit"
        },
        {
            "sha": "83cb89290d32aecca3e5f40fbf9461ed6e9b9de8",
            "filename": ".github/workflows/self-scheduled-intel-gaudi3-caller.yml",
            "status": "added",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/.github%2Fworkflows%2Fself-scheduled-intel-gaudi3-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/.github%2Fworkflows%2Fself-scheduled-intel-gaudi3-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-intel-gaudi3-caller.yml?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -0,0 +1,67 @@\n+name: Self-hosted runner (Intel Gaudi3 scheduled CI caller)\n+\n+on:\n+  repository_dispatch:\n+  workflow_dispatch:\n+  schedule:\n+    - cron: \"17 2 * * *\"\n+\n+jobs:\n+  model-ci:\n+    name: Model CI\n+    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml\n+    with:\n+      job: run_models_gpu\n+      ci_event: Scheduled CI (Intel) - Gaudi3\n+      runner_scale_set: itac-bm-emr-gaudi3-dell\n+      slack_report_channel: \"#transformers-ci-daily-intel-gaudi3\"\n+      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3\n+\n+    secrets: inherit\n+\n+  pipeline-ci:\n+    name: Pipeline CI\n+    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml\n+    with:\n+      job: run_pipelines_gpu\n+      ci_event: Scheduled CI (Intel) - Gaudi3\n+      runner_scale_set: itac-bm-emr-gaudi3-dell\n+      slack_report_channel: \"#transformers-ci-daily-intel-gaudi3\"\n+      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3\n+\n+    secrets: inherit\n+\n+  example-ci:\n+    name: Example CI\n+    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml\n+    with:\n+      job: run_examples_gpu\n+      ci_event: Scheduled CI (Intel) - Gaudi3\n+      runner_scale_set: itac-bm-emr-gaudi3-dell\n+      slack_report_channel: \"#transformers-ci-daily-intel-gaudi3\"\n+      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3\n+\n+    secrets: inherit\n+\n+  deepspeed-ci:\n+    name: DeepSpeed CI\n+    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml\n+    with:\n+      job: run_deepspeed_gpu\n+      ci_event: Scheduled CI (Intel) - Gaudi3\n+      runner_scale_set: itac-bm-emr-gaudi3-dell\n+      slack_report_channel: \"#transformers-ci-daily-intel-gaudi3\"\n+      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3\n+\n+    secrets: inherit\n+\n+  trainer-fsdp-ci:\n+    name: Trainer/FSDP CI\n+    uses: ./.github/workflows/self-scheduled-intel-gaudi.yml\n+    with:\n+      job: run_trainer_and_fsdp_gpu\n+      ci_event: Scheduled CI (Intel) - Gaudi3\n+      runner_scale_set: itac-bm-emr-gaudi3-dell\n+      slack_report_channel: \"#transformers-ci-daily-intel-gaudi3\"\n+      report_repo_id: optimum-intel/transformers_daily_ci_intel_gaudi3\n+    secrets: inherit"
        },
        {
            "sha": "4a1b44146c3093c78992688540066b8dd4efa8a9",
            "filename": "src/transformers/data/processors/squad.py",
            "status": "modified",
            "additions": 24,
            "deletions": 6,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -23,7 +23,7 @@\n \n from ...models.bert.tokenization_bert import whitespace_tokenize\n from ...tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase, TruncationStrategy\n-from ...utils import is_tf_available, is_torch_available, logging\n+from ...utils import is_tf_available, is_torch_available, is_torch_hpu_available, logging\n from .utils import DataProcessor\n \n \n@@ -361,11 +361,29 @@ def squad_convert_examples_to_features(\n         is_training=not evaluate,\n     )\n     ```\"\"\"\n-    # Defining helper methods\n-    features = []\n \n-    threads = min(threads, cpu_count())\n-    with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:\n+    if not is_torch_hpu_available():\n+        threads = min(threads, cpu_count())\n+        with Pool(threads, initializer=squad_convert_example_to_features_init, initargs=(tokenizer,)) as p:\n+            annotate_ = partial(\n+                squad_convert_example_to_features,\n+                max_seq_length=max_seq_length,\n+                doc_stride=doc_stride,\n+                max_query_length=max_query_length,\n+                padding_strategy=padding_strategy,\n+                is_training=is_training,\n+            )\n+            features = list(\n+                tqdm(\n+                    p.imap(annotate_, examples, chunksize=32),\n+                    total=len(examples),\n+                    desc=\"convert squad examples to features\",\n+                    disable=not tqdm_enabled,\n+                )\n+            )\n+    else:\n+        # Non-parallel version for hpu https://github.com/huggingface/transformers/pull/38790#discussion_r2156470902\n+        squad_convert_example_to_features_init(tokenizer_for_convert=tokenizer)\n         annotate_ = partial(\n             squad_convert_example_to_features,\n             max_seq_length=max_seq_length,\n@@ -376,7 +394,7 @@ def squad_convert_examples_to_features(\n         )\n         features = list(\n             tqdm(\n-                p.imap(annotate_, examples, chunksize=32),\n+                map(annotate_, examples),\n                 total=len(examples),\n                 desc=\"convert squad examples to features\",\n                 disable=not tqdm_enabled,"
        },
        {
            "sha": "7c3c6ccac8b4a0b3649ebb2d8ca7c79d82ea0586",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -3007,6 +3007,9 @@ def _find(self, tests, obj, name, module, source_lines, globs, seen) -> None:\n \n def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable], *args, **kwargs):\n     if device not in dispatch_table:\n+        if not callable(dispatch_table[\"default\"]):\n+            return dispatch_table[\"default\"]\n+\n         return dispatch_table[\"default\"](*args, **kwargs)\n \n     fn = dispatch_table[device]"
        },
        {
            "sha": "a933c9638d6f55aa9eaa726daaf79676e6c387cb",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 2,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -815,8 +815,8 @@ def is_torch_hpu_available():\n     ):\n         return False\n \n-    torch_hpu_min_version = \"1.5.0\"\n-    if _accelerate_available and version.parse(_accelerate_version) < version.parse(torch_hpu_min_version):\n+    torch_hpu_min_accelerate_version = \"1.5.0\"\n+    if _accelerate_available and version.parse(_accelerate_version) < version.parse(torch_hpu_min_accelerate_version):\n         return False\n \n     import torch\n@@ -850,6 +850,24 @@ def patched_masked_fill_(self, mask, value):\n \n         torch.Tensor.masked_fill_ = patched_masked_fill_\n \n+    # IlyasMoutawwakil: we patch torch.compile to use the HPU backend by default\n+    # https://github.com/huggingface/transformers/pull/38790#discussion_r2157043944\n+    # This is necessary for cases where torch.compile is used as a decorator (defaulting to inductor)\n+    # https://github.com/huggingface/transformers/blob/af6120b3eb2470b994c21421bb6eaa76576128b0/src/transformers/models/modernbert/modeling_modernbert.py#L204\n+    original_compile = torch.compile\n+\n+    def hpu_backend_compile(*args, **kwargs):\n+        if kwargs.get(\"backend\", None) not in [\"hpu_backend\", \"eager\"]:\n+            logger.warning(\n+                f\"Calling torch.compile with backend={kwargs.get('backend', None)} on a Gaudi device is not supported. \"\n+                \"We will override the backend with 'hpu_backend' to avoid errors.\"\n+            )\n+            kwargs[\"backend\"] = \"hpu_backend\"\n+\n+        return original_compile(*args, **kwargs)\n+\n+    torch.compile = hpu_backend_compile\n+\n     return True\n \n "
        },
        {
            "sha": "1a4966b09eb3b5948f184c37016b711fa0c18ecd",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -1134,10 +1134,12 @@ class TestDeepSpeedWithLauncher(TestCasePlus):\n \n     @parameterized.expand(params, name_func=parameterized_custom_name_func)\n     @require_torch_multi_accelerator\n+    @run_first\n     def test_basic_distributed(self, stage, dtype):\n         self.run_and_check(stage=stage, dtype=dtype, distributed=True)\n \n     @require_torch_fp16\n+    @run_first\n     def test_do_eval_no_train(self):\n         # testing only zero3 since zero2 makes no sense with inference\n         self.run_and_check(\n@@ -1150,6 +1152,7 @@ def test_do_eval_no_train(self):\n         )\n \n     @parameterized.expand(params, name_func=parameterized_custom_name_func)\n+    @run_first\n     def test_fp32_non_distributed(self, stage, dtype):\n         # real model needs too much GPU memory under stage2+fp32, so using tiny random model here -\n         # therefore no quality checks, just basic completion checks are done\n@@ -1166,6 +1169,7 @@ def test_fp32_non_distributed(self, stage, dtype):\n \n     @parameterized.expand(params, name_func=parameterized_custom_name_func)\n     @require_torch_multi_accelerator\n+    @run_first\n     def test_fp32_distributed(self, stage, dtype):\n         # real model needs too much GPU memory under stage2+fp32, so using tiny random model here -\n         # therefore no quality checks, just basic completion checks are done\n@@ -1181,6 +1185,7 @@ def test_fp32_distributed(self, stage, dtype):\n         )\n \n     @parameterized.expand(params, name_func=parameterized_custom_name_func)\n+    @run_first\n     def test_resume_train_not_from_ds_checkpoint(self, stage, dtype):\n         # do normal training and then resume not from the deepspeed checkpoint but explicitly from\n         # the saved model dir\n@@ -1207,6 +1212,7 @@ def test_resume_train_not_from_ds_checkpoint(self, stage, dtype):\n \n     @parameterized.expand([\"bf16\", \"fp16\", \"fp32\"])\n     @require_torch_multi_accelerator\n+    @run_first\n     def test_inference(self, dtype):\n         if dtype == \"bf16\" and not is_torch_bf16_available_on_device(torch_device):\n             self.skipTest(reason=\"test requires bfloat16 hardware support\")\n@@ -1361,6 +1367,7 @@ def run_trainer(\n         return output_dir\n \n     @parameterized.expand(params, name_func=parameterized_custom_name_func)\n+    @run_first\n     def test_clm(self, stage, dtype):\n         # this test exercises model.resize_token_embeddings() which requires param gathering outside\n         # of forward - it's not used by `run_translation.py`, but it is in `run_clm.py`\n@@ -1397,6 +1404,7 @@ def test_clm(self, stage, dtype):\n         execute_subprocess_async(cmd, env=self.get_env())\n \n     @require_torch_fp16\n+    @run_first\n     def test_clm_from_config_zero3_fp16(self):\n         # this test exercises AutoModel.from_config(config) - to ensure zero.Init is called\n "
        },
        {
            "sha": "b2c277b8621a490dc77a404e5d2a2a9c231b7438",
            "filename": "tests/deepspeed/test_model_zoo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Fdeepspeed%2Ftest_model_zoo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Fdeepspeed%2Ftest_model_zoo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_model_zoo.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -28,6 +28,7 @@\n     get_tests_dir,\n     require_deepspeed,\n     require_torch_accelerator,\n+    run_first,\n     slow,\n     torch_device,\n )\n@@ -327,6 +328,7 @@ def parameterized_custom_name_func(func, param_num, param):\n \n \n @slow\n+@run_first\n @require_deepspeed\n @require_torch_accelerator\n class TestDeepSpeedModelZoo(TestCasePlus):"
        },
        {
            "sha": "a932a1fbac67e5f253dc88a12b650f855b4851f1",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -358,6 +358,7 @@ def test_fsdp_cpu_offloading(self):\n             raise AssertionError(\"CPU offloading failed with FSDP!\")\n \n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n     @require_fsdp_v2_version\n     @require_accelerate_fsdp2\n@@ -405,6 +406,7 @@ def test_accelerate_fsdp2_integration(self):\n                 self.assertAlmostEqual(log[\"learning_rate\"], log1[\"learning_rate\"], delta=1e-5)\n \n     @require_torch_multi_accelerator\n+    @run_first\n     @slow\n     @require_fsdp\n     @require_fsdp_v2_version"
        },
        {
            "sha": "874323b1e9a44a5a6bb8397c97d170811744078a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -84,6 +84,7 @@\n     require_bitsandbytes,\n     require_deepspeed,\n     require_flash_attn,\n+    require_non_hpu,\n     require_safetensors,\n     require_torch,\n     require_torch_accelerator,\n@@ -92,6 +93,7 @@\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     require_torch_sdpa,\n+    run_first,\n     run_test_using_subprocess,\n     set_config_for_less_flaky_test,\n     set_model_for_less_flaky_test,\n@@ -2797,6 +2799,7 @@ def test_cpu_offload(self):\n                     else:\n                         torch.testing.assert_close(base_output[0], new_output[0], rtol=1e-5, atol=1e-5)\n \n+    @require_non_hpu\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_multi_accelerator\n@@ -3727,6 +3730,9 @@ def test_eager_matches_sdpa_inference(\n                 if torch_device in [\"cpu\", \"cuda\"]:\n                     atol = atols[torch_device, enable_kernels, torch_dtype]\n                     rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                elif torch_device == \"hpu\":\n+                    atol = atols[\"cuda\", enable_kernels, torch_dtype]\n+                    rtol = rtols[\"cuda\", enable_kernels, torch_dtype]\n                 elif torch_device == \"xpu\":\n                     # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n                     # which is implemented on PyTorch level using aten operators and is\n@@ -4666,6 +4672,7 @@ def test_can_load_with_device_context_manager(self):\n \n     # Here we need to run with a subprocess as otherwise setting back the default device to the default value (\"cpu\")\n     # may bring unwanted consequences on other tests. See PR #37553\n+    @run_first\n     @run_test_using_subprocess\n     @require_torch_accelerator\n     def test_can_load_with_global_device_set(self):"
        },
        {
            "sha": "878940b937f65b3d41f2bb40b651afda6c4d083d",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -3062,6 +3062,7 @@ def test_can_resume_training(self):\n     # the test slower.\n     @require_torch_non_multi_accelerator\n     @run_test_using_subprocess\n+    @run_first\n     @slow\n     def test_can_resume_training_lm(self):\n         # Check if it works for a simple language modeling example\n@@ -3517,7 +3518,6 @@ def test_load_best_model_from_safetensors(self):\n                 )\n \n     @slow\n-    @run_first\n     def test_trainer_eval_mrpc(self):\n         MODEL_ID = \"google-bert/bert-base-cased-finetuned-mrpc\"\n         tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n@@ -3534,7 +3534,6 @@ def test_trainer_eval_mrpc(self):\n             self.assertLess(result[\"eval_loss\"], 0.2)\n \n     @slow\n-    @run_first\n     def test_trainer_eval_multiple(self):\n         MODEL_ID = \"openai-community/gpt2\"\n         tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n@@ -4125,6 +4124,7 @@ def test_no_wd_param_group(self):\n             self.assertListEqual(trainer.optimizer.param_groups[1][\"params\"], no_wd_params)\n \n     @slow\n+    @run_first\n     @require_non_hpu\n     @require_torch_multi_accelerator\n     def test_end_to_end_example(self):"
        },
        {
            "sha": "54568caac37a9e55ba360cb0560af5c89e128067",
            "filename": "tests/trainer/test_trainer_distributed.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer_distributed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -22,6 +22,7 @@\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_torch_multi_accelerator,\n+    run_first,\n     torch_device,\n )\n from transformers.training_args import ParallelMode\n@@ -116,6 +117,7 @@ def __getitem__(self, i):\n \n \n class TestTrainerDistributed(TestCasePlus):\n+    @run_first\n     @require_torch_multi_accelerator\n     def test_trainer(self):\n         distributed_args = f\"\"\"--nproc_per_node={backend_device_count(torch_device)}\n@@ -199,8 +201,7 @@ def compute_metrics(p: EvalPrediction) -> dict:\n     model = RegressionModel()\n     training_args.per_device_train_batch_size = 1\n     training_args.max_steps = 1\n-    training_args.accelerator_config = {\n-        \"dispatch_batches\": False,\n-    }\n+    training_args.accelerator_config.dispatch_batches = False\n+\n     trainer = Trainer(model, training_args, train_dataset=train_dataset)\n     trainer.train()"
        },
        {
            "sha": "93cc042fe6dcc724e747be25c30032e5a92dabc2",
            "filename": "tests/trainer/test_trainer_distributed_loss.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed_loss.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -18,11 +18,13 @@\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_torch_multi_accelerator,\n+    run_first,\n     torch_device,\n )\n \n \n class TestTrainerDistributedLoss(TestCasePlus):\n+    @run_first\n     @require_torch_multi_accelerator\n     def test_trainer(self):\n         device_count = backend_device_count(torch_device)"
        },
        {
            "sha": "ded03826771fa1530aa0e8b46044ee14c507ab50",
            "filename": "tests/trainer/test_trainer_distributed_worker_seed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer_distributed_worker_seed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Ftrainer%2Ftest_trainer_distributed_worker_seed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_distributed_worker_seed.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -18,6 +18,7 @@\n     execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_torch_multi_accelerator,\n+    run_first,\n     torch_device,\n )\n \n@@ -57,6 +58,7 @@ def forward(self, x):\n \n \n class TestTrainerDistributedWorkerSeed(TestCasePlus):\n+    @run_first\n     @require_torch_multi_accelerator\n     def test_trainer(self):\n         device_count = backend_device_count(torch_device)"
        },
        {
            "sha": "903283dd4a9dfa9f533e9e046e20ea57f9f27e04",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -58,6 +58,7 @@\n     is_staging_test,\n     require_accelerate,\n     require_flax,\n+    require_non_hpu,\n     require_read_token,\n     require_safetensors,\n     require_tf,\n@@ -1002,6 +1003,7 @@ def test_checkpoint_variant_save_load_bin(self):\n \n         self.assertIsNotNone(model)\n \n+    @require_non_hpu\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_multi_accelerator"
        },
        {
            "sha": "e6d54fff2c74ce7c5ebb6e2f0f4052b387188566",
            "filename": "utils/print_env.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/utils%2Fprint_env.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/984ff89e7306ad33c46f76afc9aa78d40a8c01d8/utils%2Fprint_env.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fprint_env.py?ref=984ff89e7306ad33c46f76afc9aa78d40a8c01d8",
            "patch": "@@ -21,7 +21,7 @@\n import sys\n \n import transformers\n-from transformers import is_torch_xpu_available\n+from transformers import is_torch_hpu_available, is_torch_xpu_available\n \n \n os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n@@ -38,6 +38,9 @@\n         accelerator = \"CUDA\"\n     elif is_torch_xpu_available():\n         accelerator = \"XPU\"\n+    elif is_torch_hpu_available():\n+        accelerator = \"HPU\"\n+\n     print(\"Torch accelerator:\", accelerator)\n \n     if accelerator == \"CUDA\":\n@@ -48,6 +51,9 @@\n     elif accelerator == \"XPU\":\n         print(\"SYCL version:\", torch.version.xpu)\n         print(\"Number of XPUs available:\", torch.xpu.device_count())\n+    elif accelerator == \"HPU\":\n+        print(\"HPU version:\", torch.__version__.split(\"+\")[-1])\n+        print(\"Number of HPUs available:\", torch.hpu.device_count())\n except ImportError:\n     print(\"Torch version:\", None)\n "
        }
    ],
    "stats": {
        "total": 632,
        "additions": 618,
        "deletions": 14
    }
}