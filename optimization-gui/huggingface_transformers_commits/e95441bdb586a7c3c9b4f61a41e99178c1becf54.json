{
    "author": "wirthual",
    "message": "add type hints (#40319)\n\n* add basic type hints to import module\n\n* run make fixup\n\n* remove optional\n\n* fixes\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "e95441bdb586a7c3c9b4f61a41e99178c1becf54",
    "files": [
        {
            "sha": "aa80cbb28e7a0640d20f1d0d9bbf445fac2a78d8",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 159,
            "deletions": 159,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/e95441bdb586a7c3c9b4f61a41e99178c1becf54/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e95441bdb586a7c3c9b4f61a41e99178c1becf54/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e95441bdb586a7c3c9b4f61a41e99178c1becf54",
            "patch": "@@ -31,7 +31,7 @@\n from functools import lru_cache\n from itertools import chain\n from types import ModuleType\n-from typing import Any, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n from packaging import version\n \n@@ -371,35 +371,35 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n         logger.info(f\"Torch XLA version {_torch_xla_version} available.\")\n \n \n-def is_kenlm_available():\n+def is_kenlm_available() -> Union[tuple[bool, str], bool]:\n     return _kenlm_available\n \n \n-def is_kernels_available():\n+def is_kernels_available() -> Union[tuple[bool, str], bool]:\n     return _kernels_available\n \n \n-def is_cv2_available():\n+def is_cv2_available() -> Union[tuple[bool, str], bool]:\n     return _cv2_available\n \n \n-def is_yt_dlp_available():\n+def is_yt_dlp_available() -> Union[tuple[bool, str], bool]:\n     return _yt_dlp_available\n \n \n-def is_torch_available():\n+def is_torch_available() -> Union[tuple[bool, str], bool]:\n     return _torch_available\n \n \n-def is_libcst_available():\n+def is_libcst_available() -> Union[tuple[bool, str], bool]:\n     return _libcst_available\n \n \n-def is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION):\n+def is_accelerate_available(min_version: str = ACCELERATE_MIN_VERSION) -> bool:\n     return _accelerate_available and version.parse(_accelerate_version) >= version.parse(min_version)\n \n \n-def is_torch_accelerator_available():\n+def is_torch_accelerator_available() -> bool:\n     if is_torch_available():\n         import torch\n \n@@ -408,7 +408,7 @@ def is_torch_accelerator_available():\n     return False\n \n \n-def is_torch_deterministic():\n+def is_torch_deterministic() -> bool:\n     \"\"\"\n     Check whether pytorch uses deterministic algorithms by looking if torch.set_deterministic_debug_mode() is set to 1 or 2\"\n     \"\"\"\n@@ -423,23 +423,23 @@ def is_torch_deterministic():\n     return False\n \n \n-def is_triton_available(min_version: str = TRITON_MIN_VERSION):\n+def is_triton_available(min_version: str = TRITON_MIN_VERSION) -> bool:\n     return _triton_available and version.parse(_triton_version) >= version.parse(min_version)\n \n \n-def is_hadamard_available():\n+def is_hadamard_available() -> Union[tuple[bool, str], bool]:\n     return _hadamard_available\n \n \n-def is_hqq_available(min_version: str = HQQ_MIN_VERSION):\n+def is_hqq_available(min_version: str = HQQ_MIN_VERSION) -> bool:\n     return _hqq_available and version.parse(_hqq_version) >= version.parse(min_version)\n \n \n-def is_pygments_available():\n+def is_pygments_available() -> Union[tuple[bool, str], bool]:\n     return _pygments_available\n \n \n-def get_torch_version():\n+def get_torch_version() -> str:\n     return _torch_version\n \n \n@@ -457,7 +457,7 @@ def is_torch_sdpa_available():\n     return True\n \n \n-def is_torch_flex_attn_available():\n+def is_torch_flex_attn_available() -> bool:\n     if not is_torch_available() or _torch_version == \"N/A\":\n         return False\n \n@@ -466,75 +466,75 @@ def is_torch_flex_attn_available():\n     return version.parse(_torch_version) >= version.parse(\"2.5.0\")\n \n \n-def is_torchvision_available():\n+def is_torchvision_available() -> bool:\n     return _torchvision_available\n \n \n-def is_torchvision_v2_available():\n+def is_torchvision_v2_available() -> bool:\n     if not is_torchvision_available():\n         return False\n \n     # NOTE: We require torchvision>=0.15 as v2 transforms are available from this version: https://pytorch.org/vision/stable/transforms.html#v1-or-v2-which-one-should-i-use\n     return version.parse(_torchvision_version) >= version.parse(\"0.15\")\n \n \n-def is_galore_torch_available():\n+def is_galore_torch_available() -> Union[tuple[bool, str], bool]:\n     return _galore_torch_available\n \n \n-def is_apollo_torch_available():\n+def is_apollo_torch_available() -> Union[tuple[bool, str], bool]:\n     return _apollo_torch_available\n \n \n-def is_torch_optimi_available():\n+def is_torch_optimi_available() -> Union[tuple[bool, str], bool]:\n     return _torch_optimi_available\n \n \n-def is_lomo_available():\n+def is_lomo_available() -> Union[tuple[bool, str], bool]:\n     return _lomo_available\n \n \n-def is_grokadamw_available():\n+def is_grokadamw_available() -> Union[tuple[bool, str], bool]:\n     return _grokadamw_available\n \n \n-def is_schedulefree_available(min_version: str = SCHEDULEFREE_MIN_VERSION):\n+def is_schedulefree_available(min_version: str = SCHEDULEFREE_MIN_VERSION) -> bool:\n     return _schedulefree_available and version.parse(_schedulefree_version) >= version.parse(min_version)\n \n \n-def is_pyctcdecode_available():\n+def is_pyctcdecode_available() -> Union[tuple[bool, str], bool]:\n     return _pyctcdecode_available\n \n \n-def is_librosa_available():\n+def is_librosa_available() -> Union[tuple[bool, str], bool]:\n     return _librosa_available\n \n \n-def is_essentia_available():\n+def is_essentia_available() -> Union[tuple[bool, str], bool]:\n     return _essentia_available\n \n \n-def is_pydantic_available():\n+def is_pydantic_available() -> Union[tuple[bool, str], bool]:\n     return _pydantic_available\n \n \n-def is_fastapi_available():\n+def is_fastapi_available() -> Union[tuple[bool, str], bool]:\n     return _fastapi_available\n \n \n-def is_uvicorn_available():\n+def is_uvicorn_available() -> Union[tuple[bool, str], bool]:\n     return _uvicorn_available\n \n \n-def is_openai_available():\n+def is_openai_available() -> Union[tuple[bool, str], bool]:\n     return _openai_available\n \n \n-def is_pretty_midi_available():\n+def is_pretty_midi_available() -> Union[tuple[bool, str], bool]:\n     return _pretty_midi_available\n \n \n-def is_torch_cuda_available():\n+def is_torch_cuda_available() -> bool:\n     if is_torch_available():\n         import torch\n \n@@ -543,7 +543,7 @@ def is_torch_cuda_available():\n         return False\n \n \n-def is_cuda_platform():\n+def is_cuda_platform() -> bool:\n     if is_torch_available():\n         import torch\n \n@@ -552,7 +552,7 @@ def is_cuda_platform():\n         return False\n \n \n-def is_rocm_platform():\n+def is_rocm_platform() -> bool:\n     if is_torch_available():\n         import torch\n \n@@ -561,7 +561,7 @@ def is_rocm_platform():\n         return False\n \n \n-def is_mamba_ssm_available():\n+def is_mamba_ssm_available() -> Union[tuple[bool, str], bool]:\n     if is_torch_available():\n         import torch\n \n@@ -572,7 +572,7 @@ def is_mamba_ssm_available():\n     return False\n \n \n-def is_mamba_2_ssm_available():\n+def is_mamba_2_ssm_available() -> bool:\n     if is_torch_available():\n         import torch\n \n@@ -587,7 +587,7 @@ def is_mamba_2_ssm_available():\n     return False\n \n \n-def is_causal_conv1d_available():\n+def is_causal_conv1d_available() -> Union[tuple[bool, str], bool]:\n     if is_torch_available():\n         import torch\n \n@@ -597,19 +597,19 @@ def is_causal_conv1d_available():\n     return False\n \n \n-def is_xlstm_available():\n+def is_xlstm_available() -> Union[tuple[bool, str], bool]:\n     if is_torch_available():\n         return _is_package_available(\"xlstm\")\n     return False\n \n \n-def is_mambapy_available():\n+def is_mambapy_available() -> Union[tuple[bool, str], bool]:\n     if is_torch_available():\n         return _is_package_available(\"mambapy\")\n     return False\n \n \n-def is_torch_mps_available(min_version: Optional[str] = None):\n+def is_torch_mps_available(min_version: Optional[str] = None) -> bool:\n     if is_torch_available():\n         import torch\n \n@@ -639,11 +639,11 @@ def is_torch_bf16_gpu_available() -> bool:\n     return False\n \n \n-def is_torch_bf16_cpu_available() -> bool:\n+def is_torch_bf16_cpu_available() -> Union[tuple[bool, str], bool]:\n     return is_torch_available()\n \n \n-def is_torch_bf16_available():\n+def is_torch_bf16_available() -> bool:\n     # the original bf16 check was for gpu only, but later a cpu/bf16 combo has emerged so this util\n     # has become ambiguous and therefore deprecated\n     warnings.warn(\n@@ -655,7 +655,7 @@ def is_torch_bf16_available():\n \n \n @lru_cache\n-def is_torch_fp16_available_on_device(device):\n+def is_torch_fp16_available_on_device(device: str) -> bool:\n     if not is_torch_available():\n         return False\n \n@@ -687,7 +687,7 @@ def is_torch_fp16_available_on_device(device):\n \n \n @lru_cache\n-def is_torch_bf16_available_on_device(device):\n+def is_torch_bf16_available_on_device(device: str) -> bool:\n     if not is_torch_available():\n         return False\n \n@@ -710,7 +710,7 @@ def is_torch_bf16_available_on_device(device):\n     return True\n \n \n-def is_torch_tf32_available():\n+def is_torch_tf32_available() -> bool:\n     if not is_torch_available():\n         return False\n \n@@ -723,55 +723,55 @@ def is_torch_tf32_available():\n     return True\n \n \n-def is_torch_fx_available():\n+def is_torch_fx_available() -> Union[tuple[bool, str], bool]:\n     return is_torch_available()\n \n \n-def is_peft_available():\n+def is_peft_available() -> Union[tuple[bool, str], bool]:\n     return _peft_available\n \n \n-def is_bs4_available():\n+def is_bs4_available() -> Union[tuple[bool, str], bool]:\n     return _bs4_available\n \n \n-def is_tf_available():\n+def is_tf_available() -> bool:\n     return _tf_available\n \n \n-def is_coloredlogs_available():\n+def is_coloredlogs_available() -> Union[tuple[bool, str], bool]:\n     return _coloredlogs_available\n \n \n-def is_tf2onnx_available():\n+def is_tf2onnx_available() -> Union[tuple[bool, str], bool]:\n     return _tf2onnx_available\n \n \n-def is_onnx_available():\n+def is_onnx_available() -> Union[tuple[bool, str], bool]:\n     return _onnx_available\n \n \n-def is_flax_available():\n+def is_flax_available() -> bool:\n     return _flax_available\n \n \n-def is_flute_available():\n+def is_flute_available() -> bool:\n     try:\n         return importlib.util.find_spec(\"flute\") is not None and importlib.metadata.version(\"flute-kernel\") >= \"0.4.1\"\n     except importlib.metadata.PackageNotFoundError:\n         return False\n \n \n-def is_ftfy_available():\n+def is_ftfy_available() -> Union[tuple[bool, str], bool]:\n     return _ftfy_available\n \n \n-def is_g2p_en_available():\n+def is_g2p_en_available() -> Union[tuple[bool, str], bool]:\n     return _g2p_en_available\n \n \n @lru_cache\n-def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n+def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False) -> bool:\n     \"\"\"\n     Check if `torch_xla` is available. To train a native pytorch job in an environment with torch xla installed, set\n     the USE_TORCH_XLA to false.\n@@ -792,14 +792,14 @@ def is_torch_xla_available(check_is_tpu=False, check_is_gpu=False):\n \n \n @lru_cache\n-def is_torch_neuroncore_available(check_device=True):\n+def is_torch_neuroncore_available(check_device=True) -> bool:\n     if importlib.util.find_spec(\"torch_neuronx\") is not None:\n         return is_torch_xla_available()\n     return False\n \n \n @lru_cache\n-def is_torch_npu_available(check_device=False):\n+def is_torch_npu_available(check_device=False) -> bool:\n     \"Checks if `torch_npu` is installed and potentially if a NPU is in the environment\"\n     if not _torch_available or importlib.util.find_spec(\"torch_npu\") is None:\n         return False\n@@ -818,7 +818,7 @@ def is_torch_npu_available(check_device=False):\n \n \n @lru_cache\n-def is_torch_mlu_available(check_device=False):\n+def is_torch_mlu_available(check_device=False) -> bool:\n     \"\"\"\n     Checks if `mlu` is available via an `cndev-based` check which won't trigger the drivers and leave mlu\n     uninitialized.\n@@ -843,7 +843,7 @@ def is_torch_mlu_available(check_device=False):\n \n \n @lru_cache\n-def is_torch_musa_available(check_device=False):\n+def is_torch_musa_available(check_device=False) -> bool:\n     \"Checks if `torch_musa` is installed and potentially if a MUSA is in the environment\"\n     if not _torch_available or importlib.util.find_spec(\"torch_musa\") is None:\n         return False\n@@ -866,7 +866,7 @@ def is_torch_musa_available(check_device=False):\n \n \n @lru_cache\n-def is_torch_hpu_available():\n+def is_torch_hpu_available() -> bool:\n     \"Checks if `torch.hpu` is available and potentially if a HPU is in the environment\"\n     if (\n         not _torch_available\n@@ -963,7 +963,7 @@ def hpu_backend_compile(*args, **kwargs):\n \n \n @lru_cache\n-def is_habana_gaudi1():\n+def is_habana_gaudi1() -> bool:\n     if not is_torch_hpu_available():\n         return False\n \n@@ -973,15 +973,15 @@ def is_habana_gaudi1():\n     return htexp._get_device_type() == htexp.synDeviceType.synDeviceGaudi\n \n \n-def is_torchdynamo_available():\n+def is_torchdynamo_available() -> Union[tuple[bool, str], bool]:\n     return is_torch_available()\n \n \n-def is_torch_compile_available():\n+def is_torch_compile_available() -> Union[tuple[bool, str], bool]:\n     return is_torch_available()\n \n \n-def is_torchdynamo_compiling():\n+def is_torchdynamo_compiling() -> Union[tuple[bool, str], bool]:\n     if not is_torch_available():\n         return False\n \n@@ -1000,7 +1000,7 @@ def is_torchdynamo_compiling():\n             return False\n \n \n-def is_torchdynamo_exporting():\n+def is_torchdynamo_exporting() -> bool:\n     if not is_torch_available():\n         return False\n \n@@ -1017,61 +1017,61 @@ def is_torchdynamo_exporting():\n             return False\n \n \n-def is_torch_tensorrt_fx_available():\n+def is_torch_tensorrt_fx_available() -> bool:\n     if importlib.util.find_spec(\"torch_tensorrt\") is None:\n         return False\n     return importlib.util.find_spec(\"torch_tensorrt.fx\") is not None\n \n \n-def is_datasets_available():\n+def is_datasets_available() -> Union[tuple[bool, str], bool]:\n     return _datasets_available\n \n \n-def is_detectron2_available():\n+def is_detectron2_available() -> Union[tuple[bool, str], bool]:\n     return _detectron2_available\n \n \n-def is_rjieba_available():\n+def is_rjieba_available() -> Union[tuple[bool, str], bool]:\n     return _rjieba_available\n \n \n-def is_psutil_available():\n+def is_psutil_available() -> Union[tuple[bool, str], bool]:\n     return _psutil_available\n \n \n-def is_py3nvml_available():\n+def is_py3nvml_available() -> Union[tuple[bool, str], bool]:\n     return _py3nvml_available\n \n \n-def is_sacremoses_available():\n+def is_sacremoses_available() -> Union[tuple[bool, str], bool]:\n     return _sacremoses_available\n \n \n-def is_apex_available():\n+def is_apex_available() -> Union[tuple[bool, str], bool]:\n     return _apex_available\n \n \n-def is_aqlm_available():\n+def is_aqlm_available() -> Union[tuple[bool, str], bool]:\n     return _aqlm_available\n \n \n-def is_vptq_available(min_version: str = VPTQ_MIN_VERSION):\n+def is_vptq_available(min_version: str = VPTQ_MIN_VERSION) -> bool:\n     return _vptq_available and version.parse(_vptq_version) >= version.parse(min_version)\n \n \n-def is_av_available():\n+def is_av_available() -> bool:\n     return _av_available\n \n \n-def is_decord_available():\n+def is_decord_available() -> bool:\n     return _decord_available\n \n \n-def is_torchcodec_available():\n+def is_torchcodec_available() -> bool:\n     return _torchcodec_available\n \n \n-def is_ninja_available():\n+def is_ninja_available() -> bool:\n     r\"\"\"\n     Code comes from *torch.utils.cpp_extension.is_ninja_available()*. Returns `True` if the\n     [ninja](https://ninja-build.org/) build system is available on the system, `False` otherwise.\n@@ -1084,7 +1084,7 @@ def is_ninja_available():\n         return True\n \n \n-def is_ipex_available(min_version: str = \"\"):\n+def is_ipex_available(min_version: str = \"\") -> bool:\n     def get_major_and_minor_from_version(full_version):\n         return str(version.parse(full_version).major) + \".\" + str(version.parse(full_version).minor)\n \n@@ -1105,7 +1105,7 @@ def get_major_and_minor_from_version(full_version):\n \n \n @lru_cache\n-def is_torch_xpu_available(check_device=False):\n+def is_torch_xpu_available(check_device: bool = False) -> bool:\n     \"\"\"\n     Checks if XPU acceleration is available either via native PyTorch (>=2.6),\n     `intel_extension_for_pytorch` or via stock PyTorch (>=2.4) and potentially\n@@ -1134,7 +1134,7 @@ def is_torch_xpu_available(check_device=False):\n \n \n @lru_cache\n-def is_bitsandbytes_available(check_library_only=False) -> bool:\n+def is_bitsandbytes_available(check_library_only: bool = False) -> bool:\n     if not _bitsandbytes_available:\n         return False\n \n@@ -1164,7 +1164,7 @@ def is_bitsandbytes_multi_backend_available() -> bool:\n     return \"multi_backend\" in getattr(bnb, \"features\", set())\n \n \n-def is_flash_attn_2_available():\n+def is_flash_attn_2_available() -> bool:\n     if not is_torch_available():\n         return False\n \n@@ -1189,7 +1189,7 @@ def is_flash_attn_2_available():\n \n \n @lru_cache\n-def is_flash_attn_3_available():\n+def is_flash_attn_3_available() -> bool:\n     if not is_torch_available():\n         return False\n \n@@ -1208,23 +1208,23 @@ def is_flash_attn_3_available():\n \n \n @lru_cache\n-def is_flash_attn_greater_or_equal_2_10():\n+def is_flash_attn_greater_or_equal_2_10() -> bool:\n     if not _is_package_available(\"flash_attn\"):\n         return False\n \n     return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")\n \n \n @lru_cache\n-def is_flash_attn_greater_or_equal(library_version: str):\n+def is_flash_attn_greater_or_equal(library_version: str) -> bool:\n     if not _is_package_available(\"flash_attn\"):\n         return False\n \n     return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(library_version)\n \n \n @lru_cache\n-def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n+def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n     \"\"\"\n     Accepts a library version and returns True if the current version of the library is greater than or equal to the\n     given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n@@ -1242,7 +1242,7 @@ def is_torch_greater_or_equal(library_version: str, accept_dev: bool = False):\n \n \n @lru_cache\n-def is_torch_less_or_equal(library_version: str, accept_dev: bool = False):\n+def is_torch_less_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n     \"\"\"\n     Accepts a library version and returns True if the current version of the library is less than or equal to the\n     given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n@@ -1260,7 +1260,7 @@ def is_torch_less_or_equal(library_version: str, accept_dev: bool = False):\n \n \n @lru_cache\n-def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False):\n+def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool = False) -> bool:\n     if not _is_package_available(\"huggingface_hub\"):\n         return False\n \n@@ -1273,7 +1273,7 @@ def is_huggingface_hub_greater_or_equal(library_version: str, accept_dev: bool =\n \n \n @lru_cache\n-def is_quanto_greater(library_version: str, accept_dev: bool = False):\n+def is_quanto_greater(library_version: str, accept_dev: bool = False) -> bool:\n     \"\"\"\n     Accepts a library version and returns True if the current version of the library is greater than or equal to the\n     given version. If `accept_dev` is True, it will also accept development versions (e.g. 2.7.0.dev20250320 matches\n@@ -1294,49 +1294,49 @@ def is_torchdistx_available():\n     return _torchdistx_available\n \n \n-def is_faiss_available():\n+def is_faiss_available() -> bool:\n     return _faiss_available\n \n \n-def is_scipy_available():\n+def is_scipy_available() -> Union[tuple[bool, str], bool]:\n     return _scipy_available\n \n \n-def is_sklearn_available():\n+def is_sklearn_available() -> Union[tuple[bool, str], bool]:\n     return _sklearn_available\n \n \n-def is_sentencepiece_available():\n+def is_sentencepiece_available() -> Union[tuple[bool, str], bool]:\n     return _sentencepiece_available\n \n \n-def is_seqio_available():\n+def is_seqio_available() -> Union[tuple[bool, str], bool]:\n     return _is_seqio_available\n \n \n-def is_gguf_available(min_version: str = GGUF_MIN_VERSION):\n+def is_gguf_available(min_version: str = GGUF_MIN_VERSION) -> bool:\n     return _is_gguf_available and version.parse(_gguf_version) >= version.parse(min_version)\n \n \n-def is_protobuf_available():\n+def is_protobuf_available() -> bool:\n     if importlib.util.find_spec(\"google\") is None:\n         return False\n     return importlib.util.find_spec(\"google.protobuf\") is not None\n \n \n-def is_fsdp_available(min_version: str = FSDP_MIN_VERSION):\n+def is_fsdp_available(min_version: str = FSDP_MIN_VERSION) -> bool:\n     return is_torch_available() and version.parse(_torch_version) >= version.parse(min_version)\n \n \n-def is_optimum_available():\n+def is_optimum_available() -> Union[tuple[bool, str], bool]:\n     return _optimum_available\n \n \n-def is_auto_awq_available():\n+def is_auto_awq_available() -> bool:\n     return _auto_awq_available\n \n \n-def is_auto_round_available(min_version: str = AUTOROUND_MIN_VERSION):\n+def is_auto_round_available(min_version: str = AUTOROUND_MIN_VERSION) -> bool:\n     return _auto_round_available and version.parse(_auto_round_version) >= version.parse(min_version)\n \n \n@@ -1345,56 +1345,56 @@ def is_optimum_quanto_available():\n     return _is_optimum_quanto_available\n \n \n-def is_quark_available():\n+def is_quark_available() -> Union[tuple[bool, str], bool]:\n     return _quark_available\n \n \n-def is_fp_quant_available():\n+def is_fp_quant_available() -> bool:\n     return _fp_quant_available and version.parse(_fp_quant_version) >= version.parse(\"0.1.6\")\n \n \n-def is_qutlass_available():\n+def is_qutlass_available() -> Union[tuple[bool, str], bool]:\n     return _qutlass_available\n \n \n-def is_compressed_tensors_available():\n+def is_compressed_tensors_available() -> bool:\n     return _compressed_tensors_available\n \n \n-def is_auto_gptq_available():\n+def is_auto_gptq_available() -> Union[tuple[bool, str], bool]:\n     return _auto_gptq_available\n \n \n-def is_gptqmodel_available():\n+def is_gptqmodel_available() -> Union[tuple[bool, str], bool]:\n     return _gptqmodel_available\n \n \n-def is_eetq_available():\n+def is_eetq_available() -> Union[tuple[bool, str], bool]:\n     return _eetq_available\n \n \n-def is_fbgemm_gpu_available():\n+def is_fbgemm_gpu_available() -> Union[tuple[bool, str], bool]:\n     return _fbgemm_gpu_available\n \n \n-def is_levenshtein_available():\n+def is_levenshtein_available() -> Union[tuple[bool, str], bool]:\n     return _levenshtein_available\n \n \n-def is_optimum_neuron_available():\n+def is_optimum_neuron_available() -> Union[tuple[bool, str], bool]:\n     return _optimum_available and _is_package_available(\"optimum.neuron\")\n \n \n-def is_safetensors_available():\n+def is_safetensors_available() -> Union[tuple[bool, str], bool]:\n     return _safetensors_available\n \n \n-def is_tokenizers_available():\n+def is_tokenizers_available() -> Union[tuple[bool, str], bool]:\n     return _tokenizers_available\n \n \n @lru_cache\n-def is_vision_available():\n+def is_vision_available() -> bool:\n     _pil_available = importlib.util.find_spec(\"PIL\") is not None\n     if _pil_available:\n         try:\n@@ -1408,27 +1408,27 @@ def is_vision_available():\n     return _pil_available\n \n \n-def is_pytesseract_available():\n+def is_pytesseract_available() -> Union[tuple[bool, str], bool]:\n     return _pytesseract_available\n \n \n-def is_pytest_available():\n+def is_pytest_available() -> Union[tuple[bool, str], bool]:\n     return _pytest_available\n \n \n-def is_spacy_available():\n+def is_spacy_available() -> Union[tuple[bool, str], bool]:\n     return _spacy_available\n \n \n-def is_tensorflow_text_available():\n+def is_tensorflow_text_available() -> Union[tuple[bool, str], bool]:\n     return is_tf_available() and _tensorflow_text_available\n \n \n-def is_keras_nlp_available():\n+def is_keras_nlp_available() -> Union[tuple[bool, str], bool]:\n     return is_tensorflow_text_available() and _keras_nlp_available\n \n \n-def is_in_notebook():\n+def is_in_notebook() -> bool:\n     try:\n         # Check if we are running inside Marimo\n         if \"marimo\" in sys.modules:\n@@ -1448,19 +1448,19 @@ def is_in_notebook():\n         return False\n \n \n-def is_pytorch_quantization_available():\n+def is_pytorch_quantization_available() -> Union[tuple[bool, str], bool]:\n     return _pytorch_quantization_available\n \n \n-def is_tensorflow_probability_available():\n+def is_tensorflow_probability_available() -> Union[tuple[bool, str], bool]:\n     return _tensorflow_probability_available\n \n \n-def is_pandas_available():\n+def is_pandas_available() -> Union[tuple[bool, str], bool]:\n     return _pandas_available\n \n \n-def is_sagemaker_dp_enabled():\n+def is_sagemaker_dp_enabled() -> bool:\n     # Get the sagemaker specific env variable.\n     sagemaker_params = os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\")\n     try:\n@@ -1474,7 +1474,7 @@ def is_sagemaker_dp_enabled():\n     return _smdistributed_available\n \n \n-def is_sagemaker_mp_enabled():\n+def is_sagemaker_mp_enabled() -> bool:\n     # Get the sagemaker specific mp parameters from smp_options variable.\n     smp_options = os.getenv(\"SM_HP_MP_PARAMETERS\", \"{}\")\n     try:\n@@ -1498,52 +1498,52 @@ def is_sagemaker_mp_enabled():\n     return _smdistributed_available\n \n \n-def is_training_run_on_sagemaker():\n+def is_training_run_on_sagemaker() -> bool:\n     return \"SAGEMAKER_JOB_NAME\" in os.environ\n \n \n-def is_soundfile_available():\n+def is_soundfile_available() -> Union[tuple[bool, str], bool]:\n     return _soundfile_available\n \n \n-def is_timm_available():\n+def is_timm_available() -> Union[tuple[bool, str], bool]:\n     return _timm_available\n \n \n-def is_natten_available():\n+def is_natten_available() -> Union[tuple[bool, str], bool]:\n     return _natten_available\n \n \n-def is_nltk_available():\n+def is_nltk_available() -> Union[tuple[bool, str], bool]:\n     return _nltk_available\n \n \n-def is_torchaudio_available():\n+def is_torchaudio_available() -> Union[tuple[bool, str], bool]:\n     return _torchaudio_available\n \n \n-def is_torchao_available(min_version: str = TORCHAO_MIN_VERSION):\n+def is_torchao_available(min_version: str = TORCHAO_MIN_VERSION) -> bool:\n     return _torchao_available and version.parse(_torchao_version) >= version.parse(min_version)\n \n \n-def is_speech_available():\n+def is_speech_available() -> Union[tuple[bool, str], bool]:\n     # For now this depends on torchaudio but the exact dependency might evolve in the future.\n     return _torchaudio_available\n \n \n-def is_spqr_available():\n+def is_spqr_available() -> Union[tuple[bool, str], bool]:\n     return _spqr_available\n \n \n-def is_phonemizer_available():\n+def is_phonemizer_available() -> Union[tuple[bool, str], bool]:\n     return _phonemizer_available\n \n \n-def is_uroman_available():\n+def is_uroman_available() -> Union[tuple[bool, str], bool]:\n     return _uroman_available\n \n \n-def torch_only_method(fn):\n+def torch_only_method(fn: Callable) -> Callable:\n     def wrapper(*args, **kwargs):\n         if not _torch_available:\n             raise ImportError(\n@@ -1556,19 +1556,19 @@ def wrapper(*args, **kwargs):\n     return wrapper\n \n \n-def is_ccl_available():\n+def is_ccl_available() -> bool:\n     return _is_ccl_available\n \n \n-def is_sudachi_available():\n+def is_sudachi_available() -> bool:\n     return _sudachipy_available\n \n \n-def get_sudachi_version():\n+def get_sudachi_version() -> bool:\n     return _sudachipy_version\n \n \n-def is_sudachi_projection_available():\n+def is_sudachi_projection_available() -> bool:\n     if not is_sudachi_available():\n         return False\n \n@@ -1577,54 +1577,54 @@ def is_sudachi_projection_available():\n     return version.parse(_sudachipy_version) >= version.parse(\"0.6.8\")\n \n \n-def is_jumanpp_available():\n+def is_jumanpp_available() -> bool:\n     return (importlib.util.find_spec(\"rhoknp\") is not None) and (shutil.which(\"jumanpp\") is not None)\n \n \n-def is_cython_available():\n+def is_cython_available() -> bool:\n     return importlib.util.find_spec(\"pyximport\") is not None\n \n \n-def is_jieba_available():\n+def is_jieba_available() -> Union[tuple[bool, str], bool]:\n     return _jieba_available\n \n \n-def is_jinja_available():\n+def is_jinja_available() -> Union[tuple[bool, str], bool]:\n     return _jinja_available\n \n \n-def is_mlx_available():\n+def is_mlx_available() -> Union[tuple[bool, str], bool]:\n     return _mlx_available\n \n \n-def is_num2words_available():\n+def is_num2words_available() -> Union[tuple[bool, str], bool]:\n     return _num2words_available\n \n \n-def is_tiktoken_available():\n+def is_tiktoken_available() -> Union[tuple[bool, str], bool]:\n     return _tiktoken_available and _blobfile_available\n \n \n-def is_liger_kernel_available():\n+def is_liger_kernel_available() -> bool:\n     if not _liger_kernel_available:\n         return False\n \n     return version.parse(importlib.metadata.version(\"liger_kernel\")) >= version.parse(\"0.3.0\")\n \n \n-def is_rich_available():\n+def is_rich_available() -> Union[tuple[bool, str], bool]:\n     return _rich_available\n \n \n-def is_matplotlib_available():\n+def is_matplotlib_available() -> Union[tuple[bool, str], bool]:\n     return _matplotlib_available\n \n \n-def is_mistral_common_available():\n+def is_mistral_common_available() -> Union[tuple[bool, str], bool]:\n     return _mistral_common_available\n \n \n-def check_torch_load_is_safe():\n+def check_torch_load_is_safe() -> None:\n     if not is_torch_greater_or_equal(\"2.6\"):\n         raise ValueError(\n             \"Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \"\n@@ -2451,7 +2451,7 @@ def inner_fn(fun):\n }\n \n \n-def fetch__all__(file_content):\n+def fetch__all__(file_content) -> list[str]:\n     \"\"\"\n     Returns the content of the __all__ variable in the file content.\n     Returns None if not defined, otherwise returns a list of strings.\n@@ -2483,7 +2483,7 @@ def fetch__all__(file_content):\n \n     # __all__ is defined on multiple lines\n     else:\n-        _all = []\n+        _all: list[str] = []\n         for __all__line_index in range(1, len(lines)):\n             if lines[__all__line_index].strip() == \"]\":\n                 return _all\n@@ -2855,7 +2855,7 @@ def define_import_structure(module_path: str, prefix: Optional[str] = None) -> I\n         return spread_dict\n \n \n-def clear_import_cache():\n+def clear_import_cache() -> None:\n     \"\"\"\n     Clear cached Transformers modules to allow reloading modified code.\n "
        }
    ],
    "stats": {
        "total": 318,
        "additions": 159,
        "deletions": 159
    }
}