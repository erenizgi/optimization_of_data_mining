{
    "author": "emapco",
    "message": "fix: prevent model access error during Optuna hyperparameter tuning (#36395)\n\n* fix: prevent model access error during Optuna hyperparameter tuning\n\nThe `transformers.integrations.integration_utils.run_hp_search_optuna` function releases model memory and sets trainer.model to None after each trial. This causes an AttributeError when  subsequent Trainer.train calls attempt to access the model before reinitialization. This is only an issue when `fp16_full_eval` or `bf16_full_eval` flags are enabled.\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "082834dd79d14a2e53332c0c1a7c19852ab8973f",
    "files": [
        {
            "sha": "b6dffe8d8598805edd679c1fa29d29547b2e073e",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/082834dd79d14a2e53332c0c1a7c19852ab8973f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082834dd79d14a2e53332c0c1a7c19852ab8973f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=082834dd79d14a2e53332c0c1a7c19852ab8973f",
            "patch": "@@ -2180,7 +2180,12 @@ def train(\n \n         # do_train is not a reliable argument, as it might not be set and .train() still called, so\n         # the following is a workaround:\n-        if (args.fp16_full_eval or args.bf16_full_eval) and not args.do_train and not self.is_model_parallel:\n+        if (\n+            (args.fp16_full_eval or args.bf16_full_eval)\n+            and not args.do_train\n+            and not self.is_model_parallel\n+            and self.model_init is None\n+        ):\n             self._move_model_to_device(self.model, args.device)\n \n         if \"model_path\" in kwargs:"
        },
        {
            "sha": "0a78a524f4040c33bd58431bd4d740b92fe73598",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/082834dd79d14a2e53332c0c1a7c19852ab8973f/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082834dd79d14a2e53332c0c1a7c19852ab8973f/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=082834dd79d14a2e53332c0c1a7c19852ab8973f",
            "patch": "@@ -4998,6 +4998,38 @@ def compute_objective(metrics: Dict[str, float]) -> List[float]:\n             )\n \n \n+@require_torch\n+@require_optuna\n+class TrainerHyperParameterOptunaIntegrationTestWithFullEval(unittest.TestCase):\n+    def test_hyperparameter_search(self):\n+        def hp_space(trial):\n+            return {}\n+\n+        def model_init(trial):\n+            if trial is not None:\n+                a = trial.suggest_int(\"a\", -4, 4)\n+                b = trial.suggest_int(\"b\", -4, 4)\n+            else:\n+                a = 0\n+                b = 0\n+            config = RegressionModelConfig(a=a, b=b, double_output=False)\n+\n+            return RegressionPreTrainedModel(config)\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmp_dir,\n+                disable_tqdm=True,\n+                model_init=model_init,\n+                fp16_full_eval=True,\n+            )\n+            trainer.hyperparameter_search(\n+                direction=\"minimize\",\n+                hp_space=hp_space,\n+                n_trials=2,\n+            )\n+\n+\n @require_torch\n @require_ray\n class TrainerHyperParameterRayIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 39,
        "additions": 38,
        "deletions": 1
    }
}