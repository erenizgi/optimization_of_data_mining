{
    "author": "MekkCyber",
    "message": "Add kernelize to transformers (#38205)\n\n* fix\n\n* fix\n\n* fix flow\n\n* remove non compiling path\n\n* change\n\n* style\n\n* fix\n\n* update\n\n* update pin\n\n* revert",
    "sha": "08bf7f1afee8c1127a28053cf452c44cf7e04d9c",
    "files": [
        {
            "sha": "253e6fd0a9cb35d8e0c08543bbcf4568dc0ff827",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=08bf7f1afee8c1127a28053cf452c44cf7e04d9c",
            "patch": "@@ -128,7 +128,7 @@\n     # Keras pin - this is to make sure Keras 3 doesn't destroy us. Remove or change when we have proper support.\n     \"keras>2.9,<2.16\",\n     \"keras-nlp>=0.3.1,<0.14.0\",  # keras-nlp 0.14 doesn't support keras 2, see pin on keras.\n-    \"kernels>=0.4.4,<0.5\",\n+    \"kernels>=0.6.1,<0.7\",\n     \"librosa\",\n     \"natten>=0.14.6,<0.15.0\",\n     \"nltk<=3.8.1\","
        },
        {
            "sha": "8b2abc406f6eaaaeb9c814f4b6ebf1ec439bd8ad",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=08bf7f1afee8c1127a28053cf452c44cf7e04d9c",
            "patch": "@@ -34,7 +34,7 @@\n     \"kenlm\": \"kenlm\",\n     \"keras\": \"keras>2.9,<2.16\",\n     \"keras-nlp\": \"keras-nlp>=0.3.1,<0.14.0\",\n-    \"kernels\": \"kernels>=0.4.4,<0.5\",\n+    \"kernels\": \"kernels>=0.6.1,<0.7\",\n     \"librosa\": \"librosa\",\n     \"natten\": \"natten>=0.14.6,<0.15.0\",\n     \"nltk\": \"nltk<=3.8.1\","
        },
        {
            "sha": "7aa6c48f4c50dda7407e65125b8ba7c1b8d78abb",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 4,
            "deletions": 41,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=08bf7f1afee8c1127a28053cf452c44cf7e04d9c",
            "patch": "@@ -13,18 +13,14 @@\n # limitations under the License.\n from typing import Union\n \n-from ..utils import is_torchdynamo_compiling\n-\n \n try:\n     from kernels import (\n         Device,\n         LayerRepository,\n         register_kernel_mapping,\n         replace_kernel_forward_from_hub,\n-    )\n-    from kernels import (\n-        use_kernel_forward_from_hub as original_use_kernel_forward_from_hub,\n+        use_kernel_forward_from_hub,\n     )\n \n     _hub_kernels_available = True\n@@ -45,9 +41,9 @@\n         },\n         \"RMSNorm\": {\n             \"cuda\": LayerRepository(\n-                repo_id=\"kernels-community/triton-layer-norm\",\n-                layer_name=\"LlamaRMSNorm\",\n-                revision=\"pure-layer-test\",\n+                repo_id=\"kernels-community/liger_kernels\",\n+                layer_name=\"LigerRMSNorm\",\n+                # revision=\"pure-layer-test\",\n             )\n         },\n         \"MLP\": {\n@@ -60,39 +56,6 @@\n \n     register_kernel_mapping(_KERNEL_MAPPING)\n \n-    def use_kernel_forward_from_hub(*args, **kwargs):\n-        \"\"\"\n-        Expands `kernels`' `use_kernel_forward_from_hub` to NOT use a kernel at compile time. This should be removed\n-        when `kernels` supports `torch.compile`.\n-\n-        If the layer has a `config` attribute, we can also set `config.disable_custom_kernels = True` to disable the\n-        kernel.\n-        \"\"\"\n-\n-        def decorator_with_compile_path(cls):\n-            # Keeps a reference to the original forward method\n-            original_forward = cls.forward\n-\n-            # Applies the original decorator\n-            decorator = original_use_kernel_forward_from_hub(*args, **kwargs)\n-            cls = decorator(cls)\n-\n-            # Replaces the kernel forward with a compile-friendly version\n-            kernel_forward = cls.forward\n-\n-            def forward_with_compile_path(*forward_args, **forward_kwargs):\n-                disable_custom_kernels = hasattr(cls, \"config\") and getattr(cls.config, \"disable_custom_kernels\", None)\n-                if is_torchdynamo_compiling() or disable_custom_kernels:\n-                    return original_forward(*forward_args, **forward_kwargs)\n-                else:\n-                    return kernel_forward(*forward_args, **forward_kwargs)\n-\n-            cls.forward = forward_with_compile_path\n-\n-            return cls\n-\n-        return decorator_with_compile_path\n-\n \n except ImportError:\n     # Stub to make decorators int transformers work when `kernels`"
        },
        {
            "sha": "4774a72df7b66e103d054514234a9552ed218158",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/08bf7f1afee8c1127a28053cf452c44cf7e04d9c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=08bf7f1afee8c1127a28053cf452c44cf7e04d9c",
            "patch": "@@ -4281,6 +4281,7 @@ def from_pretrained(\n         tp_size = kwargs.pop(\"tp_size\", None)\n         device_mesh = kwargs.pop(\"device_mesh\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n+        use_kernels = kwargs.pop(\"use_kernels\", False)\n \n         key_mapping = kwargs.pop(\"key_mapping\", None)\n         # Load models with hardcoded key mapping on class for VLMs only, to keep BC and standardize model\n@@ -4733,6 +4734,12 @@ def _assign_original_dtype(module):\n         # Set model in evaluation mode to deactivate DropOut modules by default\n         model.eval()\n \n+        # check if using kernels\n+        if use_kernels:\n+            from kernels import Device, kernelize\n+\n+            kernelize(model, device=Device(type=model.device.type))\n+\n         # If it is a model with generation capabilities, attempt to load generation files (generation config,\n         # custom generate function)\n         if model.can_generate() and generation_config is not None:"
        }
    ],
    "stats": {
        "total": 56,
        "additions": 13,
        "deletions": 43
    }
}