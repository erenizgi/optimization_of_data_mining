{
    "author": "cakiki",
    "message": "[MINOR:TYPO] Update hubert.md (#36733)\n\n* [MINOR:TYPO] Update hubert.md\n\n- typo fix (wave2vec instead of hubert)\r\n- make code snippet copiable and runnable\n\n* Run tests",
    "sha": "e3af4fec91bdac3381e07f6e60fb8b74c3957c11",
    "files": [
        {
            "sha": "67e7d78beb635bb11c788b16fb6456697d20ca00",
            "filename": "docs/source/en/model_doc/hubert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3af4fec91bdac3381e07f6e60fb8b74c3957c11/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3af4fec91bdac3381e07f6e60fb8b74c3957c11/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhubert.md?ref=e3af4fec91bdac3381e07f6e60fb8b74c3957c11",
            "patch": "@@ -71,9 +71,10 @@ pip install -U flash-attn --no-build-isolation\n Below is an expected speedup diagram comparing the pure inference time between the native implementation in transformers of `facebook/hubert-large-ls960-ft`, the flash-attention-2 and the sdpa (scale-dot-product-attention) version. We show the average speedup obtained on the `librispeech_asr` `clean` validation split: \n \n ```python\n->>> from transformers import Wav2Vec2Model\n+>>> from transformers import HubertModel\n+>>> import torch\n \n-model = Wav2Vec2Model.from_pretrained(\"facebook/hubert-large-ls960-ft\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)\n+>>> model = HubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\")\n ...\n ```\n "
        }
    ],
    "stats": {
        "total": 5,
        "additions": 3,
        "deletions": 2
    }
}