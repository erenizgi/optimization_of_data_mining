{
    "author": "cyyever",
    "message": "Enable  FURB rules in ruff (#41395)\n\n* Apply ruff FURB rules\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Enable ruff FURB rules\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Revert changes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "12a50f294d50e3d0e124511f2b6f43625f73ffce",
    "files": [
        {
            "sha": "d85e31a7c2c3dccea914be0615d88df4aadcd239",
            "filename": "examples/legacy/seq2seq/pack_dataset.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fpack_dataset.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -60,8 +60,8 @@ def pack_data_dir(tok, data_dir: Path, max_tokens, save_path):\n     save_path.mkdir(exist_ok=True)\n     for split in [\"train\"]:\n         src_path, tgt_path = data_dir / f\"{split}.source\", data_dir / f\"{split}.target\"\n-        src_docs = [x.rstrip() for x in Path(src_path).open().readlines()]\n-        tgt_docs = [x.rstrip() for x in Path(tgt_path).open().readlines()]\n+        src_docs = [x.rstrip() for x in Path(src_path).open()]\n+        tgt_docs = [x.rstrip() for x in Path(tgt_path).open()]\n         packed_src, packed_tgt = pack_examples(tok, src_docs, tgt_docs, max_tokens)\n         print(f\"packed {split} split from {len(src_docs)} examples -> {len(packed_src)}.\")\n         Path(save_path / f\"{split}.source\").open(\"w\").write(\"\\n\".join(packed_src))"
        },
        {
            "sha": "d46544b96153e4f93a6d9a69664dc03e98fd79ec",
            "filename": "examples/legacy/seq2seq/rouge_cli.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Frouge_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Frouge_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frouge_cli.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -19,8 +19,8 @@\n \n def calculate_rouge_path(pred_path, tgt_path, save_path=None, **kwargs):\n     \"\"\"Kwargs will be passed to calculate_rouge\"\"\"\n-    pred_lns = [x.strip() for x in open(pred_path).readlines()]\n-    tgt_lns = [x.strip() for x in open(tgt_path).readlines()][: len(pred_lns)]\n+    pred_lns = [x.strip() for x in open(pred_path)]\n+    tgt_lns = [x.strip() for x in open(tgt_path)][: len(pred_lns)]\n     metrics = calculate_rouge(pred_lns, tgt_lns, **kwargs)\n     if save_path is not None:\n         save_json(metrics, save_path, indent=None)"
        },
        {
            "sha": "53fab8af9970cf51fadd782f77d19da837ee38a2",
            "filename": "examples/legacy/seq2seq/run_distributed_eval.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_distributed_eval.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -205,7 +205,7 @@ def run_generate():\n             return\n         tgt_file = Path(args.data_dir).joinpath(args.type_path + \".target\")\n         with open(tgt_file) as f:\n-            labels = [x.rstrip() for x in f.readlines()][: len(preds)]\n+            labels = [x.rstrip() for x in f][: len(preds)]\n \n         # Calculate metrics, save metrics,  and save _generations.txt\n         calc_bleu = \"translation\" in args.task"
        },
        {
            "sha": "180143046bb6f30ca6584c991469fdd132c36dd9",
            "filename": "examples/legacy/seq2seq/run_eval.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Frun_eval.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Frun_eval.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Frun_eval.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -130,7 +130,7 @@ def run_generate(verbose=True):\n     parsed_args = parse_numeric_n_bool_cl_kwargs(rest)\n     if parsed_args and verbose:\n         print(f\"parsed the following generate kwargs: {parsed_args}\")\n-    examples = [\" \" + x.rstrip() if \"t5\" in args.model_name else x.rstrip() for x in open(args.input_path).readlines()]\n+    examples = [\" \" + x.rstrip() if \"t5\" in args.model_name else x.rstrip() for x in open(args.input_path)]\n     if args.n_obs > 0:\n         examples = examples[: args.n_obs]\n     Path(args.save_path).parent.mkdir(exist_ok=True)\n@@ -159,8 +159,8 @@ def run_generate(verbose=True):\n \n     # Compute scores\n     score_fn = calculate_bleu if \"translation\" in args.task else calculate_rouge\n-    output_lns = [x.rstrip() for x in open(args.save_path).readlines()]\n-    reference_lns = [x.rstrip() for x in open(args.reference_path).readlines()][: len(output_lns)]\n+    output_lns = [x.rstrip() for x in open(args.save_path)]\n+    reference_lns = [x.rstrip() for x in open(args.reference_path)][: len(output_lns)]\n     scores: dict = score_fn(output_lns, reference_lns)\n     scores.update(runtime_metrics)\n "
        },
        {
            "sha": "d296c846fbb7024beef72204bb32e3bb0dbe329c",
            "filename": "examples/legacy/seq2seq/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Fseq2seq%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Futils.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -162,7 +162,7 @@ def __len__(self):\n \n     @staticmethod\n     def get_char_lens(data_file):\n-        return [len(x) for x in Path(data_file).open().readlines()]\n+        return [len(x) for x in Path(data_file).open()]\n \n     @cached_property\n     def tgt_lens(self):"
        },
        {
            "sha": "8a4cef710db149b3d508026777df8923467f650f",
            "filename": "examples/legacy/token-classification/scripts/preprocess.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Ftoken-classification%2Fscripts%2Fpreprocess.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -31,7 +31,7 @@\n             continue\n \n         if (subword_len_counter + current_subwords_len) > max_len:\n-            print(\"\")\n+            print()\n             print(line)\n             subword_len_counter = current_subwords_len\n             continue"
        },
        {
            "sha": "bf172eb2567f6d7e4ea204b1e72a09b83159aa9a",
            "filename": "examples/pytorch/token-classification/run_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -616,8 +616,7 @@ def compute_metrics(p):\n         output_predictions_file = os.path.join(training_args.output_dir, \"predictions.txt\")\n         if trainer.is_world_process_zero():\n             with open(output_predictions_file, \"w\") as writer:\n-                for prediction in true_predictions:\n-                    writer.write(\" \".join(prediction) + \"\\n\")\n+                writer.writelines(\" \".join(prediction) + \"\\n\" for prediction in true_predictions)\n \n     kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"token-classification\"}\n     if data_args.dataset_name is not None:"
        },
        {
            "sha": "54ec1618e3840b2c886b83851550efe8b2ed98f2",
            "filename": "pyproject.toml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/pyproject.toml",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/pyproject.toml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/pyproject.toml?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -32,7 +32,7 @@ line-length = 119\n ignore = [\"C901\", \"E501\", \"E741\", \"F402\", \"F823\", \"SIM1\", \"SIM300\", \"SIM212\", \"SIM905\", \"UP009\", \"UP015\", \"UP031\", \"UP028\", \"UP004\", \"UP045\", \"UP007\"]\n # RUF013: Checks for the use of implicit Optional\n #  in type annotations when the default parameter value is None.\n-select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"PERF102\", \"PLC1802\", \"PLC0208\", \"SIM\", \"UP\", \"PIE794\"]\n+select = [\"C\", \"E\", \"F\", \"I\", \"W\", \"RUF013\", \"PERF102\", \"PLC1802\", \"PLC0208\", \"SIM\", \"UP\", \"PIE794\", \"FURB\"]\n extend-safe-fixes = [\"UP006\"]\n \n # Ignore import violations in all `__init__.py` files."
        },
        {
            "sha": "9b16357a83ec4bfe39e40e4fc4bc6ae4d7696414",
            "filename": "src/transformers/models/bartpho/tokenization_bartpho.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbartpho%2Ftokenization_bartpho.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -140,7 +140,7 @@ def __init__(\n                 self.fairseq_tokens_to_ids[str(token)] = cnt\n                 cnt += 1\n         with open(monolingual_vocab_file, \"r\", encoding=\"utf-8\") as f:\n-            for line in f.readlines():\n+            for line in f:\n                 token = line.strip().split()[0]\n                 self.fairseq_tokens_to_ids[token] = len(self.fairseq_tokens_to_ids)\n         if str(mask_token) not in self.fairseq_tokens_to_ids:"
        },
        {
            "sha": "5e79b02f5716dc1e72e4fecb201cb2400aeab58b",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -2171,7 +2171,7 @@ def forward(\n             new_fpn_feature_map = fpn_block(fused_feature_map)\n             fpn_feature_maps.append(new_fpn_feature_map)\n \n-        fpn_feature_maps = fpn_feature_maps[::-1]\n+        fpn_feature_maps.reverse()\n \n         # bottom-up PAN\n         pan_feature_maps = [fpn_feature_maps[0]]"
        },
        {
            "sha": "ac8597361522057a904771c2dc5bee3878fa9af5",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -265,7 +265,7 @@ def __init__(self, config, conv_width, n_depth, reverse_dilation=False):\n             blocks.append(JukeboxResConv1DBlock(config, conv_width, block_depth, res_scale))\n \n         if reverse_dilation:\n-            blocks = blocks[::-1]\n+            blocks.reverse()\n         self.resnet_block = nn.ModuleList(blocks)\n \n     def forward(self, hidden_states):"
        },
        {
            "sha": "16c9eabdcd655cf49c93779936c869b4424b8837",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -617,7 +617,7 @@ def forward_pyramid(\n     def forward(\n         self,\n         coarse_features: torch.Tensor,\n-        residual_features: list[torch.Tensor],\n+        residual_features: list[torch.Tensor] | tuple[torch.Tensor],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         For each image pair, compute the fine features of pixels."
        },
        {
            "sha": "06789af620e142fae1d71870124746e357c3390e",
            "filename": "src/transformers/models/ibert/quant_modules.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -723,7 +723,7 @@ def batch_frexp(inputs, max_bit=31):\n     tmp_m = []\n     for m in output_m:\n         int_m_shifted = int(\n-            decimal.Decimal(m * (2**max_bit)).quantize(decimal.Decimal(\"1\"), rounding=decimal.ROUND_HALF_UP)\n+            decimal.Decimal(m * (2**max_bit)).quantize(decimal.Decimal(1), rounding=decimal.ROUND_HALF_UP)\n         )\n         tmp_m.append(int_m_shifted)\n     output_m = np.array(tmp_m)"
        },
        {
            "sha": "0e038b46b8e25fbd8c28f94e5ffd4223e9cf0a1a",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -1033,7 +1033,7 @@ def get_input_ids_and_entity_token_spans(text, entity_spans):\n             if head_token_span[0] < tail_token_span[0]:\n                 first_entity_token_spans[0] = (head_token_span[0], head_token_span[1] + 2)\n                 first_entity_token_spans[1] = (tail_token_span[0] + 2, tail_token_span[1] + 4)\n-                token_span_with_special_token_ids = reversed(token_span_with_special_token_ids)\n+                token_span_with_special_token_ids.reverse()\n             else:\n                 first_entity_token_spans[0] = (head_token_span[0] + 2, head_token_span[1] + 4)\n                 first_entity_token_spans[1] = (tail_token_span[0], tail_token_span[1] + 2)"
        },
        {
            "sha": "4ecf6e9ab5dcc7a3d24d9f62d4ecb9a4ac87600d",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -868,7 +868,7 @@ def get_input_ids_and_entity_token_spans(text, entity_spans):\n             if head_token_span[0] < tail_token_span[0]:\n                 first_entity_token_spans[0] = (head_token_span[0], head_token_span[1] + 2)\n                 first_entity_token_spans[1] = (tail_token_span[0] + 2, tail_token_span[1] + 4)\n-                token_span_with_special_token_ids = reversed(token_span_with_special_token_ids)\n+                token_span_with_special_token_ids.reverse()\n             else:\n                 first_entity_token_spans[0] = (head_token_span[0] + 2, head_token_span[1] + 4)\n                 first_entity_token_spans[1] = (tail_token_span[0], tail_token_span[1] + 2)"
        },
        {
            "sha": "05159b06e335aeb40d3ee4be603aed061b9a0d50",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -1262,7 +1262,7 @@ def forward(\n             new_fpn_feature_map = fpn_block(fused_feature_map)\n             fpn_feature_maps.append(new_fpn_feature_map)\n \n-        fpn_feature_maps = fpn_feature_maps[::-1]\n+        fpn_feature_maps.reverse()\n \n         # bottom-up PAN\n         pan_feature_maps = [fpn_feature_maps[0]]"
        },
        {
            "sha": "6f85dacad092487be4906acca53410f01a01d617",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -1218,7 +1218,7 @@ def forward(\n             new_fpn_feature_map = fpn_block(fused_feature_map)\n             fpn_feature_maps.append(new_fpn_feature_map)\n \n-        fpn_feature_maps = fpn_feature_maps[::-1]\n+        fpn_feature_maps.reverse()\n \n         # bottom-up PAN\n         pan_feature_maps = [fpn_feature_maps[0]]"
        },
        {
            "sha": "4fb1267f47cd7e88b1a32761115ce201ad179f31",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -457,7 +457,7 @@ def __init__(\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n \n     def _compute_window_shift(self, target_window_size, target_shift_size) -> tuple[tuple[int, int], tuple[int, int]]:\n-        window_size = [r if r <= w else w for r, w in zip(self.input_resolution, target_window_size)]\n+        window_size = [min(r, w) for r, w in zip(self.input_resolution, target_window_size)]\n         shift_size = [0 if r <= w else s for r, w, s in zip(self.input_resolution, window_size, target_shift_size)]\n         return window_size, shift_size\n "
        },
        {
            "sha": "0d87c23ffc69c7b192849d0bee6d0b2a2c2a3725",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -626,7 +626,7 @@ def __init__(\n         self.layernorm_after = nn.LayerNorm(dim, eps=config.layer_norm_eps)\n \n     def _compute_window_shift(self, target_window_size, target_shift_size) -> tuple[tuple[int, int], tuple[int, int]]:\n-        window_size = [r if r <= w else w for r, w in zip(self.input_resolution, target_window_size)]\n+        window_size = [min(r, w) for r, w in zip(self.input_resolution, target_window_size)]\n         shift_size = [0 if r <= w else s for r, w, s in zip(self.input_resolution, window_size, target_shift_size)]\n         return window_size, shift_size\n "
        },
        {
            "sha": "73552f7c2b20f7ec4111bf9790955380d5268acf",
            "filename": "src/transformers/models/vits/convert_original_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fvits%2Fconvert_original_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fvits%2Fconvert_original_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fconvert_original_checkpoint.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -329,7 +329,7 @@ def convert_checkpoint(\n         phonemize = True\n     else:\n         # Save vocab as temporary json file\n-        symbols = [line.replace(\"\\n\", \"\") for line in open(vocab_path, encoding=\"utf-8\").readlines()]\n+        symbols = [line.replace(\"\\n\", \"\") for line in open(vocab_path, encoding=\"utf-8\")]\n         symbol_to_id = {s: i for i, s in enumerate(symbols)}\n         # MMS-TTS does not use a <pad> token, so we set to the token used to space characters\n         _pad = symbols[0]"
        },
        {
            "sha": "f18b26a4e6a4597b01d4c4e106ea02856a1e94ee",
            "filename": "src/transformers/models/vivit/convert_vivit_flax_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -36,8 +36,7 @@ def download_checkpoint(path):\n \n     with open(path, \"wb\") as f:\n         with requests.get(url, stream=True) as req:\n-            for chunk in req.iter_content(chunk_size=2048):\n-                f.write(chunk)\n+            f.writelines(req.iter_content(chunk_size=2048))\n \n \n def get_vivit_config() -> VivitConfig:"
        },
        {
            "sha": "ca0b61c73e5b6e50004cd9df87bb099c9e41d167",
            "filename": "src/transformers/models/whisper/convert_openai_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconvert_openai_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconvert_openai_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconvert_openai_to_hf.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -317,8 +317,7 @@ def convert_tiktoken_to_hf(\n \n         with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n             writer.write(\"#version: 0.2\\n\")\n-            for bpe_tokens in merges:\n-                writer.write(bpe_tokens + \"\\n\")\n+            writer.writelines(bpe_tokens + \"\\n\" for bpe_tokens in merges)\n \n         hf_tokenizer = WhisperTokenizer(vocab_file, merge_file)\n "
        },
        {
            "sha": "f78b17896cb6b7551789f32cccee89b75d309851",
            "filename": "tests/models/bartpho/test_tokenization_bartpho.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbartpho%2Ftest_tokenization_bartpho.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -40,8 +40,7 @@ def setUpClass(cls):\n \n         cls.monolingual_vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"monolingual_vocab_file\"])\n         with open(cls.monolingual_vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            for token in vocab_tokens:\n-                fp.write(f\"{token} {vocab_tokens[token]}\\n\")\n+            fp.writelines(f\"{token} {vocab_tokens[token]}\\n\" for token in vocab_tokens)\n \n         tokenizer = BartphoTokenizer(SAMPLE_VOCAB, cls.monolingual_vocab_file, **cls.special_tokens_map)\n         tokenizer.save_pretrained(cls.tmpdirname)"
        },
        {
            "sha": "9c4aefecb9c9a6b7c61ce5dce2807377cc0fc276",
            "filename": "tests/models/bertweet/test_tokenization_bertweet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbertweet%2Ftest_tokenization_bertweet.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -38,8 +38,7 @@ def setUpClass(cls):\n         cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n         cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n         with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            for token in vocab_tokens:\n-                fp.write(f\"{token} {vocab_tokens[token]}\\n\")\n+            fp.writelines(f\"{token} {vocab_tokens[token]}\\n\" for token in vocab_tokens)\n         with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n "
        },
        {
            "sha": "922a9cac155330eb33e68fc68bb66733ddfd873c",
            "filename": "tests/models/phobert/test_tokenization_phobert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphobert%2Ftest_tokenization_phobert.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -39,8 +39,7 @@ def setUpClass(cls):\n         cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n \n         with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            for token in vocab_tokens:\n-                fp.write(f\"{token} {vocab_tokens[token]}\\n\")\n+            fp.writelines(f\"{token} {vocab_tokens[token]}\\n\" for token in vocab_tokens)\n         with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(\"\\n\".join(merges))\n "
        },
        {
            "sha": "2cdc2d8643b0152e3ad50be84bb6811259afe87c",
            "filename": "utils/fetch_hub_objects_for_ci.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/utils%2Ffetch_hub_objects_for_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/utils%2Ffetch_hub_objects_for_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ffetch_hub_objects_for_ci.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -210,8 +210,7 @@ def url_to_local_path(url, return_url_if_not_found=True):\n             response.raise_for_status()\n \n             with open(filename, \"wb\") as f:\n-                for chunk in response.iter_content(chunk_size=8192):\n-                    f.write(chunk)\n+                f.writelines(response.iter_content(chunk_size=8192))\n             print(f\"Successfully downloaded: {filename}\")\n         except requests.exceptions.RequestException as e:\n             print(f\"Error downloading {filename}: {e}\")"
        },
        {
            "sha": "2c2ecc6c54189725e504cdb65d7aa4baa56aedf9",
            "filename": "utils/modular_integrations.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12a50f294d50e3d0e124511f2b6f43625f73ffce/utils%2Fmodular_integrations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12a50f294d50e3d0e124511f2b6f43625f73ffce/utils%2Fmodular_integrations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_integrations.py?ref=12a50f294d50e3d0e124511f2b6f43625f73ffce",
            "patch": "@@ -34,7 +34,7 @@ def convert_relative_import_to_absolute(\n     rel_level = len(import_node.relative)\n \n     # Strip file extension and split into parts\n-    file_path_no_ext = file_path[:-3] if file_path.endswith(\".py\") else file_path\n+    file_path_no_ext = file_path.removesuffix(\".py\")\n     file_parts = file_path_no_ext.split(os.path.sep)\n \n     # Ensure the file path includes the package name"
        }
    ],
    "stats": {
        "total": 69,
        "additions": 31,
        "deletions": 38
    }
}