{
    "author": "zucchini-nlp",
    "message": "[chat template] return assistant mask in processors (#38545)\n\n* messed up the git history, squash commits\n\n* raise error if slow and refine tests\n\n* index was off by one\n\n* fix the test",
    "sha": "bcc009193761356784218dac587f5f191e05eb2c",
    "files": [
        {
            "sha": "0dbac1a08a023412a1ff70b958af0c4330e98c6a",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 5,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc009193761356784218dac587f5f191e05eb2c/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc009193761356784218dac587f5f191e05eb2c/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=bcc009193761356784218dac587f5f191e05eb2c",
            "patch": "@@ -15,6 +15,7 @@\n Processing saving/loading class for common processors.\n \"\"\"\n \n+import bisect\n import copy\n import inspect\n import json\n@@ -1468,6 +1469,8 @@ def apply_chat_template(\n                 # It's a template string, render it directly\n                 chat_template = chat_template\n \n+        is_tokenizers_fast = hasattr(self, \"tokenizer\") and self.tokenizer.__class__.__name__.endswith(\"Fast\")\n+\n         if kwargs.get(\"continue_final_message\", False):\n             if kwargs.get(\"add_generation_prompt\", False):\n                 raise ValueError(\n@@ -1476,6 +1479,15 @@ def apply_chat_template(\n             if kwargs.get(\"return_assistant_tokens_mask\", False):\n                 raise ValueError(\"continue_final_message is not compatible with return_assistant_tokens_mask.\")\n \n+        if kwargs.get(\"return_assistant_tokens_mask\", False):\n+            if not is_tokenizers_fast:\n+                raise ValueError(\n+                    \"`return_assistant_tokens_mask` is not possible with slow tokenizers. Make sure you have `tokenizers` installed. \"\n+                    \"If the error persists, open an issue to support a Fast tokenizer for your model.\"\n+                )\n+            else:\n+                kwargs[\"return_offsets_mapping\"] = True  # force offset mapping so we can infer token boundaries\n+\n         # Fill sets of kwargs that should be used by different parts of template\n         processed_kwargs = {\n             \"mm_load_kwargs\": {},\n@@ -1605,19 +1617,27 @@ def apply_chat_template(\n                 video_metadata=batch_video_metadata,\n                 **kwargs,\n             )\n+\n             if return_dict:\n                 if processed_kwargs[\"template_kwargs\"].get(\"return_assistant_tokens_mask\", False):\n                     assistant_masks = []\n+                    offset_mapping = out.pop(\"offset_mapping\")\n                     input_ids = out[\"input_ids\"]\n                     for i in range(len(input_ids)):\n                         current_mask = [0] * len(input_ids[i])\n+                        offsets = offset_mapping[i]\n+                        offset_starts = [start for start, end in offsets]\n                         for assistant_start_char, assistant_end_char in generation_indices[i]:\n-                            start_token = out.char_to_token(i, assistant_start_char)\n-                            end_token = out.char_to_token(i, assistant_end_char - 1)\n-                            if start_token is None:\n+                            start_pos = bisect.bisect_left(offset_starts, assistant_start_char)\n+                            end_pos = bisect.bisect_left(offset_starts, assistant_end_char)\n+\n+                            if not (\n+                                start_pos >= 0\n+                                and offsets[start_pos][0] <= assistant_start_char < offsets[start_pos][1]\n+                            ):\n                                 # start_token is out of bounds maybe due to truncation.\n-                                break\n-                            for token_id in range(start_token, end_token + 1 if end_token else len(input_ids[i])):\n+                                continue\n+                            for token_id in range(start_pos, end_pos if end_pos else len(input_ids[i])):\n                                 current_mask[token_id] = 1\n                         assistant_masks.append(current_mask)\n                     out[\"assistant_masks\"] = assistant_masks"
        },
        {
            "sha": "2abb7eb2d66f8345a2a86834a46c2cd2a8524239",
            "filename": "tests/models/csm/test_processor_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc009193761356784218dac587f5f191e05eb2c/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc009193761356784218dac587f5f191e05eb2c/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py?ref=bcc009193761356784218dac587f5f191e05eb2c",
            "patch": "@@ -137,3 +137,8 @@ def test_apply_chat_template(self):\n             [[128000, 58, 15, 60, 2028, 374, 264, 1296, 11914, 13, 128001, 128002, 128002, 128002, 128003]]\n         )\n         torch.testing.assert_close(input_ids, expected_ids)\n+\n+    @require_torch\n+    @unittest.skip(\"CSM doesn't need assistant masks as an audio generation model\")\n+    def test_apply_chat_template_assistant_mask(self):\n+        pass"
        },
        {
            "sha": "0bbe17e65607caa021d1e8fd1cce9b0c0cfa7ad1",
            "filename": "tests/models/shieldgemma2/test_processing_shieldgemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc009193761356784218dac587f5f191e05eb2c/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc009193761356784218dac587f5f191e05eb2c/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py?ref=bcc009193761356784218dac587f5f191e05eb2c",
            "patch": "@@ -206,3 +206,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n     @unittest.skip(\"Parent test needs to be adapted for ShieldGemma 2.\")\n     def test_kwargs_overrides_default_image_processor_kwargs(self):\n         pass\n+\n+    @unittest.skip(\"ShieldGemma requires images in input, and fails in text-only processing\")\n+    def test_apply_chat_template_assistant_mask(self):\n+        pass"
        },
        {
            "sha": "2bb5b9c847b47a7feaacac994fc55452922055c9",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 79,
            "deletions": 1,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc009193761356784218dac587f5f191e05eb2c/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc009193761356784218dac587f5f191e05eb2c/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=bcc009193761356784218dac587f5f191e05eb2c",
            "patch": "@@ -100,7 +100,11 @@ def get_component(self, attribute, **kwargs):\n         assert attribute in self.processor_class.attributes\n         component_class_name = getattr(self.processor_class, f\"{attribute}_class\")\n         if isinstance(component_class_name, tuple):\n-            component_class_name = component_class_name[0]\n+            if attribute == \"image_processor\":\n+                # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n+                component_class_name = component_class_name[0]\n+            else:\n+                component_class_name = component_class_name[-1]\n \n         component_class = processor_class_from_name(component_class_name)\n         component = component_class.from_pretrained(self.tmpdirname, **kwargs)  # noqa\n@@ -1149,3 +1153,77 @@ def test_chat_template_jinja_kwargs(self):\n         )\n         expected_prompt = \"You are a helpful assistant.<|special_start|>user\\nWhich of these animals is making the sound?<|special_end|>\\nYou are a helpful assistant.<|special_start|>assistant\\nIt is a cow.<|special_end|>\\n\"\n         self.assertEqual(formatted_prompt, expected_prompt)\n+\n+    @require_torch\n+    def test_apply_chat_template_assistant_mask(self):\n+        processor = self.get_processor()\n+\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is the capital of France?\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"The capital of France is Paris.\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What about Italy?\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"The capital of Italy is Rome.\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        dummy_template = (\n+            \"{% for message in messages %}\"\n+            \"{% if (message['role'] != 'assistant') %}\"\n+            \"{{'<|special_start|>' + message['role'] + '\\n' + message['content'][0]['text'] + '<|special_end|>' + '\\n'}}\"\n+            \"{% elif (message['role'] == 'assistant')%}\"\n+            \"{{'<|special_start|>' + message['role'] + '\\n'}}\"\n+            \"{% generation %}\"\n+            \"{{message['content'][0]['text'] + '<|special_end|>' + '\\n'}}\"\n+            \"{% endgeneration %}\"\n+            \"{% endif %}\"\n+            \"{% endfor %}\"\n+        )\n+\n+        inputs = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=False,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            return_assistant_tokens_mask=True,\n+            chat_template=dummy_template,\n+        )\n+        self.assertTrue(\"assistant_masks\" in inputs)\n+        self.assertEqual(len(inputs[\"assistant_masks\"]), len(inputs[\"input_ids\"]))\n+\n+        mask = inputs[\"assistant_masks\"].bool()\n+        assistant_ids = inputs[\"input_ids\"][mask]\n+\n+        assistant_text = (\n+            \"The capital of France is Paris.<|special_end|>\\nThe capital of Italy is Rome.<|special_end|>\\n\"\n+        )\n+\n+        # Some tokenizers add extra spaces which aren't then removed when decoding, so we need to check token ids\n+        # if we can't get identical text outputs\n+        text_is_same = assistant_text == processor.decode(assistant_ids, clean_up_tokenization_spaces=True)\n+        ids_is_same = processor.tokenizer.encode(assistant_text, add_special_tokens=False), assistant_ids.tolist()\n+        self.assertTrue(text_is_same or ids_is_same)"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 113,
        "deletions": 6
    }
}