{
    "author": "sameerajashyam",
    "message": "[siglip] fix pooling comment (#39378)\n\n* feat(siglip2): add forward pass with pooled output logic in Siglip2TextModel\n\n* test(siglip2): add test_text_model.py to verify pooled output behavior\n\n* style(siglip2): fix formatting in test_text_model.py using Ruff\n\n* fix(siglip2): remove misleading 'sticky EOS' comment and sync modular-classic files\n\n* fix(siglip2): remove misleading 'sticky EOS' comment and sync modular-classic files\n\n* chore(siglip2): regenerate classic model after modular change\n\n* Update",
    "sha": "6017f5e8ed33d48096cdf8630d1cc7cbf2550c90",
    "files": [
        {
            "sha": "d8fc3ed62838ce28c58f2fa3aa813c8dabfb1f72",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6017f5e8ed33d48096cdf8630d1cc7cbf2550c90/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6017f5e8ed33d48096cdf8630d1cc7cbf2550c90/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=6017f5e8ed33d48096cdf8630d1cc7cbf2550c90",
            "patch": "@@ -664,7 +664,7 @@ def forward(\n         last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.final_layer_norm(last_hidden_state)\n \n-        # Assuming \"sticky\" EOS tokenization, last token is always EOS.\n+        # The model uses the last token's hidden state, which may be padding.\n         pooled_output = last_hidden_state[:, -1, :]\n         pooled_output = self.head(pooled_output)\n "
        },
        {
            "sha": "25543b901ab693ddb5fed93504c86ab90b24fbcb",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6017f5e8ed33d48096cdf8630d1cc7cbf2550c90/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6017f5e8ed33d48096cdf8630d1cc7cbf2550c90/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=6017f5e8ed33d48096cdf8630d1cc7cbf2550c90",
            "patch": "@@ -688,7 +688,7 @@ def forward(\n         last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.final_layer_norm(last_hidden_state)\n \n-        # Assuming \"sticky\" EOS tokenization, last token is always EOS.\n+        # The model uses the last token's hidden state, which may be padding.\n         pooled_output = last_hidden_state[:, -1, :]\n         pooled_output = self.head(pooled_output)\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}