{
    "author": "NielsRogge",
    "message": "Add GraniteRMSNorm (#33177)\n\n* Add GraniteRMSNorm\r\n\r\n* [run_slow] granite",
    "sha": "b9bc691e8d0113e08833dd1df80b038cb1f6b483",
    "files": [
        {
            "sha": "90aa345b0eef433ef7a41a551748eb699d8ba1d6",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 28,
            "deletions": 3,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9bc691e8d0113e08833dd1df80b038cb1f6b483/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9bc691e8d0113e08833dd1df80b038cb1f6b483/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=b9bc691e8d0113e08833dd1df80b038cb1f6b483",
            "patch": "@@ -30,6 +30,7 @@\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -99,6 +100,30 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     return causal_mask\n \n \n+# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Granite\n+class GraniteRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        GraniteRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+ALL_LAYERNORM_LAYERS.append(GraniteRMSNorm)\n+\n+\n class GraniteRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteConfig):\n         super().__init__()\n@@ -534,8 +559,8 @@ def __init__(self, config: GraniteConfig, layer_idx: int):\n         self.self_attn = GRANITE_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n \n         self.mlp = GraniteMLP(config)\n-        self.input_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.post_attention_layernorm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.input_layernorm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n         self.residual_multiplier = config.residual_multiplier\n \n@@ -749,7 +774,7 @@ def __init__(self, config: GraniteConfig):\n         self.layers = nn.ModuleList(\n             [GraniteDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self.norm = nn.RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.norm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.gradient_checkpointing = False\n \n         self.embedding_multiplier = config.embedding_multiplier"
        },
        {
            "sha": "f3663c09902f525fb982bbf011d07fec307b0e02",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9bc691e8d0113e08833dd1df80b038cb1f6b483/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9bc691e8d0113e08833dd1df80b038cb1f6b483/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=b9bc691e8d0113e08833dd1df80b038cb1f6b483",
            "patch": "@@ -24,7 +24,7 @@\n from .utils import is_torch_xla_available, logging\n \n \n-ALL_LAYERNORM_LAYERS = [nn.LayerNorm, nn.RMSNorm]\n+ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n \n logger = logging.get_logger(__name__)\n "
        }
    ],
    "stats": {
        "total": 33,
        "additions": 29,
        "deletions": 4
    }
}