{
    "author": "faaany",
    "message": "[docs] no hard coding cuda as bnb has multi-backend support (#35867)\n\n* change cuda to DEVICE\n\n* Update docs/source/en/llm_tutorial.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "0a1a8e3c7e63dd42ac5e052202d16e8bf1df0b89",
    "files": [
        {
            "sha": "b0cb96293b68466dd8e42229a2d50b5804ac7341",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/0a1a8e3c7e63dd42ac5e052202d16e8bf1df0b89/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0a1a8e3c7e63dd42ac5e052202d16e8bf1df0b89/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=0a1a8e3c7e63dd42ac5e052202d16e8bf1df0b89",
            "patch": "@@ -40,6 +40,7 @@ Before you begin, make sure you have all the necessary libraries installed:\n ```bash\n pip install transformers bitsandbytes>=0.39.0 -q\n ```\n+Bitsandbytes supports multiple backends in addition to CUDA-based GPUs. Refer to the multi-backend installation [guide](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend) to learn more.\n \n \n ## Generate text\n@@ -101,9 +102,11 @@ Next, you need to preprocess your text input with a [tokenizer](tokenizer_summar\n \n ```py\n >>> from transformers import AutoTokenizer\n+>>> from accelerate.test_utils.testing import get_backend\n \n+>>> DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n->>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(DEVICE)\n ```\n \n The `model_inputs` variable holds the tokenized text input, as well as the attention mask. While [`~generation.GenerationMixin.generate`] does its best effort to infer the attention mask when it is not passed, we recommend passing it whenever possible for optimal results.\n@@ -122,7 +125,7 @@ Finally, you don't need to do it one sequence at a time! You can batch your inpu\n >>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n >>> model_inputs = tokenizer(\n ...     [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n-... ).to(\"cuda\")\n+... ).to(DEVICE)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n ['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n@@ -152,7 +155,7 @@ If not specified in the [`~generation.GenerationConfig`] file, `generate` return\n \n \n ```py\n->>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(DEVICE)\n \n >>> # By default, the output will contain up to 20 tokens\n >>> generated_ids = model.generate(**model_inputs)\n@@ -174,7 +177,7 @@ By default, and unless specified in the [`~generation.GenerationConfig`] file, `\n >>> from transformers import set_seed\n >>> set_seed(42)\n \n->>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(DEVICE)\n \n >>> # LLM + greedy decoding = repetitive, boring output\n >>> generated_ids = model.generate(**model_inputs)\n@@ -196,7 +199,7 @@ LLMs are [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt\n >>> # which is shorter, has padding on the right side. Generation fails to capture the logic.\n >>> model_inputs = tokenizer(\n ...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(\"cuda\")\n+... ).to(DEVICE)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 33333333333'\n@@ -206,7 +209,7 @@ LLMs are [decoder-only](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt\n >>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n >>> model_inputs = tokenizer(\n ...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(\"cuda\")\n+... ).to(DEVICE)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 3, 4, 5, 6,'\n@@ -223,7 +226,7 @@ Some models and tasks expect a certain input prompt format to work properly. Whe\n ... )\n >>> set_seed(0)\n >>> prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n->>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(DEVICE)\n >>> input_length = model_inputs.input_ids.shape[1]\n >>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n >>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n@@ -239,7 +242,7 @@ Some models and tasks expect a certain input prompt format to work properly. Whe\n ...     },\n ...     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ... ]\n->>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(DEVICE)\n >>> input_length = model_inputs.shape[1]\n >>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n >>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 11,
        "deletions": 8
    }
}