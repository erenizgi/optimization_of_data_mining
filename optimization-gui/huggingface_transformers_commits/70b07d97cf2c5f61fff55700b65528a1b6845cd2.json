{
    "author": "unknown",
    "message": "Default `synced_gpus` to `True` when using `FullyShardedDataParallel` (#33483)\n\n* Default synced_gpus to True when using FullyShardedDataParallel\r\n\r\nFixes #30228\r\n\r\nRelated:\r\n\r\n* https://github.com/pytorch/pytorch/issues/100069\r\n* https://github.com/pytorch/pytorch/issues/123962\r\n\r\nSimilar to DeepSpeed ZeRO Stage 3, when using FSDP with multiple GPUs and differently sized data per rank, the ranks reach different synchronization points at the same time, leading to deadlock\r\n\r\nTo avoid this, we can automatically set synced_gpus to True if we detect that a PreTrainedModel is being managed by FSDP using _is_fsdp_managed_module, which was added in 2.0.0 for torch.compile: https://github.com/pytorch/pytorch/blob/v2.0.0/torch/distributed/fsdp/_dynamo_utils.py\r\n\r\n* Remove test file\r\n\r\n* ruff formatting\r\n\r\n* ruff format\r\n\r\n* Update copyright year\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* Add test for FSDP-wrapped model generation\r\n\r\nBefore #33483, these tests would have hung for 10 minutes before crashing due to a timeout error\r\n\r\n* Ruff format\r\n\r\n* Move argparse import\r\n\r\n* Remove barrier\r\n\r\nI think this might cause more problems if one of the workers was killed\r\n\r\n* Move import into function to decrease load time\r\n\r\nhttps://github.com/huggingface/transformers/pull/33483#discussion_r1787972735\r\n\r\n* Add test for accelerate and Trainer\r\n\r\nhttps://github.com/huggingface/transformers/pull/33483#discussion_r1790309675\r\n\r\n* Refactor imports\r\n\r\n* Ruff format\r\n\r\n* Use nullcontext\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "70b07d97cf2c5f61fff55700b65528a1b6845cd2",
    "files": [
        {
            "sha": "5da4878513eb223b2cdc60b89d25aae9a64655cc",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -35,6 +35,7 @@\n )\n from ..configuration_utils import PretrainedConfig\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ..integrations.fsdp import is_fsdp_managed_module\n from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n@@ -1913,9 +1914,9 @@ def generate(\n                 for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                 Retrieval](https://arxiv.org/abs/2010.00904).\n             synced_gpus (`bool`, *optional*):\n-                Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n-                `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n-                generating before other GPUs. Otherwise it'll be set to `False`.\n+                Whether to continue running the while loop until max_length. Unless overridden, this flag will be set\n+                to `True` if using `FullyShardedDataParallel` or DeepSpeed ZeRO Stage 3 with multiple GPUs to avoid\n+                deadlocking if one GPU finishes generating before other GPUs. Otherwise, defaults to `False`.\n             assistant_model (`PreTrainedModel`, *optional*):\n                 An assistant model that can be used to accelerate generation. The assistant model must have the exact\n                 same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n@@ -1962,10 +1963,7 @@ def generate(\n \n         # 2. Set generation parameters if not already defined\n         if synced_gpus is None:\n-            if is_deepspeed_zero3_enabled() and dist.get_world_size() > 1:\n-                synced_gpus = True\n-            else:\n-                synced_gpus = False\n+            synced_gpus = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n \n         logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n         stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n@@ -2499,7 +2497,8 @@ def _dola_decoding(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n@@ -2702,7 +2701,8 @@ def _contrastive_search(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n@@ -3105,7 +3105,8 @@ def _sample(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n@@ -3307,7 +3308,8 @@ def _beam_search(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             model_kwargs:\n                 Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n                 an encoder-decoder model the kwargs should include `encoder_outputs`.\n@@ -3585,7 +3587,8 @@ def _group_beam_search(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             model_kwargs:\n                 Additional model specific kwargs that will be forwarded to the `forward` function of the model. If\n                 model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n@@ -3874,7 +3877,8 @@ def _constrained_beam_search(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             model_kwargs:\n                 Additional model specific kwargs will be forwarded to the `forward` function of the model. If model is\n                 an encoder-decoder model the kwargs should include `encoder_outputs`.\n@@ -4112,7 +4116,8 @@ def _assisted_decoding(\n             generation_config ([`~generation.GenerationConfig`]):\n                 The generation configuration to be used as parametrization of the decoding method.\n             synced_gpus (`bool`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing."
        },
        {
            "sha": "093e0af29844e4b76ff2ecc98df9b36b826c5134",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -54,6 +54,7 @@\n     ],\n     \"eetq\": [\"replace_with_eetq_linear\"],\n     \"fbgemm_fp8\": [\"FbgemmFp8Linear\", \"replace_with_fbgemm_fp8_linear\"],\n+    \"fsdp\": [\"is_fsdp_managed_module\"],\n     \"ggml\": [\n         \"GGUF_CONFIG_MAPPING\",\n         \"GGUF_TENSOR_MAPPING\",\n@@ -155,6 +156,7 @@\n     )\n     from .eetq import replace_with_eetq_linear\n     from .fbgemm_fp8 import FbgemmFp8Linear, replace_with_fbgemm_fp8_linear\n+    from .fsdp import is_fsdp_managed_module\n     from .ggml import (\n         GGUF_CONFIG_MAPPING,\n         GGUF_TENSOR_MAPPING,"
        },
        {
            "sha": "7bcb11fe74e4d603b6b43322604bb9cc0074cc4a",
            "filename": "src/transformers/integrations/fsdp.py",
            "status": "added",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fintegrations%2Ffsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fintegrations%2Ffsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffsdp.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -0,0 +1,33 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n+from ..utils import is_torch_available\n+\n+\n+if TYPE_CHECKING:\n+    from torch import nn\n+\n+\n+def is_fsdp_managed_module(module: nn.Module) -> bool:\n+    if not is_torch_available():\n+        return False\n+\n+    import torch.distributed.fsdp\n+\n+    return isinstance(module, torch.distributed.fsdp.FullyShardedDataParallel) or getattr(\n+        module, \"_is_fsdp_managed_module\", False\n+    )"
        },
        {
            "sha": "590509eaf9057cae82c2339a8b20e0f5ea46d472",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -826,7 +827,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -836,8 +837,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,"
        },
        {
            "sha": "3cbf2cc0bf0dcd8a485e15d336fd02ad43fe1aff",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -24,6 +24,7 @@\n from ....activations import ACT2FN\n from ....file_utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward\n from ....integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ....integrations.fsdp import is_fsdp_managed_module\n from ....modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ....modeling_outputs import BaseModelOutput, CausalLMOutput\n from ....modeling_utils import (\n@@ -579,7 +580,7 @@ def forward(\n                     f\"but it is for {head_mask.size()[0]}.\"\n                 )\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n@@ -588,8 +589,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         encoder_layer.__call__,"
        },
        {
            "sha": "ad21d768e35c8002cd798dbe7e20fe40aa2a037a",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -968,7 +969,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -978,8 +979,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -1055,7 +1056,7 @@ def forward(\n         hidden_states = hidden_states + position_embeddings\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1065,8 +1066,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func("
        },
        {
            "sha": "1588aa28aa2dbfc874a89e3617c635ac92142217",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -24,6 +24,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1055,7 +1056,7 @@ def forward(\n                     f\"The head_mask should be specified for {len(self.layers)} layers, but it is for\"\n                     f\" {head_mask.size()[0]}.\"\n                 )\n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -1065,8 +1066,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n \n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n@@ -1312,7 +1313,7 @@ def forward(\n                         f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n                         f\" {head_mask.size()[0]}.\"\n                     )\n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for idx, decoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -1322,8 +1323,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n \n                 past_key_value = past_key_values[idx] if past_key_values is not None else None\n "
        },
        {
            "sha": "8d7f6ad3c7c682e232506c5a4ade151b17430cab",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -1506,7 +1506,8 @@ def generate(\n                 generation config. If a stopping criteria is passed that is already created with the arguments or a\n                 generation config an error is thrown. This feature is intended for advanced users.\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n@@ -2513,7 +2514,8 @@ def generate(\n                 generation config. If a stopping criteria is passed that is already created with the arguments or a\n                 generation config an error is thrown. This feature is intended for advanced users.\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing."
        },
        {
            "sha": "96b8d29db83da17239ef68a1eb5769d51dcc2756",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -1428,7 +1428,8 @@ def generate(\n                 generation config. If a stopping criteria is passed that is already created with the arguments or a\n                 generation config an error is thrown. This feature is intended for advanced users.\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n@@ -2364,7 +2365,8 @@ def generate(\n                 generation config. If a stopping criteria is passed that is already created with the arguments or a\n                 generation config an error is thrown. This feature is intended for advanced users.\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing."
        },
        {
            "sha": "cedefc4f4642f7c84679171eed0171bafdbeb85f",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -24,6 +24,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     MoEModelOutput,\n@@ -1370,7 +1371,7 @@ def forward(\n                         f\"The `{mask_name}` should be specified for {len(self.layers)} layers, but it is for\"\n                         f\" {head_mask.size()[0]}.\"\n                     )\n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for idx, decoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -1380,13 +1381,13 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n+            if not skip_the_layer or synced_gpus:\n                 layer_head_mask = head_mask[idx] if head_mask is not None else None\n                 cross_attn_layer_head_mask = cross_attn_head_mask[idx] if cross_attn_head_mask is not None else None\n \n                 past_key_value = past_key_values[idx] if past_key_values is not None else None\n \n-                # under deepspeed zero3 all gpus must run in sync\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     if use_cache:\n                         logger.warning_once("
        },
        {
            "sha": "adc01ec40f959941ec53c2a716b37c42a6da0fb1",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -823,7 +824,7 @@ def forward(\n         else:\n             relative_position_embeddings = None\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for i, layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -835,8 +836,8 @@ def forward(\n             skip_the_layer = (\n                 True if self.training and (dropout_probability < self.config.speech_encoder_layerdrop) else False\n             )\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -2863,7 +2864,8 @@ def generate(\n                 for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                 Retrieval](https://arxiv.org/abs/2010.00904).\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model.\n@@ -3149,7 +3151,8 @@ def generate(\n                 for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                 Retrieval](https://arxiv.org/abs/2010.00904).\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model."
        },
        {
            "sha": "21265faa225127a603a077f09c322f8064826a7d",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -770,7 +771,7 @@ def forward(\n \n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for i, layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -782,8 +783,8 @@ def forward(\n             skip_the_layer = (\n                 True if self.training and (dropout_probability < self.config.speech_encoder_layerdrop) else False\n             )\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -3121,7 +3122,8 @@ def generate(\n                 for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                 Retrieval](https://arxiv.org/abs/2010.00904).\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model.\n@@ -3417,7 +3419,8 @@ def generate(\n                 for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                 Retrieval](https://arxiv.org/abs/2010.00904).\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             kwargs (`Dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model."
        },
        {
            "sha": "8638d93385843d88f938903e12c345d1705fad14",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -921,7 +922,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -931,8 +932,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,"
        },
        {
            "sha": "dbe57c01d9839ef106e3e728bf16b0b63fca078f",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1318,7 +1319,7 @@ def forward(\n \n         position_bias = self.embed_positions(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -1341,8 +1342,8 @@ def forward(\n                 dropout_probability = torch.rand([])\n                 skip_the_layer = dropout_probability < self.layerdrop\n \n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         encoder_layer.__call__,\n@@ -1603,7 +1604,7 @@ def forward(\n                 encoder_attention_mask, hidden_states.dtype, tgt_len=input_shape[-1]\n             )\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n@@ -1636,7 +1637,7 @@ def forward(\n             if self.training:\n                 dropout_probability = torch.rand([])\n                 skip_the_layer = dropout_probability < self.layerdrop\n-            if skip_the_layer and not deepspeed_zero3_is_enabled:\n+            if skip_the_layer and not synced_gpus:\n                 continue\n \n             past_key_value = past_key_values[idx] if past_key_values is not None else None"
        },
        {
            "sha": "eab23032475c40c62bf0ea920df8e092e08abb1b",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput, Wav2Vec2BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -1004,7 +1005,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1014,8 +1015,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -1091,7 +1092,7 @@ def forward(\n         hidden_states = hidden_states + position_embeddings\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1101,8 +1102,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func("
        },
        {
            "sha": "31d5071dbe24bfa97e51602c9cc43370fec45fd0",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -1021,7 +1022,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1031,8 +1032,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -1108,7 +1109,7 @@ def forward(\n         hidden_states = hidden_states + position_embeddings\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1118,8 +1119,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func("
        },
        {
            "sha": "66834167d15e06ff78984b4a24788b72619410ec",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -1139,7 +1140,7 @@ def forward(\n \n         hidden_states = hidden_states * padding_mask\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for encoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1149,8 +1150,8 @@ def forward(\n             dropout_probability = np.random.uniform(0, 1)\n \n             skip_the_layer = self.training and (dropout_probability < self.layerdrop)\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         encoder_layer.__call__,"
        },
        {
            "sha": "2648722111d52ecefe03f39ebb7b40c2e43efa12",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -1038,7 +1039,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1048,8 +1049,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -1124,7 +1125,7 @@ def forward(\n         hidden_states = hidden_states + position_embeddings\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for layer in self.layers:\n             if output_hidden_states:\n@@ -1134,8 +1135,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func("
        },
        {
            "sha": "6f1d5576df7316809c67f02adc613f36335630a0",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -26,6 +26,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -730,7 +731,7 @@ def forward(\n         else:\n             relative_position_embeddings = None\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for i, layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -740,8 +741,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,"
        },
        {
            "sha": "933bf8f6dc0bcd062e315f8d16fa139698007cef",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -893,7 +894,7 @@ def forward(\n         else:\n             relative_position_embeddings = None\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n \n         for i, layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -903,8 +904,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = True if self.training and (dropout_probability < self.config.layerdrop) else False\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,"
        },
        {
            "sha": "4df192fda5efa33ffd1b61c56596d1eaa62d8ad7",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n+from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -697,7 +698,7 @@ def forward(\n         hidden_states = self.layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n         position_bias = None\n \n         for i, layer in enumerate(self.layers):\n@@ -708,8 +709,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = self.training and i > 0 and (dropout_probability < self.config.layerdrop)\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func(\n                         layer.__call__,\n@@ -781,7 +782,7 @@ def forward(\n         hidden_states = hidden_states + position_embeddings\n         hidden_states = self.dropout(hidden_states)\n \n-        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n+        synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n         position_bias = None\n \n         for i, layer in enumerate(self.layers):\n@@ -792,8 +793,8 @@ def forward(\n             dropout_probability = torch.rand([])\n \n             skip_the_layer = self.training and i > 0 and (dropout_probability < self.config.layerdrop)\n-            if not skip_the_layer or deepspeed_zero3_is_enabled:\n-                # under deepspeed zero3 all gpus must run in sync\n+            if not skip_the_layer or synced_gpus:\n+                # under fsdp or deepspeed zero3 all gpus must run in sync\n                 # XXX: could optimize this like synced_gpus in generate_utils but not sure if it's worth the code complication\n                 if self.gradient_checkpointing and self.training:\n                     layer_outputs = self._gradient_checkpointing_func("
        },
        {
            "sha": "0ecdcb4dbdeadc5e288daaccd85d56438c9332fd",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -357,7 +357,8 @@ def generate(\n                 for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                 Retrieval](https://arxiv.org/abs/2010.00904).\n             synced_gpus (`bool`, *optional*, defaults to `False`):\n-                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n+                Whether to continue running the while loop until max_length (needed to avoid deadlocking with\n+                `FullyShardedDataParallel` and DeepSpeed ZeRO Stage 3).\n             return_timestamps (`bool`, *optional*):\n                 Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n             task (`str`, *optional*):"
        },
        {
            "sha": "20b9f6dad231d127ae611fa4e8e75d583132d4f6",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -4031,7 +4031,7 @@ def evaluation_loop(\n             start_time = time.time()\n             model = (\n                 self.accelerator.prepare(model)\n-                if self.is_deepspeed_enabled\n+                if self.is_deepspeed_enabled or self.is_fsdp_enabled\n                 else self.accelerator.prepare_model(model, evaluation_mode=True)\n             )\n             self.model_preparation_time = round(time.time() - start_time, 4)\n@@ -4634,7 +4634,7 @@ def prediction_loop(\n         if len(self.accelerator._models) == 0 and model is self.model:\n             model = (\n                 self.accelerator.prepare(model)\n-                if self.is_deepspeed_enabled\n+                if self.is_deepspeed_enabled or self.is_fsdp_enabled\n                 else self.accelerator.prepare_model(model, evaluation_mode=True)\n             )\n "
        },
        {
            "sha": "07d0571e44c9b9868f2b21f600078b19be3c6d72",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -12,17 +12,20 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import contextlib\n import warnings\n from copy import deepcopy\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n+from torch.distributed.fsdp import FullyShardedDataParallel\n from torch.utils.data import Dataset\n \n from .generation.configuration_utils import GenerationConfig\n from .integrations.deepspeed import is_deepspeed_zero3_enabled\n+from .integrations.fsdp import is_fsdp_managed_module\n from .trainer import Trainer\n from .utils import is_datasets_available, logging\n from .utils.deprecation import deprecate_kwarg\n@@ -303,10 +306,8 @@ def prediction_step(\n         if \"max_length\" in gen_kwargs and gen_kwargs[\"max_length\"] is None:\n             gen_kwargs.pop(\"max_length\")\n \n-        default_synced_gpus = True if is_deepspeed_zero3_enabled() else False\n-        gen_kwargs[\"synced_gpus\"] = (\n-            gen_kwargs[\"synced_gpus\"] if gen_kwargs.get(\"synced_gpus\") is not None else default_synced_gpus\n-        )\n+        default_synced_gpus = is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self.model)\n+        gen_kwargs[\"synced_gpus\"] = gen_kwargs.get(\"synced_gpus\", default_synced_gpus)\n \n         generation_inputs = inputs.copy()\n         # If the `decoder_input_ids` was created from `labels`, evict the former, so that the model can freely generate\n@@ -319,7 +320,15 @@ def prediction_step(\n             generation_inputs = {\n                 k: v for k, v in inputs.items() if k not in (\"decoder_input_ids\", \"decoder_attention_mask\")\n             }\n-        generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n+\n+        summon_full_params_context = (\n+            FullyShardedDataParallel.summon_full_params(self.model)\n+            if isinstance(self.model, FullyShardedDataParallel)\n+            else contextlib.nullcontext()\n+        )\n+\n+        with summon_full_params_context:\n+            generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n \n         # Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\n         # TODO: remove this hack when the legacy code that initializes generation_config from a model config is"
        },
        {
            "sha": "904ccdea63baf97b6c90624f3a46f36c6c729c98",
            "filename": "tests/generation/test_fsdp.py",
            "status": "added",
            "additions": 148,
            "deletions": 0,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/tests%2Fgeneration%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/tests%2Fgeneration%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_fsdp.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -0,0 +1,148 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+from typing import Any, Callable\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    execute_subprocess_async,\n+    get_torch_dist_unique_port,\n+    require_torch_multi_gpu,\n+)\n+\n+\n+if is_torch_available():\n+    import functools\n+\n+    import torch\n+    import torch.distributed\n+    from torch.distributed._composable.fsdp import fully_shard, register_fsdp_forward_method\n+    from torch.distributed.device_mesh import init_device_mesh\n+    from torch.distributed.fsdp import FullyShardedDataParallel\n+    from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n+\n+    from transformers import AutoModelForCausalLM, AutoTokenizer\n+    from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n+\n+    data = 4 * [\n+        \"Hello world!\",\n+        \"The quick brown fox jumps over the lazy dog.\",\n+    ]\n+\n+    def manage_process_group(func: Callable[..., Any]) -> Callable[..., Any]:\n+        \"\"\"Manage the creation and destruction of the distributed process group for the wrapped function.\"\"\"\n+\n+        def wrapped(*args: Any, **kwargs: Any) -> Any:\n+            torch.distributed.init_process_group(world_size=torch.cuda.device_count())\n+            try:\n+                return func(*args, **kwargs)\n+            finally:\n+                torch.distributed.destroy_process_group()\n+\n+        return wrapped\n+\n+    @manage_process_group\n+    def fsdp_generate():\n+        torch.cuda.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n+\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(device)\n+\n+        fsdp_model = FullyShardedDataParallel(\n+            model,\n+            auto_wrap_policy=functools.partial(transformer_auto_wrap_policy, transformer_layer_cls={GPT2Block}),\n+            limit_all_gathers=True,\n+            use_orig_params=True,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        batch = tokenizer(data[rank], return_tensors=\"pt\", return_attention_mask=True).to(device)\n+\n+        with FullyShardedDataParallel.summon_full_params(fsdp_model):\n+            _ = fsdp_model.module.generate(\n+                input_ids=batch[\"input_ids\"],\n+                attention_mask=batch[\"attention_mask\"],\n+                max_length=30,\n+            )\n+\n+    @manage_process_group\n+    def fsdp2_generate():\n+        torch.cuda.set_device(device := torch.device(rank := torch.distributed.get_rank()))\n+\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(device)\n+\n+        mesh = init_device_mesh(\"cuda\", (torch.distributed.get_world_size(),))\n+        for submodule in model.modules():\n+            if isinstance(submodule, GPT2Block):\n+                fully_shard(submodule, mesh=mesh)\n+        fully_shard(model, mesh=mesh)\n+\n+        register_fsdp_forward_method(model, \"generate\")\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        batch = tokenizer(data[rank], return_tensors=\"pt\", return_attention_mask=True).to(device)\n+\n+        _ = model.generate(\n+            input_ids=batch[\"input_ids\"],\n+            attention_mask=batch[\"attention_mask\"],\n+            max_length=30,\n+        )\n+\n+\n+class TestFSDPGeneration(TestCasePlus):\n+    @require_torch_multi_gpu\n+    def test_fsdp_generate(self):\n+        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n+            --master_port={get_torch_dist_unique_port()}\n+            {self.test_file_dir}/test_fsdp.py\n+        \"\"\".split()\n+        args = \"--fsdp\".split()\n+        cmd = [\"torchrun\"] + distributed_args + args\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n+\n+    @require_torch_multi_gpu\n+    def test_fsdp2_generate(self):\n+        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n+            --master_port={get_torch_dist_unique_port()}\n+            {self.test_file_dir}/test_fsdp.py\n+        \"\"\".split()\n+        args = \"--fsdp2\".split()\n+        cmd = [\"torchrun\"] + distributed_args + args\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n+\n+\n+if __name__ == \"__main__\":\n+    # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:\n+    #\n+    # PYTHONPATH=\"src\" python -m torch.distributed.run --nproc_per_node 2 --output_dir output_dir ./tests/generation/test_fsdp.py --fsdp\n+\n+    class CLIArgs(argparse.Namespace):\n+        fsdp: bool\n+        fsdp2: bool\n+\n+    parser = argparse.ArgumentParser()\n+    group = parser.add_mutually_exclusive_group()\n+    group.add_argument(\"--fsdp\", action=\"store_true\")\n+    group.add_argument(\"--fsdp2\", action=\"store_true\")\n+    args = parser.parse_args(namespace=CLIArgs())\n+\n+    if args.fsdp:\n+        fsdp_generate()\n+    elif args.fsdp2:\n+        fsdp2_generate()\n+    else:\n+        raise ValueError(\"Missing test selection\")"
        },
        {
            "sha": "994a82a8db0c448ae4044a3c423f9363c7032420",
            "filename": "tests/trainer/test_trainer_fsdp.py",
            "status": "added",
            "additions": 114,
            "deletions": 0,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/70b07d97cf2c5f61fff55700b65528a1b6845cd2/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/70b07d97cf2c5f61fff55700b65528a1b6845cd2/tests%2Ftrainer%2Ftest_trainer_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_fsdp.py?ref=70b07d97cf2c5f61fff55700b65528a1b6845cd2",
            "patch": "@@ -0,0 +1,114 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Dict\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    TestCasePlus,\n+    execute_subprocess_async,\n+    get_torch_dist_unique_port,\n+    require_accelerate,\n+    require_torch_multi_gpu,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.distributed\n+    import torch.utils.data\n+\n+    from transformers import (\n+        AutoModelForCausalLM,\n+        AutoTokenizer,\n+        DataCollatorForSeq2Seq,\n+        EvalPrediction,\n+        GenerationConfig,\n+        HfArgumentParser,\n+        PreTrainedTokenizerBase,\n+        Seq2SeqTrainer,\n+        Seq2SeqTrainingArguments,\n+    )\n+\n+    class DummyTextDataset(torch.utils.data.Dataset[str]):\n+        def __init__(self, tokenizer: PreTrainedTokenizerBase) -> None:\n+            data = 4 * [\n+                \"Hello world!\",\n+                \"The quick brown fox jumps over the lazy dog.\",\n+            ]\n+            self.data = [\n+                {k: v.squeeze(0) for k, v in tokenizer(item, return_tensors=\"pt\", return_attention_mask=True).items()}\n+                for item in data\n+            ]\n+            for item in self.data:\n+                item[\"labels\"] = item[\"input_ids\"]\n+\n+        def __len__(self) -> int:\n+            return len(self.data)\n+\n+        def __getitem__(self, i: int) -> str:\n+            return self.data[i]\n+\n+\n+class TestFSDPTrainer(TestCasePlus):\n+    @require_accelerate\n+    @require_torch_multi_gpu\n+    def test_trainer(self):\n+        output_dir = self.get_auto_remove_tmp_dir()\n+        cmd = [\n+            \"accelerate\",\n+            \"launch\",\n+            \"--use_fsdp\",\n+            \"--main_process_port\",\n+            f\"{get_torch_dist_unique_port()}\",\n+            \"--num_processes\",\n+            f\"{torch.cuda.device_count()}\",\n+            \"--fsdp_transformer_layer_cls_to_wrap\",\n+            \"GPT2Block\",\n+            f\"{self.test_file_dir}/test_trainer_fsdp.py\",\n+            \"--output_dir\",\n+            f\"{output_dir}\",\n+            \"--report_to\",\n+            \"none\",\n+        ]\n+        execute_subprocess_async(cmd, env=self.get_env())\n+        # successful return here == success - any errors would have caused an error in the sub-call\n+\n+\n+if __name__ == \"__main__\":\n+    parser = HfArgumentParser((Seq2SeqTrainingArguments,))\n+    training_args = parser.parse_args_into_dataclasses()[0]\n+    training_args.per_device_eval_batch_size = 1\n+    training_args.use_legacy_prediction_loop = False\n+    training_args.predict_with_generate = True\n+    training_args.generation_config = GenerationConfig(max_length=30)\n+\n+    pretrained_model_name = \"hf-internal-testing/tiny-random-gpt2\"\n+    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n+    tokenizer.pad_token = tokenizer.eos_token\n+    device = torch.device(torch.distributed.get_rank())\n+    model = AutoModelForCausalLM.from_pretrained(pretrained_model_name).to(device)\n+\n+    def compute_metrics(p: EvalPrediction) -> Dict[str, bool]:\n+        return {\"accuracy\": (p.predictions == p.label_ids).mean()}\n+\n+    trainer = Seq2SeqTrainer(\n+        model=model,\n+        args=training_args,\n+        data_collator=DataCollatorForSeq2Seq(tokenizer, model),\n+        eval_dataset=DummyTextDataset(tokenizer),\n+        compute_metrics=compute_metrics,\n+    )\n+\n+    metrics = trainer.evaluate()"
        }
    ],
    "stats": {
        "total": 532,
        "additions": 434,
        "deletions": 98
    }
}