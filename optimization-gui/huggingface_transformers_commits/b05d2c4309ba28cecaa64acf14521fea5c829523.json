{
    "author": "SunMarc",
    "message": "Fix dtype quantizer (#42882)\n\n* fix dtype quantizer\n\n* fix\n\n* rm print\n\n* fix\n\n* style\n\n* fix\n\n* revert\n\n* bitnet\n\n* fix\n\n* gogo\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* warn instead\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "b05d2c4309ba28cecaa64acf14521fea5c829523",
    "files": [
        {
            "sha": "5b599d860be8bc826fad69bc5e717bf7bc68d935",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -1019,10 +1019,6 @@ def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n         Checks and removes if there are any keys in the dict that should not be serialized when saving the config.\n         Runs recursive check on the dict, to remove from all sub configs.\n         \"\"\"\n-        if hasattr(self, \"quantization_config\"):\n-            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n-            _ = d.pop(\"_pre_quantization_dtype\", None)\n-\n         if \"_auto_class\" in d:\n             del d[\"_auto_class\"]\n         if \"_output_attentions\" in d:"
        },
        {
            "sha": "14f1f1beda464e2887dae6e34575300275f9b744",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 14,
            "deletions": 12,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -233,7 +233,7 @@ def replace_with_bnb_linear(\n \n \n # Copied from PEFT: https://github.com/huggingface/peft/blob/47b3712898539569c02ec5b3ed4a6c36811331a1/src/peft/utils/integrations.py#L41\n-def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", dtype: \"torch.dtype\", state=None):\n+def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", state=None):\n     \"\"\"\n     Helper function to dequantize 4bit or 8bit bnb weights.\n \n@@ -248,10 +248,7 @@ def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", dtype: \"torch.dtype\", st\n \n     if cls_name == \"Params4bit\":\n         output_tensor = bnb.functional.dequantize_4bit(weight.data, weight.quant_state)\n-        logger.warning_once(\n-            f\"The model is going to be dequantized in {output_tensor.dtype} - if you want to upcast it to another dtype, make sure to pass the desired dtype when quantizing the model through `bnb_4bit_quant_type` argument of `BitsAndBytesConfig`\"\n-        )\n-        return output_tensor.to(dtype)\n+        return output_tensor\n \n     if state.SCB is None:\n         state.SCB = weight.SCB\n@@ -263,7 +260,7 @@ def dequantize_bnb_weight(weight: \"torch.nn.Parameter\", dtype: \"torch.dtype\", st\n         # Multiply by (scale/127) to dequantize.\n         dequantized = weight.data * state.SCB.view(-1, 1) * 7.874015718698502e-3\n \n-    return dequantized.to(dtype)\n+    return dequantized\n \n \n def _create_accelerate_new_hook(old_hook):\n@@ -283,10 +280,7 @@ def _create_accelerate_new_hook(old_hook):\n     return new_hook\n \n \n-def dequantize_and_replace(\n-    model,\n-    quantization_config=None,\n-):\n+def dequantize_and_replace(model, quantization_config=None, dtype=None):\n     \"\"\"\n     Converts a quantized model into its dequantized original version. The newly converted model will have\n     some performance drop compared to the original model before quantization - use it only for specific usecases\n@@ -297,14 +291,22 @@ def dequantize_and_replace(\n     quant_method = quantization_config.quantization_method()\n \n     target_cls = bnb.nn.Linear8bitLt if quant_method == \"llm_int8\" else bnb.nn.Linear4bit\n-\n     for module_name, module in model.named_modules():\n         if isinstance(module, target_cls):\n             with init_empty_weights():\n                 bias = getattr(module, \"bias\", None)\n                 new_module = torch.nn.Linear(module.in_features, module.out_features, bias=bias is not None)\n             state = module.state if quant_method == \"llm_int8\" else None\n-            new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, model.dtype, state))\n+            new_module.weight = torch.nn.Parameter(dequantize_bnb_weight(module.weight, state))\n+            weight = dequantize_bnb_weight(module.weight, state)\n+            if dtype is None:\n+                logger.warning_once(\n+                    f\"The modules are dequantized in {weight.dtype}. If you want to change the dtype, please specify `dtype` in `dequantize`. \"\n+                )\n+            else:\n+                logger.warning_once(f\"The modules are dequantized in {weight.dtype} and casted to {dtype}.\")\n+                weight = weight.to(dtype)\n+            new_module.weight = torch.nn.Parameter(weight)\n             if bias is not None:\n                 new_module.bias = bias\n             if hasattr(module, \"_hf_hook\"):"
        },
        {
            "sha": "219b8b1d16ea820cc0625d6b79519a445d2bb268",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -20,8 +20,8 @@ def get_target_dtype(query: torch.Tensor, module: torch.nn.Module) -> torch.dtyp\n                 else torch.get_autocast_gpu_dtype()\n             )\n         # Handle the case where the model is quantized\n-        elif hasattr(module.config, \"_pre_quantization_dtype\"):\n-            return module.config._pre_quantization_dtype\n+        elif hasattr(module.config, \"quantization_config\"):\n+            return module.config.dtype\n         else:\n             return next(layer for layer in module.modules() if isinstance(layer, torch.nn.Linear)).weight.dtype\n     return None"
        },
        {
            "sha": "abc202678b3e8d5ea9894f34465790b4b66f7afc",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 19,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -792,6 +792,7 @@ def _get_dtype(\n     sharded_metadata: Optional[dict],\n     state_dict: Optional[dict],\n     weights_only: bool,\n+    hf_quantizer: Optional[HfQuantizer] = None,\n ) -> tuple[PreTrainedConfig, torch.dtype]:\n     \"\"\"Find the correct `dtype` to use based on provided arguments. Also update the `config` based on the\n     inferred dtype. We do the following:\n@@ -840,6 +841,9 @@ def _get_dtype(\n         # set torch.get_default_dtype() (usually fp32) as the default dtype if `None` is provided\n         dtype = torch.get_default_dtype()\n \n+    if hf_quantizer is not None:\n+        hf_quantizer.update_dtype(dtype)\n+\n     # Get the main dtype\n     if isinstance(dtype, dict):\n         main_dtype = dtype.get(\"\", torch.get_default_dtype())\n@@ -1433,7 +1437,7 @@ def tp_plan(self, plan: dict[str, str] | None):\n     def pp_plan(self, plan: dict[str, tuple[str, str]]):\n         self._pp_plan = plan\n \n-    def dequantize(self):\n+    def dequantize(self, dtype=None):\n         \"\"\"\n         Potentially dequantize the model in case it has been quantized by a quantization method that support\n         dequantization.\n@@ -1443,7 +1447,7 @@ def dequantize(self):\n         if hf_quantizer is None:\n             raise ValueError(\"You need to first quantize your model in order to dequantize it\")\n \n-        return hf_quantizer.dequantize(self)\n+        return hf_quantizer.dequantize(self, dtype=dtype)\n \n     def _backward_compatibility_gradient_checkpointing(self):\n         if self.supports_gradient_checkpointing and getattr(self.config, \"gradient_checkpointing\", False):\n@@ -3875,8 +3879,8 @@ def from_pretrained(\n         if \"attn_implementation\" in kwargs:\n             config._attn_implementation = kwargs.pop(\"attn_implementation\")\n \n-        hf_quantizer, config, dtype, device_map = get_hf_quantizer(\n-            config, quantization_config, dtype, device_map, weights_only, user_agent\n+        hf_quantizer, config, device_map = get_hf_quantizer(\n+            config, quantization_config, device_map, weights_only, user_agent\n         )\n \n         if gguf_file:\n@@ -3923,7 +3927,9 @@ def from_pretrained(\n             ]\n \n         # Find the correct dtype based on current state\n-        config, dtype = _get_dtype(dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only)\n+        config, dtype = _get_dtype(\n+            dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only, hf_quantizer\n+        )\n \n         config.name_or_path = pretrained_model_name_or_path\n         model_init_context = cls.get_init_context(dtype, is_quantized, _is_ds_init_called)\n@@ -3932,22 +3938,18 @@ def from_pretrained(\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)\n \n+            if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n+                hf_quantizer.preprocess_model(\n+                    model=model,\n+                    dtype=dtype,\n+                    device_map=device_map,\n+                    checkpoint_files=checkpoint_files,\n+                    use_kernels=use_kernels,\n+                )\n+\n         # Obtain the weight conversion mapping for this model if any are registered\n         weight_conversions = get_model_conversion_mapping(model, key_mapping, hf_quantizer)\n \n-        # make sure we use the model's config since the __init__ call might have copied it\n-        config = model.config\n-\n-        if hf_quantizer is not None:  # replace module with quantized modules (does not touch weights)\n-            hf_quantizer.preprocess_model(\n-                model=model,\n-                device_map=device_map,\n-                keep_in_fp32_modules=model._keep_in_fp32_modules,  # TODO prob no longer needed?\n-                config=config,\n-                checkpoint_files=checkpoint_files,\n-                use_kernels=use_kernels,\n-            )\n-\n         if _torch_distributed_available and device_mesh is not None:  # add hooks to nn.Modules: no weights\n             model = distribute_model(model, tp_plan, distributed_config, device_mesh, tp_size)\n \n@@ -3994,7 +3996,9 @@ def from_pretrained(\n \n         if hf_quantizer is not None:\n             model.hf_quantizer = hf_quantizer\n-            hf_quantizer.postprocess_model(model, config=config)  # usually a no-op but sometimes needed\n+            hf_quantizer.postprocess_model(\n+                model\n+            )  # usually a no-op but sometimes needed, e.g to remove the quant config when dequantizing\n \n         if _adapter_model_path is not None:\n             adapter_kwargs[\"key_mapping\"] = key_mapping"
        },
        {
            "sha": "08ca18074206fc0e39c58853905791524709ee42",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -361,8 +361,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "9d661ad2ab5dd39f3ba5f66f7d287e5676c14fd1",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -236,8 +236,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "f0633e0613fe24ab64621bd98958e8f9b2020c10",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -521,8 +521,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.query_key_value.weight.dtype\n "
        },
        {
            "sha": "f117e47cefd80b53b9077942fb9b6bc6f758c6a3",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -345,7 +345,7 @@ def cuda_kernels_forward(\n \n             # In case the model has been quantized, we need a hack to properly call the `nn.Linear` module\n             # at the price of a small overhead.\n-            if hasattr(self.config, \"_pre_quantization_dtype\"):\n+            if hasattr(self.config, \"quantization_config\"):\n                 discrete_time_step = (self.dt_proj(time_step) - self.dt_proj.bias).transpose(1, 2)\n             else:\n                 discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)"
        },
        {
            "sha": "77c9ffd95fed65f94be312f6ef2e6bfd2f341eae",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -357,7 +357,7 @@ def cuda_kernels_forward(\n \n             # In case the model has been quantized, we need a hack to properly call the `nn.Linear` module\n             # at the price of a small overhead.\n-            if hasattr(self.config, \"_pre_quantization_dtype\"):\n+            if hasattr(self.config, \"quantization_config\"):\n                 discrete_time_step = (self.dt_proj(time_step) - self.dt_proj.bias).transpose(1, 2)\n             else:\n                 discrete_time_step = self.dt_proj.weight @ time_step.transpose(1, 2)"
        },
        {
            "sha": "6776a88322ae9540254f8c945ce274b08a64b3d1",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -237,8 +237,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "0e79c0bdfeaef8fa997b6c419a018aa8aa388617",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -334,8 +334,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "9fdf1bb86cc86c80eb386ae9393ccbe314bd9e7f",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -600,8 +600,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "9b3630c11d24c8ab04961d20322e10ca1a204bc7",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -814,8 +814,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "5f6e57aad3dbe75df135a221dc50e6ea4104d5c8",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -609,8 +609,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "a919db68a880a9e1c9ec43efcc1c7529666385b1",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -397,8 +397,8 @@ def forward(\n                     else torch.get_autocast_gpu_dtype()\n                 )\n             # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+            elif hasattr(self.config, \"quantization_config\"):\n+                target_dtype = self.config.dtype\n             else:\n                 target_dtype = self.q_proj.weight.dtype\n "
        },
        {
            "sha": "9e4be035583ea25d9b1edffb8d967eb85bc16c26",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -302,7 +302,7 @@ def register_quantizer_fn(cls):\n     return register_quantizer_fn\n \n \n-def get_hf_quantizer(config, quantization_config, dtype, device_map, weights_only, user_agent):\n+def get_hf_quantizer(config, quantization_config, device_map, weights_only, user_agent):\n     pre_quantized = hasattr(config, \"quantization_config\")\n     if pre_quantized and not AutoHfQuantizer.supports_quant_method(config.quantization_config):\n         pre_quantized = False\n@@ -324,11 +324,9 @@ def get_hf_quantizer(config, quantization_config, dtype, device_map, weights_onl\n \n     if hf_quantizer is not None:\n         hf_quantizer.validate_environment(\n-            dtype=dtype,\n             device_map=device_map,\n             weights_only=weights_only,\n         )\n-        dtype = hf_quantizer.update_dtype(dtype)\n         device_map = hf_quantizer.update_device_map(device_map)\n         config = hf_quantizer.update_tp_plan(config)\n         config = hf_quantizer.update_ep_plan(config)\n@@ -337,4 +335,4 @@ def get_hf_quantizer(config, quantization_config, dtype, device_map, weights_onl\n         if not getattr(hf_quantizer.quantization_config, \"dequantize\", False):\n             quant_method = hf_quantizer.quantization_config.quant_method\n             user_agent[\"quant\"] = getattr(quant_method, \"value\", quant_method)\n-    return hf_quantizer, config, dtype, device_map\n+    return hf_quantizer, config, device_map"
        },
        {
            "sha": "27cec13b24c10e4657be5228f9167ac030e62378",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 8,
            "deletions": 24,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -31,16 +31,6 @@\n logger = logging.get_logger(__file__)\n \n \n-def _assign_original_dtype(module, original_dtype):\n-    # not very nice in a recursive function but it avoids a circular import\n-    from ..modeling_utils import PreTrainedModel\n-\n-    for child in module.children():\n-        if isinstance(child, PreTrainedModel):\n-            child.config._pre_quantization_dtype = original_dtype\n-        _assign_original_dtype(child, original_dtype)\n-\n-\n def get_keys_to_not_convert(model) -> list:\n     r\"\"\"\n     Function to automatically detect keys to not convert for usage like quantization. For example for CausalLM modules\n@@ -176,7 +166,7 @@ def update_ep_plan(self, config):\n     def _process_model_before_weight_loading(self, model, **kwargs):\n         return model\n \n-    def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n+    def preprocess_model(self, model: \"PreTrainedModel\", dtype=None, **kwargs):\n         \"\"\"\n         Setting model attributes and/or converting model before weights loading. At this point\n         the model should be initialized on the meta device so you can freely manipulate the skeleton\n@@ -194,14 +184,6 @@ def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpo\n             self._convert_model_for_quantization(model)\n         self._process_model_before_weight_loading(model, **kwargs)\n \n-        # We store the original dtype for quantized models as we cannot easily retrieve it\n-        # once the weights have been quantized\n-        # Note that once you have loaded a quantized model, you can't change its dtype so this will\n-        # remain a single source of truth\n-        original_dtype = dtype if dtype is not None else torch.get_default_dtype()\n-        config._pre_quantization_dtype = original_dtype\n-        _assign_original_dtype(model, original_dtype)\n-\n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         return model\n \n@@ -231,18 +213,20 @@ def remove_quantization_config(self, model):\n             del model.hf_quantizer\n         if hasattr(model.config, \"quantization_config\"):\n             del model.config.quantization_config\n-        if hasattr(model.config, \"_pre_quantization_dtype\"):\n-            del model.config._pre_quantization_dtype\n         if hasattr(model, \"quantization_method\"):\n             del model.quantization_method\n         model.is_quantized = False\n \n-    def dequantize(self, model):\n+    def dequantize(self, model, dtype=None):\n         \"\"\"\n         Potentially dequantize the model to retrieve the original model, with some loss in accuracy / performance.\n         Note not all quantization schemes support this.\n         \"\"\"\n-        model = self._dequantize(model)\n+        if dtype is None:\n+            # using the same dtype we used to load the model. If we don't do that, we might have issues with modules we didn't quantize.\n+            # or we need to upcast everything to the same dtype\n+            dtype = model.config.dtype\n+        model = self._dequantize(model, dtype=dtype)\n         self.remove_quantization_config(model)\n \n         return model\n@@ -258,7 +242,7 @@ def get_accelerator_warm_up_factor(self):\n         # weight loading)\n         return 4\n \n-    def _dequantize(self, model):\n+    def _dequantize(self, model, dtype=None):\n         raise NotImplementedError(\n             f\"{self.quantization_config.quant_method} has no implementation of `dequantize`, please raise an issue on GitHub.\"\n         )"
        },
        {
            "sha": "d4991610bba459477998a42b8634a266e3471aa8",
            "filename": "src/transformers/quantizers/quantizer_aqlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -23,13 +23,10 @@\n     from ..modeling_utils import PreTrainedModel\n \n from ..integrations import replace_with_aqlm_linear\n-from ..utils import is_accelerate_available, is_aqlm_available, is_torch_available, logging\n+from ..utils import is_accelerate_available, is_aqlm_available, logging\n from ..utils.quantization_config import QuantizationConfigMixin\n \n \n-if is_torch_available():\n-    import torch\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -50,20 +47,6 @@ def validate_environment(self, *args, **kwargs):\n         if not is_aqlm_available():\n             raise ImportError(\"Using `aqlm` quantization requires AQLM: `pip install aqlm[gpu,cpu]`\")\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            if torch.cuda.is_available():\n-                dtype = torch.float16\n-                logger.info(\n-                    \"CUDA available. Assuming AQLM inference on GPU and loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\"\n-                )\n-            else:\n-                dtype = torch.float32\n-                logger.info(\n-                    \"CUDA is unavailable. Assuming AQLM inference on CPU and loading the model in `torch.float32`. To overwrite it, set `dtype` manually.\"\n-                )\n-        return dtype\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\","
        },
        {
            "sha": "3a52314013066b21fb90ed5646e101ec7d67cd09",
            "filename": "src/transformers/quantizers/quantizer_auto_round.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -19,13 +19,10 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_auto_round_available, is_torch_available, logging\n+from ..utils import is_auto_round_available, logging\n from ..utils.quantization_config import QuantizationConfigMixin\n \n \n-if is_torch_available():\n-    import torch\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -47,12 +44,6 @@ def validate_environment(self, *args, **kwargs):\n                 \"Loading an AutoRound quantized model requires auto-round library (`pip install 'auto-round>=0.5'`)\"\n             )\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.bfloat16\n-            logger.info(\"Loading the model in `torch.bfloat16`. To overwrite it, set `dtype` manually.\")\n-        return dtype\n-\n     def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         if model.__class__.main_input_name != \"input_ids\":\n             logger.warning(\"AutoRound offers only limited support for models that are not strictly text-based.\")"
        },
        {
            "sha": "692401b4c48646d1fb4dd964c8fb140385ddf1eb",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -53,10 +53,7 @@ def validate_environment(self, **kwargs):\n             raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n \n     def update_dtype(self, dtype):\n-        if dtype is None:\n-            dtype = torch.float16\n-            logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\")\n-        elif dtype == torch.bfloat16 and (torch.cuda.is_available() or torch.xpu.is_available()):\n+        if dtype == torch.bfloat16 and (torch.cuda.is_available() or torch.xpu.is_available()):\n             logger.warning(\n                 \"`torch.bfloat16` is not supported for AWQ CUDA/XPU kernels yet. Casting to `torch.float16`.\"\n             )\n@@ -65,13 +62,11 @@ def update_dtype(self, dtype):\n             logger.warning(\"We suggest you to set `dtype=torch.float16` for better efficiency on CUDA/XPU with AWQ.\")\n         return dtype\n \n-    def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: list[str] | None = None, **kwargs\n-    ):\n+    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         from ..integrations import replace_quantization_scales, replace_with_awq_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules, add_default_skips=True\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules, add_default_skips=True\n         )\n \n         model = replace_with_awq_linear("
        },
        {
            "sha": "5bac524dd95b7f2d2acf2dfde7508696db522391",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -68,13 +68,12 @@ def validate_environment(self, *args, **kwargs):\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_bitnet_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         model = replace_with_bitnet_linear("
        },
        {
            "sha": "3a53e5380dffea0d43497f796c5543fec1a9368e",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 18,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -122,20 +122,6 @@ def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int |\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        # TODO: remove ? is it still true ? we will move to dtype = \"auto\" so it will likely be either fp16 or bf16\n-        if dtype is None:\n-            # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n-            logger.info(\n-                \"Overriding dtype=%s with `dtype=torch.float16` due to \"\n-                \"requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n-                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n-                \" dtype=torch.float16 to remove this warning.\",\n-                dtype,\n-            )\n-            dtype = torch.float16\n-        return dtype\n-\n     def update_device_map(self, device_map):\n         if device_map is None:\n             if torch.cuda.is_available():\n@@ -159,13 +145,12 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         device_map,\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_bnb_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n+            model, self.quantization_config.llm_int8_skip_modules, model._keep_in_fp32_modules\n         )\n \n         if self.quantization_config.llm_int8_enable_fp32_cpu_offload:\n@@ -192,10 +177,10 @@ def is_serializable(self):\n     def is_trainable(self) -> bool:\n         return True\n \n-    def _dequantize(self, model):\n+    def _dequantize(self, model, dtype=None):\n         from ..integrations import dequantize_and_replace\n \n-        model = dequantize_and_replace(model, quantization_config=self.quantization_config)\n+        model = dequantize_and_replace(model, quantization_config=self.quantization_config, dtype=dtype)\n         return model\n \n     def get_quantize_ops(self):"
        },
        {
            "sha": "c37975eb50d1d99900a4d841c0c948a746d82f74",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 17,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -83,19 +83,6 @@ def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int |\n         max_memory = {key: val * 0.90 for key, val in max_memory.items()}\n         return max_memory\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            # We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\n-            logger.info(\n-                \"Overriding dtype=%s with `dtype=torch.float16` due to \"\n-                \"requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \"\n-                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n-                \" dtype=torch.float16 to remove this warning.\",\n-                dtype,\n-            )\n-            dtype = torch.float16\n-        return dtype\n-\n     def update_device_map(self, device_map):\n         if device_map is None:\n             if torch.cuda.is_available():\n@@ -133,13 +120,12 @@ def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n         device_map,\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_bnb_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.llm_int8_skip_modules, keep_in_fp32_modules\n+            model, self.quantization_config.llm_int8_skip_modules, model._keep_in_fp32_modules\n         )\n \n         if self.quantization_config.llm_int8_enable_fp32_cpu_offload:\n@@ -161,10 +147,10 @@ def is_serializable(self):\n     def is_trainable(self) -> bool:\n         return True\n \n-    def _dequantize(self, model):\n+    def _dequantize(self, model, dtype=None):\n         from ..integrations import dequantize_and_replace\n \n-        model = dequantize_and_replace(model, quantization_config=self.quantization_config)\n+        model = dequantize_and_replace(model, quantization_config=self.quantization_config, dtype=dtype)\n         return model\n \n     def get_quantize_ops(self):"
        },
        {
            "sha": "586f47ac0d1ca8196ecc3f11f68a47078d2b7862",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -59,10 +59,7 @@ def validate_environment(self, *args, **kwargs):\n             )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            logger.info(\"Loading model using torch.float16 for compressed-tensors quantization\")\n-            dtype = torch.float16\n-        elif dtype != torch.float16:\n+        if dtype != torch.float16:\n             logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with compressed_tensors.\")\n         return dtype\n "
        },
        {
            "sha": "7bef2c7dea1bd77a8969ed07106e040c10f55336",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -64,16 +64,7 @@ def validate_environment(self, *args, **kwargs):\n                 )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.float16\n-            logger.info(\n-                \"Overriding dtype=%s with `dtype=torch.float16` due to \"\n-                \"requirements of `eetq` to enable model loading in 8-bit. \"\n-                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n-                \" dtype=torch.float16 to remove this warning.\",\n-                dtype,\n-            )\n-        elif dtype != torch.float16:\n+        if dtype != torch.float16:\n             logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with EETQ.\")\n         return dtype\n \n@@ -92,13 +83,12 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_eetq_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         model = replace_with_eetq_linear("
        },
        {
            "sha": "5a49c323be9b6c24e43c14b7a234156dcb73f85e",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -84,19 +84,11 @@ def validate_environment(self, *args, **kwargs):\n                 )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.bfloat16\n-            logger.info(\n-                \"Overriding dtype=%s with `dtype=torch.bloat16` due to \"\n-                \"requirements of `fbgemm-gpu` to enable model loading in fp8. \"\n-                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n-                \" dtype=torch.bfloat16 to remove this warning.\",\n-                dtype,\n-            )\n-        elif dtype == torch.float16:\n-            raise ValueError(\n-                \"You cannot use FP8 with dtype=torch.float16. We recommend you passing dtype=torch.bfloat16\"\n+        if dtype != torch.bfloat16:\n+            logger.warning_once(\n+                f\"Setting dtype to {dtype}, but only bfloat16 is supported right now. Overwriting torch_dtype to bfloat16.\"\n             )\n+            dtype = torch.bfloat16\n         return dtype\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n@@ -119,13 +111,12 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_fbgemm_fp8_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         model = replace_with_fbgemm_fp8_linear("
        },
        {
            "sha": "bd052e78b262ab3a03ae5cfc8cb3d1f7405965df",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -85,13 +85,12 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations.finegrained_fp8 import replace_with_fp8_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         model = replace_with_fp8_linear("
        },
        {
            "sha": "1422dec9c35257bbeb276137005fa70d0a1e749c",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -78,11 +78,11 @@ def validate_environment(self, device_map, **kwargs):\n                 )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            logger.info(\"`dtype` is None. Setting `dtype=torch.bfloat16` for qutlass compatibility.\")\n+        if dtype != torch.bfloat16:\n+            logger.warning_once(\n+                f\"Setting dtype to {dtype}, but only bfloat16 is supported right now. Overwriting torch_dtype to bfloat16.\"\n+            )\n             dtype = torch.bfloat16\n-        elif dtype != torch.bfloat16:\n-            raise ValueError(f\"Invalid `dtype` {dtype}. fp_quant quantization only supports `dtype=torch.bfloat16`.\")\n         return dtype\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:"
        },
        {
            "sha": "8e8f1a108b053c7e6a3aaf5eaf864719935c2130",
            "filename": "src/transformers/quantizers/quantizer_gptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -66,10 +66,7 @@ def validate_environment(self, *args, **kwargs):\n             raise ImportError(\"The gptqmodel version should be >= 1.4.3, optimum version should >= 1.24.0\")\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.float16\n-            logger.info(\"Loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\")\n-        elif dtype != torch.float16:\n+        if dtype != torch.float16:\n             logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with GPTQ.\")\n         return dtype\n "
        },
        {
            "sha": "950cd272b67e0df4711f99c23630e43bfe85c86b",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -69,10 +69,7 @@ def validate_environment(self, device_map, **kwargs):\n                 )\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            logger.info(\"`dtype` is None. Setting `dtype=torch.float16` for FLUTE compatibility.\")\n-            dtype = torch.float16\n-        elif dtype != torch.float16 and dtype != torch.bfloat16:\n+        if dtype != torch.float16 and dtype != torch.bfloat16:\n             raise ValueError(\n                 f\"Invalid `dtype` {dtype}. HIGGS quantization only supports `dtype=torch.float16` or `dtype=torch.bfloat16`.\"\n             )\n@@ -116,13 +113,12 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_higgs_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         replace_with_higgs_linear("
        },
        {
            "sha": "6b078c512440f3e565a1800409e98c7ecbd4ada4",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -135,18 +135,6 @@ def validate_environment(self, *args, **kwargs):\n                     \"Please use a quantized checkpoint or remove the CPU or disk device from the device_map.\"\n                 )\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.bfloat16\n-            logger.info(\n-                \"Overriding dtype=%s with `dtype=torch.bfloat16` due to \"\n-                \"requirements of `fbgemm-gpu` to enable model loading in fp4. \"\n-                \"Pass your own dtype to specify the dtype of the remaining non-linear layers or pass\"\n-                \" dtype=torch.bfloat16 to remove this warning.\",\n-                dtype,\n-            )\n-        return dtype\n-\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations import Mxfp4GptOssExperts\n \n@@ -167,7 +155,6 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         use_kernels: bool = False,\n         **kwargs,\n     ):\n@@ -182,7 +169,7 @@ def _process_model_before_weight_loading(\n             self.quantization_config.dequantize = True\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         model = replace_with_mxfp4_linear("
        },
        {
            "sha": "a2e6aaebcbc6fc5f877046bb5c40f2e809a03655",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -95,13 +95,11 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         target_dtype = mapping[self.quantization_config.weights]\n         return target_dtype\n \n-    def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: list[str] | None = None, **kwargs\n-    ):\n+    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         from ..integrations import replace_with_quanto_layers\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n \n         model = replace_with_quanto_layers("
        },
        {
            "sha": "419660c1f9208f5fcdc0d404c67d6b5f63109f3e",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -51,24 +51,19 @@ def validate_environment(self, *args, **kwargs):\n             raise ImportError(\"Using `spqr` quantization requires SpQR: `pip install spqr_quant[gpu]`\")\n \n     def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.float16\n-            logger.info(\"Assuming SpQR inference on GPU and loading the model in `torch.float16`.\")\n-        elif dtype != torch.float16:\n+        if dtype != torch.float16:\n             raise ValueError(\n-                \"You cannot use any type other than torch.float16 for SpQR. Please either leave it None or set it to\"\n-                \"torch.float16 explicitly.\"\n+                \"You cannot use any type other than torch.float16 for SpQR. Please set it totorch.float16 explicitly.\"\n             )\n         return dtype\n \n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n         replace_with_spqr_linear(\n             model,"
        },
        {
            "sha": "9c022ceb5d241b40a9bb3e18f11cf8ad14a6d51b",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 7,
            "deletions": 34,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -134,22 +134,11 @@ def validate_environment(self, *args, **kwargs):\n \n     def update_dtype(self, dtype):\n         if self.quantization_config.quant_type == \"int4_weight_only\":\n-            if dtype is not None and dtype != torch.bfloat16:\n+            if dtype != torch.bfloat16:\n                 logger.warning_once(\n-                    f\"Setting dtype to {dtype} for int4_weight_only quantization, but only bfloat16 is supported right now. Please set the dtype to bfloat16.\"\n-                )\n-            if dtype is None:\n-                logger.warning_once(\n-                    \"Setting dtype to torch.bfloat16 for int4_weight_only quantization since only bfloat16 is supported right now. Please set dtype=torch.bfloat16 to remove this warning.\"\n+                    f\"Setting dtype to {dtype} for int4_weight_only quantization, but only bfloat16 is supported right now. Overwriting torch_dtype to bfloat16.\"\n                 )\n                 dtype = torch.bfloat16\n-        if self.quantization_config.quant_type == \"int8_dynamic_activation_int8_weight\":\n-            if dtype is None:\n-                logger.info(\n-                    \"Setting dtype to torch.float32 for int8_dynamic_activation_int8_weight quantization as no dtype was specified in from_pretrained\"\n-                )\n-                # we need to set the dtype, otherwise we have dtype mismatch when performing the quantized linear op\n-                dtype = torch.float32\n         return dtype\n \n     def get_state_dict_and_metadata(self, model):\n@@ -203,11 +192,9 @@ def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int |\n         max_memory = {key: val * 0.9 for key, val in max_memory.items()}\n         return max_memory\n \n-    def _process_model_before_weight_loading(\n-        self, model: \"PreTrainedModel\", keep_in_fp32_modules: list[str] | None = None, **kwargs\n-    ):\n+    def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", checkpoint_files=None, **kwargs):\n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n         if self.quantization_config.include_input_output_embeddings:\n             input_emb = model.get_input_embeddings()\n@@ -217,7 +204,9 @@ def _process_model_before_weight_loading(\n             self.modules_to_not_convert = [\n                 x for x in self.modules_to_not_convert if x not in input_emb_names + output_emb_names\n             ]\n-        return\n+        if checkpoint_files is not None:\n+            # Torchao needs access to all metadata later\n+            self.set_metadata(checkpoint_files)\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         if self.pre_quantized:\n@@ -253,22 +242,6 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n \n         return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n \n-    def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n-        \"\"\"\n-        Setting model attributes and/or converting model before weights loading. At this point\n-        the model should be initialized on the meta device so you can freely manipulate the skeleton\n-        of the model in order to replace modules in-place. Make sure to override the abstract method `_process_model_before_weight_loading`.\n-\n-        Args:\n-            model (`~transformers.PreTrainedModel`):\n-                The model to quantize\n-            kwargs (`dict`, *optional*):\n-                The keyword arguments that are passed along `_process_model_before_weight_loading`.\n-        \"\"\"\n-        super().preprocess_model(model, config, dtype, checkpoint_files, **kwargs)\n-        # Torchao needs access to all metadata later\n-        self.set_metadata(checkpoint_files)\n-\n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n         if self.quantization_config.quant_type == \"autoquant\":"
        },
        {
            "sha": "0e2716eb6eaf5e3e01dfefa71528cd8f9b434d34",
            "filename": "src/transformers/quantizers/quantizer_vptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -49,24 +49,15 @@ def validate_environment(self, *args, **kwargs):\n         if not torch.cuda.is_available():\n             raise RuntimeError(\"GPU is required to run VTPQ quantized model.\")\n \n-    def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if dtype is None:\n-            dtype = torch.float16\n-            logger.info(\n-                \"Assuming VPTQ inference on GPU and loading the model in `torch.float16`. To overwrite it, set `dtype` manually.\"\n-            )\n-        return dtype\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n-        keep_in_fp32_modules: list[str] | None = None,\n         **kwargs,\n     ):\n         from ..integrations import replace_with_vptq_linear\n \n         self.modules_to_not_convert = self.get_modules_to_not_convert(\n-            model, self.quantization_config.modules_to_not_convert, keep_in_fp32_modules\n+            model, self.quantization_config.modules_to_not_convert, model._keep_in_fp32_modules\n         )\n         replace_with_vptq_linear(\n             model,"
        },
        {
            "sha": "65c3ebaf8eaa4a0fe0f21bfd35f85d2383ffd9ed",
            "filename": "tests/quantization/bitnet_integration/test_bitnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -65,7 +65,9 @@ def setUpClass(cls):\n         Load the model\n         \"\"\"\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n-        cls.quantized_model = AutoModelForCausalLM.from_pretrained(cls.model_name, device_map=torch_device)\n+        cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n+            cls.model_name, dtype=torch.bfloat16, device_map=torch_device\n+        )\n \n     def tearDown(self):\n         gc.collect()\n@@ -92,16 +94,15 @@ def test_replace_with_bitlinear(self):\n             if isinstance(module, BitLinear):\n                 nb_bitnet_linear += 1\n \n-        self.assertEqual(nb_linears - 1, nb_bitnet_linear)\n+        self.assertEqual(nb_linears, nb_bitnet_linear)\n \n     def test_quantized_model(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly\n         \"\"\"\n         input_text = \"What are we having for dinner?\"\n-        expected_output = \"What are we having for dinner? What are we going to do for fun this weekend?\"\n+        expected_output = \"What are we having for dinner? What are we going to do for fun? What are\"\n         input_ids = self.tokenizer(input_text, return_tensors=\"pt\").to(torch_device)\n-\n         output = self.quantized_model.generate(**input_ids, max_new_tokens=11, do_sample=False)\n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n "
        },
        {
            "sha": "3f55ac47fca5a98138cfe08d6302548ecbef1c26",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -126,7 +126,10 @@ def setUp(self):\n         # Models and tokenizer\n         self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n         self.model_4bit = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, quantization_config=BitsAndBytesConfig(load_in_4bit=True), device_map=\"auto\"\n+            self.model_name,\n+            dtype=torch.float16,\n+            quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+            device_map=\"auto\",\n         )\n \n     def tearDown(self):\n@@ -238,7 +241,6 @@ def test_generate_quality_dequantize(self):\n         Test that loading the model and unquantize it produce correct results\n         \"\"\"\n         bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n-\n         model_4bit = AutoModelForCausalLM.from_pretrained(\n             self.model_name, quantization_config=bnb_config, device_map=\"auto\"\n         )"
        },
        {
            "sha": "02dbfb65642bb10cab602f431a55651a379da597",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b05d2c4309ba28cecaa64acf14521fea5c829523/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b05d2c4309ba28cecaa64acf14521fea5c829523/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=b05d2c4309ba28cecaa64acf14521fea5c829523",
            "patch": "@@ -127,7 +127,10 @@ def setUp(self):\n         # Models and tokenizer\n         self.model_fp16 = AutoModelForCausalLM.from_pretrained(self.model_name, dtype=torch.float16, device_map=\"auto\")\n         self.model_8bit = AutoModelForCausalLM.from_pretrained(\n-            self.model_name, quantization_config=BitsAndBytesConfig(load_in_8bit=True), device_map=\"auto\"\n+            self.model_name,\n+            dtype=torch.float16,\n+            quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n+            device_map=\"auto\",\n         )\n \n     def tearDown(self):"
        }
    ],
    "stats": {
        "total": 405,
        "additions": 124,
        "deletions": 281
    }
}