{
    "author": "alyosha-swamy",
    "message": "Add afmoe model (#42168)\n\n* Add AFMoE model support\n\n* Address review feedback for AFMoE implementation\n\n* Add flex attention support to AFMoE model\n\n* Fix expert_bias routing in AFMoE\n\n* Remove test-results directory\n\n* Address PR review feedback for AFMoE model\n\n* fix(afmoe): ensure RMSNorm output dtype matches input dtype)\n\n* properly return attn weights\n\n* fix most tests\n\n* cleanup\nRemove shared expert if else as defaults to 2\nRemove `route_norm` as it default to `True`.\n\nMake test smaller faster\n\n* fix input embeds api\n\n* update rope API, smaller test and should be good to go\n\n* oups wront place to skip unittest\n\n* quality\n\n* update\n\n* rope parameter docstring fill\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>",
    "sha": "cac0a28c83cf87b7a05495de3177099c635ba852",
    "files": [
        {
            "sha": "75f5a999831095e354dc45840e788a33ca5e2d78",
            "filename": ".gitignore",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/.gitignore",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/.gitignore",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.gitignore?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -175,3 +175,4 @@ tags\n \n # Cursor IDE files\n .cursor/\n+test-results/"
        },
        {
            "sha": "f60dfc556c16acb0dbe23710c8f687a5bef3242c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -384,6 +384,8 @@\n     title: Main Classes\n   - sections:\n     - sections:\n+      - local: model_doc/afmoe\n+        title: AFMoE\n       - local: model_doc/albert\n         title: ALBERT\n       - local: model_doc/apertus"
        },
        {
            "sha": "4297701742aad4f9c296b62fe130e28b67d3c4df",
            "filename": "docs/source/en/model_doc/afmoe.md",
            "status": "added",
            "additions": 130,
            "deletions": 0,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/docs%2Fsource%2Fen%2Fmodel_doc%2Fafmoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/docs%2Fsource%2Fen%2Fmodel_doc%2Fafmoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fafmoe.md?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1,130 @@\n+<!--Copyright 2025 Arcee AI and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-11-18.*\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# AFMoE\n+\n+AFMoE (Arcee Foundational Mixture of Experts) is a decoder-only transformer model that extends the Llama architecture with a sparse Mixture of Experts (MoE) approach. The model combines token-choice routing with shared experts and employs several architectural innovations for efficient inference and improved performance.\n+\n+## Key Architecture Features\n+\n+AFMoE introduces several key modifications to the standard transformer architecture:\n+\n+- **Mixture of Experts with Shared Experts**: Combines routed experts (activated per-token via learned routing) with always-active shared experts for stable base computation\n+- **Token-Choice Routing**: Uses sigmoid or softmax-based routing with normalization and scaling for expert selection\n+- **Q/K Normalization and Gating**: Applies RMSNorm to query and key projections and uses sigmoid gating on attention outputs for improved stability\n+- **Hybrid Attention Patterns**: Alternates between sliding window attention and full attention across layers for efficiency with long contexts\n+- **Dual Normalization**: Uses pre- and post-normalization around both attention and MLP blocks for training stability\n+- **Configurable Dense Layers**: Allows initial layers to use dense MLPs before transitioning to sparse MoE layers\n+\n+The model supports extended context lengths with RoPE embeddings and includes all standard Transformers features including Flash Attention 2, SDPA, gradient checkpointing, and quantization support.\n+\n+> [!TIP]\n+> AFMoE is particularly well-suited for scenarios requiring efficient scaling through sparsity while maintaining strong performance. The shared experts provide a stable computation baseline while routed experts enable model capacity scaling.\n+\n+The example below demonstrates how to generate text with AFMoE using [`Pipeline`] or the [`AutoModel`].\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"text-generation\",\n+    model=\"arcee-ai/Trinity-Mini\",\n+    torch_dtype=torch.bfloat16,\n+    device=0\n+)\n+\n+output = pipeline(\"The key innovation in mixture of experts is\")\n+print(output[0][\"generated_text\"])\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AfmoeForCausalLM\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"arcee-ai/Trinity-Mini\")\n+model = AfmoeForCausalLM.from_pretrained(\n+    \"arcee-ai/Trinity-Mini\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+\n+inputs = tokenizer(\"The key innovation in mixture of experts is\", return_tensors=\"pt\")\n+with torch.no_grad():\n+    outputs = model.generate(**inputs, max_new_tokens=50)\n+\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## Model Architecture Details\n+\n+### Expert Routing\n+\n+AFMoE uses token-choice routing where each token independently selects top-k experts based on router logits. The routing mechanism includes:\n+\n+- Configurable scoring function (sigmoid or softmax)\n+- Optional route normalization for balanced expert utilization\n+- Route scaling to control expert contribution strength\n+- Bias correction for expert selection\n+\n+### Shared Experts\n+\n+Unlike standard MoE models, AFMoE includes shared experts that are always activated for every token, providing:\n+\n+- A stable computation baseline across all tokens\n+- Reduced variance in model outputs\n+- Better handling of out-of-distribution inputs\n+\n+### Attention Mechanism\n+\n+The hybrid attention pattern alternates between:\n+\n+- **Sliding Window Attention**: For efficiency on long sequences, with configurable window size\n+- **Full Attention**: Applied every N layers (configurable via `global_attn_every_n_layers`) for global context\n+\n+All attention layers include Q/K normalization and output gating for improved training dynamics.\n+\n+## AfmoeConfig\n+\n+[[autodoc]] AfmoeConfig\n+\n+## AfmoeModel\n+\n+[[autodoc]] AfmoeModel\n+    - forward\n+\n+## AfmoeForCausalLM\n+\n+[[autodoc]] AfmoeForCausalLM\n+    - forward"
        },
        {
            "sha": "ae7dc9d6e8d1591500fe4292c7cc80d248fe61bc",
            "filename": "splitted_tests.txt",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/splitted_tests.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/splitted_tests.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/splitted_tests.txt?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1 @@\n+tests/models/afmoe/test_modeling_afmoe.py"
        },
        {
            "sha": "eec0be2b3edb90c8d9493b3a3263d1ccb2f3e466",
            "filename": "src/transformers/models/afmoe/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2F__init__.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_afmoe import *\n+    from .modeling_afmoe import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "88904050d386a2f397a7f461230e8d60ed8a04e8",
            "filename": "src/transformers/models/afmoe/configuration_afmoe.py",
            "status": "added",
            "additions": 210,
            "deletions": 0,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2Fconfiguration_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2Fconfiguration_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fconfiguration_afmoe.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1,210 @@\n+# coding=utf-8\n+# Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"AFMoE model configuration\"\"\"\n+\n+from typing import Optional\n+\n+from ...configuration_utils import PreTrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import RopeParameters\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class AfmoeConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`AfmoeModel`]. It is used to instantiate an\n+    AFMoE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of [arcee-ai/Trinity-Mini](https://huggingface.co/arcee-ai/Trinity-Mini).\n+\n+    AFMoE is an Adaptive Feedforward MoE (Mixture of Experts) model with token-choice routing, shared experts, and a\n+    hybrid attention mechanism combining sliding window and full attention patterns.\n+\n+    Configuration objects inherit from [`PreTrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PreTrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 200192):\n+            Vocabulary size of the AFMoE model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`AfmoeModel`].\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 6144):\n+            Dimension of the dense MLP representations.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1408):\n+            Intermediate size of the routed expert MLPs.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_dense_layers (`int`, *optional*, defaults to 1):\n+            Number of initial dense layers before MoE layers begin. Layers with index < num_dense_layers will use\n+            standard dense MLPs instead of MoE.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            The dimension of each attention head.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the MLP blocks.\n+        max_position_embeddings (`int`, *optional*, defaults to 16384):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the RMS normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_parameters (`RopeParameters`, *optional*):\n+            Dictionary containing the configuration parameters for the RoPE embeddings. The dictionary should contain\n+            a value for `rope_theta` and optionally parameters used for scaling in case you want to use RoPE\n+            with longer `max_position_embeddings`.\n+        num_experts (`int`, *optional*, defaults to 64):\n+            Number of routed experts in MoE layers.\n+        num_experts_per_tok (`int`, *optional*, defaults to 6):\n+            Number of experts to route each token to. This is the top-k value for the token-choice routing.\n+        num_shared_experts (`int`, *optional*, defaults to 2):\n+            Number of shared experts that are always activated for all tokens.\n+        route_scale (`float`, *optional*, defaults to 1.0):\n+            Scaling factor applied to routing weights.\n+        global_attn_every_n_layers (`int`, *optional*, defaults to 4):\n+            The frequency of full attention layers. Every Nth layer will use full attention, while others use sliding\n+            window attention.\n+        sliding_window (`int`, *optional*, defaults to 1024):\n+            Sliding window size for local attention layers.\n+        layer_types (`list[str]`, *optional*):\n+            A list that explicitly maps each layer index with its attention type. Each element should be either\n+            \"sliding_attention\" or \"full_attention\". If not provided, it will be automatically generated based on\n+            `global_attn_every_n_layers`.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        mup_enabled (`bool`, *optional*, defaults to `False`):\n+            Whether to enable muP (Maximal Update Parametrization) input scaling. When enabled, input embeddings\n+            are scaled by `sqrt(hidden_size)`.\n+\n+    Example:\n+    ```python\n+    >>> from transformers import AfmoeModel, AfmoeConfig\n+\n+    >>> # Initializing an AFMoE configuration\n+    >>> configuration = AfmoeConfig()\n+\n+    >>> # Initializing a model from the afmoe-small-sft-v1 style configuration\n+    >>> model = AfmoeModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"afmoe\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    # Default pipeline parallel plan for base model\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size: Optional[int] = 200192,\n+        hidden_size: Optional[int] = 2048,\n+        intermediate_size: Optional[int] = 6144,\n+        moe_intermediate_size: Optional[int] = 1408,\n+        num_hidden_layers: Optional[int] = 32,\n+        num_dense_layers: Optional[int] = 1,\n+        num_attention_heads: Optional[int] = 16,\n+        num_key_value_heads: Optional[int] = None,\n+        head_dim: Optional[int] = 128,\n+        hidden_act: Optional[str] = \"silu\",\n+        max_position_embeddings: Optional[int] = 16384,\n+        initializer_range: Optional[float] = 0.02,\n+        rms_norm_eps: Optional[float] = 1e-5,\n+        use_cache: Optional[bool] = True,\n+        tie_word_embeddings: Optional[bool] = False,\n+        rope_theta: Optional[float] = 10000.0,\n+        rope_parameters: Optional[RopeParameters | dict[str, RopeParameters]] = None,\n+        num_experts: Optional[int] = 64,\n+        num_experts_per_tok: Optional[int] = 6,\n+        num_shared_experts: Optional[int] = 2,\n+        route_scale: Optional[float] = 1.0,\n+        global_attn_every_n_layers: Optional[int] = 4,\n+        sliding_window: Optional[int] = 1024,\n+        layer_types: Optional[list] = None,\n+        attention_dropout: Optional[float] = 0.0,\n+        mup_enabled: Optional[bool] = False,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_dense_layers = num_dense_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_parameters = rope_parameters\n+\n+        # MoE specific\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_experts = num_experts\n+        self.num_shared_experts = num_shared_experts\n+        self.route_scale = route_scale\n+        self.attention_bias = False\n+\n+        # Attention specific\n+        self.attention_dropout = attention_dropout\n+        self.global_attn_every_n_layers = global_attn_every_n_layers\n+        self.sliding_window = sliding_window\n+        self.mup_enabled = mup_enabled\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % global_attn_every_n_layers) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"AfmoeConfig\"]"
        },
        {
            "sha": "da021b62853a9d969d86ce6936cbf3a037a6b384",
            "filename": "src/transformers/models/afmoe/modeling_afmoe.py",
            "status": "added",
            "additions": 721,
            "deletions": 0,
            "changes": 721,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodeling_afmoe.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1,721 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/afmoe/modular_afmoe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_afmoe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_func_from_hub\n+from ...integrations.hub_kernels import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from .configuration_afmoe import AfmoeConfig\n+\n+\n+class AfmoeRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: AfmoeConfig, device=None):\n+        super().__init__()\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+\n+        self.rope_type = self.config.rope_parameters[\"rope_type\"]\n+        rope_init_fn: Callable = self.compute_default_rope_parameters\n+        if self.rope_type != \"default\":\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = inv_freq\n+\n+    @staticmethod\n+    def compute_default_rope_parameters(\n+        config: Optional[AfmoeConfig] = None,\n+        device: Optional[\"torch.device\"] = None,\n+        seq_len: Optional[int] = None,\n+    ) -> tuple[\"torch.Tensor\", float]:\n+        \"\"\"\n+        Computes the inverse frequencies according to the original RoPE implementation\n+        Args:\n+            config ([`~transformers.PreTrainedConfig`]):\n+                The model configuration.\n+            device (`torch.device`):\n+                The device to use for initialization of the inverse frequencies.\n+            seq_len (`int`, *optional*):\n+                The current sequence length. Unused for this type of RoPE.\n+        Returns:\n+            Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n+            post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n+        \"\"\"\n+        base = config.rope_parameters[\"rope_theta\"]\n+        dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+\n+        attention_factor = 1.0  # Unused in this type of RoPE\n+\n+        # Compute the inverse frequencies\n+        inv_freq = 1.0 / (\n+            base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim)\n+        )\n+        return inv_freq, attention_factor\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class AfmoeRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        AfmoeRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return (self.weight * hidden_states).to(input_dtype)  # main diff with Llama\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class AfmoeMLP(nn.Module):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size if intermediate_size is None else intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class AfmoeTokenChoiceRouter(nn.Module):\n+    \"\"\"\n+    Token-choice top-K router for MoE routing.\n+\n+    This router assigns each token to the top-K experts based on sigmoid scores, matching the released checkpoints.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_experts\n+        self.route_scale = config.route_scale\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor, expert_bias: torch.Tensor):\n+        _, _, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+\n+        scores = torch.sigmoid(self.gate(hidden_states).to(torch.float32))\n+\n+        _, selected_experts = torch.topk(scores + expert_bias, k=self.top_k, dim=1)\n+        top_scores = scores.gather(dim=1, index=selected_experts)\n+        denominator = top_scores.sum(dim=-1, keepdim=True) + 1e-20\n+        top_scores = top_scores / denominator\n+        top_scores = top_scores * self.route_scale\n+        return top_scores, selected_experts\n+\n+\n+class AfmoeExperts(nn.ModuleList):\n+    \"\"\"\n+    Container holding the routed experts.\n+\n+    This mirrors the Experts pattern used across other MoE models to ease checkpoint conversion.\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_experts\n+        for _ in range(self.num_experts):\n+            self.append(AfmoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch, seq, hidden)\n+            selected_experts: (batch, seq, top_k)\n+            routing_weights: (batch, seq, top_k)\n+        \"\"\"\n+        batch_size, seq_len, hidden_dim = hidden_states.shape\n+        if seq_len == 0:\n+            return hidden_states.new_zeros(batch_size, 0, hidden_dim)\n+        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n+        top_k = selected_experts.shape[-1]\n+\n+        # Map every token routing decision to a unique position so we can process expert by expert.\n+        token_indices = torch.arange(\n+            hidden_states_flat.shape[0], device=hidden_states.device, dtype=torch.long\n+        ).repeat_interleave(top_k)\n+        expert_indices = selected_experts.reshape(-1)\n+        routing_weights = routing_weights.reshape(-1)\n+\n+        sorting = torch.argsort(expert_indices, stable=True)\n+        token_indices = token_indices[sorting]\n+        expert_indices = expert_indices[sorting]\n+        routing_weights = routing_weights[sorting]\n+\n+        dispatched_tokens = hidden_states_flat.index_select(0, token_indices)\n+        expert_outputs = torch.zeros_like(dispatched_tokens)\n+\n+        unique_experts, counts = torch.unique_consecutive(expert_indices, return_counts=True)\n+        start = 0\n+        for expert_id, count in zip(unique_experts.tolist(), counts.tolist()):\n+            if count == 0:\n+                continue\n+            end = start + count\n+            expert_input = dispatched_tokens[start:end]\n+            expert_output = self[expert_id](expert_input)\n+            expert_outputs[start:end] = expert_output\n+            start = end\n+\n+        weighted_outputs = (expert_outputs.to(torch.float32) * routing_weights.unsqueeze(-1)).to(hidden_states.dtype)\n+        aggregated = torch.zeros_like(hidden_states_flat)\n+        scatter_indices = token_indices.unsqueeze(-1).expand_as(weighted_outputs)\n+        aggregated.scatter_add_(0, scatter_indices, weighted_outputs)\n+        return aggregated.view(batch_size, seq_len, hidden_dim)\n+\n+\n+class AfmoeMoE(nn.Module):\n+    \"\"\"\n+    Mixture of Experts (MoE) module for AFMoE.\n+\n+    This module implements a sparse MoE layer with both shared experts (always active) and\n+    routed experts (activated based on token-choice routing).\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.router = AfmoeTokenChoiceRouter(config)\n+        self.shared_experts = AfmoeMLP(config, config.moe_intermediate_size * config.num_shared_experts)\n+        self.experts = AfmoeExperts(config)\n+        self.expert_bias = nn.Parameter(torch.zeros(config.num_experts, dtype=torch.float32), requires_grad=False)\n+\n+    def forward(self, hidden_states):\n+        batch_size, seq_len, hidden_dim = hidden_states.shape\n+        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n+\n+        # Get routing decisions\n+        top_scores, selected_experts = self.router(hidden_states, self.expert_bias)\n+        top_scores = top_scores.view(batch_size, seq_len, self.config.num_experts_per_tok)\n+        selected_experts = selected_experts.view(batch_size, seq_len, self.config.num_experts_per_tok)\n+\n+        # Process through shared experts\n+        shared_output = self.shared_experts(hidden_states_flat).view(batch_size, seq_len, hidden_dim)\n+        routed_output = self.experts(hidden_states, selected_experts, top_scores)\n+        return shared_output + routed_output\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+@use_kernel_func_from_hub(\"rotary_pos_emb\")\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class AfmoeAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention module with optional sliding window and gating.\n+\n+    This attention mechanism supports both full attention and sliding window attention,\n+    and includes Q/K normalization and gating of the output. It inherits from [`LlamaAttention`] to minimize the amount\n+    of custom logic we need to maintain.\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.rotary_fn = apply_rotary_pos_emb\n+        # Parent LlamaAttention already sets: layer_idx, num_heads, num_key_value_heads, num_key_value_groups, head_dim\n+        # We only add AFMoE-specific attributes\n+        self.is_local_attention = config.layer_types[layer_idx] == \"sliding_attention\"\n+        self.sliding_window = config.sliding_window if self.is_local_attention else None\n+\n+        self.q_norm = AfmoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = AfmoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.gate_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape)\n+        gate_states = self.gate_proj(hidden_states)\n+\n+        query_states = self.q_norm(query_states).transpose(1, 2)\n+        key_states = self.k_norm(key_states).transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        if self.is_local_attention:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        output = output.view(*input_shape, -1).contiguous()\n+        output = output * torch.sigmoid(gate_states)\n+        attn_output = self.o_proj(output)\n+        return attn_output, attn_weights\n+\n+\n+class AfmoeDecoderLayer(GradientCheckpointingLayer):\n+    \"\"\"\n+    AFMoE decoder layer with dual normalization.\n+\n+    This layer applies self-attention followed by either a dense MLP or MoE block,\n+    with dual normalization (pre and post) around each component.\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.layer_idx = layer_idx\n+\n+        self.self_attn = AfmoeAttention(config=config, layer_idx=layer_idx)\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        # Dual normalization for attention\n+        self.input_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # Dual normalization for FFN\n+        self.pre_mlp_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_mlp_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # MoE or dense FFN\n+        self.moe_enabled = layer_idx >= config.num_dense_layers\n+        if self.moe_enabled:\n+            self.mlp = AfmoeMoE(config)\n+        else:\n+            self.mlp = AfmoeMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+\n+        # Self Attention with dual normalization\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # FFN with dual normalization\n+        residual = hidden_states\n+        hidden_states = self.pre_mlp_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_mlp_layernorm(hidden_states)\n+\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+class AfmoePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config: AfmoeConfig\n+    base_model_prefix = \"model\"\n+    _no_split_modules = [\"AfmoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _can_record_outputs = {\n+        \"hidden_states\": AfmoeDecoderLayer,\n+        \"attentions\": AfmoeAttention,\n+    }\n+    _keep_in_fp32_modules = [\n+        \"input_layernorm\",\n+        \"post_attention_layernorm\",\n+        \"pre_mlp_layernorm\",\n+        \"post_mlp_layernorm\",\n+        \"q_norm\",\n+        \"k_norm\",\n+        \"norm\",\n+        \"expert_bias\",\n+    ]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    supports_gradient_checkpointing = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+        elif isinstance(module, nn.Embedding):\n+            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                nn.init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, AfmoeRMSNorm):\n+            nn.init.ones_(module.weight)\n+        elif isinstance(module, AfmoeTokenChoiceRouter):\n+            nn.init.zeros_(module.gate.weight)\n+        elif isinstance(module, AfmoeMoE):\n+            nn.init.zeros_(module.expert_bias)\n+\n+\n+@auto_docstring\n+class AfmoeModel(AfmoePreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`AfmoeDecoderLayer`]\n+\n+    Args:\n+        config: AfmoeConfig\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [AfmoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = AfmoeRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs()\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens,\n+                past_seen_tokens + inputs_embeds.shape[1],\n+                device=inputs_embeds.device,\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+\n+        # Apply muP input scaling if enabled\n+        if self.config.mup_enabled:\n+            hidden_states = hidden_states * (self.config.hidden_size**0.5)\n+\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+@auto_docstring\n+class AfmoeForCausalLM(AfmoePreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = AfmoeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AfmoeForCausalLM\n+\n+        >>> model = AfmoeForCausalLM.from_pretrained(\"meta-afmoe/Afmoe-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-afmoe/Afmoe-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"AfmoeForCausalLM\", \"AfmoeModel\", \"AfmoePreTrainedModel\"]"
        },
        {
            "sha": "3a8269bff70f0bfbeef90a9247f763a893379a61",
            "filename": "src/transformers/models/afmoe/modular_afmoe.py",
            "status": "added",
            "additions": 479,
            "deletions": 0,
            "changes": 479,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fafmoe%2Fmodular_afmoe.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1,479 @@\n+# coding=utf-8\n+# Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch AFMoE model.\"\"\"\n+\n+from collections.abc import Callable\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n+from ..gpt_oss.modeling_gpt_oss import GptOssRMSNorm\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from ..qwen2_moe.modeling_qwen2_moe import Qwen2MoeMLP\n+from .configuration_afmoe import AfmoeConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class AfmoeRotaryEmbedding(LlamaRotaryEmbedding):\n+    pass\n+\n+\n+class AfmoeRMSNorm(GptOssRMSNorm):\n+    pass\n+\n+\n+class AfmoeMLP(Qwen2MoeMLP):\n+    pass\n+\n+\n+class AfmoeTokenChoiceRouter(nn.Module):\n+    \"\"\"\n+    Token-choice top-K router for MoE routing.\n+\n+    This router assigns each token to the top-K experts based on sigmoid scores, matching the released checkpoints.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_experts\n+        self.route_scale = config.route_scale\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n+\n+    def forward(self, hidden_states: torch.Tensor, expert_bias: torch.Tensor):\n+        _, _, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+\n+        scores = torch.sigmoid(self.gate(hidden_states).to(torch.float32))\n+\n+        _, selected_experts = torch.topk(scores + expert_bias, k=self.top_k, dim=1)\n+        top_scores = scores.gather(dim=1, index=selected_experts)\n+        denominator = top_scores.sum(dim=-1, keepdim=True) + 1e-20\n+        top_scores = top_scores / denominator\n+        top_scores = top_scores * self.route_scale\n+        return top_scores, selected_experts\n+\n+\n+class AfmoeExperts(nn.ModuleList):\n+    \"\"\"\n+    Container holding the routed experts.\n+\n+    This mirrors the Experts pattern used across other MoE models to ease checkpoint conversion.\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig):\n+        super().__init__()\n+        self.top_k = config.num_experts_per_tok\n+        self.num_experts = config.num_experts\n+        for _ in range(self.num_experts):\n+            self.append(AfmoeMLP(config, intermediate_size=config.moe_intermediate_size))\n+\n+    def forward(\n+        self, hidden_states: torch.Tensor, selected_experts: torch.Tensor, routing_weights: torch.Tensor\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states: (batch, seq, hidden)\n+            selected_experts: (batch, seq, top_k)\n+            routing_weights: (batch, seq, top_k)\n+        \"\"\"\n+        batch_size, seq_len, hidden_dim = hidden_states.shape\n+        if seq_len == 0:\n+            return hidden_states.new_zeros(batch_size, 0, hidden_dim)\n+        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n+        top_k = selected_experts.shape[-1]\n+\n+        # Map every token routing decision to a unique position so we can process expert by expert.\n+        token_indices = torch.arange(\n+            hidden_states_flat.shape[0], device=hidden_states.device, dtype=torch.long\n+        ).repeat_interleave(top_k)\n+        expert_indices = selected_experts.reshape(-1)\n+        routing_weights = routing_weights.reshape(-1)\n+\n+        sorting = torch.argsort(expert_indices, stable=True)\n+        token_indices = token_indices[sorting]\n+        expert_indices = expert_indices[sorting]\n+        routing_weights = routing_weights[sorting]\n+\n+        dispatched_tokens = hidden_states_flat.index_select(0, token_indices)\n+        expert_outputs = torch.zeros_like(dispatched_tokens)\n+\n+        unique_experts, counts = torch.unique_consecutive(expert_indices, return_counts=True)\n+        start = 0\n+        for expert_id, count in zip(unique_experts.tolist(), counts.tolist()):\n+            if count == 0:\n+                continue\n+            end = start + count\n+            expert_input = dispatched_tokens[start:end]\n+            expert_output = self[expert_id](expert_input)\n+            expert_outputs[start:end] = expert_output\n+            start = end\n+\n+        weighted_outputs = (expert_outputs.to(torch.float32) * routing_weights.unsqueeze(-1)).to(hidden_states.dtype)\n+        aggregated = torch.zeros_like(hidden_states_flat)\n+        scatter_indices = token_indices.unsqueeze(-1).expand_as(weighted_outputs)\n+        aggregated.scatter_add_(0, scatter_indices, weighted_outputs)\n+        return aggregated.view(batch_size, seq_len, hidden_dim)\n+\n+\n+class AfmoeMoE(nn.Module):\n+    \"\"\"\n+    Mixture of Experts (MoE) module for AFMoE.\n+\n+    This module implements a sparse MoE layer with both shared experts (always active) and\n+    routed experts (activated based on token-choice routing).\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.router = AfmoeTokenChoiceRouter(config)\n+        self.shared_experts = AfmoeMLP(config, config.moe_intermediate_size * config.num_shared_experts)\n+        self.experts = AfmoeExperts(config)\n+        self.expert_bias = nn.Parameter(torch.zeros(config.num_experts, dtype=torch.float32), requires_grad=False)\n+\n+    def forward(self, hidden_states):\n+        batch_size, seq_len, hidden_dim = hidden_states.shape\n+        hidden_states_flat = hidden_states.view(-1, hidden_dim)\n+\n+        # Get routing decisions\n+        top_scores, selected_experts = self.router(hidden_states, self.expert_bias)\n+        top_scores = top_scores.view(batch_size, seq_len, self.config.num_experts_per_tok)\n+        selected_experts = selected_experts.view(batch_size, seq_len, self.config.num_experts_per_tok)\n+\n+        # Process through shared experts\n+        shared_output = self.shared_experts(hidden_states_flat).view(batch_size, seq_len, hidden_dim)\n+        routed_output = self.experts(hidden_states, selected_experts, top_scores)\n+        return shared_output + routed_output\n+\n+\n+class AfmoeAttention(LlamaAttention):\n+    \"\"\"\n+    Multi-headed attention module with optional sliding window and gating.\n+\n+    This attention mechanism supports both full attention and sliding window attention,\n+    and includes Q/K normalization and gating of the output. It inherits from [`LlamaAttention`] to minimize the amount\n+    of custom logic we need to maintain.\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # Parent LlamaAttention already sets: layer_idx, num_heads, num_key_value_heads, num_key_value_groups, head_dim\n+        # We only add AFMoE-specific attributes\n+        self.is_local_attention = config.layer_types[layer_idx] == \"sliding_attention\"\n+        self.sliding_window = config.sliding_window if self.is_local_attention else None\n+\n+        self.q_norm = AfmoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.k_norm = AfmoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.gate_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape)\n+        gate_states = self.gate_proj(hidden_states)\n+\n+        query_states = self.q_norm(query_states).transpose(1, 2)\n+        key_states = self.k_norm(key_states).transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        if self.is_local_attention:\n+            cos, sin = position_embeddings\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        output = output.view(*input_shape, -1).contiguous()\n+        output = output * torch.sigmoid(gate_states)\n+        attn_output = self.o_proj(output)\n+        return attn_output, attn_weights\n+\n+\n+class AfmoeDecoderLayer(GradientCheckpointingLayer):\n+    \"\"\"\n+    AFMoE decoder layer with dual normalization.\n+\n+    This layer applies self-attention followed by either a dense MLP or MoE block,\n+    with dual normalization (pre and post) around each component.\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.layer_idx = layer_idx\n+\n+        self.self_attn = AfmoeAttention(config=config, layer_idx=layer_idx)\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        # Dual normalization for attention\n+        self.input_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # Dual normalization for FFN\n+        self.pre_mlp_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_mlp_layernorm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        # MoE or dense FFN\n+        self.moe_enabled = layer_idx >= config.num_dense_layers\n+        if self.moe_enabled:\n+            self.mlp = AfmoeMoE(config)\n+        else:\n+            self.mlp = AfmoeMLP(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+\n+        # Self Attention with dual normalization\n+        hidden_states = self.input_layernorm(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # FFN with dual normalization\n+        residual = hidden_states\n+        hidden_states = self.pre_mlp_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_mlp_layernorm(hidden_states)\n+\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+class AfmoePreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config: AfmoeConfig\n+    base_model_prefix = \"model\"\n+    _no_split_modules = [\"AfmoeDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _can_record_outputs = {\n+        \"hidden_states\": AfmoeDecoderLayer,\n+        \"attentions\": AfmoeAttention,\n+    }\n+    _keep_in_fp32_modules = [\n+        \"input_layernorm\",\n+        \"post_attention_layernorm\",\n+        \"pre_mlp_layernorm\",\n+        \"post_mlp_layernorm\",\n+        \"q_norm\",\n+        \"k_norm\",\n+        \"norm\",\n+        \"expert_bias\",\n+    ]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    supports_gradient_checkpointing = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+        elif isinstance(module, nn.Embedding):\n+            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                nn.init.zeros_(module.weight[module.padding_idx])\n+        elif isinstance(module, AfmoeRMSNorm):\n+            nn.init.ones_(module.weight)\n+        elif isinstance(module, AfmoeTokenChoiceRouter):\n+            nn.init.zeros_(module.gate.weight)\n+        elif isinstance(module, AfmoeMoE):\n+            nn.init.zeros_(module.expert_bias)\n+\n+\n+@auto_docstring\n+class AfmoeModel(AfmoePreTrainedModel):\n+    \"\"\"\n+    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`AfmoeDecoderLayer`]\n+\n+    Args:\n+        config: AfmoeConfig\n+    \"\"\"\n+\n+    def __init__(self, config: AfmoeConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [AfmoeDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = AfmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = AfmoeRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    @check_model_inputs()\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens,\n+                past_seen_tokens + inputs_embeds.shape[1],\n+                device=inputs_embeds.device,\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+            }\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+\n+        # Apply muP input scaling if enabled\n+        if self.config.mup_enabled:\n+            hidden_states = hidden_states * (self.config.hidden_size**0.5)\n+\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return MoeModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+class AfmoeForCausalLM(LlamaForCausalLM, AfmoePreTrainedModel, GenerationMixin):\n+    def __init__(self, config):\n+        AfmoePreTrainedModel.__init__(self, config)\n+        self.model = AfmoeModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.post_init()\n+\n+\n+__all__ = [\n+    \"AfmoeForCausalLM\",\n+    \"AfmoeModel\",\n+    \"AfmoePreTrainedModel\",\n+]"
        },
        {
            "sha": "3c5dedcf292ec4cb642bee681e9d38a6a51ba8e3",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -35,6 +35,7 @@\n CONFIG_MAPPING_NAMES = OrderedDict[str, str](\n     [\n         # Add configs here\n+        (\"afmoe\", \"AfmoeConfig\"),\n         (\"aimv2\", \"Aimv2Config\"),\n         (\"aimv2_vision_model\", \"Aimv2VisionConfig\"),\n         (\"albert\", \"AlbertConfig\"),\n@@ -458,6 +459,7 @@\n MODEL_NAMES_MAPPING = OrderedDict[str, str](\n     [\n         # Add full (and cased) model names here\n+        (\"afmoe\", \"AFMoE\"),\n         (\"aimv2\", \"AIMv2\"),\n         (\"aimv2_vision_model\", \"Aimv2VisionModel\"),\n         (\"albert\", \"ALBERT\"),"
        },
        {
            "sha": "64111d5e1b5ff837f1f8f68c17f48906c5a049db",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -43,6 +43,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n MODEL_MAPPING_NAMES = OrderedDict(\n     [\n         # Base model mapping\n+        (\"afmoe\", \"AfmoeModel\"),\n         (\"aimv2\", \"Aimv2Model\"),\n         (\"aimv2_vision_model\", \"Aimv2VisionModel\"),\n         (\"albert\", \"AlbertModel\"),\n@@ -613,6 +614,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n MODEL_FOR_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Causal LM mapping\n+        (\"afmoe\", \"AfmoeForCausalLM\"),\n         (\"apertus\", \"ApertusForCausalLM\"),\n         (\"arcee\", \"ArceeForCausalLM\"),\n         (\"aria_text\", \"AriaTextForCausalLM\"),"
        },
        {
            "sha": "8b137891791fe96927ad78e64b0aad7bded08bdc",
            "filename": "tests/models/afmoe/__init__.py",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/tests%2Fmodels%2Fafmoe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/tests%2Fmodels%2Fafmoe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fafmoe%2F__init__.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1 @@\n+"
        },
        {
            "sha": "61f6b07552ec7ec09a065c73d47c753ffc61c5b6",
            "filename": "tests/models/afmoe/test_modeling_afmoe.py",
            "status": "added",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/tests%2Fmodels%2Fafmoe%2Ftest_modeling_afmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/tests%2Fmodels%2Fafmoe%2Ftest_modeling_afmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fafmoe%2Ftest_modeling_afmoe.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -0,0 +1,123 @@\n+# Copyright 2025 Arcee AI and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import require_torch\n+\n+\n+if is_torch_available():\n+    from transformers import AfmoeForCausalLM, AfmoeModel\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class AfmoeModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        base_model_class = AfmoeModel\n+\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=4,\n+        seq_length=128,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_token_type_ids=False,\n+        use_labels=True,\n+        vocab_size=64,\n+        hidden_size=32,\n+        intermediate_size=16,\n+        moe_intermediate_size=16,\n+        num_hidden_layers=2,\n+        num_dense_layers=1,\n+        num_attention_heads=16,\n+        num_key_value_heads=16,\n+        head_dim=128,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=16384,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=False,\n+        rope_theta=10000.0,\n+        rope_parameters=None,\n+        num_experts=4,\n+        num_experts_per_tok=2,\n+        num_shared_experts=2,\n+        route_norm=True,\n+        route_scale=1.0,\n+        global_attn_every_n_layers=2,\n+        sliding_window=128,\n+        attention_dropout=0.0,\n+    ):\n+        super().__init__(\n+            parent=parent,\n+            batch_size=batch_size,\n+            seq_length=seq_length,\n+            is_training=is_training,\n+            use_input_mask=use_input_mask,\n+            use_token_type_ids=use_token_type_ids,\n+            use_labels=use_labels,\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            intermediate_size=intermediate_size,\n+            hidden_act=hidden_act,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+        )\n+        self.use_cache = use_cache\n+        self.head_dim = head_dim\n+        self.rms_norm_eps = rms_norm_eps\n+        self.rope_theta = rope_theta\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.num_dense_layers = num_dense_layers\n+        self.num_experts = num_experts\n+        self.num_experts_per_tok = num_experts_per_tok\n+        self.num_shared_experts = num_shared_experts\n+        self.route_norm = route_norm\n+        self.route_scale = route_scale\n+        self.global_attn_every_n_layers = global_attn_every_n_layers\n+        self.sliding_window = sliding_window\n+        self.attention_dropout = attention_dropout\n+\n+\n+@require_torch\n+class AfmoeModelTest(CausalLMModelTest, unittest.TestCase):\n+    model_tester_class = AfmoeModelTester\n+    all_model_classes = (AfmoeModel, AfmoeForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": AfmoeModel, \"text-generation\": AfmoeForCausalLM} if is_torch_available() else {}\n+    )\n+\n+    @unittest.skip(\"Afmoe applies key/query norm which doesn't work with packing\")\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Afmoe  applies key/query norm which doesn't work with packing\")\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"Afmoe  applies key/query norm which doesn't work with packing\")\n+    def test_model_rope_scaling_frequencies(self):\n+        pass\n+\n+    @unittest.skip(\"Afmoe has moe, output can be different\")\n+    def test_model_outputs_equivalence(self, **kwargs):\n+        pass\n+\n+    # TODO: Add integration tests once we have a checkpoint on the Hub"
        },
        {
            "sha": "433c40842b41fd550c70fb49c721d63e5c769431",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac0a28c83cf87b7a05495de3177099c635ba852/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac0a28c83cf87b7a05495de3177099c635ba852/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=cac0a28c83cf87b7a05495de3177099c635ba852",
            "patch": "@@ -32,6 +32,10 @@\n CONFIG_MAPPING = transformers.models.auto.configuration_auto.CONFIG_MAPPING\n \n SPECIAL_CASES_TO_ALLOW = {\n+    \"AfmoeConfig\": [\n+        \"global_attn_every_n_layers\",  # used internally in config to generate `layer_types`\n+        \"rope_scaling\",  # used internally in config to generate `rope_parameters`\n+    ],\n     \"xLSTMConfig\": [\"add_out_norm\", \"chunkwise_kernel\", \"sequence_kernel\", \"step_kernel\"],\n     \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n     \"Ernie4_5_MoeConfig\": [\"tie_word_embeddings\"],"
        }
    ],
    "stats": {
        "total": 1703,
        "additions": 1703,
        "deletions": 0
    }
}