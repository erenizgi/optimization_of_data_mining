{
    "author": "HyunZ118",
    "message": "ğŸŒ [i18n-KO] Translated `imageprocessor.md` to Korean (#39557)\n\n* feat: manual translation\n\n* docs: fix ko/_toctree.yml\n\n* Apply suggestions from code review\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update docs/source/ko/image_processors.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "20ee3a73f0f887b593191e99d2b5ac5e4f143b35",
    "files": [
        {
            "sha": "df2d53c49a96b1c730207ca710697f9d7402c9fb",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ee3a73f0f887b593191e99d2b5ac5e4f143b35/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ee3a73f0f887b593191e99d2b5ac5e4f143b35/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=20ee3a73f0f887b593191e99d2b5ac5e4f143b35",
            "patch": "@@ -29,8 +29,8 @@\n   - sections:\n     - local: fast_tokenizers\n       title: ğŸ¤— Tokenizers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í† í¬ë‚˜ì´ì € ì‚¬ìš©í•˜ê¸°\n-    - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Image processors\n+    - local: image_processors\n+      title: ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Video processors\n     - local: in_translation"
        },
        {
            "sha": "eddccb799ecfe492bdb5b851edce98b5b5013eb2",
            "filename": "docs/source/ko/image_processors.md",
            "status": "added",
            "additions": 223,
            "deletions": 0,
            "changes": 223,
            "blob_url": "https://github.com/huggingface/transformers/blob/20ee3a73f0f887b593191e99d2b5ac5e4f143b35/docs%2Fsource%2Fko%2Fimage_processors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/20ee3a73f0f887b593191e99d2b5ac5e4f143b35/docs%2Fsource%2Fko%2Fimage_processors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fimage_processors.md?ref=20ee3a73f0f887b593191e99d2b5ac5e4f143b35",
            "patch": "@@ -0,0 +1,223 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ(Image processor) [[image-processors]]\n+\n+ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ë¥¼ í”½ì…€ ê°’, ì¦‰ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒê³¼ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ í”½ì…€ ê°’ì€ ë¹„ì „ ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë•Œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹í•˜ë ¤ë©´ ì…ë ¥ë˜ëŠ” ì´ë¯¸ì§€ì˜ í˜•ì‹ì´ í•™ìŠµ ë‹¹ì‹œ ì‚¬ìš©í–ˆë˜ ë°ì´í„°ì™€ ë˜‘ê°™ì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‘ì—…ì„ í†µí•´ ì´ë¯¸ì§€ í˜•ì‹ì„ í†µì¼ì‹œì¼œì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n+\n+- ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ëŠ” [`~BaseImageProcessor.center_crop`] \n+- í”½ì…€ ê°’ì„ ì •ê·œí™”í•˜ëŠ” [`~BaseImageProcessor.normalize`] ë˜ëŠ” í¬ê¸°ë¥¼ ì¬ì¡°ì •í•˜ëŠ” [`~BaseImageProcessor.rescale`]\n+\n+Hugging Face [Hub](https://hf.co)ë‚˜ ë¡œì»¬ ë””ë ‰í† ë¦¬ì— ìˆëŠ” ë¹„ì „ ëª¨ë¸ì—ì„œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì˜ ì„¤ì •(ì´ë¯¸ì§€ í¬ê¸°, ì •ê·œí™” ë° ë¦¬ì‚¬ì´ì¦ˆ ì—¬ë¶€ ë“±)ì„ ë¶ˆëŸ¬ì˜¤ë ¤ë©´ [`~ImageProcessingMixin.from_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ê° ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì„¤ì •ì€ [preprocessor_config.json](https://huggingface.co/google/vit-base-patch16-224/blob/main/preprocessor_config.json) íŒŒì¼ì— ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers import AutoImageProcessor\n+\n+image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n+```\n+\n+ì´ë¯¸ì§€ë¥¼ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì— ì „ë‹¬í•˜ì—¬ í”½ì…€ ê°’ìœ¼ë¡œ ë³€í™˜í•˜ê³ , `return_tensors=\"pt\"` ë¥¼ ì„¤ì •í•˜ì—¬ PyTorch í…ì„œë¥¼ ë°˜í™˜ë°›ìœ¼ì„¸ìš”. ì´ë¯¸ì§€ê°€ í…ì„œë¡œ ì–´ë–»ê²Œ ë³´ì´ëŠ”ì§€ ê¶ê¸ˆí•˜ë‹¤ë©´ ì…ë ¥ê°’ì„ í•œë²ˆ ì¶œë ¥í•´ë³´ì‹œëŠ”ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤!\n+\n+```py\n+from PIL import Image\n+import requests\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/image_processor_example.png\"\n+image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+inputs = image_processor(image, return_tensors=\"pt\")\n+```\n+\n+ì´ ê°€ì´ë“œì—ì„œëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤ì™€ ë¹„ì „ ëª¨ë¸ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë°©ë²•ì— ëŒ€í•´ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤.\n+\n+## ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ í´ë˜ìŠ¤(Image processor classes) [[image-processor-classes]]\n+\n+ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë“¤ì€ [`~BaseImageProcessor.center_crop`], [`~BaseImageProcessor.normalize`], [`~BaseImageProcessor.rescale`] í•¨ìˆ˜ë¥¼ ì œê³µí•˜ëŠ” [`BaseImageProcessor`] í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì—ëŠ” ë‘ ê°€ì§€ ì¢…ë¥˜ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+- [`BaseImageProcessor`]ëŠ” íŒŒì´ì¬ ê¸°ë°˜ êµ¬í˜„ì²´ì…ë‹ˆë‹¤.\n+- [`BaseImageProcessorFast`]ëŠ” ë” ë¹ ë¥¸ [torchvision-backed](https://pytorch.org/vision/stable/index.html) ë²„ì „ì…ë‹ˆë‹¤. [torch.Tensor](https://pytorch.org/docs/stable/tensors.html)ì…ë ¥ì˜ ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ìµœëŒ€ 33ë°° ë” ë¹ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. [`BaseImageProcessorFast`]ëŠ” í˜„ì¬ ëª¨ë“  ë¹„ì „ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— ëª¨ë¸ì˜ API ë¬¸ì„œë¥¼ ì°¸ì¡°í•˜ì—¬ ì§€ì› ì—¬ë¶€ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”.\n+\n+ê° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì €ì¥í•˜ê¸° ìœ„í•œ [`~ImageProcessingMixin.from_pretrained`]ì™€ [`~ImageProcessingMixin.save_pretrained`] ë©”ì†Œë“œë¥¼ ì œê³µí•˜ëŠ” [`ImageProcessingMixin`] í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ ê¸°ëŠ¥ì„ í™•ì¥ì‹œí‚µë‹ˆë‹¤.\n+\n+ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë°©ë²•ì€ [`AutoImageProcessor`]ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ ëª¨ë¸ë³„ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+<hfoptions id=\"image-processor-classes\">\n+<hfoption id=\"AutoImageProcessor\">\n+\n+[AutoClass](./model_doc/auto) APIëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œê°€ ì–´ë–¤ ëª¨ë¸ê³¼ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ ì§ì ‘ ì§€ì •í•˜ì§€ ì•Šê³ ë„ í¸ë¦¬í•˜ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.\n+\n+[`~AutoImageProcessor.from_pretrained`]ë¥¼ ì‚¬ìš©í•´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ë§Œì•½ ë¹ ë¥¸ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ê³  ì‹¶ë‹¤ë©´ `use_fast=True`ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import AutoImageProcessor\n+\n+image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\", use_fast=True)\n+```\n+\n+</hfoption>\n+<hfoption id=\"model-specific image processor\">\n+\n+ê° ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” íŠ¹ì • ë¹„ì „ ëª¨ë¸ì— ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ í”„ë¡œì„¸ì„œì˜ ì„¤ì • íŒŒì¼ì—ëŠ” í•´ë‹¹ ëª¨ë¸ì´ í•„ìš”ë¡œ í•˜ëŠ” ì´ë¯¸ì§€ í¬ê¸°ë‚˜ ì •ê·œí™”, ë¦¬ì‚¬ì´ì¦ˆ ì ìš© ì—¬ë¶€ ê°™ì€ ì •ë³´ê°€ ë‹´ê²¨ìˆìŠµë‹ˆë‹¤.\n+\n+ì´ëŸ¬í•œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ëª¨ë¸ë³„ í´ë˜ìŠ¤ì—ì„œ ì§ì ‘ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìœ¼ë©°, ë” ë¹ ë¥¸ ë²„ì „ì˜ ì§€ì› ì—¬ë¶€ëŠ” í•´ë‹¹ ëª¨ë¸ì˜ API ë¬¸ì„œì—ì„œ í™•ì¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n+\n+```py\n+from transformers import ViTImageProcessor\n+\n+image_processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n+```\n+\n+ë¹ ë¥¸ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¶ˆëŸ¬ì˜¤ê¸° ìœ„í•´ fast êµ¬í˜„ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•´ë³´ì„¸ìš”.\n+\n+```py\n+from transformers import ViTImageProcessorFast\n+\n+image_processor = ViTImageProcessorFast.from_pretrained(\"google/vit-base-patch16-224\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## ë¹ ë¥¸ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ(Fast image processors) [[fast-image-processors]]\n+\n+[`BaseImageProcessorFast`]ëŠ” [torchvision](https://pytorch.org/vision/stable/index.html)ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, íŠ¹íˆ GPUì—ì„œ ì²˜ë¦¬í•  ë•Œ ì†ë„ê°€ í›¨ì”¬ ë¹ ë¦…ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” ê¸°ì¡´ [`BaseImageProcessor`]ì™€ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ì„¤ê³„ë˜ì—ˆê¸° ë•Œë¬¸ì—, ëª¨ë¸ì´ ì§€ì›í•œë‹¤ë©´ ë³„ë„ ìˆ˜ì • ì—†ì´ ë°”ë¡œ êµì²´í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [torchvision](https://pytorch.org/get-started/locally/#mac-installation)ì„ ì„¤ì¹˜í•œ ë’¤ `use_fast` íŒŒë¼ë¯¸í„°ë¥¼ `True`ë¡œ ì§€ì •í•´ì£¼ì‹œë©´ ë©ë‹ˆë‹¤.\n+\n+\n+```py\n+from transformers import AutoImageProcessor\n+\n+processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", use_fast=True)\n+```\n+\n+`device` íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•´ ì–´ëŠ ì¥ì¹˜ì—ì„œ ì²˜ë¦¬í• ì§€ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ ì…ë ¥ê°’ì´ í…ì„œ(tensor)ë¼ë©´ ê·¸ í…ì„œì™€ ë™ì¼í•œ ì¥ì¹˜ì—ì„œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš°ì—ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPUì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤. ì•„ë˜ëŠ” ë¹ ë¥¸ í”„ë¡œì„¸ì„œë¥¼ GPUì—ì„œ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\n+\n+```py\n+from torchvision.io import read_image\n+from transformers import DetrImageProcessorFast\n+\n+images = read_image(\"image.jpg\")\n+processor = DetrImageProcessorFast.from_pretrained(\"facebook/detr-resnet-50\")\n+images_processed = processor(images, return_tensors=\"pt\", device=\"cuda\")\n+```\n+\n+<details>\n+<summary>Benchmarks</summary>\n+\n+ì´ ë²¤ì¹˜ë§ˆí¬ëŠ” NVIDIA A10G Tensor Core GPUê°€ ì¥ì°©ëœ [AWS EC2 g5.2xlarge](https://aws.amazon.com/ec2/instance-types/g5/) ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì¸¡ì •ëœ ê²°ê³¼ì…ë‹ˆë‹¤.\n+\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_padded.png\" />\n+</div>\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_detr_fast_batched_compiled.png\" />\n+</div>\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_single.png\" />\n+</div>\n+<div class=\"flex\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/benchmark_results_full_pipeline_rt_detr_fast_batched.png\" />\n+</div>\n+</details>\n+\n+## ì „ì²˜ë¦¬(Preprocess) [[preprocess]]\n+\n+Transformersì˜ ë¹„ì „ ëª¨ë¸ì€ ì…ë ¥ê°’ìœ¼ë¡œ PyTorch í…ì„œ í˜•íƒœì˜ í”½ì…€ ê°’ì„ ë°›ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì´ë¯¸ì§€ë¥¼ ë°”ë¡œ ì´ í”½ì…€ ê°’ í…ì„œ(ë°°ì¹˜ í¬ê¸°, ì±„ë„ ìˆ˜, ë†’ì´, ë„ˆë¹„)ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” í¬ê¸°ë¡œ ì´ë¯¸ì§€ë¥¼ ì¡°ì ˆí•˜ê³ , í”½ì…€ ê°’ ë˜í•œ ëª¨ë¸ ê¸°ì¤€ì— ë§ì¶° ì •ê·œí™”í•˜ê±°ë‚˜ ì¬ì¡°ì •í•©ë‹ˆë‹¤.\n+\n+ì´ëŸ¬í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ëŠ” ì´ë¯¸ì§€ ì¦ê°•ê³¼ëŠ” ë‹¤ë¥¸ ê°œë…ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ì¦ê°•ì€ í•™ìŠµ ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ê³¼ì í•©ì„ ë§‰ê¸° ìœ„í•´ ì´ë¯¸ì§€ì— ì˜ë„ì ì¸ ë³€í™”(ë°ê¸°, ìƒ‰ìƒ, íšŒì „ ë“±)ë¥¼ ì£¼ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë°˜ë©´, ì´ë¯¸ì§€ ì „ì²˜ë¦¬ëŠ” ì´ë¯¸ì§€ë¥¼ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì…ë ¥ í˜•ì‹ì— ì •í™•íˆ ë§ì¶°ì£¼ëŠ” ì‘ì—…ì—ë§Œ ì§‘ì¤‘í•©ë‹ˆë‹¤.\n+\n+ì¼ë°˜ì ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´, ì´ë¯¸ì§€ëŠ” ë³´í†µ ì¦ê°• ê³¼ì •ì„ ê±°ì¹œ ë’¤ ì „ì²˜ë¦¬ë˜ì–´ ëª¨ë¸ì— ì…ë ¥ë©ë‹ˆë‹¤. ì´ë•Œ ì¦ê°• ì‘ì—…ì€ [Albumentations](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_albumentations.ipynb), [Kornia](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification_kornia.ipynb)) ì™€ ê°™ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ì´í›„ ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.\n+\n+ì´ë²ˆ ê°€ì´ë“œì—ì„œëŠ” ì´ë¯¸ì§€ ì¦ê°•ì„ ìœ„í•´ torchvisionì˜ [transforms](https://pytorch.org/vision/stable/transforms.html) ëª¨ë“ˆì„ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n+\n+ìš°ì„  [food101](https://hf.co/datasets/food101) ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë§Œ ìƒ˜í”Œë¡œ ë¶ˆëŸ¬ì™€ì„œ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.\n+\n+```py\n+from datasets import load_dataset\n+\n+dataset = load_dataset(\"food101\", split=\"train[:100]\")\n+```\n+\n+[transforms](https://pytorch.org/vision/stable/transforms.html) ëª¨ë“ˆì˜ [Compose](https://pytorch.org/vision/master/generated/torchvision.transforms.Compose.html)APIëŠ” ì—¬ëŸ¬ ë³€í™˜ì„ í•˜ë‚˜ë¡œ ë¬¶ì–´ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ìë¥´ê³  ë¦¬ì‚¬ì´ì¦ˆí•˜ëŠ” [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)ê³¼ ìƒ‰ìƒì„ ë¬´ì‘ìœ„ë¡œ ë°”ê¾¸ëŠ” [ColorJitter](https://pytorch.org/vision/main/generated/torchvision.transforms.ColorJitter.html)ë¥¼ í•¨ê»˜ ì‚¬ìš©í•´ë³´ê² ìŠµë‹ˆë‹¤.\n+\n+ì´ë•Œ ì˜ë¼ë‚¼ ì´ë¯¸ì§€ì˜ í¬ê¸°ëŠ” ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì— ë”°ë¼ ì •í™•í•œ ë†’ì´ì™€ ë„ˆë¹„ê°€ í•„ìš”í•  ë•Œë„ ìˆê³ , ê°€ì¥ ì§§ì€ ë³€ `shortest_edge` ê°’ë§Œ í•„ìš”í•  ë•Œë„ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from torchvision.transforms import RandomResizedCrop, ColorJitter, Compose\n+\n+size = (\n+    image_processor.size[\"shortest_edge\"]\n+    if \"shortest_edge\" in image_processor.size\n+    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n+)\n+_transforms = Compose([RandomResizedCrop(size), ColorJitter(brightness=0.5, hue=0.5)])\n+```\n+\n+ì¤€ë¹„ëœ ë³€í™˜ê°’ ë“¤ì„ ì´ë¯¸ì§€ì— ì ìš©í•˜ê³ , RGB í˜•ì‹ìœ¼ë¡œ ë°”ê¿”ì¤ë‹ˆë‹¤. ê·¸ ë‹¤ìŒ, ì´ë ‡ê²Œ ì¦ê°•ëœ ì´ë¯¸ì§€ë¥¼ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œì— ë„£ì–´ í”½ì…€ ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n+\n+ì—¬ê¸°ì„œ `do_resize`íŒŒë¼ë¯¸í„°ë¥¼ `False`ë¡œ ì„¤ì •í•œ ì´ìœ ëŠ”, ì•ì„  ì¦ê°• ë‹¨ê³„ì—ì„œ [RandomResizedCrop](https://pytorch.org/vision/main/generated/torchvision.transforms.RandomResizedCrop.html)ì„ í†µí•´ ì´ë¯¸ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¡°ì ˆí–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë§Œì•½ ì¦ê°• ê³¼ì •ì„ ìƒëµí•œë‹¤ë©´, ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” `image_mean`ê³¼ `image_std`ê°’(ì „ì²˜ë¦¬ê¸° ì„¤ì • íŒŒì¼ì— ì €ì¥ë¨)ì„ ì‚¬ìš©í•´ ìë™ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆì™€ ì •ê·œí™”ë¥¼ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.\n+\n+```py\n+def transforms(examples):\n+    images = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n+    examples[\"pixel_values\"] = image_processor(images, do_resize=False, return_tensors=\"pt\")[\"pixel_values\"]\n+    return examples\n+```\n+\n+[`~datasets.Dataset.set_transform`]ì„ ì‚¬ìš©í•˜ë©´ ê²°í•©ëœ ì¦ê°• ë° ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì „ì²´ ë°ì´í„°ì…‹ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ì ìš©ë©ë‹ˆë‹¤.\n+\n+```py\n+dataset.set_transform(transforms)\n+```\n+\n+ì´ì œ ì²˜ë¦¬ëœ í”½ì…€ ê°’ì„ ë‹¤ì‹œ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ì—¬ ì¦ê°• ë° ì „ì²˜ë¦¬ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‚˜ì™”ëŠ”ì§€ ì§ì ‘ í™•ì¸í•´ ë´…ì‹œë‹¤.\n+\n+```py\n+import numpy as np\n+import matplotlib.pyplot as plt\n+\n+img = dataset[0][\"pixel_values\"]\n+plt.imshow(img.permute(1, 2, 0))\n+```\n+\n+<div class=\"flex gap-4\">\n+  <div>\n+    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/vision-preprocess-tutorial.png\" />\n+    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">ì´ì „</figcaption>\n+  </div>\n+  <div>\n+    <img class=\"rounded-xl\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png\" />\n+    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">ì´í›„</figcaption>\n+  </div>\n+</div>\n+\n+ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œëŠ” ì „ì²˜ë¦¬ë¿ë§Œ ì•„ë‹ˆë¼, ê°ì²´ íƒì§€ë‚˜ ë¶„í• ê³¼ ê°™ì€ ë¹„ì „ ì‘ì—…ì—ì„œ ëª¨ë¸ì˜ ê²°ê³¼ê°’ì„ ë°”ìš´ë”© ë°•ìŠ¤ë‚˜ ë¶„í•  ë§µì²˜ëŸ¼ ì˜ë¯¸ ìˆëŠ” ì˜ˆì¸¡ìœ¼ë¡œ ë°”ê¿”ì£¼ëŠ” í›„ì²˜ë¦¬ ê¸°ëŠ¥ë„ ê°–ì¶”ê³  ìˆìŠµë‹ˆë‹¤.\n+\n+### íŒ¨ë”©(Padding) [[padding]]\n+\n+[DETR](./model_doc/detr)ê³¼ ê°™ì€ ì¼ë¶€ ëª¨ë¸ì€ í›ˆë ¨ ì¤‘ì— [scale augmentation](https://paperswithcode.com/method/image-scale-augmentation)ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— í•œ ë°°ì¹˜ ë‚´ì— í¬í•¨ëœ ì´ë¯¸ì§€ë“¤ì˜ í¬ê¸°ê°€ ì œê°ê° ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì‹œë‹¤ì‹œí”¼ í¬ê¸°ê°€ ì„œë¡œ ë‹¤ë¥¸ ì´ë¯¸ì§€ë“¤ì€ í•˜ë‚˜ì˜ ë°°ì¹˜ë¡œ ë¬¶ì„ ìˆ˜ ì—†ì£ .\n+\n+ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ë©´ ì´ë¯¸ì§€ì— íŠ¹ìˆ˜ íŒ¨ë”© í† í°ì¸ `0`ì„ ì±„ì›Œ ë„£ì–´ í¬ê¸°ë¥¼ í†µì¼ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤. [pad](https://github.com/huggingface/transformers/blob/9578c2597e2d88b6f0b304b5a05864fd613ddcc1/src/transformers/models/detr/image_processing_detr.py#L1151) ë©”ì†Œë“œë¡œ íŒ¨ë”©ì„ ì ìš©í•˜ê³ , ì´ë ‡ê²Œ í¬ê¸°ê°€ í†µì¼ëœ ì´ë¯¸ì§€ë“¤ì„ ë°°ì¹˜ë¡œ ë¬¶ê¸° ìœ„í•´ ì‚¬ìš©ì ì •ì˜ `collate` í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+```py\n+def collate_fn(batch):\n+    pixel_values = [item[\"pixel_values\"] for item in batch]\n+    encoding = image_processor.pad(pixel_values, return_tensors=\"pt\")\n+    labels = [item[\"labels\"] for item in batch]\n+    batch = {}\n+    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n+    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n+    batch[\"labels\"] = labels\n+    return batch\n+```"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 225,
        "deletions": 2
    }
}