{
    "author": "Cyrilvallez",
    "message": "Add loading speed test (#36671)\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* trigger CIs\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py\n\n* better error messages\n\n* Update test_modeling_utils.py\n\n* Update test_modeling_utils.py",
    "sha": "2a004f9ff1a8f118318f0f82f64b74a2655d26d0",
    "files": [
        {
            "sha": "c51ca2c4384cf66dfb0141da136375132f800c53",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a004f9ff1a8f118318f0f82f64b74a2655d26d0/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a004f9ff1a8f118318f0f82f64b74a2655d26d0/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=2a004f9ff1a8f118318f0f82f64b74a2655d26d0",
            "patch": "@@ -17,8 +17,10 @@\n import json\n import os\n import os.path\n+import subprocess\n import sys\n import tempfile\n+import textwrap\n import threading\n import unittest\n import unittest.mock as mock\n@@ -28,6 +30,7 @@\n \n import requests\n from huggingface_hub import HfApi, HfFolder\n+from parameterized import parameterized\n from pytest import mark\n from requests.exceptions import HTTPError\n \n@@ -55,10 +58,12 @@\n     is_staging_test,\n     require_accelerate,\n     require_flax,\n+    require_read_token,\n     require_safetensors,\n     require_tf,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_usr_bin_time,\n     slow,\n@@ -1900,6 +1905,61 @@ def test_unknown_quantization_config(self):\n             self.assertEqual(len(cm.records), 1)\n             self.assertTrue(cm.records[0].message.startswith(\"Unknown quantization type, got\"))\n \n+    @parameterized.expand([(\"Qwen/Qwen2.5-3B-Instruct\", 10), (\"meta-llama/Llama-2-7b-chat-hf\", 10)])\n+    @slow\n+    @require_read_token\n+    @require_torch_gpu\n+    def test_loading_is_fast_on_gpu(self, model_id: str, max_loading_time: float):\n+        \"\"\"\n+        This test is used to avoid regresion on https://github.com/huggingface/transformers/pull/36380.\n+        10s should be more than enough for both models, and allows for some margin as loading time are quite\n+        unstable. Before #36380, it used to take more than 40s, so 10s is still reasonable.\n+        Note that we run this test in a subprocess, to ensure that cuda is not already initialized/warmed-up.\n+        \"\"\"\n+        # First download the weights if not already on disk\n+        _ = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n+\n+        script_to_run = textwrap.dedent(\n+            \"\"\"\n+            import torch\n+            import time\n+            import argparse\n+            from transformers import AutoModelForCausalLM\n+\n+            parser = argparse.ArgumentParser()\n+            parser.add_argument(\"model_id\", type=str)\n+            parser.add_argument(\"max_loading_time\", type=float)\n+            args = parser.parse_args()\n+\n+            device = torch.device(\"cuda:0\")\n+\n+            torch.cuda.synchronize(device)\n+            t0 = time.time()\n+            model = AutoModelForCausalLM.from_pretrained(args.model_id, torch_dtype=torch.float16, device_map=device)\n+            torch.cuda.synchronize(device)\n+            dt = time.time() - t0\n+\n+            # Assert loading is faster (it should be more than enough in both cases)\n+            if dt > args.max_loading_time:\n+                raise ValueError(f\"Loading took {dt:.2f}s! It should not take more than {args.max_loading_time}s\")\n+            # Ensure everything is correctly loaded on gpu\n+            bad_device_params = {k for k, v in model.named_parameters() if v.device != device}\n+            if len(bad_device_params) > 0:\n+                raise ValueError(f\"The following parameters are not on GPU: {bad_device_params}\")\n+            \"\"\"\n+        )\n+\n+        with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n+            tmp.write(script_to_run)\n+            tmp.flush()\n+            tmp.seek(0)\n+            cmd = f\"python {tmp.name} {model_id} {max_loading_time}\".split()\n+            try:\n+                # We cannot use a timeout of `max_loading_time` as cuda initialization can take up to 15-20s\n+                _ = subprocess.run(cmd, capture_output=True, env=self.get_env(), text=True, check=True, timeout=60)\n+            except subprocess.CalledProcessError as e:\n+                raise Exception(f\"The following error was captured: {e.stderr}\")\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 60,
        "deletions": 0
    }
}