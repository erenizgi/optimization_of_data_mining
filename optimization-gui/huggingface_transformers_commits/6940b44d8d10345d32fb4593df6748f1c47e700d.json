{
    "author": "ArthurZucker",
    "message": "Auto convert tekken.json (#42299)\n\n* auto convert tekken.json\n\n* fix conversion\n\n* simplify\n\n* nit\n\n* model info based on the fly fix\n\n* up\n\n* last nit\n\n* fixup\n\n* call it fix mistral regex\n\n* fix behaviour for local or only tok is saved\n\n* style\n\n* rm comment at wrong palce\n\n* fix escaping\n\n* style\n\n* fix backend tokenizer attr to _tokenizer\n\n* update\n\n* up\n\n* update\n\n* fix the last red tests",
    "sha": "6940b44d8d10345d32fb4593df6748f1c47e700d",
    "files": [
        {
            "sha": "8bafa5d017b0ebd9ec6ee3547c8bfdf13ef67394",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 85,
            "deletions": 1,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/6940b44d8d10345d32fb4593df6748f1c47e700d/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6940b44d8d10345d32fb4593df6748f1c47e700d/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=6940b44d8d10345d32fb4593df6748f1c47e700d",
            "patch": "@@ -19,11 +19,13 @@\n \"\"\"\n \n import warnings\n+from functools import lru_cache\n from typing import Optional\n \n from packaging import version\n from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, processors\n from tokenizers.models import BPE, Unigram, WordPiece\n+from tqdm import tqdm\n \n from .utils import is_protobuf_available, is_sentencepiece_available, logging, requires_backends\n from .utils.import_utils import PROTOBUF_IMPORT_ERROR\n@@ -1692,6 +1694,85 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n+class MistralConverter:\n+    def __init__(\n+        self,\n+        vocab_file=None,\n+        pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n+        add_prefix_space=False,\n+        additional_special_tokens=None,\n+        **kwargs,\n+    ):\n+        self.vocab_file = vocab_file\n+        self.pattern = pattern\n+        self.add_prefix_space = add_prefix_space\n+        self.additional_special_tokens = (\n+            additional_special_tokens.keys()\n+            if isinstance(additional_special_tokens, dict)\n+            else additional_special_tokens\n+        )\n+\n+    def extract_vocab_merges_from_model(self, tiktoken_url: str):\n+        import base64\n+        import json\n+\n+        with open(self.vocab_file, \"r\", encoding=\"utf-8\") as f:\n+            untyped = json.load(f)\n+        self.pattern = untyped[\"config\"][\"pattern\"]\n+        self.additional_special_tokens = [\n+            AddedToken(k[\"token_str\"], special=k[\"is_control\"]) for k in untyped[\"special_tokens\"]\n+        ]\n+        bpe_ranks = untyped[\"vocab\"]\n+        byte_encoder = bytes_to_unicode()\n+\n+        @lru_cache\n+        def token_bytes_to_string(b):\n+            return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n+\n+        merges = []\n+        vocab = {}\n+        for idx, token in enumerate(self.additional_special_tokens):\n+            vocab[token.content] = idx\n+        bpe_ranks = [base64.b64decode(k[\"token_bytes\"]) for k in bpe_ranks]\n+        rank_set = set(bpe_ranks)\n+        for rank, token in enumerate(tqdm(bpe_ranks, desc=\"Converting tekken.json to tokenizer.json\")):\n+            vocab[token_bytes_to_string(token)] = rank\n+            if len(token) == 1:\n+                continue\n+            local = []\n+            for index in range(1, len(token)):\n+                piece_l, piece_r = token[:index], token[index:]\n+                if piece_l in rank_set and piece_r in rank_set and (piece_l + piece_r) in rank_set:\n+                    local.append((piece_l, piece_r, rank))\n+            local = sorted(local, key=lambda x: (bpe_ranks.index(x[0]), bpe_ranks.index(x[1])), reverse=False)\n+            merges.extend(local)\n+        merges = sorted(merges, key=lambda val: val[2], reverse=False)\n+        merges = [(token_bytes_to_string(val[0]), token_bytes_to_string(val[1])) for val in merges]\n+        return vocab, merges\n+\n+    def tokenizer(self):\n+        vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)\n+        tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=False))\n+        if hasattr(tokenizer.model, \"ignore_merges\"):\n+            tokenizer.model.ignore_merges = True\n+        return tokenizer\n+\n+    def converted(self) -> Tokenizer:\n+        tokenizer = self.tokenizer()\n+        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.Split(Regex(self.pattern), behavior=\"isolated\", invert=False),\n+                pre_tokenizers.ByteLevel(add_prefix_space=self.add_prefix_space, use_regex=False),\n+            ]\n+        )\n+        tokenizer.decoder = decoders.ByteLevel()\n+\n+        tokenizer.add_tokens(self.additional_special_tokens)\n+        tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n+\n+        return tokenizer\n+\n+\n SLOW_TO_FAST_CONVERTERS = {\n     \"AlbertTokenizer\": AlbertConverter,\n     \"BartTokenizer\": RobertaConverter,\n@@ -1771,7 +1852,10 @@ def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=False) -> Tokeni\n     if tokenizer_class_name in SLOW_TO_FAST_CONVERTERS and not from_tiktoken:\n         converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name]\n         return converter_class(transformer_tokenizer).converted()\n-\n+    elif transformer_tokenizer.vocab_file.endswith(\"tekken.json\"):\n+        transformer_tokenizer.original_tokenizer = transformer_tokenizer\n+        logger.info(\"Converting from Mistral tekken.json\")\n+        return MistralConverter(transformer_tokenizer.vocab_file).converted()\n     else:\n         try:\n             logger.info(\"Converting from Tiktoken\")"
        },
        {
            "sha": "5edda2f5be8c3cbd416f7126699071e450714554",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6940b44d8d10345d32fb4593df6748f1c47e700d/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6940b44d8d10345d32fb4593df6748f1c47e700d/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=6940b44d8d10345d32fb4593df6748f1c47e700d",
            "patch": "@@ -748,10 +748,8 @@\n         (\n             \"voxtral\",\n             (\n-                \"MistralCommonTokenizer\"\n-                if is_mistral_common_available()\n-                else (\"LlamaTokenizer\" if is_sentencepiece_available() else None),\n-                \"LlamaTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n+                \"MistralCommonTokenizer\" if is_mistral_common_available() else None,\n+                \"PreTrainedTokenizerFast\" if is_tokenizers_available() and not is_mistral_common_available() else None,\n             ),\n         ),\n         (\"wav2vec2\", (\"Wav2Vec2CTCTokenizer\", None)),"
        },
        {
            "sha": "14e516778fce920f719a17674ca624b57a407409",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 85,
            "deletions": 1,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/6940b44d8d10345d32fb4593df6748f1c47e700d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6940b44d8d10345d32fb4593df6748f1c47e700d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=6940b44d8d10345d32fb4593df6748f1c47e700d",
            "patch": "@@ -33,6 +33,7 @@\n from typing import TYPE_CHECKING, Any, Literal, NamedTuple, Optional, Union, overload\n \n import numpy as np\n+from huggingface_hub import list_repo_files\n from packaging import version\n \n from . import __version__\n@@ -2098,7 +2099,21 @@ def from_pretrained(\n                             template = template.removesuffix(\".jinja\")\n                             vocab_files[f\"chat_template_{template}\"] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n \n-        # Get files from url, cache, or disk depending on the case\n+        if not is_local and not local_files_only:\n+            try:\n+                remote_files = list_repo_files(pretrained_model_name_or_path)\n+            except Exception:\n+                remote_files = []\n+        else:\n+            remote_files = os.listdir(pretrained_model_name_or_path)\n+\n+        if \"tokenizer_file\" in vocab_files and not re.search(vocab_files[\"tokenizer_file\"], \"\".join(remote_files)):\n+            # mistral tokenizer names are different, but we can still convert them if\n+            # mistral common is not there\n+            other_pattern = re.escape(\"tekken.json|tokenizer.model.*\")\n+            if match := re.search(other_pattern, \"\\n\".join(remote_files)):\n+                vocab_files[\"vocab_file\"] = match.group()\n+\n         resolved_vocab_files = {}\n         for file_id, file_path in vocab_files.items():\n             if file_path is None:\n@@ -2417,6 +2432,75 @@ def _from_pretrained(\n                 \"Special tokens have been added in the vocabulary, make sure the associated word embeddings are\"\n                 \" fine-tuned or trained.\"\n             )\n+        try:\n+            vocab_size = tokenizer.vocab_size\n+        except NotImplementedError:\n+            vocab_size = 0\n+\n+        if (\n+            vocab_size > 100000\n+            and hasattr(tokenizer, \"_tokenizer\")\n+            and getattr(tokenizer._tokenizer, \"pre_tokenizer\", None) is not None\n+        ):\n+            from huggingface_hub import model_info\n+\n+            def is_base_mistral(model_id: str) -> bool:\n+                model = model_info(model_id)\n+                if model.tags is not None:\n+                    if re.search(\"base_model:.*mistralai\", \"\".join(model.tags)):\n+                        return True\n+                return False\n+\n+            if _is_local or is_base_mistral(pretrained_model_name_or_path):\n+                _config_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    \"config.json\",\n+                    cache_dir=cache_dir,\n+                    token=token,\n+                    local_files_only=local_files_only,\n+                    _raise_exceptions_for_missing_entries=False,\n+                    _raise_exceptions_for_connection_errors=False,\n+                    _commit_hash=_commit_hash,\n+                )\n+                if _config_file is not None:\n+                    with open(_config_file, encoding=\"utf-8\") as f:\n+                        _config = json.load(f)\n+                    transformers_version = _config.get(\"transformers_version\")\n+\n+                    if transformers_version and version.parse(transformers_version) <= version.parse(\"4.57.2\"):\n+                        if _is_local and _config.model_type not in [\n+                            \"mistral\",\n+                            \"mistral3\",\n+                            \"voxstral\",\n+                            \"ministral\",\n+                            \"pixtral\",\n+                        ]:\n+                            return tokenizer\n+\n+                # Expose the `fix_mistral_regex` flag on the tokenizer when provided, even if no correction is applied.\n+                if \"fix_mistral_regex\" in init_kwargs:\n+                    setattr(tokenizer, \"fix_mistral_regex\", init_kwargs[\"fix_mistral_regex\"])\n+\n+                fix_mistral_regex = kwargs.get(\"fix_mistral_regex\")  # not init kwargs\n+                # only warn if its not explicitly passed\n+                if fix_mistral_regex is None and not getattr(tokenizer, \"fix_mistral_regex\", False):\n+                    setattr(tokenizer, \"fix_mistral_regex\", False)\n+                    logger.warning(\n+                        f\"The tokenizer you are loading from '{pretrained_model_name_or_path}'\"\n+                        f\" with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. \"\n+                        \" This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\"\n+                    )\n+                elif fix_mistral_regex is True or getattr(tokenizer, \"fix_mistral_regex\", False):\n+                    setattr(tokenizer, \"fix_mistral_regex\", True)\n+                    import tokenizers\n+\n+                    tokenizer.backend_tokenizer.pre_tokenizer[0] = tokenizers.pre_tokenizers.Split(\n+                        pattern=tokenizers.Regex(\n+                            r\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*|\\p{N}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n+                        ),\n+                        behavior=\"isolated\",\n+                    )\n+\n         return tokenizer\n \n     @staticmethod"
        }
    ],
    "stats": {
        "total": 178,
        "additions": 172,
        "deletions": 6
    }
}