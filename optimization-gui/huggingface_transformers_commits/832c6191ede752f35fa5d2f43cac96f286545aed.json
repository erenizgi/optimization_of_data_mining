{
    "author": "jxmorris12",
    "message": "Add inputs_embeds param to ModernBertModel (#35373)\n\n* update modular_modernbert -- add inputs_embeds param to ModernBertModel\r\n\r\n* Fix implementation issues; extend to other classes; docstring\r\n\r\nFirst of all, the inputs_embeds shouldn't fully replace `self.embeddings(input_ids)`, because this call also does layer normalization and dropout. So, now both input_ids and inputs_embeds is passed to the ModernBertEmbeddings, much like how BertEmbeddings is implemented.\r\n\r\nI also added `inputs_embeds` to the docstring, and propagated the changes to the other model classes.\r\n\r\nI also introduced an error if input_ids and input_embeds are both or neither provided.\r\n\r\nLastly, I fixed an issue with device being based solely on input_ids with attention_mask.\r\n\r\n* Propagate inputs_embeds to ModernBertForMaskedLM correctly\r\n\r\nAlso reintroduce inputs_embeds test\r\n\r\n---------\r\n\r\nCo-authored-by: Tom Aarsen <Cubiegamedev@gmail.com>",
    "sha": "832c6191ede752f35fa5d2f43cac96f286545aed",
    "files": [
        {
            "sha": "c3dcb26ad883ddcb0c53a3db44b7ab274da387c6",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 68,
            "deletions": 26,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/832c6191ede752f35fa5d2f43cac96f286545aed/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/832c6191ede752f35fa5d2f43cac96f286545aed/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=832c6191ede752f35fa5d2f43cac96f286545aed",
            "patch": "@@ -205,12 +205,17 @@ def __init__(self, config: ModernBertConfig):\n     def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n         return self.drop(self.norm(self.tok_embeddings(input_ids)))\n \n-    def forward(self, input_ids: torch.LongTensor, position_ids: Optional[torch.LongTensor] = None) -> torch.Tensor:\n-        hidden_states = (\n-            self.compiled_embeddings(input_ids)\n-            if self.config.reference_compile\n-            else self.drop(self.norm(self.tok_embeddings(input_ids)))\n-        )\n+    def forward(\n+        self, input_ids: torch.LongTensor = None, inputs_embeds: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n+        if inputs_embeds is not None:\n+            hidden_states = self.drop(self.norm(inputs_embeds))\n+        else:\n+            hidden_states = (\n+                self.compiled_embeddings(input_ids)\n+                if self.config.reference_compile\n+                else self.drop(self.norm(self.tok_embeddings(input_ids)))\n+            )\n         return hidden_states\n \n \n@@ -791,6 +796,10 @@ def _pad_modernbert_output(\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):\n             Indices of the non-padding tokens in the input sequence. Used for unpadding the output.\n         cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):\n@@ -842,10 +851,11 @@ def set_input_embeddings(self, value):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n         max_seqlen: Optional[int] = None,\n@@ -861,35 +871,49 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n         self._maybe_set_compile()\n-        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+\n+        if input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n \n         if batch_size is None and seq_len is None:\n-            batch_size, seq_len = input_ids.shape[:2]\n+            if inputs_embeds is not None:\n+                batch_size, seq_len = inputs_embeds.shape[:2]\n+            else:\n+                batch_size, seq_len = input_ids.shape[:2]\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n+            attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.bool)\n \n         repad = False\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if indices is None and cu_seqlens is None and max_seqlen is None:\n                 repad = True\n-                with torch.no_grad():\n-                    input_ids, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n-                        inputs=input_ids, attention_mask=attention_mask\n+                if inputs_embeds is None:\n+                    with torch.no_grad():\n+                        input_ids, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n+                            inputs=input_ids, attention_mask=attention_mask\n+                        )\n+                else:\n+                    inputs_embeds, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n+                        inputs=inputs_embeds, attention_mask=attention_mask\n                     )\n         else:\n             if position_ids is None:\n-                position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n+                position_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n \n             attention_mask, sliding_window_mask = self._update_attention_mask(\n                 attention_mask, output_attentions=output_attentions\n             )\n \n-        hidden_states = self.embeddings(input_ids)\n+        hidden_states = self.embeddings(input_ids=input_ids, inputs_embeds=inputs_embeds)\n \n         for encoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1025,10 +1049,11 @@ def compiled_head(self, output: torch.Tensor) -> torch.Tensor:\n     )\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor],\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n@@ -1045,19 +1070,32 @@ def forward(\n \n         if self.config._attn_implementation == \"flash_attention_2\":\n             if indices is None and cu_seqlens is None and max_seqlen is None:\n-                batch_size, seq_len = input_ids.shape[:2]\n+                if batch_size is None and seq_len is None:\n+                    if inputs_embeds is not None:\n+                        batch_size, seq_len = inputs_embeds.shape[:2]\n+                    else:\n+                        batch_size, seq_len = input_ids.shape[:2]\n+                device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n                 if attention_mask is None:\n-                    attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n-                with torch.no_grad():\n-                    input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n-                        inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n+                    attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.bool)\n+\n+                if inputs_embeds is None:\n+                    with torch.no_grad():\n+                        input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n+                            inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n+                        )\n+                else:\n+                    inputs_embeds, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n+                        inputs=inputs_embeds, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n                     )\n \n         outputs = self.model(\n-            input_ids,\n+            input_ids=input_ids,\n             attention_mask=attention_mask,\n             sliding_window_mask=sliding_window_mask,\n             position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             indices=indices,\n             cu_seqlens=cu_seqlens,\n             max_seqlen=max_seqlen,\n@@ -1130,10 +1168,11 @@ def __init__(self, config: ModernBertConfig):\n     )\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor],\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n@@ -1155,10 +1194,11 @@ def forward(\n         self._maybe_set_compile()\n \n         outputs = self.model(\n-            input_ids,\n+            input_ids=input_ids,\n             attention_mask=attention_mask,\n             sliding_window_mask=sliding_window_mask,\n             position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             indices=indices,\n             cu_seqlens=cu_seqlens,\n             max_seqlen=max_seqlen,\n@@ -1241,10 +1281,11 @@ def __init__(self, config: ModernBertConfig):\n     )\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor],\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n@@ -1263,10 +1304,11 @@ def forward(\n         self._maybe_set_compile()\n \n         outputs = self.model(\n-            input_ids,\n+            input_ids=input_ids,\n             attention_mask=attention_mask,\n             sliding_window_mask=sliding_window_mask,\n             position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             indices=indices,\n             cu_seqlens=cu_seqlens,\n             max_seqlen=max_seqlen,"
        },
        {
            "sha": "aaad42a7e67c93a61b5eb2548e7110420f7ac89a",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 68,
            "deletions": 26,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/832c6191ede752f35fa5d2f43cac96f286545aed/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/832c6191ede752f35fa5d2f43cac96f286545aed/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=832c6191ede752f35fa5d2f43cac96f286545aed",
            "patch": "@@ -464,12 +464,17 @@ def __init__(self, config: ModernBertConfig):\n     def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n         return self.drop(self.norm(self.tok_embeddings(input_ids)))\n \n-    def forward(self, input_ids: torch.LongTensor, position_ids: Optional[torch.LongTensor] = None) -> torch.Tensor:\n-        hidden_states = (\n-            self.compiled_embeddings(input_ids)\n-            if self.config.reference_compile\n-            else self.drop(self.norm(self.tok_embeddings(input_ids)))\n-        )\n+    def forward(\n+        self, input_ids: torch.LongTensor = None, inputs_embeds: Optional[torch.Tensor] = None\n+    ) -> torch.Tensor:\n+        if inputs_embeds is not None:\n+            hidden_states = self.drop(self.norm(inputs_embeds))\n+        else:\n+            hidden_states = (\n+                self.compiled_embeddings(input_ids)\n+                if self.config.reference_compile\n+                else self.drop(self.norm(self.tok_embeddings(input_ids)))\n+            )\n         return hidden_states\n \n \n@@ -944,6 +949,10 @@ def resize_token_embeddings(self, *args, **kwargs):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n         indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):\n             Indices of the non-padding tokens in the input sequence. Used for unpadding the output.\n         cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):\n@@ -995,10 +1004,11 @@ def set_input_embeddings(self, value):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n         max_seqlen: Optional[int] = None,\n@@ -1014,35 +1024,49 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n \n         self._maybe_set_compile()\n-        self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+\n+        if input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n \n         if batch_size is None and seq_len is None:\n-            batch_size, seq_len = input_ids.shape[:2]\n+            if inputs_embeds is not None:\n+                batch_size, seq_len = inputs_embeds.shape[:2]\n+            else:\n+                batch_size, seq_len = input_ids.shape[:2]\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n         if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n+            attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.bool)\n \n         repad = False\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if indices is None and cu_seqlens is None and max_seqlen is None:\n                 repad = True\n-                with torch.no_grad():\n-                    input_ids, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n-                        inputs=input_ids, attention_mask=attention_mask\n+                if inputs_embeds is None:\n+                    with torch.no_grad():\n+                        input_ids, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n+                            inputs=input_ids, attention_mask=attention_mask\n+                        )\n+                else:\n+                    inputs_embeds, indices, cu_seqlens, max_seqlen, *_ = _unpad_modernbert_input(\n+                        inputs=inputs_embeds, attention_mask=attention_mask\n                     )\n         else:\n             if position_ids is None:\n-                position_ids = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n+                position_ids = torch.arange(seq_len, device=device).unsqueeze(0)\n \n             attention_mask, sliding_window_mask = self._update_attention_mask(\n                 attention_mask, output_attentions=output_attentions\n             )\n \n-        hidden_states = self.embeddings(input_ids)\n+        hidden_states = self.embeddings(input_ids=input_ids, inputs_embeds=inputs_embeds)\n \n         for encoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1178,10 +1202,11 @@ def compiled_head(self, output: torch.Tensor) -> torch.Tensor:\n     )\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor],\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n@@ -1198,19 +1223,32 @@ def forward(\n \n         if self.config._attn_implementation == \"flash_attention_2\":\n             if indices is None and cu_seqlens is None and max_seqlen is None:\n-                batch_size, seq_len = input_ids.shape[:2]\n+                if batch_size is None and seq_len is None:\n+                    if inputs_embeds is not None:\n+                        batch_size, seq_len = inputs_embeds.shape[:2]\n+                    else:\n+                        batch_size, seq_len = input_ids.shape[:2]\n+                device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n                 if attention_mask is None:\n-                    attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n-                with torch.no_grad():\n-                    input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n-                        inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n+                    attention_mask = torch.ones((batch_size, seq_len), device=device, dtype=torch.bool)\n+\n+                if inputs_embeds is None:\n+                    with torch.no_grad():\n+                        input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n+                            inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n+                        )\n+                else:\n+                    inputs_embeds, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n+                        inputs=inputs_embeds, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n                     )\n \n         outputs = self.model(\n-            input_ids,\n+            input_ids=input_ids,\n             attention_mask=attention_mask,\n             sliding_window_mask=sliding_window_mask,\n             position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             indices=indices,\n             cu_seqlens=cu_seqlens,\n             max_seqlen=max_seqlen,\n@@ -1283,10 +1321,11 @@ def __init__(self, config: ModernBertConfig):\n     )\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor],\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n@@ -1308,10 +1347,11 @@ def forward(\n         self._maybe_set_compile()\n \n         outputs = self.model(\n-            input_ids,\n+            input_ids=input_ids,\n             attention_mask=attention_mask,\n             sliding_window_mask=sliding_window_mask,\n             position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             indices=indices,\n             cu_seqlens=cu_seqlens,\n             max_seqlen=max_seqlen,\n@@ -1394,10 +1434,11 @@ def __init__(self, config: ModernBertConfig):\n     )\n     def forward(\n         self,\n-        input_ids: Optional[torch.Tensor],\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         sliding_window_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         indices: Optional[torch.Tensor] = None,\n         cu_seqlens: Optional[torch.Tensor] = None,\n@@ -1416,10 +1457,11 @@ def forward(\n         self._maybe_set_compile()\n \n         outputs = self.model(\n-            input_ids,\n+            input_ids=input_ids,\n             attention_mask=attention_mask,\n             sliding_window_mask=sliding_window_mask,\n             position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n             indices=indices,\n             cu_seqlens=cu_seqlens,\n             max_seqlen=max_seqlen,"
        },
        {
            "sha": "08e77505b5b787747905dada35081af1df93fefa",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/832c6191ede752f35fa5d2f43cac96f286545aed/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/832c6191ede752f35fa5d2f43cac96f286545aed/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=832c6191ede752f35fa5d2f43cac96f286545aed",
            "patch": "@@ -146,7 +146,11 @@ def get_config(self):\n             # If we're testing `test_retain_grad_hidden_states_attentions`, we normally get an error\n             # that compilation doesn't work. Users can then set compile=False when loading the model,\n             # much like here. We're testing whether it works once they've done that.\n-            if test_name == \"test_retain_grad_hidden_states_attentions\":\n+\n+            # If we're testing `test_inputs_embeds_matches_input_ids`, then we'd like to test with `reference_compile`\n+            # set to False, otherwise the input_ids with compiled input embeddings will not match the inputs_embeds\n+            # with atol=1e-8 and rtol=1e-5\n+            if test_name in (\"test_retain_grad_hidden_states_attentions\", \"test_inputs_embeds_matches_input_ids\"):\n                 config.reference_compile = False\n             # Some tests require attentions to be outputted, in that case we'll set the attention implementation to eager\n             # as the others don't support outputted attentions\n@@ -294,10 +298,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    @unittest.skip(\"ModernBert doesn't use `inputs_embeds` as input.\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n     def test_for_masked_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)"
        }
    ],
    "stats": {
        "total": 198,
        "additions": 141,
        "deletions": 57
    }
}