{
    "author": "ArthurZucker",
    "message": "Refactor some core stuff (#36539)\n\n* some config changes\n\n* update\n\n* current state\n\n* update\n\n* update\n\n* updates and cleanup\n\n* something that works\n\n* fixup\n\n* fixes\n\n* nits\n\n* nit\n\n* nits and fix\n\n* Update src/transformers/integrations/tensor_parallel.py\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* Update src/transformers/integrations/tensor_parallel.py\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>\n\n* cleanup\n\n* style\n\n* safe import\n\n* fix\n\n* updates\n\n* rename stuff an clean\n\n* style\n\n* small updates\n\n* ups\n\n* oups\n\n* nit\n\n* protect imports\n\n* update tp\n\n* rodfl\n\n* arf\n\n* turbo nit on init\n\n* fix import error\n\n* frumble gumbgle\n\n* try to fix the import error\n\n* should fix the non model test\n\n* update keep in float32\n\n* update\n\n* fix\n\n* nits\n\n* fix subvconfigs\n\n* test was weird\n\n* nit\n\n* fix failing test\n\n* fix instruct blip\n\n* fixes\n\n* style\n\n* x.com\n\n* fix overwrite\n\n* ok last bit of failing test\n\n---------\n\nCo-authored-by: Lysandre Debut <hi@lysand.re>",
    "sha": "1c4b62b219323a31011bac3bd3cece7675d9e4c3",
    "files": [
        {
            "sha": "9b29b0d2b952ca96c46754cc00fbc79920ad4642",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -824,25 +824,27 @@ def to_diff_dict(self) -> Dict[str, Any]:\n \n         serializable_config_dict = {}\n \n-        # only serialize values that differ from the default config\n+        # Only serialize values that differ from the default config,\n+        # except always keep the 'config' attribute.\n         for key, value in config_dict.items():\n             if (\n                 isinstance(getattr(self, key, None), PretrainedConfig)\n                 and key in class_config_dict\n                 and isinstance(class_config_dict[key], dict)\n+                or key in self.sub_configs\n             ):\n                 # For nested configs we need to clean the diff recursively\n-                diff = recursive_diff_dict(value, class_config_dict[key], config_obj=getattr(self, key, None))\n+                diff = recursive_diff_dict(value, default_config_dict, config_obj=getattr(self, key, None))\n                 if \"model_type\" in value:\n                     # Needs to be set even if it's not in the diff\n                     diff[\"model_type\"] = value[\"model_type\"]\n-                if len(diff) > 0:\n-                    serializable_config_dict[key] = diff\n+                serializable_config_dict[key] = diff\n             elif (\n                 key not in default_config_dict\n                 or key == \"transformers_version\"\n+                or key == \"vocab_file\"\n                 or value != default_config_dict[key]\n-                or (key in class_config_dict and value != class_config_dict[key])\n+                or (key in default_config_dict and value != class_config_dict.get(key, value))\n             ):\n                 serializable_config_dict[key] = value\n \n@@ -867,6 +869,9 @@ def to_diff_dict(self) -> Dict[str, Any]:\n         if \"base_model_pp_plan\" in serializable_config_dict:\n             del serializable_config_dict[\"base_model_pp_plan\"]\n \n+        if \"_name_or_path\" in serializable_config_dict:\n+            del serializable_config_dict[\"_name_or_path\"]\n+\n         return serializable_config_dict\n \n     def to_dict(self) -> Dict[str, Any]:\n@@ -1178,16 +1183,17 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     \"\"\"\n     Helper function to recursively take the diff between two nested dictionaries. The resulting diff only contains the\n     values from `dict_a` that are different from values in `dict_b`.\n+\n+    dict_b : the default config dictionnary. We want to remove values that are in this one\n     \"\"\"\n     diff = {}\n     default = config_obj.__class__().to_dict() if config_obj is not None else {}\n     for key, value in dict_a.items():\n         obj_value = getattr(config_obj, str(key), None)\n         if isinstance(obj_value, PretrainedConfig) and key in dict_b and isinstance(dict_b[key], dict):\n             diff_value = recursive_diff_dict(value, dict_b[key], config_obj=obj_value)\n-            if len(diff_value) > 0:\n-                diff[key] = diff_value\n-        elif key not in dict_b or value != dict_b[key] or key not in default or value != default[key]:\n+            diff[key] = diff_value\n+        elif key not in dict_b or (value != default[key]):\n             diff[key] = value\n     return diff\n "
        },
        {
            "sha": "7e413cd06405f21157021fa29893c1ce2381d6f9",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 25,
            "deletions": 1,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n from typing import TYPE_CHECKING\n \n-from ..utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available\n+from ..utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_torch_greater_or_equal\n \n \n _import_structure = {\n@@ -128,6 +128,18 @@\n         \"convert_and_export_with_cache\",\n     ]\n \n+try:\n+    if not is_torch_greater_or_equal(\"2.3\"):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"tensor_parallel\"] = [\n+        \"shard_and_distribute_module\",\n+        \"SUPPORTED_TP_STYLES\",\n+        \"translate_to_torch_parallel_style\",\n+    ]\n+\n if TYPE_CHECKING:\n     from .aqlm import replace_with_aqlm_linear\n     from .awq import (\n@@ -231,6 +243,18 @@\n     else:\n         from .executorch import TorchExportableModuleWithStaticCache, convert_and_export_with_cache\n \n+    try:\n+        if not is_torch_greater_or_equal(\"2.3\"):\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .tensor_parallel import (\n+            SUPPORTED_TP_STYLES,\n+            shard_and_distribute_module,\n+            translate_to_torch_parallel_style,\n+        )\n+\n else:\n     import sys\n "
        },
        {
            "sha": "9e8a0dec7633c94f0d50283d5dac104272a69312",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "added",
            "additions": 544,
            "deletions": 0,
            "changes": 544,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -0,0 +1,544 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from __future__ import annotations\n+\n+import re\n+from functools import lru_cache, partial\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from ..utils import is_torch_greater_or_equal, logging\n+\n+\n+ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n+\n+logger = logging.get_logger(__name__)\n+\n+# Cache this result has it's a C FFI call which can be pretty time-consuming\n+_torch_distributed_available = torch.distributed.is_available()\n+\n+\n+if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n+    from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n+\n+\n+def _blocks_to_block_sizes(total_size: int, blocks: Union[int, List[int]]) -> List[int]:\n+    \"\"\"\n+    Convert block count or proportions to block sizes.\n+\n+    This function accepts\n+\n+    - The number of blocks (int), in which case the block size is\n+      total_size//blocks; or\n+    - A list of block sizes (List[int]).\n+\n+    In the second case, if sum(blocks) < total_size, the ratios between\n+    the block sizes will be preserved. For instance, if blocks is\n+    [2, 1, 1] and total_size is 1024, the returned block sizes are\n+    [512, 256, 256].\n+    \"\"\"\n+    if isinstance(blocks, list):\n+        total_blocks = sum(blocks)\n+        assert total_size % total_blocks == 0, f\"Cannot split {total_size} in proportional blocks: {blocks}\"\n+        part_size = total_size // total_blocks\n+        return [part_size * block for block in blocks]\n+    else:\n+        assert total_size % blocks == 0, f\"Prepacked is not divisible by {blocks}\"\n+        single_size = total_size // blocks\n+        return [single_size] * blocks\n+\n+\n+def get_packed_weights(param, empty_param, device_mesh, rank, dim):\n+    \"\"\"\n+    When weights are packed (gate_up_proj), we need to make sure each shard gets its correct share.\n+    So if you have: gate_proj       ( 16, 5120, 8190)\n+    and             up_proj         ( 16, 5120, 8190)\n+    packed as       gate_up_proj    ( 16, 5120, 2 * 8190)\n+    And you shard along the last dimension, you need to interleave the gate and up values:\n+\n+    Now, if we shard along the last dimension across TP_size (Tensor Parallelism size), we must interleave the values from gate and up projections correctly.\n+\n+    Let's take TP_size = 4 for an example:\n+\n+    Packed tensor `gate_up_proj`\n+    ---------------------------------------------------------------\n+    [ G0  G1  G2  G3 | G4  G5  G6  G7 | ... | U0  U1  U2  U3 | U4  U5  U6  U7 | ... ]\n+     ↑─────────────↑   ↑─────────────↑        ↑─────────────↑  ↑─────────────↑\n+       Gate Slice 0      Gate Slice 1            Up Slice 0       Up Slice 1\n+\n+    Explanation:\n+    - The first half of the tensor (left of the center) holds the gate_proj values.\n+    - The second half (right of the center) holds the up_proj values.\n+    - For TP=4, we divide each half into 4 slices. In this example, we show two slices for brevity.\n+    - Each shard receives one slice from the gate part and the corresponding slice from the up part.\n+\n+    For instance:\n+    • Shard 0 gets: [ Gate Slice 0, Up Slice 0 ] = [ G0, G1, G2, G3, U0, U1, U2, U3 ]\n+    • Shard 1 gets: [ Gate Slice 1, Up Slice 1 ] = [ G4, G5, G6, G7, U4, U5, U6, U7 ]\n+    • … and so on.\n+\n+    This ensures that each shard receives an equal portion of both gate and up projections, maintaining consistency across tensor parallelism.\n+    \"\"\"\n+    slice_ = param\n+    total_size = empty_param.shape[dim]\n+    world_size = device_mesh.size()\n+    block_sizes = _blocks_to_block_sizes(total_size=total_size, blocks=2)\n+\n+    tensors_slices = []\n+    block_offset = 0\n+    for block_size in block_sizes:\n+        shard_block_size = block_size // world_size\n+        start = rank * shard_block_size\n+        stop = (rank + 1) * shard_block_size\n+        tensors_slices += range(block_offset + start, block_offset + stop)\n+        block_offset += block_size\n+\n+    if dim == 0:\n+        tensor = slice_[tensors_slices, ...]\n+    elif dim == 1 or dim == -2:\n+        tensor = slice_[:, tensors_slices, ...]\n+    elif dim == 2 or dim == -1:\n+        tensor = slice_[..., tensors_slices]\n+    else:\n+        raise ValueError(f\"Unsupported dim {dim}, only dim 0, 1 or 2 are supported\")\n+    return tensor\n+\n+\n+def get_tensor_shard(param, empty_param, device_mesh, rank, dim):\n+    if dim == 0:\n+        size_ = empty_param.shape[0]\n+        param = param[rank * (size_ // device_mesh.size()) : (rank + 1) * (size_ // device_mesh.size()), ...]\n+    elif dim == 1 or dim == -2:\n+        size_ = empty_param.shape[-2]\n+        param = param[..., rank * (size_ // device_mesh.size()) : (rank + 1) * (size_ // device_mesh.size()), :]\n+    elif dim == 2 or dim == -1:\n+        size_ = empty_param.shape[-1]\n+        param = param[..., rank * (size_ // device_mesh.size()) : (rank + 1) * (size_ // device_mesh.size())]\n+    else:\n+        raise ValueError(f\"Unsupported dim {dim}, only dim 0, 1 or 2 are supported\")\n+    return param\n+\n+\n+def distribute_module(\n+    module: nn.Module,\n+    device_mesh=None,\n+    input_fn=None,\n+    output_fn=None,\n+) -> nn.Module:\n+    \"\"\"\n+    Copy pasted from torch's function but we remove the communications (partitionning)\n+    as well as buffer registering that is similarly not efficient.\n+    \"\"\"\n+    if len(module._forward_pre_hooks) == 0:\n+        if input_fn is not None:\n+            module.register_forward_pre_hook(lambda mod, inputs: input_fn(mod, inputs, device_mesh))\n+        if output_fn is not None:\n+            module.register_forward_hook(lambda mod, inputs, outputs: output_fn(mod, outputs, device_mesh))\n+    return module\n+\n+\n+class TensorParallelLayer:\n+    \"\"\"\n+    General tensor parallel layer for transformers.\n+    \"\"\"\n+\n+    use_dtensor = True\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh): ...\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh): ...\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        raise NotImplementedError\n+\n+    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n+        if self.use_dtensor:\n+            distribute_module(\n+                module,\n+                device_mesh,\n+                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n+                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n+            )\n+\n+\n+# use_dtensor needs to be set to false for nn.Parameter when you want to view, chunk, slice\n+# you name it. Whatever you want to do that is a bit unconventional, you need local tensors\n+class GatherParallel(TensorParallelLayer):\n+    \"\"\"\n+    Simple class used to define the hooks to add to a layer when we just want to gather the outputs\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        input_layouts: Optional[Placement] = None,\n+        output_layouts: Optional[Placement] = None,\n+        use_local_output: bool = True,\n+    ):\n+        super().__init__()\n+        self.input_layouts = (input_layouts or Replicate(),)\n+        self.output_layouts = output_layouts\n+        self.desired_input_layouts = (Replicate(),)\n+        self.use_local_output = use_local_output\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        if isinstance(inputs[0], DTensor):\n+            inputs[0] = inputs[0].to_local()\n+        return inputs\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n+        return outputs\n+\n+\n+class IsolatedParallel(TensorParallelLayer):\n+    \"\"\"\n+    This class is used to isolate computation in a TP layer from the rest of the world.\n+    Parameters need to be LOCAL, so not dtensors\n+    \"\"\"\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh=None):\n+        # annotate module input placements/sharding with input_layouts\n+        input_tensor = inputs[0]\n+        if isinstance(input_tensor, DTensor):\n+            input_tensor = input_tensor.to_local()\n+        return input_tensor\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh=None):\n+        # TODO: figure out dynamo support for instance method and switch this to instance method\n+        return outputs\n+\n+    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n+        distribute_module(\n+            module,\n+            device_mesh,\n+            partial(self._prepare_input_fn),\n+            partial(self._prepare_output_fn),\n+        )\n+\n+\n+class ColwiseParallel(TensorParallelLayer):\n+    \"\"\"\n+    General tensor parallel layer for transformers.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        input_layouts: Optional[Placement] = None,\n+        output_layouts: Optional[Placement] = None,\n+        use_local_output: bool = True,\n+        use_dtensor=True,\n+    ):\n+        super().__init__()\n+        self.input_layouts = (input_layouts or Replicate(),)\n+        self.output_layouts = (output_layouts or Shard(-1),)\n+        self.desired_input_layouts = (Replicate(),)\n+        self.use_local_output = use_local_output\n+        self.use_dtensor = use_dtensor\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        # TODO: figure out dynamo support for instance method and switch this to instance method\n+        # annotate module input placements/sharding with input_layouts\n+        input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+\n+        # transform the input layouts to the desired layouts of ColwiseParallel\n+        if input_layouts != desired_input_layouts:\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n+        return input_tensor\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n+        # means Colwise as Linear is input * weight^T + bias, where\n+        # weight would become Shard(1)\n+        if param_type == \"bias\":\n+            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n+            shard = [Shard(-1)]\n+        else:\n+            shard = [Shard(-2)]\n+            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2)\n+\n+        parameter = parameter.to(param_casting_dtype)\n+        if to_contiguous:\n+            parameter = parameter.contiguous()\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n+        return nn.Parameter(parameter)\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        # outputs is a shard on last dimension DTensor, i.e. Shard(-1)\n+        if outputs.placements != output_layouts:\n+            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n+        # back to local tensor\n+        return outputs.to_local() if use_local_output else outputs\n+\n+\n+class PackedColwiseParallel(ColwiseParallel):\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n+        # means Colwise as Linear is input * weight^T + bias, where\n+        # weight would become Shard(1)\n+        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -2)\n+        parameter = parameter.to(param_casting_dtype)\n+        if to_contiguous:\n+            parameter = parameter.contiguous()\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-2)], run_check=False)\n+        return nn.Parameter(parameter)\n+\n+\n+class RowwiseParallel(TensorParallelLayer):\n+    \"\"\"\n+    Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.\n+    Users can compose it with ColwiseParallel to achieve the sharding of more complicated modules.\n+    (i.e. MLP, Attention)\n+\n+    Keyword Args:\n+        input_layouts (Placement, optional):\n+            The DTensor layout of input tensor for the nn.Module, this is used to annotate the input tensor to\n+            become a DTensor. If not specified, we assume the input tensor to be sharded on the last dimension.\n+        output_layouts (Placement, optional):\n+            The DTensor layout of the output for the nn.Module, this is used to ensure the output of the nn.Module\n+            with the user desired layout. If not specified, the output tensor is replicated.\n+        use_local_output (bool, optional):\n+            Whether to use local :class:`torch.Tensor` instead of :class:`DTensor` for the module output, default: True.\n+    Returns:\n+        A :class:`ParallelStyle` object that represents Rowwise sharding of the nn.Module.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        *,\n+        input_layouts: Optional[Placement] = None,\n+        output_layouts: Optional[Placement] = None,\n+        use_local_output: bool = True,\n+        use_dtensor=True,\n+    ):\n+        super().__init__()\n+        self.input_layouts = (input_layouts or Shard(-1),)\n+        self.output_layouts = (output_layouts or Replicate(),)\n+        self.use_local_output = use_local_output\n+        self.use_dtensor = use_dtensor\n+\n+    @staticmethod\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        input_tensor = inputs[0]\n+        if not isinstance(input_tensor, DTensor):\n+            input_tensor = DTensor.from_local(input_tensor, device_mesh, input_layouts, run_check=False)\n+\n+        if input_layouts != desired_input_layouts:\n+            input_tensor = input_tensor.redistribute(placements=desired_input_layouts, async_op=True)\n+        return input_tensor\n+\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # Rowwise shard weight to Shard(1), bias to Replicate(), weight be Shard(1)\n+        # means Rowwise as nn.Linear is input * weight^T + bias, where\n+        # weight would become Shard(0)\n+        if param_type != \"bias\":\n+            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n+            shard = [Shard(-1)]\n+        else:\n+            shard = [Replicate()]\n+            parameter = param[:]\n+\n+        parameter = parameter.to(param_casting_dtype)\n+        if to_contiguous:\n+            parameter = parameter.contiguous()\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n+        return nn.Parameter(parameter)\n+\n+    @staticmethod\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        # Rowwise sharding produces partial output, depending on output layouts:\n+        # 1. to replicate -> allreduce\n+        # 2. to shard -> reduce_scatter\n+        if outputs.placements != output_layouts:\n+            outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n+        # back to local tensor if use_local_output is True\n+        return outputs.to_local() if use_local_output else outputs\n+\n+    def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n+        module._distribute_module_applied = True\n+        if self.use_dtensor:\n+            if isinstance(module, nn.Linear):\n+                # rowwise linear runtime sharding requires input tensor shard on last dim\n+                self.desired_input_layouts: Tuple[Placement, ...] = (Shard(-1),)\n+            elif isinstance(module, nn.Embedding):\n+                # rowwise embedding runtime sharding requires input tensor replicated\n+                self.desired_input_layouts = (Replicate(),)\n+            elif isinstance(module, nn.Parameter):\n+                # rowwise embedding runtime sharding requires input tensor replicated\n+                self.desired_input_layouts = (Shard(-1),)\n+            else:\n+                raise NotImplementedError(\"RowwiseParallel currently only support nn.Linear and nn.Embedding!\")\n+\n+            distribute_module(\n+                module,\n+                device_mesh,\n+                partial(self._prepare_input_fn, self.input_layouts, self.desired_input_layouts),\n+                partial(self._prepare_output_fn, self.output_layouts, self.use_local_output),\n+            )\n+\n+\n+class PackedRowwiseParallel(RowwiseParallel):\n+    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+        # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n+        # means Colwise as Linear is input * weight^T + bias, where\n+        # weight would become Shard(1)\n+        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -1)\n+        parameter = parameter.to(param_casting_dtype)\n+        if to_contiguous:\n+            parameter = parameter.contiguous()\n+        if self.use_dtensor:\n+            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n+        return nn.Parameter(parameter)\n+\n+\n+SUPPORTED_TP_STYLES = {\n+    \"colwise\",\n+    \"rowwise\",\n+    \"colwise_rep\",\n+    \"rowwise_rep\",\n+    \"local_colwise\",\n+    \"local_rowwise\",\n+    \"local\",\n+    \"gather\",\n+    \"local_packed_rowwise\",\n+}\n+\n+\n+@lru_cache\n+def translate_to_torch_parallel_style(style: str):\n+    \"\"\"\n+    In model configurations, we use a neutral type (string) to specify parallel\n+    styles, here we translate them into torch.distributed tensor-parallel\n+    types.\n+    \"\"\"\n+    if not isinstance(style, str):\n+        raise ValueError(f\"Unsupported parallel style type {type(style)}, expected str\")\n+\n+    if style == \"colwise\":\n+        return ColwiseParallel()\n+    elif style == \"rowwise\":\n+        return RowwiseParallel()\n+    elif style == \"colwise_rep\":\n+        return ColwiseParallel(output_layouts=Replicate())\n+    elif style == \"rowwise_rep\":\n+        return RowwiseParallel(input_layouts=Replicate())\n+    elif style == \"local_colwise\":\n+        return ColwiseParallel(use_dtensor=False)\n+    elif style == \"local_rowwise\":\n+        return RowwiseParallel(use_dtensor=False)\n+    elif style == \"local\":\n+        return IsolatedParallel()\n+    elif style == \"gather\":\n+        return GatherParallel()\n+    elif style == \"local_packed_rowwise\":\n+        return PackedRowwiseParallel(use_dtensor=False)\n+    else:\n+        raise ValueError(f\"Unsupported parallel style value: {style}\")\n+\n+\n+def add_tensor_parallel_hooks_to_module(model, module, tp_plan, layer_name, current_module_plan, device_mesh):\n+    \"\"\"\n+    Add hooks to the module holding the layer. Meaning:\n+    ```\n+    class MyModel(nn.Module):\n+        def __init__(self):\n+            self.layer = nn.Linear(10, 10)\n+    ```\n+    has state_dict like:\n+    ```\n+    {\n+        \"layer.weight\": torch.Tensor,\n+        \"layer.bias\": torch.Tensor\n+    }\n+    ```\n+    we add hooks to `MyModel` as well as `layer` to make sure that the tensors are correctly sharded and gathered.\n+    \"\"\"\n+\n+    # 1. We add hooks to the layer being loaded:\n+    if current_module_plan is not None:\n+        tp_layer = translate_to_torch_parallel_style(current_module_plan)\n+        tp_layer.prepare_module_tp(module, device_mesh)\n+\n+    # 2. We add hooks to the parrent module if needed\n+    if \".\" in layer_name:\n+        parrent_layer_name = layer_name.rsplit(\".\", 1)[0]\n+        generic_name = re.sub(r\"\\d+\", \"*\", parrent_layer_name)\n+        # The module itself needs hooks\n+        if module_plan := tp_plan.get(generic_name, False):\n+            tp_layer = translate_to_torch_parallel_style(module_plan)\n+            module_to_tp_ = model.get_submodule(parrent_layer_name)\n+            tp_layer.prepare_module_tp(module_to_tp_, device_mesh)\n+\n+\n+def shard_and_distribute_module(\n+    model, param, empty_param, parameter_name, param_casting_dtype, is_contiguous, rank, device_mesh\n+):\n+    r\"\"\"\n+    Main uses cases:\n+    - column / rowise parallelism, you just shard all the weights of the layer (weight and bias)\n+    - packed layers: you slice the weights, then shard like above\n+    - custom operation:\n+        - you want to add an all-gather at the end of a local layer.\n+        - you want to have a layer that is isolated from the rest of the world (because torch.DTensor does not work well with `.view` for instance)\n+\n+    \"\"\"\n+    param_name, param_type = parameter_name.rsplit(\".\", 1) if \".\" in parameter_name else parameter_name\n+    tp_plan = model._tp_plan\n+    module_to_tp = model.get_submodule(param_name)\n+    current_module_plan = None\n+    generic_param_name = re.sub(r\"\\d+\", \"*\", parameter_name)\n+    if generic_param_name in tp_plan:\n+        current_module_plan = tp_plan[generic_param_name]\n+    elif \".\" in generic_param_name and generic_param_name.rsplit(\".\", 1)[0] in tp_plan:\n+        current_module_plan = tp_plan[generic_param_name.rsplit(\".\", 1)[0]]\n+\n+    # Add hooks to the module if not done yet\n+    # add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)\n+    if not getattr(module_to_tp, \"_is_hooked\", False):\n+        add_tensor_parallel_hooks_to_module(model, module_to_tp, tp_plan, param_name, current_module_plan, device_mesh)\n+        module_to_tp._is_hooked = True\n+\n+    if current_module_plan is not None:\n+        tp_layer = translate_to_torch_parallel_style(current_module_plan)\n+        param = tp_layer.partition_tensor(\n+            param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n+        )\n+    else:\n+        param = param[:]\n+        if is_contiguous:\n+            param = param.contiguous()\n+\n+    # SUPER IMPORTANT we have to use setattr\n+    # otherwise loading is crazy slow\n+    if not isinstance(param, torch.nn.Parameter):\n+        param = torch.nn.Parameter(param)\n+    setattr(module_to_tp, param_type, param)\n+    # module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n+    return param"
        },
        {
            "sha": "4a1a683c6bb5c390353559a26506e5fbbd3cf3d8",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 111,
            "deletions": 97,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -54,17 +54,20 @@\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.sdpa_attention import sdpa_attention_forward\n+from .integrations.tensor_parallel import (\n+    SUPPORTED_TP_STYLES,\n+    shard_and_distribute_module,\n+    translate_to_torch_parallel_style,\n+)\n from .loss.loss_utils import LOSS_MAPPING\n from .pytorch_utils import (  # noqa: F401\n     Conv1D,\n     apply_chunking_to_forward,\n-    distribute_module,\n     find_pruneable_heads_and_indices,\n     id_tensor_storage,\n     prune_conv1d_layer,\n     prune_layer,\n     prune_linear_layer,\n-    translate_to_torch_parallel_style,\n )\n from .quantizers import AutoHfQuantizer, HfQuantizer\n from .quantizers.quantizers_utils import get_module_from_name\n@@ -151,6 +154,7 @@\n _init_weights = True\n _is_quantized = False\n _is_ds_init_called = False\n+_torch_distributed_available = torch.distributed.is_available()\n \n \n def is_fsdp_enabled():\n@@ -181,8 +185,6 @@ def is_local_dist_rank_0():\n if is_peft_available():\n     from .utils import find_adapter_config_file\n \n-if is_torch_greater_or_equal(\"2.5\"):\n-    from torch.distributed.tensor import DTensor, Shard\n \n SpecificPreTrainedModelType = TypeVar(\"SpecificPreTrainedModelType\", bound=\"PreTrainedModel\")\n \n@@ -756,6 +758,40 @@ def _move_model_to_meta(model, loaded_state_dict_keys, start_prefix):\n             setattr(submodule, param_name, new_val)\n \n \n+def fix_tensor_type_and_device(\n+    model, param_name, param, dtype=None, keep_in_fp32_modules=None\n+) -> Union[str, torch.dtype]:\n+    # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\n+    # uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\n+    # Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\n+\n+    old_param = model\n+    if \".\" in param_name:\n+        pre, _ = param_name.rsplit(\".\", 1)\n+\n+        old_param = model.get_submodule(pre)\n+        if not isinstance(old_param, (torch.nn.Parameter, torch.Tensor)):\n+            old_param = None\n+\n+        is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n+        # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n+        # in int/uint/bool and not cast them.\n+        param_casting_dtype = None\n+        is_param_float8_e4m3fn = is_torch_e4m3fn_available and param.dtype == torch.float8_e4m3fn\n+        if param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n+            if keep_in_fp32_modules is not None and keep_in_fp32_modules.search(param_name):\n+                param_casting_dtype = torch.float32\n+            elif dtype is not None:\n+                param_casting_dtype = dtype\n+            elif old_param is not None:\n+                param_casting_dtype = old_param.dtype\n+        return old_param is not None and old_param.is_contiguous(), param_casting_dtype\n+    else:\n+        return False, None\n+\n+    return\n+\n+\n @torch.no_grad()\n def _load_state_dict_into_meta_model(\n     model: torch.nn.Module,\n@@ -787,18 +823,12 @@ def _load_state_dict_into_meta_model(\n     It also initialize tensor parallelism for each module if needed.\n \n     \"\"\"\n-    tensor_device = None\n+    tensor_device = \"cpu\"\n     if device_map is not None and device_map.get(\"\", None) is not None:\n         tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n     if device_map is not None:\n         device_map_regex = \"|\".join(sorted(device_map.keys(), reverse=True))\n \n-    # we need this later to initialize tensor parallelism\n-    if device_mesh is not None:\n-        full_tp_plan = model.config.base_model_tp_plan\n-        for submodule in model.modules():\n-            full_tp_plan.update(getattr(submodule, \"_tp_plan\", {}))\n-\n     file_pointer = None\n     bin_state_dict = None\n     if shard_file.endswith(\".safetensors\"):\n@@ -818,8 +848,6 @@ def _load_state_dict_into_meta_model(\n \n     is_quantized = hf_quantizer is not None\n \n-    is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n-\n     for serialized_param_name, empty_param in state_dict.items():\n         # serialized_param_name is the raw, serialized name\n         # fixed_param_name is the model's equivalent\n@@ -829,87 +857,37 @@ def _load_state_dict_into_meta_model(\n             continue\n \n         # we need to use serialized_param_name as file pointer is untouched\n-        param = (\n-            file_pointer.get_slice(serialized_param_name)\n-            if shard_file.endswith(\".safetensors\")\n-            else bin_state_dict[serialized_param_name]\n-        )\n-\n-        # For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model, and which\n-        # uses `param.copy_(input_param)` that preserves the contiguity of the parameter in the model.\n-        # Reference: https://github.com/pytorch/pytorch/blob/db79ceb110f6646523019a59bbd7b838f43d4a86/torch/nn/modules/module.py#L2040C29-L2040C29\n-\n-        old_param = model\n-        splits = fixed_param_name.split(\".\")\n-        for split in splits:\n-            # We shouldn't hit the default value unless for quant methods like hqq that modifies expected_keys.\n-            old_param = getattr(old_param, split, None)\n-            if old_param is None:\n-                break\n-\n-        if not isinstance(old_param, (torch.nn.Parameter, torch.Tensor)):\n-            old_param = None\n+        if shard_file.endswith(\".safetensors\"):\n+            param = file_pointer.get_slice(serialized_param_name)\n+        elif shard_file.endswith(\".gguf\"):\n+            param = empty_param  # For gguf the dict is actually not empty!\n+        else:\n+            param = bin_state_dict[serialized_param_name]\n \n-        # We convert floating dtypes to the `dtype` passed except for float8_e4m3fn type. We also want to keep the buffers/params\n-        # in int/uint/bool and not cast them.\n-        param_casting_dtype = None\n-        is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n-        if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n-            if keep_in_fp32_modules is not None and keep_in_fp32_modules.search(fixed_param_name):\n-                param_casting_dtype = torch.float32\n-            elif dtype is not None:\n-                param_casting_dtype = dtype\n-            elif old_param is not None:\n-                param_casting_dtype = old_param.dtype\n+        to_contiguous, param_casting_dtype = fix_tensor_type_and_device(\n+            model,\n+            param_name=fixed_param_name,\n+            param=empty_param,\n+            dtype=dtype,\n+            keep_in_fp32_modules=keep_in_fp32_modules,\n+        )\n \n         if device_mesh is not None:  # In this case, the param is already on the correct device!\n-            module_to_tp, param_type = get_module_from_name(model, fixed_param_name)\n-            current_module_plan = None\n-            full_tp_plan_ = \"|\".join(full_tp_plan.keys()).replace(\"*\", \"[0-9]+\")\n-            if plan := re.search(full_tp_plan_, fixed_param_name):\n-                match = re.sub(\"[0-9]+\", \"*\", plan[0])\n-                current_module_plan = full_tp_plan[match]\n-\n-            if current_module_plan is not None:\n-                tp_layer = translate_to_torch_parallel_style(current_module_plan)\n-                rank = tensor_device\n-                row, col = empty_param.shape\n-                if \"rowwise\" == current_module_plan:\n-                    param = param[:, rank * (col // device_mesh.size()) : (rank + 1) * (col // device_mesh.size())]\n-                    shard = Shard(1)\n-                    tp_layer.desired_input_layouts = (Shard(-1),)\n-                elif \"colwise\" == current_module_plan:\n-                    param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n-                    shard = Shard(0)\n-                else:\n-                    param = param[rank * (row // device_mesh.size()) : (rank + 1) * (row // device_mesh.size()), :]\n-                    shard = Shard(0)\n-                if param_casting_dtype is not None:\n-                    param = param.to(param_casting_dtype)\n-                if old_param.is_contiguous():\n-                    param = param.contiguous()\n-                local_parameter = DTensor.from_local(\n-                    param,\n-                    device_mesh=device_mesh,\n-                    placements=[shard] * device_mesh.ndim,\n-                )\n-                if isinstance(module_to_tp.weight, nn.Parameter):\n-                    local_parameter = torch.nn.Parameter(local_parameter)\n-                module_to_tp.weight = local_parameter\n-                input_fn = partial(tp_layer._prepare_input_fn, tp_layer.input_layouts, tp_layer.desired_input_layouts)\n-                output_fn = partial(tp_layer._prepare_output_fn, tp_layer.output_layouts, tp_layer.use_local_output)\n-                distribute_module(module_to_tp, device_mesh, None, input_fn, output_fn)\n-            else:\n-                param = param[:]\n-                if old_param is not None and old_param.is_contiguous():\n-                    param = param.contiguous()\n-                module_to_tp.load_state_dict({param_type: param}, strict=False, assign=True)\n-\n+            shard_and_distribute_module(\n+                model,\n+                param,\n+                empty_param,\n+                fixed_param_name,\n+                param_casting_dtype,\n+                to_contiguous,\n+                tensor_device,  # the rank\n+                device_mesh,\n+            )\n         else:\n             param = param[:]\n             if param_casting_dtype is not None:\n                 param = param.to(param_casting_dtype)\n-            if old_param is not None and old_param.is_contiguous():\n+            if to_contiguous:\n                 param = param.contiguous()\n \n             if device_map is None:\n@@ -966,6 +944,7 @@ def _load_state_dict_into_meta_model(\n                         val_kwargs[\"requires_grad\"] = False\n                     value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n                     setattr(module, param_type, value)\n+\n     if file_pointer is not None:\n         file_pointer.__exit__(None, None, None)\n \n@@ -1409,7 +1388,10 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n     # A tensor parallel plan to be applied to the model when TP is enabled. For\n     # top-level models, this attribute is currently defined in respective model\n     # code. For base models, this attribute comes from\n-    # `config.base_model_tp_plan` during `post_init`.\n+    # `config.base_model_tp_plan` during `__init__`.\n+    # It should identify the layers exactly: if you want to TP model.language_model.layers.fc1\n+    # by passing `tp_plan` to the init, it should be {\"model.language_model.layers.fc1\":\"colwise\"}\n+    # for example.\n     _tp_plan = None\n \n     # A pipeline parallel plan specifying the layers which may not be present\n@@ -1475,18 +1457,32 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n         # when a different component (e.g. language_model) is used.\n         self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n \n+        self._no_split_modules = self._no_split_modules or []\n+\n     def post_init(self):\n         \"\"\"\n         A method executed at the end of each Transformer model initialization, to execute code that needs the model's\n         modules properly initialized (such as weight initialization).\n         \"\"\"\n         self.init_weights()\n         self._backward_compatibility_gradient_checkpointing()\n+\n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         if self.base_model is self:\n-            self._tp_plan = self.config.base_model_tp_plan\n             self._pp_plan = self.config.base_model_pp_plan\n \n+        self._tp_plan = self._tp_plan or self.config.base_model_tp_plan or {}\n+        for name, module in self.named_children():\n+            if plan := getattr(module, \"_tp_plan\", None):\n+                self._tp_plan.update({f\"{name}.{k}\": v for k, v in plan.items()})\n+\n+        if self._tp_plan is not None and is_torch_greater_or_equal(\"2.3\"):\n+            for _, v in self._tp_plan.items():\n+                if v not in SUPPORTED_TP_STYLES:\n+                    raise ValueError(\n+                        f\"Unsupported tensor parallel style {v}. Supported styles are {SUPPORTED_TP_STYLES}\"\n+                    )\n+\n     def dequantize(self):\n         \"\"\"\n         Potentially dequantize the model in case it has been quantized by a quantization method that support\n@@ -4315,7 +4311,8 @@ def from_pretrained(\n             model = cls(config, *model_args, **model_kwargs)\n \n         if device_mesh is not None and not model.supports_tp_plan:\n-            raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n+            if config.base_model_tp_plan is None and config.get_text_config().base_model_tp_plan is None:\n+                raise NotImplementedError(\"This model does not have a tensor parallel plan.\")\n \n         # make sure we use the model's config since the __init__ call might have copied it\n         config = model.config\n@@ -4453,7 +4450,7 @@ def from_pretrained(\n                 model,\n                 state_dict,\n                 loaded_state_dict_keys,  # XXX: rename?\n-                resolved_archive_file,\n+                resolved_archive_file or gguf_file,\n                 pretrained_model_name_or_path,\n                 ignore_mismatched_sizes=ignore_mismatched_sizes,\n                 sharded_metadata=sharded_metadata,\n@@ -4565,7 +4562,6 @@ def from_pretrained(\n     @staticmethod\n     def _fix_state_dict_key_on_load(key) -> Tuple[str, bool]:\n         \"\"\"Replace legacy parameter names with their modern equivalents. E.g. beta -> bias, gamma -> weight.\"\"\"\n-\n         # Rename LayerNorm beta & gamma params for some early models ported from Tensorflow (e.g. Bert)\n         # This rename is logged.\n         if key.endswith(\"LayerNorm.beta\"):\n@@ -4590,6 +4586,13 @@ def _fix_state_dict_key_on_load(key) -> Tuple[str, bool]:\n         return key, False\n \n     def rename_key(self, key):\n+        \"\"\"\n+        When we load a LlamaModel from a checkpoint made using LlamaForCausalLM, the keys have an extra\n+        prefix, which can be accessed in the `LlamaModel` via the `self.base_model_prefix` attribute.\n+\n+        But, what if there is an extra layer on top of it? You load a MistralModel from a LlavaForConditionalGeneration?\n+        In that what you actually want is to cut whatever is left of the key.\n+        \"\"\"\n         new_key = key\n         if len(self.base_model_prefix) > 0:\n             if not hasattr(self, self.base_model_prefix) and key.startswith(self.base_model_prefix):\n@@ -4940,7 +4943,7 @@ def _load_pretrained_model(\n                     keep_in_fp32_modules=keep_in_fp32_modules,\n                     unexpected_keys=unexpected_keys,\n                     device_mesh=device_mesh,\n-                    resolved_archive_file=resolved_archive_file,\n+                    shard_file=resolved_archive_file,\n                     weights_only=weights_only,\n                 )\n             else:\n@@ -5019,7 +5022,7 @@ def _load_pretrained_model(\n                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n                             model_to_load,\n                             state_dict,\n-                            start_prefix,\n+                            prefix,\n                             expected_keys,\n                             device_map=device_map,\n                             offload_folder=offload_folder,\n@@ -5898,10 +5901,21 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n \n     for param_name, device in accelerator_device_map.items():\n         try:\n-            param = model.get_parameter(param_name)\n+            param = getattr(model, param_name)\n         except AttributeError:\n-            param = model.get_buffer(param_name)\n-        parameter_count[device] += int(math.prod(param.shape) * allocation_factor)\n+            if \".\" in param_name:\n+                param_name, param_type = param_name.rsplit(\".\", 1)\n+                param = getattr(model.get_submodule(param_name), param_type)\n+            else:\n+                param = model.get_buffer(param_name)\n+\n+        param_size = int(math.prod(param.shape) * allocation_factor)\n+\n+        if _torch_distributed_available and torch.distributed.is_initialized():\n+            generic_name = re.sub(r\"\\d+\", \"*\", param_name)\n+            param_size //= torch.distributed.get_world_size() if not model._tp_plan.get(generic_name, False) else 1\n+\n+        parameter_count[device] += param_size\n \n     dtype = dtype if dtype is not None else torch.float32\n "
        },
        {
            "sha": "7a6475c152d18e24f5bb9b14350b9963d1d3b39e",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -419,7 +419,7 @@ class Blip2PreTrainedModel(PreTrainedModel):\n         \"OPTDecoderLayer\",\n     ]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _keep_in_fp32_modules = [\"wo\"]\n+    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -1799,7 +1799,7 @@ def forward(\n )\n class Blip2TextModelWithProjection(Blip2PreTrainedModel):\n     supports_gradient_checkpointing = False\n-    _keep_in_fp32_modules = []\n+    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1898,7 +1898,7 @@ def forward(\n )\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    _keep_in_fp32_modules = []\n+    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -2371,7 +2371,7 @@ def generate(\n )\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    _keep_in_fp32_modules = []\n+    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)"
        },
        {
            "sha": "d685dd6e99b39453670edee5c27d860e80b78594",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -322,7 +322,6 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n         \"InstructBlipQFormerMultiHeadAttention\",\n         \"InstructBlipQFormerSelfOutput\",\n     ]\n-    _keep_in_fp32_modules = []\n \n     # Copied from transformers.models.blip_2.modeling_blip_2.Blip2PreTrainedModel._init_weights with Blip2->InstructBlip\n     def _init_weights(self, module):\n@@ -1293,6 +1292,7 @@ class InstructBlipForConditionalGeneration(InstructBlipPreTrainedModel, Generati\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n+    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n \n     def __init__(self, config: InstructBlipConfig):\n         super().__init__(config)"
        },
        {
            "sha": "8648d53b87011ce4c7c820c64d601f6d8f858aee",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -323,7 +323,6 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n         \"InstructBlipVideoQFormerMultiHeadAttention\",\n         \"InstructBlipVideoQFormerSelfOutput\",\n     ]\n-    _keep_in_fp32_modules = []\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -1287,6 +1286,7 @@ class InstructBlipVideoForConditionalGeneration(InstructBlipVideoPreTrainedModel\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n+    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n \n     def __init__(self, config: InstructBlipVideoConfig):\n         super().__init__(config)"
        },
        {
            "sha": "3b28adc1ee66b6ad4404ca46a4880cefd2b9ddd0",
            "filename": "tests/models/llava/test_configuration_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "patch": "@@ -58,13 +58,13 @@ def test_arbitrary_reload(self):\n         \"\"\"\n         Simple test for reloading arbirarily composed subconfigs\n         \"\"\"\n-        default_values = LlavaConfig().to_dict()\n-        default_values[\"vision_config\"][\"model_type\"] = \"qwen2_vl\"\n+        default_values = LlavaConfig().to_diff_dict()\n+        default_values[\"vision_config\"][\"model_type\"] = \"pixtral\"\n         default_values[\"text_config\"][\"model_type\"] = \"opt\"\n-\n+        self.maxDiff = None\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             config = LlavaConfig(**default_values)\n             config.save_pretrained(tmp_dir)\n \n             reloaded = LlavaConfig.from_pretrained(tmp_dir)\n-            assert config.to_dict() == reloaded.to_dict()\n+            self.assertDictEqual(config.to_dict(), reloaded.to_dict())"
        },
        {
            "sha": "6a564e552428381cd4881203f5c220a462f54485",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c4b62b219323a31011bac3bd3cece7675d9e4c3/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c4b62b219323a31011bac3bd3cece7675d9e4c3/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=1c4b62b219323a31011bac3bd3cece7675d9e4c3",
            "previous_filename": "tests/tp/test_tp.py"
        }
    ],
    "stats": {
        "total": 820,
        "additions": 704,
        "deletions": 116
    }
}