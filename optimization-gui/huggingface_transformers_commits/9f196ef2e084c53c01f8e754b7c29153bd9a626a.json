{
    "author": "hlky",
    "message": "Replace `accelerator.use_fp16` in examples (#33513)\n\n* Replace `accelerator.use_fp16` in examples\r\n\r\n* pad_to_multiple_of=16 for fp8",
    "sha": "9f196ef2e084c53c01f8e754b7c29153bd9a626a",
    "files": [
        {
            "sha": "3987b6d20d5e17cb2b34e87e9fa4a258f464624a",
            "filename": "examples/pytorch/multiple-choice/run_swag_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -473,9 +473,14 @@ def preprocess_function(examples):\n         # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorForMultipleChoice(\n-            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n-        )\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorForMultipleChoice(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "f8e2f56f8e08b488f7f98cc42c08b5d7a8da941a",
            "filename": "examples/pytorch/question-answering/run_qa_beam_search_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_beam_search_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -670,7 +670,14 @@ def prepare_validation_features(examples):\n         # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "f0a22e51637d282ccc275ca5abbab6fe5ddc915e",
            "filename": "examples/pytorch/question-answering/run_qa_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fquestion-answering%2Frun_qa_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -685,7 +685,14 @@ def prepare_validation_features(examples):\n         # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "21da10700052ea6eabe2d157d95435fbe126cd63",
            "filename": "examples/pytorch/summarization/run_summarization_no_trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -534,11 +534,17 @@ def preprocess_function(examples):\n         logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n \n     label_pad_token_id = -100 if args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n+    if accelerator.mixed_precision == \"fp8\":\n+        pad_to_multiple_of = 16\n+    elif accelerator.mixed_precision != \"no\":\n+        pad_to_multiple_of = 8\n+    else:\n+        pad_to_multiple_of = None\n     data_collator = DataCollatorForSeq2Seq(\n         tokenizer,\n         model=model,\n         label_pad_token_id=label_pad_token_id,\n-        pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n+        pad_to_multiple_of=pad_to_multiple_of,\n     )\n \n     def postprocess_text(preds, labels):"
        },
        {
            "sha": "da9193ab1cfaa2e9a7fc1d19678b40879aaf7f2f",
            "filename": "examples/pytorch/text-classification/run_glue_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -426,7 +426,14 @@ def preprocess_function(examples):\n         # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "77016e2a6cb822a4d7596c78577da7c43ac9e9be",
            "filename": "examples/pytorch/token-classification/run_ner_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -541,9 +541,14 @@ def tokenize_and_align_labels(examples):\n         # Otherwise, `DataCollatorForTokenClassification` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorForTokenClassification(\n-            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n-        )\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "70ef92284db010641b033c2d910081bea0ab98b5",
            "filename": "examples/pytorch/translation/run_translation_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftranslation%2Frun_translation_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -517,11 +517,18 @@ def preprocess_function(examples):\n         # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n         data_collator = DataCollatorForSeq2Seq(\n             tokenizer,\n             model=model,\n             label_pad_token_id=label_pad_token_id,\n-            pad_to_multiple_of=8 if accelerator.use_fp16 else None,\n+            pad_to_multiple_of=pad_to_multiple_of,\n         )\n \n     train_dataloader = DataLoader("
        },
        {
            "sha": "1552acbd42c21dcd686fd4de1b4a166a5b088771",
            "filename": "examples/research_projects/luke/run_luke_ner_no_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fresearch_projects%2Fluke%2Frun_luke_ner_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fresearch_projects%2Fluke%2Frun_luke_ner_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fluke%2Frun_luke_ner_no_trainer.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -542,9 +542,14 @@ def tokenize_and_align_labels(examples):\n         # Otherwise, `DataCollatorForTokenClassification` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorForLukeTokenClassification(\n-            tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None)\n-        )\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorForLukeTokenClassification(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "4bf9eb28df2810129040f1fb1bf825ddbba19db0",
            "filename": "examples/research_projects/self-training-text-classification/finetuning.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fresearch_projects%2Fself-training-text-classification%2Ffinetuning.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/examples%2Fresearch_projects%2Fself-training-text-classification%2Ffinetuning.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fself-training-text-classification%2Ffinetuning.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -704,7 +704,14 @@ def preprocess_function(examples):\n         # precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple of\n         # 8s, which will enable the use of Tensor Cores on NVIDIA hardware with\n         # compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset,"
        },
        {
            "sha": "0b27b49212937b25446a32acd40a6dcceef89c72",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f196ef2e084c53c01f8e754b7c29153bd9a626a/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f196ef2e084c53c01f8e754b7c29153bd9a626a/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=9f196ef2e084c53c01f8e754b7c29153bd9a626a",
            "patch": "@@ -836,7 +836,14 @@ def tokenize_function(examples):\n         # Otherwise, `DataCollatorWithPadding` will apply dynamic padding for us (by padding to the maximum length of\n         # the samples passed). When using mixed precision, we add `pad_to_multiple_of=8` to pad all tensors to multiple\n         # of 8s, which will enable the use of Tensor Cores on NVIDIA hardware with compute capability >= 7.5 (Volta).\n-        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=(8 if accelerator.use_fp16 else None))\n+        # For fp8, we pad to multiple of 16.\n+        if accelerator.mixed_precision == \"fp8\":\n+            pad_to_multiple_of = 16\n+        elif accelerator.mixed_precision != \"no\":\n+            pad_to_multiple_of = 8\n+        else:\n+            pad_to_multiple_of = None\n+        data_collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 79,
        "deletions": 16
    }
}