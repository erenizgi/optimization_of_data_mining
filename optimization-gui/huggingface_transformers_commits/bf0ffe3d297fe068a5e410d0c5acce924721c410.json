{
    "author": "ylacombe",
    "message": "[Tests] Diverse Whisper fixes (#33665)\n\n* fix beam indices in token_timestamps\r\n\r\n* fix attention_mask in FA2\r\n\r\n* correct translation example with the right example\r\n\r\n* correct how somes tests are using outputs + correct num_frames\r\n\r\n* fix shortform batch prev cond tests\r\n\r\n* make fix-copies\r\n\r\n* make fix-copies\r\n\r\n* take care of shifting beam indices\r\n\r\n* [run-slow] whisper\r\n\r\n* [run-slow] whisper",
    "sha": "bf0ffe3d297fe068a5e410d0c5acce924721c410",
    "files": [
        {
            "sha": "6422baac5feb5ec2901c175b0903536726caa68d",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0ffe3d297fe068a5e410d0c5acce924721c410/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0ffe3d297fe068a5e410d0c5acce924721c410/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=bf0ffe3d297fe068a5e410d0c5acce924721c410",
            "patch": "@@ -291,7 +291,7 @@ def forward(\n \n         causal_mask = attention_mask\n         if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            causal_mask = attention_mask[:, : key_states.shape[-2]]\n \n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need"
        },
        {
            "sha": "32e54e0a121d7b53c21c823eec709dd73dfe29eb",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0ffe3d297fe068a5e410d0c5acce924721c410/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0ffe3d297fe068a5e410d0c5acce924721c410/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=bf0ffe3d297fe068a5e410d0c5acce924721c410",
            "patch": "@@ -173,7 +173,9 @@ def _pad_to_max_length(\n \n \n class WhisperGenerationMixin(GenerationMixin):\n-    def _extract_token_timestamps(self, generate_outputs, alignment_heads, time_precision=0.02, num_frames=None):\n+    def _extract_token_timestamps(\n+        self, generate_outputs, alignment_heads, time_precision=0.02, num_frames=None, num_input_ids=None\n+    ):\n         \"\"\"\n         Calculates token-level timestamps using the encoder-decoder cross-attentions and dynamic time-warping (DTW) to\n         map each output token to a position in the input audio. If `num_frames` is specified, the encoder-decoder\n@@ -200,11 +202,18 @@ def _extract_token_timestamps(self, generate_outputs, alignment_heads, time_prec\n             # since the beam search strategy chooses the most probable sequences at the end of the search.\n             # In that case, the cross_attentions weights are too long and we have to make sure that they have the right output_length\n             weight_length = (generate_outputs.beam_indices != -1).sum(-1).max()\n+            weight_length = weight_length if num_input_ids is None else weight_length + num_input_ids\n+\n+            # beam search takes `decoder_input_ids` into account in the `beam_indices` length\n+            # but forgot to shift the beam_indices by the number of `decoder_input_ids`\n+            beam_indices = torch.zeros_like(generate_outputs.beam_indices[:, :weight_length])\n+            # we actually shif the beam indices here\n+            beam_indices[:, num_input_ids:] = generate_outputs.beam_indices[:, : weight_length - num_input_ids]\n+\n             weights = weights[:, :, :weight_length]\n \n             # If beam index is still -1, it means that the associated token id is EOS\n             # We need to replace the index with 0 since index_select gives an error if any of the indexes is -1.\n-            beam_indices = generate_outputs.beam_indices[:, :weight_length]\n             beam_indices = beam_indices.masked_fill(beam_indices == -1, 0)\n \n             # Select the cross attention from the right beam for each output sequences\n@@ -218,8 +227,10 @@ def _extract_token_timestamps(self, generate_outputs, alignment_heads, time_prec\n \n         # make sure timestamps are as long as weights\n         input_length = weight_length or cross_attentions[0].shape[2]\n-        timestamps = torch.zeros_like(generate_outputs.sequences, dtype=torch.float32)[:, : input_length + 1]\n-        batch_size = timestamps.shape[0]\n+        batch_size = generate_outputs.sequences.shape[0]\n+        timestamps = torch.zeros(\n+            (batch_size, input_length + 1), dtype=torch.float32, device=generate_outputs.sequences.device\n+        )\n \n         if num_frames is not None:\n             # two cases:\n@@ -239,6 +250,7 @@ def _extract_token_timestamps(self, generate_outputs, alignment_heads, time_prec\n             else:\n                 # num_frames is of shape (batch_size,) whereas batch_size is truely batch_size*num_return_sequences\n                 repeat_time = batch_size if isinstance(num_frames, int) else batch_size // len(num_frames)\n+                num_frames = num_frames.cpu() if isinstance(num_frames, (torch.Tensor)) else num_frames\n                 num_frames = np.repeat(num_frames, repeat_time)\n \n         if num_frames is None or isinstance(num_frames, int):\n@@ -948,7 +960,10 @@ def _postprocess_outputs(\n         if return_token_timestamps and hasattr(generation_config, \"alignment_heads\"):\n             num_frames = getattr(generation_config, \"num_frames\", None)\n             seek_outputs[\"token_timestamps\"] = self._extract_token_timestamps(\n-                seek_outputs, generation_config.alignment_heads, num_frames=num_frames\n+                seek_outputs,\n+                generation_config.alignment_heads,\n+                num_frames=num_frames,\n+                num_input_ids=decoder_input_ids.shape[-1],\n             )\n             seek_outputs[\"token_timestamps\"] = seek_outputs[\"token_timestamps\"][:, start_idx:]\n "
        },
        {
            "sha": "4a38ad0a5e7777c1ae65b01cf1f2b5da9129477e",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0ffe3d297fe068a5e410d0c5acce924721c410/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0ffe3d297fe068a5e410d0c5acce924721c410/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=bf0ffe3d297fe068a5e410d0c5acce924721c410",
            "patch": "@@ -422,7 +422,7 @@ def forward(\n \n         causal_mask = attention_mask\n         if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            causal_mask = attention_mask[:, : key_states.shape[-2]]\n \n         # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n         # therefore the input hidden states gets silently casted in float32. Hence, we need"
        },
        {
            "sha": "2925de5f225c376ec5707571b7d7e9ae8e8d9131",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf0ffe3d297fe068a5e410d0c5acce924721c410/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf0ffe3d297fe068a5e410d0c5acce924721c410/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=bf0ffe3d297fe068a5e410d0c5acce924721c410",
            "patch": "@@ -1916,14 +1916,14 @@ def test_large_generation_multilingual(self):\n             input_features, do_sample=False, max_length=20, language=\"<|de|>\", task=\"transcribe\"\n         )\n         transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-        EXPECTED_TRANSCRIPT = \" Mein sechster Sohn scheint, wenigstens auf den ersten Blick,\"\n+        EXPECTED_TRANSCRIPT = \" Denken Sie, soeben walten meine Gedanken bei Ihnen in Adela\"\n         self.assertEqual(transcript, EXPECTED_TRANSCRIPT)\n \n         generated_ids = model.generate(\n             input_features, do_sample=False, max_length=20, language=\"<|de|>\", task=\"translate\"\n         )\n         transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-        EXPECTED_TRANSCRIPT = \" My sixth son seems, at least at first glance, the most deeply-minded\"\n+        EXPECTED_TRANSCRIPT = \" Think, my thoughts were just rolling with you in Adelaide, and I\"\n         self.assertEqual(transcript, EXPECTED_TRANSCRIPT)\n \n     @slow\n@@ -2238,7 +2238,7 @@ def test_tiny_token_timestamp_generation(self):\n             input_features, max_length=448, return_timestamps=True, return_token_timestamps=True\n         )\n \n-        self.assertEqual(generate_outputs.sequences.shape, generate_outputs.token_timestamps.shape)\n+        self.assertEqual(generate_outputs[\"sequences\"].shape, generate_outputs[\"token_timestamps\"].shape)\n \n         # fmt: off\n         EXPECTED_OUTPUT = torch.tensor([\n@@ -2249,7 +2249,7 @@ def test_tiny_token_timestamp_generation(self):\n         ])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generate_outputs.token_timestamps.to(\"cpu\"), EXPECTED_OUTPUT))\n+        self.assertTrue(torch.allclose(generate_outputs[\"token_timestamps\"].to(\"cpu\"), EXPECTED_OUTPUT))\n \n     @slow\n     def test_large_token_timestamp_generation(self):\n@@ -2268,7 +2268,7 @@ def test_large_token_timestamp_generation(self):\n             **input_features, max_length=448, return_timestamps=True, return_token_timestamps=True\n         )\n \n-        self.assertEqual(generate_outputs.sequences.shape, generate_outputs.token_timestamps.shape)\n+        self.assertEqual(generate_outputs[\"sequences\"].shape, generate_outputs[\"token_timestamps\"].shape)\n \n         # fmt: off\n         EXPECTED_OUTPUT = torch.tensor([\n@@ -2279,7 +2279,7 @@ def test_large_token_timestamp_generation(self):\n         ])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generate_outputs.token_timestamps.to(\"cpu\"), EXPECTED_OUTPUT))\n+        self.assertTrue(torch.allclose(generate_outputs[\"token_timestamps\"].to(\"cpu\"), EXPECTED_OUTPUT))\n \n     @slow\n     def test_tiny_token_timestamp_batch_generation(self):\n@@ -2306,9 +2306,9 @@ def test_tiny_token_timestamp_batch_generation(self):\n         )\n \n         # task id and lang id prompts should not have timestamp tokens\n-        self.assertEqual(generate_outputs.sequences.shape[-1] - 2, generate_outputs.token_timestamps.shape[-1])\n+        self.assertEqual(generate_outputs[\"sequences\"].shape[-1] - 2, generate_outputs[\"token_timestamps\"].shape[-1])\n \n-        self.assertEqual(len(generate_outputs.sequences), num_return_sequences * num_samples)\n+        self.assertEqual(len(generate_outputs[\"sequences\"]), num_return_sequences * num_samples)\n \n     @slow\n     def test_tiny_token_timestamp_generation_longform(self):\n@@ -2799,7 +2799,7 @@ def test_whisper_shortform_single_batch_prev_cond(self):\n \n         torch.manual_seed(0)\n         result = model.generate(input_features, **gen_kwargs)\n-        decoded = processor.batch_decode(result.sequences, skip_special_tokens=True)\n+        decoded = processor.batch_decode(result, skip_special_tokens=True)\n \n         assert decoded == EXPECTED_TEXT\n \n@@ -2814,7 +2814,7 @@ def test_whisper_shortform_single_batch_prev_cond(self):\n \n         torch.manual_seed(0)\n         result = model.generate(input_features, **gen_kwargs)\n-        decoded = processor.batch_decode(result.sequences, skip_special_tokens=True)\n+        decoded = processor.batch_decode(result, skip_special_tokens=True)\n \n         assert decoded == EXPECTED_TEXT1\n \n@@ -3114,7 +3114,7 @@ def test_whisper_shortform_multi_batch_hard_prev_cond(self):\n         }\n \n         result = model.generate(**inputs, **gen_kwargs)\n-        decoded_all = processor.batch_decode(result.sequences, skip_special_tokens=True)\n+        decoded_all = processor.batch_decode(result, skip_special_tokens=True)\n \n         for i in range(num_samples):\n             if isinstance(EXPECTED_TEXT[i], str):"
        }
    ],
    "stats": {
        "total": 51,
        "additions": 33,
        "deletions": 18
    }
}