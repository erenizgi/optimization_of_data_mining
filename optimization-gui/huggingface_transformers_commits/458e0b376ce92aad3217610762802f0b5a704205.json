{
    "author": "druvdub",
    "message": "Update bamba model card (#38853)\n\n* Update bamba model card\n\n* Update the doc for bamba\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nBamba paragraph\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nBamba collection url\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nUpdate Padding-Free Training to Notes heading\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nupdate examples\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nUpdate additional info\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nconsistent casing\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nsimplify sentences\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Include pipeline and cli examples + fix formatting\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bamba.md\r\n\r\nupdate cli id\n\n* Update quantization example\n\n* Fix auto code formatter changes\n\n* Update cli command + include BambaModel\n\n* Update docs/source/en/model_doc/bamba.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "458e0b376ce92aad3217610762802f0b5a704205",
    "files": [
        {
            "sha": "81f8f79a58a7c6b04e5c6019f606689af8ab7492",
            "filename": "docs/source/en/model_doc/bamba.md",
            "status": "modified",
            "additions": 97,
            "deletions": 54,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/458e0b376ce92aad3217610762802f0b5a704205/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/458e0b376ce92aad3217610762802f0b5a704205/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md?ref=458e0b376ce92aad3217610762802f0b5a704205",
            "patch": "@@ -14,84 +14,127 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # Bamba\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[Bamba](https://huggingface.co/blog/bamba) is a 9B parameter decoder-only language model built on the [Mamba-2](./mamba2) architecture. It is pretrained in two stages - it starts by training on 2T tokens from the [Dolma v1.7](https://huggingface.co/datasets/allenai/dolma) dataset and then trained on an additional 200B tokens from [FineWeb](https://huggingface.co/datasets/HuggingFaceFW/fineweb) and [Cosmopedia](https://huggingface.co/datasets/HuggingFaceTB/cosmopedia).\n \n-## Overview\n+You can find all the original Bamba checkpoints under the [Bamba](https://huggingface.co/collections/ibm-ai-platform/bamba-674f1388b9bbc98b413c7bab) collection.\n \n-Bamba-9B is a decoder-only language model based on the [Mamba-2](https://github.com/state-spaces/mamba) architecture and is designed to handle a wide range of text generation tasks. It is trained from scratch using a two-stage training approach. In the first stage, the model is trained on 2 trillion tokens from the Dolma v1.7 dataset. In the second stage, it undergoes additional training on 200 billion tokens, leveraging a carefully curated blend of high-quality data to further refine its performance and enhance output quality.\n+> [!TIP]\n+> This model was contributed by [ani300](https://github.com/ani300) and [fabianlim](https://github.com/fabianlim).\n+>\n+> Click on the Bamba models in the right sidebar for more examples of how to apply Bamba to different text generation tasks.\n \n-Checkout all Bamba-9B model checkpoints [here](https://github.com/foundation-model-stack/bamba).\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`], and from the command line.\n \n-## BambaConfig\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-| Model            | Params       | # Layers | Hidden Dim. | Attention Heads | GQA | KV Heads | Context Length |  Tied Embeddings |\n-|-------------------|--------------|----------|-------------|-----------------|-----|----------|----------------|------------------|\n-| Bamba  | 9B (9.78B)   | 32       | 4096        | 32              | Yes | 8        | 4096           | True |\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"text-generation\",\n+    model=\"ibm-ai-platform/Bamba-9B-v2\",\n+    torch_dtype=torch.bfloat16,\n+    device=0\n+)\n+pipeline(\"Plants create energy through a process known as\")\n+```\n \n-[[autodoc]] BambaConfig\n+</hfoption>\n \n-<!---\n-## Usage Tips\n+<hfoption id=\"AutoModel\">\n \n-Tips:\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-- The architecture is based on Mamba-2 models.\n+tokenizer = AutoTokenizer.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\")\n+model = AutoModelForCausalLM.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\", torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n \n-## BambaModel\n+output = model.generate(**input_ids)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n \n-[[autodoc]] BambaModel\n-    - forward\n--->\n+</hfoption>\n \n-## BambaForCausalLM\n+<hfoption id=\"transformers CLI\">\n+```bash\n+echo \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model ibm-ai-platform/Bamba-9B-v2 --device 0\n+```\n+</hfoption>\n+</hfoptions>\n \n-```python\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-model = AutoModelForCausalLM.from_pretrained(\"ibm-fms/Bamba-9B\")\n-tokenizer = AutoTokenizer.from_pretrained(\"ibm-fms/Bamba-9B\")\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4.\n \n-message = [\"Mamba is a snake with following properties  \"]\n-inputs = tokenizer(message, return_tensors='pt', return_token_type_ids=False)\n-response = model.generate(**inputs, max_new_tokens=64)\n-print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+tokenizer = AutoTokenizer.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\")\n+model = AutoModelForCausalLM.from_pretrained(\n+   \"ibm-ai-platform/Bamba-9B-v2\",\n+   quantization_config=quantization_config,\n+   device_map=\"auto\",\n+   attn_implementation=\"sdpa\"\n+)\n+\n+inputs = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+output = model.generate(**inputs)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n \n+## Notes\n \n-## Padding-Free Training\n+- Bamba supports padding-free training which concatenates distinct training examples while still processing inputs as separate batches. It can significantly accelerate inference by [~2x](https://github.com/huggingface/transformers/pull/35861#issue-2807873129) (depending on model and data distribution) and reduce memory-usage if there are examples of varying lengths by avoiding unnecessary compute and memory overhead from padding tokens.\n \n-Bamba supports padding-free training in which distinct training examples can be concatenated\n-together while nevertheless processing the inputs as though they belonged to separate batches. When\n-the examples are of varying lengths, padding-free training can provide significant speed ups and\n-memory savings compared to batching the examples together and using padding, as the unnecessary\n-compute and memory due to padding is avoided entirely. The performance gains depend on factors such\n-as the model and the data distribution, but throughput gains up to [~2x are commonly\n-seen](https://github.com/huggingface/transformers/pull/35861#issue-2807873129).\n+  Padding-free training requires the `flash-attn`, `mamba-ssm`, and `causal-conv1d` packages and the following arguments must be passed to the model in addition to `input_ids` and `labels`.\n \n-Using padding-free training with Bamba requires the `flash-attn`, `mamba-ssm`, and `causal-conv1d`\n-packages, and the following arguments must be passed to the model in addition to `input_ids` and\n-`labels`:\n-* `position_ids: torch.LongTensor`: the position index of each token in each sequence.\n-* `seq_idx: torch.IntTensor`: the index of each sequence in the batch.\n-* Each of the [`FlashAttentionKwargs`]\n-    * `cu_seq_lens_q: torch.LongTensor`: The cumulative sequence lengths of all queries.\n-    * `cu_seq_lens_k: torch.LongTensor`: The cumulative sequence lengths of all keys.\n-    * `max_length_q: int`: the longest query length in the batch.\n-    * `max_length_k: int`: the longest key length in the batch.\n+  - `position_ids: torch.LongTensor`: the position index of each token in each sequence.\n+  - `seq_idx: torch.IntTensor`: the index of each sequence in the batch.\n+  - Each of the [`FlashAttentionKwargs`]\n+    - `cu_seq_lens_q: torch.LongTensor`: the cumulative sequence lengths of all queries.\n+    - `cu_seq_lens_k: torch.LongTensor`: the cumulative sequence lengths of all keys.\n+    - `max_length_q: int`: the longest query length in the batch.\n+    - `max_length_k: int`: the longest key length in the batch.\n \n-The `attention_mask` inputs should not be provided. The [`DataCollatorWithFlattening`] can be used\n-to programmatically generate the above set of additional arguments using `return_seq_idx=True` and\n-`return_flash_attn_kwargs=True`. See [this blog post](https://huggingface.co/blog/packing-with-FA2)\n-for additional information.\n+  The `attention_mask` inputs should not be provided. The [`DataCollatorWithFlattening`] programmatically generates the set of additional arguments above using `return_seq_idx=True` and `return_flash_attn_kwargs=True`. See the [Improving Hugging Face Training Efficiency Through Packing with Flash Attention](https://huggingface.co/blog/packing-with-FA2) blog post for additional information.\n \n+  ```python\n+  from transformers import DataCollatorWithFlattening\n \n-[[autodoc]] BambaForCausalLM\n+  # Example of using padding-free training\n+  data_collator = DataCollatorWithFlattening(\n+      tokenizer=tokenizer,\n+      return_seq_idx=True,\n+      return_flash_attn_kwargs=True\n+  )\n+  ```\n+\n+## BambaConfig\n+\n+[[autodoc]] BambaConfig\n+\n+## BambaModel\n+\n+[[autodoc]] BambaModel\n     - forward\n \n-This HF implementation is contributed by [ani300](https://github.com/ani300) and [fabianlim](https://github.com/fabianlim).\n+## BambaForCausalLM\n+\n+[[autodoc]] BambaForCausalLM\n+    - forward"
        }
    ],
    "stats": {
        "total": 151,
        "additions": 97,
        "deletions": 54
    }
}