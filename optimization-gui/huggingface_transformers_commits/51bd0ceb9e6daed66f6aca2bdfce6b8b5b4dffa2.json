{
    "author": "michaelfeil",
    "message": "Update configuration_qwen2.py (#36735)\n\n* Update configuration_qwen2_moe.py\n\n* Update modeling_qwen2_moe.py\n\n* ruff fmt\n\n* docstring add qkv_bias",
    "sha": "51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2",
    "files": [
        {
            "sha": "e0b1c122529351ea542ef1235260416fe362abcd",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2",
            "patch": "@@ -133,7 +133,8 @@ class Qwen2MoeConfig(PretrainedConfig):\n             Indicate which layers use Qwen2MoeMLP rather than Qwen2MoeSparseMoeBlock\n             The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n             If `mlp_only_layers` is empty, `decoder_sparse_step` is used to determine the sparsity.\n-\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to the queries, keys and values.\n     ```python\n     >>> from transformers import Qwen2MoeModel, Qwen2MoeConfig\n \n@@ -195,6 +196,7 @@ def __init__(\n         output_router_logits=False,\n         router_aux_loss_coef=0.001,\n         mlp_only_layers=None,\n+        qkv_bias=True,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -231,6 +233,7 @@ def __init__(\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n+        self.qkv_bias = qkv_bias\n \n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,"
        },
        {
            "sha": "c99c74c1fa3e3f0c117f5bc9dfeebe768441db98",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2",
            "patch": "@@ -327,9 +327,9 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n                 f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                 f\" and `num_heads`: {self.num_heads}).\"\n             )\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)\n+        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=self.config.qkv_bias)\n+        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.config.qkv_bias)\n+        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=self.config.qkv_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n         self.rotary_emb = Qwen2MoeRotaryEmbedding(config=self.config)"
        },
        {
            "sha": "027b713359eafe30c2a43e2c353f419afa7960b1",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=51bd0ceb9e6daed66f6aca2bdfce6b8b5b4dffa2",
            "patch": "@@ -89,6 +89,7 @@ def __init__(\n         pad_token_id=0,\n         bos_token_id=1,\n         scope=None,\n+        qkv_bias=False,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -127,6 +128,7 @@ def __init__(\n         self.norm_topk_prob = norm_topk_prob\n         self.output_router_logits = output_router_logits\n         self.router_aux_loss_coef = router_aux_loss_coef\n+        self.qkv_bias = qkv_bias\n \n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.prepare_config_and_inputs\n     def prepare_config_and_inputs(self):\n@@ -181,6 +183,7 @@ def get_config(self):\n             initializer_range=self.initializer_range,\n             pad_token_id=self.pad_token_id,\n             bos_token_id=self.bos_token_id,\n+            qkv_bias=self.qkv_bias,\n         )\n \n     # Copied from tests.models.llama.test_modeling_llama.LlamaModelTester.create_and_check_model with Llama->Qwen2Moe"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 10,
        "deletions": 4
    }
}